<!doctype html><html lang=ja dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multimodality/mdetr/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>[21.04] MDETR | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/ja/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/ja/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/ja/papers/multimodality/mdetr/><meta data-rh=true property=og:locale content=ja><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=en><meta data-rh=true name=docusaurus_locale content=ja><meta data-rh=true name=docsearch:language content=ja><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[21.04] MDETR | DOCSAID"><meta data-rh=true name=description content=継続の芸術><meta data-rh=true property=og:description content=継続の芸術><link data-rh=true rel=icon href=/ja/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/ja/papers/multimodality/mdetr/><link data-rh=true rel=alternate href=https://docsaid.org/papers/multimodality/mdetr/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/multimodality/mdetr/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/multimodality/mdetr/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/multimodality/mdetr/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin><link rel=alternate type=application/rss+xml href=/ja/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/ja/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/ja/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin><link rel=stylesheet href=/ja/assets/css/styles.8608fdeb.css><script src=/ja/assets/js/main.26c7ffb2.js defer></script><script src=/ja/assets/js/runtime~main.09cb2d15.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label=メインコンテンツまでスキップ><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>メインコンテンツまでスキップ</a></div><nav aria-label=ナビゲーション class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class=navbar__inner><div class=navbar__items><button aria-label=ナビゲーションバーを開く aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/ja/><div class=navbar__logo><img src=/ja/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/ja/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/ja/docs/>オープンソースプロジェクト</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/ja/papers/intro>論文ノート</a><a class="navbar__item navbar__link" href=/ja/blog>ブログ</a><a class="navbar__item navbar__link" href=/ja/playground/intro>遊び場</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>日本語</a><ul class=dropdown__menu><li><a href=/papers/multimodality/mdetr/ rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/multimodality/mdetr/ rel="noopener noreferrer" class=dropdown__link lang=en>English</a><li><a href=/ja/papers/multimodality/mdetr/ rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=ja>日本語</a></ul></div><a href=https://buymeacoffee.com/zephyr_docsaid target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">サポートする<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a><a href=https://github.com/DocsaidLab target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type=button disabled title=ダークモードを切り替える(現在はライトモード) aria-label=ダークモードを切り替える(現在はライトモード) aria-live=polite aria-pressed=false><svg viewBox="0 0 24 24" width=24 height=24 class=lightToggleIcon_pyhR><path fill=currentColor d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"/></svg><svg viewBox="0 0 24 24" width=24 height=24 class=darkToggleIcon_wfgR><path fill=currentColor d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"/></svg></button></div><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="検索 (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>検索</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label=先頭へ戻る class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/ja/><img src=/ja/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/ja/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label=ドキュメントの��サイドバー class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/ja/papers/intro>論文ノート</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="'Classic CNNs (11)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/face-anti-spoofing-1>Face Anti-Spoofing (1)</a><button aria-label="'Face Anti-Spoofing (1)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="'Face Recognition (4)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/feature-fusion-7>Feature Fusion (7)</a><button aria-label="'Feature Fusion (7)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="'Lightweight (10)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/mamba-1>Mamba (1)</a><button aria-label="'Mamba (1)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="'Model Tuning (8)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/ja/papers/category/multimodality-22>Multimodality (22)</a><button aria-label="'Multimodality (22)'の目次を隠す" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/lxmert/>[19.08] LXMERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/vilbert/>[19.08] ViLBERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/visualbert/>[19.08] VisualBERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/vlbert/>[19.08] VL-BERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/uniter/>[19.09] UNITER</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/oscar/>[20.04] Oscar</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/pixelbert/>[20.04] Pixel-BERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/ernie-vil/>[20.06] ERNIE-ViL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/villa/>[20.06] VILLA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/unimo/>[20.12] UNIMO</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/vinvl/>[21.01] VinVL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/vilt/>[21.02] ViLT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/vlt5/>[21.02] VL-T5</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/clip/>[21.03] CLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/ja/papers/multimodality/mdetr/>[21.04] MDETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/albef/>[21.07] ALBEF</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/simvlm/>[21.08] SimVLM</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/meter/>[21.11] METER</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/blip/>[22.01] BLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/flamingo/>[22.04] Flamingo</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/flip/>[22.12] FLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ja/papers/multimodality/xgen-mm/>[24.08] xGen-MM</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/normalization-1>Normalization (1)</a><button aria-label="'Normalization (1)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="'Object Detection (8)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/reparameterization-7>Reparameterization (7)</a><button aria-label="'Reparameterization (7)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="'Segmentation (1)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/text-detection-11>Text Detection (11)</a><button aria-label="'Text Detection (11)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="'Text Recognition (20)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="'Text Spotting (4)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/transformers-15>Transformers (15)</a><button aria-label="'Transformers (15)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/ja/papers/category/vision-transformers-11>Vision Transformers (11)</a><button aria-label="'Vision Transformers (11)'の目次を開く" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/ja/papers/intro>All Notes: 142 entries</a></ul></nav><button type=button title=サイドバーを隠す aria-label=サイドバーを隠す class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=パンくずリストのナビゲーション><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label=ホームページ class=breadcrumbs__link href=/ja/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ja/papers/category/multimodality-22><span itemprop=name>Multimodality (22)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[21.04] MDETR</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">このページの見出し</button></div><div class="theme-doc-markdown markdown"><header><h1>[21.04] MDETR</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=継続の芸術>継続の芸術<a href=#継続の芸術 class=hash-link aria-label="継続の芸術 への直接リンク" title="継続の芸術 への直接リンク">​</a></h2>
<p><a href=https://arxiv.org/abs/2104.12763 target=_blank rel="noopener noreferrer"><strong>MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding</strong></a></p>
<hr>
<p>近年のコンピュータビジョン分野では、物体検出は最前線に立ち、多くの先進的な多モーダル理解システムの中心的役割を果たしてきました。しかし、従来の方法では検出システムがブラックボックスとして固定的な概念で画像を検出するというアプローチが取られており、この方法には固有の限界があります。</p>
<p>明らかな問題の一つは、これらのシステムが多モーダルなコンテキストを効果的に利用して共同訓練することができず、下流のモデルは検出された対象のみを利用できることです。さらに、こうした検出システムはしばしば固定されており、適応性やさらに精緻化する能力に欠けています。もっと重要なのは、これらの検出システムの語彙が非常に制限されており、自由形式のテキストで表現された新しい概念の組み合わせに対しては盲目的であることです。</p>
<p>簡単に言うと、この論文の著者は VL モデル内の物体検出のアーキテクチャを交換したいと考えています。</p>
<p>以前、ViT を使って交換した論文がありましたが、結果はあまり良くありませんでした。</p>
<ul>
<li><strong><a href=/ja/papers/multimodality/vilt/>ViLT：君が歌い終わるとき、私は登場</a></strong></li>
</ul>
<p>ViT を使うのはあまりうまくいかなかったので（ViT は物体検出専用ではないからです）、今回は別の「専用」の物体検出モデル、DETR を使って交換してみることにしました！</p>
<ul>
<li><strong><a href=/ja/papers/object-detection/detr/>DETR：分野横断的なパイオニア</a></strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=問題の定義>問題の定義<a href=#問題の定義 class=hash-link aria-label="問題の定義 への直接リンク" title="問題の定義 への直接リンク">​</a></h2>
<p>現在、多くの先進的な多モーダル理解システムは物体検出をその中心的な部分として依存していますが、これらの設計には明らかにいくつかの問題があります：</p>
<ol>
<li>
<p><strong>協調訓練の制限</strong></p>
<p>多モーダルシステムでは、協調訓練とは、画像、テキスト、音声などの複数の入力ソースからのデータを同時に使用してモデルを訓練することを意味します。システムの一部が他の部分とこの協調訓練を行えない場合、すべての利用可能な情報を十分に活用できない可能性があります。</p>
<p>例えば、画像と音声の入力を持つモデルがあり、画像検出器のみを独立して訓練し、音声入力を考慮しない場合、音声入力が画像内のオブジェクトに関する重要な情報を提供しても、モデルはそのオブジェクトを正しく認識できない可能性があります。</p>
</li>
<li>
<p><strong>検出範囲の制限</strong></p>
<p>検出システムの主な目的は、画像内の特定のオブジェクトを識別することです。しかし、これらのシステムが既知のオブジェクトにのみ集中し、画像の他の部分を無視する場合、重要なコンテキスト情報を見逃す可能性があります。例えば、複数の人と犬が写っている画像で、検出器が人と犬のみを識別し、背景の公園のシーンを無視した場合、そのシーンがなぜ人と犬がそこにいるのかを説明する重要な情報を提供している可能性があります。</p>
</li>
<li>
<p><strong>モデルの固化</strong></p>
<p>モデルが訓練されて「凍結」された後、それは更新や学習を行いません。これにより、モデルが新しい状況やデータに適応し、最適化するのが妨げられる可能性があります。例えば、検出器が夏の画像で訓練されている場合、冬の画像では微調整を行わないと、雪や厚着をした人々をうまく認識できない可能性があります。</p>
</li>
<li>
<p><strong>語彙の制限</strong></p>
<p>物体検出システムは、訓練データに基づいて特定のクラスや属性を識別します。訓練データに含まれていない新しいオブジェクトや概念に遭遇した場合、識別できない可能性があります。</p>
</li>
<li>
<p><strong>エンドツーエンドではない設計</strong></p>
<p>エンドツーエンドのシステムでは、入力から出力までの連続的な学習と最適化が可能であり、途中のステップがありません。検出器がエンドツーエンドでない場合、他のタスクとの協調訓練が制限される可能性があります。数学的には、このシステムは微分できないため、微分できなければ最適化の機会もないということになります！</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=問題の解決>問題の解決<a href=#問題の解決 class=hash-link aria-label="問題の解決 への直接リンク" title="問題の解決 への直接リンク">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=mdetr-モデル設計>MDETR モデル設計<a href=#mdetr-モデル設計 class=hash-link aria-label="MDETR モデル設計 への直接リンク" title="MDETR モデル設計 への直接リンク">​</a></h3>
<p><img decoding=async loading=lazy alt="MDETR モデルアーキテクチャ" src=/ja/assets/images/mdetr_1-9fe82970ec37a8117f878d76e4271882.jpg width=1024 height=338 class=img_ev3q></p>
<p>このモデルは非常にシンプルです。まず、テキスト部分には改良された Encoder モデルである RoBERTa を使用します。</p>
<p>テキストの特徴ベクトルを生成した後、それを元の DETR アーキテクチャに Concat の方法で組み込みます。</p>
<p>全体の構成は以下の部分で構成されています：</p>
<ul>
<li><strong>バックボーンの畳み込みエンコーディング</strong>：画像はまず畳み込みバックボーンでエンコードされ、フラット化されます。</li>
<li><strong>空間情報</strong>：2D 位置エンコーディングをフラット化されたベクトルに加えることにより、モデルは空間情報を保持します。</li>
<li><strong>テキストエンコーディング</strong>：事前学習された Transformer 言語モデルを使用してテキストをエンコードし、入力サイズと同じ隠れベクトルのシーケンスを生成します。</li>
<li><strong>モーダル関連の線形投影</strong>：画像とテキストの特徴にモーダル関連の線形投影を適用し、これらの特徴を共有されたエンコード空間に投影します。</li>
<li><strong>クロスエンコーダ</strong>：画像とテキスト特徴のシーケンスを連結し、共通の Transformer Encoder に供給します。これはモデルの核心部分です。</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=訓練方法>訓練方法<a href=#訓練方法 class=hash-link aria-label="訓練方法 への直接リンク" title="訓練方法 への直接リンク">​</a></h3>
<ol>
<li><strong>ソフトトークン予測</strong></li>
</ol>
<p><img decoding=async loading=lazy alt=ソフトトークン予測 src=/ja/assets/images/mdetr_2-f522827e2c5a6a2fd3d51965193cbabb.jpg width=1224 height=380 class=img_ev3q></p>
<p>ソフトトークンのアイデアは非常に興味深いです。ソフトトークンは、各検出対象に対応する「範囲」を予測することに焦点を当てています。これは、標準的な物体検出と最も異なる点です。具体的には、各物体のクラス分類を予測するのではなく、原文における物体に対応する「範囲」を予測します。</p>
<p>例えば、説明文が「黒猫と白い犬」であると仮定した場合、モデルは黒い動物を検出したとき、その動物が「黒猫」というテキスト部分とどのように関連しているかを予測しようとします。これは単に独立したトークンやクラスラベルに関するものではなく、テキスト内の一連のトークンに関するもので、これらのトークンは一緒に「範囲」を形成し、特定の物体を表現します。</p>
<p>この方法の利点は、同一のテキストで複数の物体への参照が重複している場合や、複数の物体が同一のテキストに対応している場合でも処理できる点です。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=コントラストアライメント>コントラストアライメント<a href=#コントラストアライメント class=hash-link aria-label="コントラストアライメント への直接リンク" title="コントラストアライメント への直接リンク">​</a></h3>
<p>コントラストアライメントは、視覚的なオブジェクトのエンコード表現が対応するテキストトークンと特徴空間内で近接することを保証することを目的としています。このアライメントは、位置情報に基づく「ソフトトークン予測」よりも強力で、特徴表現に直接作用します。</p>
<p><img decoding=async loading=lazy alt=コントラストアライメント src=/ja/assets/images/mdetr_3-4c1659405e91212323e7fdfafd0ef89b.jpg width=1024 height=169 class=img_ev3q></p>
<p>参考論文に記載されている数式は以下の通りです：</p>
<ul>
<li>L：最大トークン数。</li>
<li>N：最大物体数。</li>
<li>T+​i：与えられた物体 oi​ に対応するトークン集合。</li>
<li>Oi+​：与えられたトークン ti​ に対応する物体集合。</li>
<li>τ：温度パラメータ、直接 0.07 に設定されます。</li>
</ul>
<p>この数式の概念は非常にシンプルで、つまり「物体とテキストはお互いにうまくアライメントしなければならない」ということです。似ているほど良いということです。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=すべての損失>すべての損失<a href=#すべての損失 class=hash-link aria-label="すべての損失 への直接リンク" title="すべての損失 への直接リンク">​</a></h3>
<p>MDETR の訓練では、上記で言及したコントラスト損失に加えて、従来の DETR 論文に記載されている損失も含まれています。例えば、二項マッチングによるボックス予測損失、L1 損失、GIoU など、すべてを計算に含めます。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=データセット>データセット<a href=#データセット class=hash-link aria-label="データセット への直接リンク" title="データセット への直接リンク">​</a></h3>
<ul>
<li><strong>CLEVR</strong>：方法の結果を評価するために使用。</li>
<li><strong>Flickr30k</strong>：組み合わせデータセットを構築するために使用された画像。</li>
<li><strong>MS COCO</strong>：組み合わせデータセットを構築するために使用された画像。</li>
<li><strong>Visual Genome (VG)</strong>：組み合わせデータセットを構築するために使用された画像。</li>
<li>アノテーションデータは、引用表現データセット、VG 領域、Flickr エンティティ、GQA 訓練バランスセットから取得されています。</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=技術的な詳細>技術的な詳細<a href=#技術的な詳細 class=hash-link aria-label="技術的な詳細 への直接リンク" title="技術的な詳細 への直接リンク">​</a></h3>
<ul>
<li>
<p><strong>事前訓練された調整検出</strong></p>
<p>事前訓練の段階では、目標は自由形式のテキストで引用されたすべての物体の検出と整合性を確認することです。</p>
</li>
<li>
<p><strong>データ組み合わせ技術とその重要性</strong></p>
<ul>
<li>各画像に対して、提案されたデータセットからその関連するすべてのテキストアノテーションを取得します。異なるアノテーションが同じ画像を参照している場合、それらのアノテーションは統合されます。訓練セットと検証/テストセットの独立性を確保するため、検証またはテストセットに現れるすべての画像は訓練セットから削除されます。</li>
<li>このアルゴリズムは、文を組み合わせるために使用され、組み合わせたフレーズやテキストブロック間の重複が少ないことを確認します（GIoU ≤ 0.5）。GIoU は、2 つの長方形領域がどれだけ重なっているかを評価する指標です。組み合わせ後の文の長さは 250 文字以下に制限されます。この方法によって、130 万対の整合した画像-テキストペアを持つ大規模なデータセットが形成されました。</li>
<li>このデータ組み合わせ技術を使用する主な理由は 2 つです：<!-- -->
<ul>
<li><strong>データ効率</strong>：1 つの訓練サンプルに多くの情報を詰め込むことで、データをより効率的に活用できます。</li>
<li><strong>より良い学習信号</strong>：<!-- -->
<ul>
<li>モデルが学習中に、テキスト内で何度も出現する同じ物体のクラス間の曖昧さを識別し、解決する必要があります。</li>
<li>1 つの文のみの状況では、「ソフトトークン予測」のタスクは比較的簡単になります。なぜなら、モデルは通常その文の主題や核心的な意味を簡単に予測でき、画像に依存しなくても済むからです。</li>
<li>複数の文を組み合わせることで、モデルは画像とテキストの関係をより深く探求する必要があり、その結果、予測能力が向上します。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>モデルアーキテクチャの選定</strong></p>
<ul>
<li>テキストエンコーダには、事前訓練された RoBERTa-base を使用し、12 層の Transformer エンコーダが含まれています。</li>
<li>視覚的バックボーンには 2 つの選択肢があります：<!-- -->
<ul>
<li><strong>ResNet-101</strong>：これは Torchvision から取得したもので、ImageNet で事前訓練されています。</li>
<li><strong>EfficientNet シリーズ</strong>：EfficientNetB3 と EfficientNetB5 が使用されました。EfficientNetB3 は ImageNet で 84.1％の Top-1 精度を達成し、EfficientNetB5 は 86.1％の精度を達成しました。</li>
</ul>
</li>
<li>また、大量の未ラベルデータを用いた訓練モデルも使用されており、これは Noisy-Student の擬似ラベル技術を活用しています。</li>
<li>訓練の詳細：32 台の V100 GPU を使用し、40 エポックの事前訓練を実施しました。効果的なバッチサイズは 64 で、訓練時間は約 1 週間かかりました。</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=討論>討論<a href=#討論 class=hash-link aria-label="討論 への直接リンク" title="討論 への直接リンク">​</a></h3>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=下流タスクの表現分析>下流タスクの表現分析<a href=#下流タスクの表現分析 class=hash-link aria-label="下流タスクの表現分析 への直接リンク" title="下流タスクの表現分析 への直接リンク">​</a></h3>
<ol>
<li><strong>フレーズグラウンディング</strong></li>
</ol>
<p><img decoding=async loading=lazy alt=フレーズグラウンディング src=/ja/assets/images/mdetr_4-c8426ce0c50ebdd7a392e1127821d2fb.jpg width=972 height=872 class=img_ev3q></p>
<p>著者は Flickr30k データセットと特定の訓練/検証/テスト分割を使用しました。評価を行う際、2 つの異なる評価プロトコルを採用し、これらのプロトコルは、1 つのフレーズが画像内の複数の物体を参照する際の問題に対処することを目的としていますが、異なるアプローチを採っており、それぞれに長所と短所があります。</p>
<ol>
<li>
<p><strong>ANY-BOX-プロトコル（任意プロトコル）</strong></p>
<p>このプロトコルでは、与えられたフレーズが画像内の複数の異なる物体を参照する場合、予測された境界ボックスは、いずれかの実際の境界ボックスとの交差（IoU）が設定された閾値（通常 0.5）を超える場合に正しいと見なされます。これは、モデルが画像内の任意の参照物体を正しく識別できれば、その予測が正しいと見なされることを意味します。ただし、この方法の問題点は、モデルがすべての参照されたインスタンスを検出したかどうかを評価できないことです。</p>
</li>
<li>
<p><strong>MERGED-BOXES-プロトコル（合併ボックスプロトコル）</strong></p>
<p>このプロトコルでは、フレーズが画像内の複数の物体を参照する場合、そのフレーズに関連するすべての実際の境界ボックスがまず合併され、これらすべてを含む最小の境界ボックスが作成されます。その後、通常の方法で、この合併された境界ボックスを実際の境界ボックスとして IoU を計算します。つまり、モデルの予測は、個々の実際の境界ボックスと一致するのではなく、この合併された境界ボックスと一致する必要があります。この方法の問題点は、これらのインスタンスが画像内で遠く離れている場合に、合併されたボックスが過度に大きくなる可能性があり、個々のインスタンスに対する詳細な理解を失うことです。</p>
</li>
<li>
<p><strong>結果の比較</strong></p>
<ul>
<li>ANY-BOX 設定では、最先端の技術と比較して、MDETR は検証セットでの Recall@1 測定で 8.5 ポイントの向上を達成し、追加の事前訓練データを使用することなく成果を上げました。</li>
<li>事前訓練を行い、同じバックボーンネットワークを使用することで、MDETR はテストセットでの最良モデル性能をさらに 12.1 ポイント向上させました。</li>
</ul>
</li>
<li>
<p><strong>参照表現理解</strong></p>
</li>
</ol>
<p><img decoding=async loading=lazy alt=参照表現理解 src=/ja/assets/images/mdetr_5-363d71e92179f27e926b68e38e1cde4d.jpg width=1024 height=271 class=img_ev3q></p>
<p>これまでの研究と手法は、通常、画像に関連する事前に抽出された境界ボックスのセットを並べ替える方法に焦点を当ててきました。これらの境界ボックスは、事前訓練されたオブジェクト検出器を使用して得られたものです。</p>
<p>本論文では、より挑戦的な目標を設定しました。すなわち、与えられた参照表現とそれに対応する画像に基づいて、モデルが境界ボックスを予測することを直接訓練するという方法です。これは単に事前に抽出されたボックスを並べ替えるのではなく、モデルが直接予測するというアプローチです。</p>
<p>本モデルは、事前訓練の段階でテキスト内で参照されたすべての物体を識別できるように訓練されています。例えば、「青いドレスを着た女性がバラの茂みの横に立っている」というキャプションに対して、モデルは女性、青いドレス、バラの茂みといったすべての参照された物体の境界ボックスを予測します。しかし、参照表現に関しては、タスクの目的は単一の境界ボックスを返すことであり、それが全体の表現を代表する物体となります。この変化に適応するために、モデルはこの 3 つの特定のデータセットで微調整されました。</p>
<p>上記の表に示されている結果では、この方法が最新の手法よりもすべてのデータセットで顕著に進歩していることが示されています。</p>
<ol start=3>
<li><strong>視覚的質問応答（VQA）</strong></li>
</ol>
<p><img decoding=async loading=lazy alt=視覚的質問応答 src=/ja/assets/images/mdetr_6-fb30a864ffbf83db984cd2dfdc55dc52.jpg width=1024 height=368 class=img_ev3q></p>
<p>このモデルアーキテクチャは VQA タスクにも適用可能ですが、いくつかの設計変更が必要です。</p>
<ul>
<li><strong>モデル設計</strong>
<ul>
<li>クエリのタイプ：物体検出用の 100 個のクエリに加えて、質問タイプに対応するクエリと、質問タイプを予測するためのクエリが導入されました。GQA では、これらの質問タイプは REL、OBJ、GLOBAL、CAT、ATTR として定義されています。</li>
<li>訓練：40 エポックの事前訓練を行い、その後不均衡な GQA 分割で 125 エポックの微調整を行い、最後に均衡分割で 10 エポックの微調整を行いました。</li>
<li>損失戦略：最初の 125 エポックでは、検出損失と QA を同時に訓練しますが、QA 損失にはより大きな重みが与えられます。</li>
</ul>
</li>
</ul>
<p>このモデルは、物体クエリをデコーダへの入力として学習し、これらのエンコーディングは物体検出に使用されます。推論時には、モデルの特定の部分が質問タイプを予測し、その部分から答えを取得します。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>ヒント</div><div class=admonitionContent_BuS1><p>この論文では、従来の VQA v2 ではなく、GQA を使用しています。<p>GQA と VQA v2 は、視覚的質問応答（VQA）のために広く使用される 2 つのデータセットです。両者は、画像を与えて関連する質問に答える点では共通していますが、いくつかの重要な違いがあります：<ol>
<li>
<p><strong>データ規模とソース</strong></p>
<ul>
<li>GQA：GQA データセットは Visual Genome データセットに基づいており、約 2200 万の質問-回答ペアを含んでいます。</li>
<li>VQA v2：VQA v2 データセットは元の VQA データセットの改良版で、約 120 万の質問-回答ペアを含み、MS COCO と Abstract Scenes データセットに基づいています。</li>
</ul>
</li>
<li>
<p><strong>質問と回答の性質</strong></p>
<ul>
<li>GQA：複雑で組み合わせ的な質問に重点を置き、通常は複数の物体とその間の関係を含みます。回答は通常、複数語の記述的な回答です。</li>
<li>VQA v2：質問は非常に多様で、単純なもの（例：「これは何色ですか？」）から比較的複雑なものまであります。回答は通常、1 語または 2 語で表されます。</li>
</ul>
</li>
<li>
<p><strong>データの不均衡性</strong></p>
<ul>
<li>GQA：データセットの設計目的の一つは、VQA での不均衡性問題を解決することです。これにより、モデルが画像内容を理解せずに答えを推測することを防ぎます。</li>
<li>VQA v2：VQA v2 は VQA v1 の改良版で、対照画像を追加し、データの偏り問題を解決しようとしています。</li>
</ul>
</li>
<li>
<p><strong>シーングラフ</strong></p>
<ul>
<li>GQA：GQA には、画像内の物体のタイプ、属性、物体間の関係を詳細に記述したシーングラフが含まれています。</li>
<li>VQA v2：VQA v2 にはシーングラフが組み込まれていませんが、研究者は他のデータソースや技術を組み合わせてこれらの情報を提供することができます。</li>
</ul>
</li>
<li>
<p><strong>タスクの目的</strong></p>
<ul>
<li>GQA：基本的な VQA タスクに加えて、GQA は多モーダル推論を重視しており、モデルに画像内容と質問の文脈をより深く理解させます。</li>
<li>VQA v2：主に基本的な VQA タスクに焦点を当て、モデルの性能を改善し、データ偏り問題に対処します。</li>
</ul>
</li>
</ol><p>簡単に言うと、GQA はより複雑で詳細な物体と関係の記述を提供し、VQA v2 はデータの偏り問題に対処しながら、より多様な質問に対応しています。</div></div>
<ol start=4>
<li><strong>性能比較</strong></li>
</ol>
<p><img decoding=async loading=lazy alt=性能比較 src=/ja/assets/images/mdetr_7-e8febec9925a06c95bcae4030fb3b9df.jpg width=860 height=576 class=img_ev3q></p>
<ul>
<li>ResNet-101 バックボーンを使用したモデルは、LXMERT や VL-T5 よりも優れた性能を示しました。</li>
<li>このモデルの性能は、より多くの事前訓練データを使用した OSCAR を超えることができました。</li>
<li>EfficientNet-B5 バックボーンを使用した MDETR モデルは、さらに高い性能を達成しました。詳細は上表に示されています。</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=few-shot-は可能か>Few-shot は可能か？<a href=#few-shot-は可能か class=hash-link aria-label="Few-shot は可能か？ への直接リンク" title="Few-shot は可能か？ への直接リンク">​</a></h3>
<p><img decoding=async loading=lazy alt=Few-shotは可能か？ src=/ja/assets/images/mdetr_8-b119933acbc0bffe8251f1e7133e3b5c.jpg width=812 height=400 class=img_ev3q></p>
<p>著者たちは、CLIP のゼロショット画像分類での成功に触発され、事前訓練された MDETR モデルを使って、少量のラベル付きデータで物体検出を行う方法をさらに探求しました。CLIP とは異なり、MDETR の事前訓練データセットでは、すべてのターゲットカテゴリのバランスが保証されていません。つまり、そのデータセットには、テキストと整合したボックスが存在しないため、モデルは与えられたテキストに対して常にボックスを予測します。</p>
<p>この設計により、MDETR は本当のゼロショット転送設定で評価することができません。そのため、著者たちは代替戦略として少数ショット設定で評価を行いました。実験には、長尾分布を持つ 1.2k のクラスを含む LVIS データセットが選ばれました。このデータセットでは、ほとんどのクラスにおいて訓練サンプルが非常に少ないです。</p>
<p>この分布に適応するために、MDETR の訓練戦略は以下の通りです：各正のクラスに対して、その画像とクラスのテキスト名を訓練インスタンスとして使用し、さらにそのクラスのすべてのアノテーションを使用します。負のクラスに対しては、クラス名と空のアノテーションのみを提供します。推論時、MDETR は可能なすべてのクラス名をクエリし、各テキストプロンプトで検出されたボックスを統合します。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>ヒント</div><div class=admonitionContent_BuS1><p>例を挙げてみましょう：<p>仮に、次の 3 つのクラス「犬」「猫」「魚」があるとします。<p>手元にあるラベル付きデータは以下の通りです：<ul>
<li>画像 1 には「犬」が写っており、ラベルは「犬」。</li>
<li>画像 2 には「猫」と「魚」が写っており、ラベルはそれぞれ「猫」「魚」。</li>
</ul><p>MDETR の訓練戦略に基づいて：<p>画像 1 について：<ul>
<li>画像に「犬」が写っているので、この画像とテキスト「犬」を訓練インスタンスとして使用し、「犬」のラベルを使用。</li>
<li>画像には「猫」や「魚」が写っていないため、「猫」と「魚」のクラス名は提供しますが、ラベル（空のラベル）は提供しません。</li>
</ul><p>画像 2 について：<ul>
<li>画像に「猫」と「魚」が写っているので、この画像、テキスト「猫」と「魚」を訓練インスタンスとして使用し、それぞれのラベル「猫」と「魚」を使用。</li>
<li>画像には「犬」が写っていないため、「犬」のクラス名のみを提供し、ラベル（空のラベル）は提供しません。</li>
</ul><p>MDETR が推論を行うと、画像が与えられた場合、クエリを行い、クラス名「犬」「猫」「魚」に対してそれぞれ検出された結果を統合します。例えば、クエリ「犬」でボックスを検出し、「猫」では検出せず、「魚」ではボックスを検出した場合、最終的な結果には「犬」のボックスと「魚」のボックスが含まれます。</div></div>
<p>著者たちは、LVIS データセットの 3 つのサブセット（1%、10%、100%の画像）で MDETR の微調整を行いました。その結果、2 つのベースライン手法との比較が行われました。1 つは LVIS の完全訓練セットで直接訓練された Mask-RCNN、もう 1 つは MSCOCO で事前訓練し、その後 LVIS のサブセットで微調整された DETR です。驚くべきことに、1%のデータしか使わなくても、MDETR はテキスト事前訓練を活用して、稀少なクラスで完全微調整された DETR を超える性能を達成しました。</p>
<p>さらに、顕著な観察結果として、すべての訓練データで微調整を行った場合、稀少な物体検出性能は、10%のデータでの 20.9 AP から 100%のデータでの 7.5 AP に急降下しました。この大幅な下降は、データ内の極端なクラス不均衡による可能性があります。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=結論>結論<a href=#結論 class=hash-link aria-label="結論 への直接リンク" title="結論 への直接リンク">​</a></h2>
<p>MDETR の最も魅力的な特徴の 1 つは、その「完全微分可能性」です。</p>
<p>この設計により、モデル全体がエンドツーエンドで訓練可能となり、この一貫性がもたらす効果は、モデル同士がより密接に協調し、全体のパフォーマンスと訓練効率を向上させる機会を提供します。さらに、実際のパフォーマンスにおいて、MDETR は多くのデータセットで信じられないような成果を示し、これにより多モーダル学習の分野でしっかりと地位を確立しました。</p>
<p>また、MDETR の多機能性も大きな注目ポイントです。物体検出において優れた結果を示しただけでなく、Few-shot 検出や視覚的質問応答（VQA）などの他の下流アプリケーションでもその価値を証明しています。</p>
<p>MDETR は、ブラックボックス型物体検出器に依存しない設計方針を提供し、これが多くの研究者に対して正確で効率的なモデルを作成するための道を開くかもしれません。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>ヒント</div><div class=admonitionContent_BuS1><p>なぜ以前の論文構成では Faster RCNN を繋げなかったのでしょうか？完全微分可能で、素晴らしいですよね？<p>完全微分可能なモデルは確かに魅力的に聞こえますが、それは多くの計算リソースを必要とする可能性があります。特に、巧妙な設計なしで、単純に、直接的に、無理やり繋げるだけでは、モデルからのペナルティを受ける可能性が高いです：<ul>
<li><strong>訓練ができない。</strong></li>
</ul><p>モデル全体が微分可能である場合、その内部構造は非常に複雑になる可能性があり、これにより計算コストが高くなり、訓練の難易度が増します。研究者は調整にもっと多くの時間を費やす必要があり、これはすべてのチームが負担できるわけではありません。</div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated><b><time datetime=2024-11-28T14:04:52.000Z itemprop=dateModified>2024年11月28日</time></b>に<b>zephyr-sh</b>が<!-- -->最終更新</span></div></div></footer><div style=margin-top:3rem> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label=ドキュメントページ><a class="pagination-nav__link pagination-nav__link--prev" href=/ja/papers/multimodality/clip/><div class=pagination-nav__sublabel>前へ</div><div class=pagination-nav__label>[21.03] CLIP</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/ja/papers/multimodality/albef/><div class=pagination-nav__sublabel>次へ</div><div class=pagination-nav__label>[21.07] ALBEF</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#�継続の芸術 class="table-of-contents__link toc-highlight">継続の芸術</a><li><a href=#問題の定義 class="table-of-contents__link toc-highlight">問題の定義</a><li><a href=#問題の解決 class="table-of-contents__link toc-highlight">問題の解決</a><ul><li><a href=#mdetr-モデル設計 class="table-of-contents__link toc-highlight">MDETR モデル設計</a><li><a href=#訓練方法 class="table-of-contents__link toc-highlight">訓練方法</a><li><a href=#コントラストアライメント class="table-of-contents__link toc-highlight">コントラストアライメント</a><li><a href=#すべての損失 class="table-of-contents__link toc-highlight">すべての損失</a><li><a href=#データセット class="table-of-contents__link toc-highlight">データセット</a><li><a href=#技術的な詳細 class="table-of-contents__link toc-highlight">技術的な詳細</a><li><a href=#討論 class="table-of-contents__link toc-highlight">討論</a><li><a href=#下流タスクの表現分析 class="table-of-contents__link toc-highlight">下流タスクの表現分析</a><li><a href=#few-shot-は可能か class="table-of-contents__link toc-highlight">Few-shot は可能か？</a></ul><li><a href=#結論 class="table-of-contents__link toc-highlight">結論</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/ja/docs>オープンソースプロジェクト</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/papers/intro>論文ノート</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/blog>ブログ</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/terms-of-service>利用規約</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/privacy-policy>プライバシーポリシー</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/worklog>作業日誌</a><span class=footer__link-separator>·</span><a href=https://buymeacoffee.com/zephyr_docsaid target=_blank rel="noopener noreferrer" class=footer__link-item>サポートする<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>