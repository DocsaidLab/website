<!doctype html><html lang=ja dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>Pedestrian Detection 技術地圖 | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://docsaid.org/ja/blog/pedestrian-paper-roadmap><meta data-rh=true property=og:locale content=ja><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=en><meta data-rh=true name=docusaurus_locale content=ja><meta data-rh=true name=docusaurus_tag content=default><meta data-rh=true name=docsearch:language content=ja><meta data-rh=true name=docsearch:docusaurus_tag content=default><meta data-rh=true property=og:title content="Pedestrian Detection 技術地圖 | DOCSAID"><meta data-rh=true name=description content="行人偵測的 40 篇論文導讀。"><meta data-rh=true property=og:description content="行人偵測的 40 篇論文導讀。"><meta data-rh=true property=og:image content=https://docsaid.org/ja/img/2025/0414.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/ja/img/2025/0414.jpg><meta data-rh=true property=og:type content=article><meta data-rh=true property=article:published_time content=2025-04-14T00:00:00.000Z><meta data-rh=true property=article:author content=https://github.com/zephyr-sh><meta data-rh=true property=article:tag content=pedestrian-detection><link data-rh=true rel=icon href=/ja/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/ja/blog/pedestrian-paper-roadmap><link data-rh=true rel=alternate href=https://docsaid.org/blog/pedestrian-paper-roadmap hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/blog/pedestrian-paper-roadmap hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/blog/pedestrian-paper-roadmap hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/blog/pedestrian-paper-roadmap hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://docsaid.org/ja/blog/pedestrian-paper-roadmap","@type":"BlogPosting","author":{"@type":"Person","description":"Dosaid maintainer, Full-Stack AI Engineer","image":"https://github.com/zephyr-sh.png","name":"Z. Yuan","url":"https://github.com/zephyr-sh"},"datePublished":"2025-04-14T00:00:00.000Z","description":"行人偵測的 40 篇論文導讀。","headline":"Pedestrian Detection 技術地圖","image":{"@id":"https://docsaid.org/ja/img/2025/0414.jpg","@type":"ImageObject","caption":"title image for the blog post: Pedestrian Detection 技術地圖","contentUrl":"https://docsaid.org/ja/img/2025/0414.jpg","url":"https://docsaid.org/ja/img/2025/0414.jpg"},"isPartOf":{"@id":"https://docsaid.org/ja/blog","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://docsaid.org/ja/blog/pedestrian-paper-roadmap","name":"Pedestrian Detection 技術地圖","url":"https://docsaid.org/ja/blog/pedestrian-paper-roadmap"}</script><link rel=alternate type=application/rss+xml href=/ja/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/ja/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/ja/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/ja/assets/css/styles.28beeaa6.css><script src=/ja/assets/js/runtime~main.aad01731.js defer></script><script src=/ja/assets/js/main.0ba60a26.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/ja/img/docsaid_logo.png><link rel=preload as=image href=/ja/img/docsaid_logo_white.png><link rel=preload as=image href=https://github.com/zephyr-sh.png><link rel=preload as=image href=/ja/img/bmc-logo.svg><link rel=preload as=image href=/ja/img/icons/all_in.svg><div role=region aria-label=メインコンテンツまでスキップ><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>メインコンテンツまでスキップ</a></div><nav aria-label=ナビゲーション class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label=ナビゲーションバーを開く aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/ja/><div class=navbar__logo><img src=/ja/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/ja/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/ja/docs/>オープンソースプロジェクト</a><a class="navbar__item navbar__link" href=/ja/papers/intro>論文ノート</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/ja/blog>ブログ</a><a class="navbar__item navbar__link" href=/ja/playground/intro>遊び場</a><a class="navbar__item navbar__link" href=/ja/services>技術サービス</a><a class="navbar__item navbar__link" href=/ja/aboutus>私たちについて</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>日本語</a><ul class=dropdown__menu><li><a href=/blog/pedestrian-paper-roadmap target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/blog/pedestrian-paper-roadmap target=_self rel="noopener noreferrer" class=dropdown__link lang=en>English</a><li><a href=/ja/blog/pedestrian-paper-roadmap target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="検索 (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>検索</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-7ny38l ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=blog-hero-fullwidth><div class=postHero_mmE7 style=background-image:url(/img/2025/0414.jpg)><div class=postHeroOverlay_UDxJ><h1 class=postTitle_weFP>Pedestrian Detection 技術地圖</h1><div class=postMeta_oUa9><div class=postAuthors_wLk4><div class=postAuthor_NvIn><img class=postAuthorImg_omQD src=https://github.com/zephyr-sh.png alt="Z. Yuan"><div class=postAuthorText_C6S8><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=postAuthorLink_uKn3><span class=postAuthorName_SaVw>Z. Yuan</span></a><span class=postAuthorTitle_UTso>Dosaid maintainer, Full-Stack AI Engineer</span></div></div></div><div class=postMetaInfo__nS4><div class=postMetaRow_zK0w><span class=postDate_B0aP>2025年4月14日</span><span class=postReadingTime_roVj>16<!-- --> min read</span></div><div class=postTags_nipL><a class=postTag_inik href=/ja/blog/tags/pedestrian-detection>pedestrian-detection</a></div></div></div></div></div></div><div class="container margin-vert--lg"><div class=row><main class="col col--9"><article class=markdown style="max-width:800px;margin:2rem auto"><article class=""><div><div id=__blog-post-container class=markdown><p>最近想做個相關的專案，先從文獻回顧做起吧。</p>
<p>我去翻了一下過去的論文，過去十幾年間大概有近千篇論文，這次我們先挑個幾篇來讀一讀，看看行人偵測的技術脈絡與發展。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第一章馬路上的神經網路>第一章：馬路上的神經網路<a href=#第一章馬路上的神經網路 class=hash-link aria-label="第一章：馬路上的神經網路 への直接リンク" title="第一章：馬路上的神經網路 への直接リンク">​</a></h2>
<blockquote>
<p><strong>入門必讀：了解行人偵測在自動駕駛或其他應用領域的核心議題與技術脈絡</strong></p>
</blockquote>
<ol>
<li>
<p>[<strong>2021 - Design guidelines on deep learning–based pedestrian detection methods for supporting autonomous vehicles</strong>]
從自動駕駛應用出發，系統性歸納深度學習式行人偵測的方法、考量與設計準則，包括資料增強策略與網路架構選擇建議。</p>
</li>
<li>
<p>[<strong>2022 - Performance evaluation of cnn-based pedestrian detectors for autonomous vehicles</strong>]
評估多種 CNN-based 行人偵測器在自動駕駛場景下的效能與時間複雜度，提供對模型選型與硬體部署的重要參考。</p>
</li>
<li>
<p><a href=https://arxiv.org/pdf/2210.10489 target=_blank rel="noopener noreferrer"><strong>2022 - A robust pedestrian detection approach for autonomous vehicles</strong></a>
著眼於自動駕駛對準確率與可靠性的高要求，透過融合複數特徵層以及後處理策略，提出兼具精度與穩定度的偵測方法。</p>
</li>
<li>
<p>[<strong>2021 - Deep neural network based vehicle and pedestrian detection for autonomous driving: A survey</strong>]
行人偵測與車輛偵測常被同時討論，本文從整合視角出發，系統回顧 DNN 技術在自動駕駛下的偵測方案，涵蓋多種卷積網路與先進架構。</p>
</li>
<li>
<p><a href=https://www.mdpi.com/2079-9292/11/21/3551 target=_blank rel="noopener noreferrer"><strong>2022 - Deep learning-based pedestrian detection in autonomous vehicles: Substantial issues and challenges</strong></a>
聚焦深度學習在自動駕駛行人偵測上碰到的顯著挑戰，如夜間/惡劣天候、資料偏差、運算成本等，並總結未來可能的研究方向。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第二章標註世界的基石>第二章：標註世界的基石<a href=#第二章標註世界的基石 class=hash-link aria-label="第二章：標註世界的基石 への直接リンク" title="第二章：標註世界的基石 への直接リンク">​</a></h2>
<blockquote>
<p><strong>從經典 KITTI 到夜間、多光譜基準，這些 Dataset 與 Benchmark 塑造了行人偵測的研究生態</strong></p>
</blockquote>
<ol start=6>
<li>
<p><a href=https://projet.liris.cnrs.fr/imagine/pub/proceedings/CVPR2012/data/papers/424_O3C-04.pdf target=_blank rel="noopener noreferrer"><strong>2012 - Are we ready for autonomous driving? the kitti vision benchmark suite</strong></a>
KITTI 是自動駕駛場景的經典資料集，提供多感測器（RGB、LiDAR）輸入與完整標註，被廣泛用於行人偵測與車輛辨識評測。</p>
</li>
<li>
<p><a href=https://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/01Ped.pdf target=_blank rel="noopener noreferrer"><strong>2011 - Pedestrian detection: An evaluation of the state of the art</strong></a>
早期經典綜述，針對當時各種傳統特徵（如 HOG、Haar、LBP）及機器學習技術進行評估與比較，至今仍是理解行人偵測歷程的必備讀物。</p>
</li>
<li>
<p><a href=https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_CityPersons_A_Diverse_CVPR_2017_paper.pdf target=_blank rel="noopener noreferrer"><strong>2017 - Citypersons: A diverse dataset for pedestrian detection</strong></a>
衍生自 Cityscapes，特別標註與設計以評估行人偵測，包含擁擠場景與各種遮蔽情形，成為研究深度學習行人偵測的重要基準。</p>
</li>
<li>
<p><a href="https://ora.ox.ac.uk/objects/uuid:48f374c8-eac3-4a98-8628-92039a76c17b/download_file?file_format=pdf&safe_filename=neumann18b.pdf&type_of_work=Conference+item" target=_blank rel="noopener noreferrer"><strong>2019 - Nightowls: A pedestrians at night dataset</strong></a>
針對夜間行人打造的大型資料集，解決夜間行人能見度低、成像品質差的痛點，推動各種昏暗場景下的偵測技術演進。</p>
</li>
<li>
<p><a href=https://openaccess.thecvf.com/content_cvpr_2015/papers/Hwang_Multispectral_Pedestrian_Detection_2015_CVPR_paper.pdf target=_blank rel="noopener noreferrer"><strong>2015 - Multispectral pedestrian detection: Benchmark dataset and baseline</strong></a>
首度提出可見光 + 紅外線的多光譜 Benchmark，為研究者提供跨波段的行人偵測資料，奠定多光譜偵測評估體系的基礎。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第三章多光譜的視線>第三章：多光譜的視線<a href=#第三章多光譜的視線 class=hash-link aria-label="第三章：多光譜的視線 への直接リンク" title="第三章：多光譜的視線 への直接リンク">​</a></h2>
<blockquote>
<p><strong>可見光不足時，紅外線/熱成像融合成解方：行人偵測從單波段邁入多波段</strong></p>
</blockquote>
<ol start=11>
<li>
<p>[<strong>2018 - Multispectral pedestrian detection based on deep convolutional neural networks</strong>]
在可見光與紅外線上同時訓練深度卷積網路，結合多通道特徵學習來提升對低光與背光場景的偵測性能。</p>
</li>
<li>
<p><a href=https://thefoxofsky.github.io/files/dataset.pdf target=_blank rel="noopener noreferrer"><strong>2019 - Benchmarking a large-scale fir dataset for on-road pedestrian detection</strong></a>
提出大規模 FIR（熱紅外）資料集，並對多種偵測器進行性能評估，系統性說明在高溫或夜間偵測的挑戰。</p>
</li>
<li>
<p><a href=https://www.researchgate.net/profile/Chan-Hung-Tse-2/publication/372212101_Multispectral_Pedestrian_Detection_Via_Two-Stream_YOLO_With_Complementarity_Fusion_For_Autonomous_Driving/links/64ba0799c41fb852dd887152/Multispectral-Pedestrian-Detection-Via-Two-Stream-YOLO-With-Complementarity-Fusion-For-Autonomous-Driving.pdf target=_blank rel="noopener noreferrer"><strong>2023 - Multispectral pedestrian detection via two-stream yolo with complementarity fusion for autonomous driving</strong></a>
採用雙路 YOLO 模型分別處理 RGB 與熱成像，再在中後期進行特徵融合，實驗顯示可大幅提升在極端光源差異時的檢測率。</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187824" target=_blank rel="noopener noreferrer"><strong>2020 - Attention based multi-layer fusion of multispectral images for pedestrian detection</strong></a>
將注意力機制（attention）引入多光譜融合，透過多層次特徵的引導式權重分配，有效強化行人邊緣與細節訊號。</p>
</li>
<li>
<p><a href=https://arxiv.org/pdf/2111.00273 target=_blank rel="noopener noreferrer"><strong>2021 - Cross-modality fusion transformer for multispectral object detection</strong></a>
雖然標題泛指物件偵測，但文中針對行人（尤其夜間）特別做了深度實驗，利用 Transformer 在跨模態注意力機制的優勢，精準提取可見光與紅外互補資訊。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第四章像素邊緣的旅人>第四章：像素邊緣的旅人<a href=#第四章像素邊緣的旅人 class=hash-link aria-label="第四章：像素邊緣的旅人 への直接リンク" title="第四章：像素邊緣的旅人 への直接リンク">​</a></h2>
<blockquote>
<p><strong>多數城市環境下常見的難題：大範圍或局部遮蔽、遠距離行人縮小等挑戰</strong></p>
</blockquote>
<ol start=16>
<li>
<p><a href=https://www.sciencedirect.com/science/article/pii/S2590005623000437 target=_blank rel="noopener noreferrer"><strong>2023 - Occlusion and multi-scale pedestrian detection a review</strong></a>
系統梳理近年行人遮蔽與多尺度偵測研究方法，對於想了解這兩大難題的脈絡與解法趨勢相當受用。</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718221" target=_blank rel="noopener noreferrer"><strong>2022 - Occlusion handling and multi-scale pedestrian detection based on deep learning: a review</strong></a>
重於深度學習方法之間的差異與挑戰，特別討論目標分割、關鍵點定位、姿勢估計等技術在遮蔽情況下的有效性。</p>
</li>
<li>
<p><a href="https://d1wqtxts1xzle7.cloudfront.net/82239902/2003.08799v2-libre.pdf?1647444211=&response-content-disposition=inline%3B+filename%3DPedestrian_Detection_The_Elephant_In_The.pdf&Expires=1744617541&Signature=aoYOmdL4KXL2ldxXLTSI~wthVshxFwcwG8pWOnLeQ5S2iNus0fJjYpp7zmoPvqhWJF3QRHKkNoNjvETt5DT2wukKxy3UUwV7av66qMXxmQ6qj~5rCidGCLrGHG5SDeftbchdMhYhCFkwXFFjtV6yE3y0VNGg524BKMZ-p061syN~jDk-mBL1WYXSpJ13twbqnk4PHnu4EideOFSHXiHPF2ys0D8cS4LHxo9OrMuIhRPf7RpYDiV0fvK5XZFbs9GllaSk6NezHnmy0KDJW-WjxvAg61ySBW0UEvw1IQ0MzwYyDPQEuGyhD~ymnYHMWsrzZiNWsmH~1oyb7TY8JlUaXQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA" target=_blank rel="noopener noreferrer"><strong>2020 - Pedestrian detection: The elephant in the room</strong></a>
作者針對各資料集中「被忽略或難處理」的 occlusion case 做分析，直指當前演算法在嚴重遮擋下的痛點，呼籲社群重視此「房間裡的大象」。</p>
</li>
<li>
<p><a href=https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Robust_Small-Scale_Pedestrian_Detection_With_Cued_Recall_via_Memory_Learning_ICCV_2021_paper.pdf target=_blank rel="noopener noreferrer"><strong>2021 - Robust small-scale pedestrian detection with cued recall via memory learning</strong></a>
面對遠距離或影像中極度縮小的行人，提出「記憶學習」機制，讓模型能重新提取被忽略的細微特徵，顯著增強偵測率。</p>
</li>
<li>
<p><a href=https://ojs.aaai.org/index.php/AAAI/article/view/20001 target=_blank rel="noopener noreferrer"><strong>2022 - Towards versatile pedestrian detector with multisensory-matching and multispectral recalling memory</strong></a>
進一步結合多模態匹配與記憶網路，針對小目標與遮蔽同時應對，展現如何在單一模型下擴增偵測器的適用範圍。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第五章域之裂縫>第五章：域之裂縫<a href=#第五章域之裂縫 class=hash-link aria-label="第五章：域之裂縫 への直接リンク" title="第五章：域之裂縫 への直接リンク">​</a></h2>
<blockquote>
<p><strong>同一個偵測模型如何在日夜、紅外、低品質、不同城市場景都能穩健運作？</strong></p>
</blockquote>
<ol start=21>
<li>
<p><a href=https://www.amazon.science/publications/domain-adaptive-pedestrian-detection-in-thermal-images target=_blank rel="noopener noreferrer"><strong>2019 - Domain-adaptive pedestrian detection in thermal images</strong></a>
以對抗學習將可見光資料的偵測特徵遷移到熱成像領域，無需大規模標註 IR 資料即可達到不錯的檢測效能。</p>
</li>
<li>
<p><a href=https://www.researchgate.net/profile/Kieu-My/publication/343167450_Task-conditioned_Domain_Adaptation_for_Pedestrian_Detection_in_Thermal_Imagery/links/5f19f8a2a6fdcc9626ad1e77/Task-conditioned-Domain-Adaptation-for-Pedestrian-Detection-in-Thermal-Imagery.pdf target=_blank rel="noopener noreferrer"><strong>2020 - Task-conditioned domain adaptation for pedestrian detection in thermal imagery</strong></a>
更進一步考慮任務需求，透過條件約束增強可見光到 IR 的跨域對齊效果，縮小日夜場景的性能落差。</p>
</li>
<li>
<p><a href=https://www.researchgate.net/profile/Kieu-My/publication/335603374_Domain_Adaptation_for_Privacy-Preserving_Pedestrian_Detection_in_Thermal_Imagery/links/5d95aeeb458515c1d38efa58/Domain-Adaptation-for-Privacy-Preserving-Pedestrian-Detection-in-Thermal-Imagery.pdf target=_blank rel="noopener noreferrer"><strong>2019 - Domain adaptation for privacy-preserving pedestrian detection in thermal imagery</strong></a>
以隱私保護為出發點，強調使用熱成像替代可見光，同時利用跨域學習避免標註成本，是領域自適應在實際應用的一大案例。</p>
</li>
<li>
<p><a href=https://openaccess.thecvf.com/content_CVPRW_2019/papers/MULA/Guan_Unsupervised_Domain_Adaptation_for_Multispectral_Pedestrian_Detection_CVPRW_2019_paper.pdf target=_blank rel="noopener noreferrer"><strong>2019 - Unsupervised domain adaptation for multispectral pedestrian detection</strong></a>
結合多光譜與無監督學習，自動對齊源域（可見光）與目標域（紅外）特徵分布，以最小的人工成本達成跨波段人形偵測。</p>
</li>
<li>
<p>[<strong>2023 - Cross modality knowledge distillation for robust pedestrian detection in low light and adverse weather conditions</strong>]
以知識蒸餾（knowledge distillation）架構，讓紅外線路徑的模型教導可見光路徑，在夜間或強烈天候環境下依舊保持高偵測率。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第六章霧中之影>第六章：霧中之影<a href=#第六章霧中之影 class=hash-link aria-label="第六章：霧中之影 への直接リンク" title="第六章：霧中之影 への直接リンク">​</a></h2>
<blockquote>
<p><strong>霧霾、暴雨、昏暗街頭都是常見卻棘手的實際場景</strong></p>
</blockquote>
<ol start=26>
<li>
<p><a href="https://d1wqtxts1xzle7.cloudfront.net/64964371/Deep_Learning_Approaches_on_Pedestrian_Detection_in_Hazy_Weather-libre.pdf?1605650125=&response-content-disposition=inline%3B+filename%3DDeep_Learning_Approaches_on_Pedestrian_D.pdf&Expires=1744617719&Signature=Ew95oMy5WCXwuYQzPUw5pO1i5AbeervP5OaLKm1fQ4QZH4I9kxpwAGeUf5c3DwpzMwA5XQCHthwM9RZz9GaI~LTnRV5TUjevH4PNrRFuodW5aOOgmYhgcnln3XHY4~wF2eeZxTC7kigOz75hPeDFKwFoEnYYdcfXUV9S0BOAHNDaBbmzaXtWktDo-NQCYAFRJH~-oZILL5azgNRg7JGYFQXH111Z1rxVQcz92pn5S00h3cGfH9VD9VPD0K7biMkYfF45j7Kfz0~HcqsWPhL-mWouY-j2bLzK1-2D5u~tHwuSUXuls1ZGtppQDM1WMCVBfRWMHNdHOINX0vvyoHTUMw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA" target=_blank rel="noopener noreferrer"><strong>2019 - Deep learning approaches on pedestrian detection in hazy weather</strong></a>
提出針對霧霾環境的失真補償與深度學習架構，協助模型克服圖像清晰度下降的問題，有效提升在陰霾天氣的檢測表現。</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9044295" target=_blank rel="noopener noreferrer"><strong>2020 - Pedestrian detection in severe weather conditions</strong></a>
系統分析雨、雪、霧、低光源對偵測結果的影響，並提出多任務學習框架，利用輔助任務（如天氣分類、對比度校正）增強行人辨識。</p>
</li>
<li>
<p>[<strong>2018 - Pedestrian detection at night time in fir domain: Comprehensive study about temperature and brightness and new benchmark</strong>]
聚焦夜晚熱成像（FIR）特性，提出溫度與亮度對比分析並釋出新基準，徹底釐清夜間環境下行人溫度梯度的偵測意涵。</p>
</li>
<li>
<p>[<strong>2022 - Pedestrian detection at daytime and nighttime conditions based on yolo-v5</strong>]
雖基於 YOLO 架構，但全程針對行人偵測進行優化，並特別討論白天與夜晚在可見光下的資料差異，給出多種引導調參策略。</p>
</li>
<li>
<p><a href=https://www.mdpi.com/2079-9292/12/10/2312 target=_blank rel="noopener noreferrer"><strong>2023 - All-weather pedestrian detection based on double-stream multispectral network</strong></a>
進一步深化至「全天候」場景，將可見光與紅外線整合為雙流式網路，在雨、霧、夜間等不利條件下顯示出高穩定度。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第七章深度與雷射交響曲>第七章：深度與雷射交響曲<a href=#第七章深度與雷射交響曲 class=hash-link aria-label="第七章：深度與雷射交響曲 への直接リンク" title="第七章：深度與雷射交響曲 への直接リンク">​</a></h2>
<blockquote>
<p><strong>在自動駕駛與機器人領域，除了相機外，LiDAR、深度攝影機同樣扮演關鍵角色</strong></p>
</blockquote>
<ol start=31>
<li>
<p>[<strong>2022 - Active pedestrian detection for excavator robots based on multi-sensor fusion</strong>]
針對工程機械的安全需求，整合 RGB 與感測器訊號，將「主動偵測」策略引入挖掘機系統，提升機器人週邊環境感知。</p>
</li>
<li>
<p>[<strong>2022 - Pedestrian detection and tracking based on 2d lidar and rgb-d camera</strong>]
單純依靠 2D LiDAR 難以判斷行人輪廓，故引入 RGB-D 融合對應，以動態聚類方式辨識並追蹤行人，是室內外應用的參考示例。</p>
</li>
<li>
<p>[<strong>2019 - An efficient 3d pedestrian detector with calibrated rgb camera and 3d lidar</strong>]
利用同步標定後的 RGB + 3D LiDAR 資料，提出可在空間中回歸立體包圍盒的偵測器，顯示多感測器能顯著降低誤檢與漏檢。</p>
</li>
<li>
<p><a href=https://www.mdpi.com/2076-3417/12/4/1799 target=_blank rel="noopener noreferrer"><strong>2022 - Lidar-based dense pedestrian detection and tracking</strong></a>
專注於「密集行人」環境，透過高密度 LiDAR 點雲與前景估計模型結合，解決人群中互相遮擋與重疊的定位困境。</p>
</li>
<li>
<p><a href=https://rasd3.github.io/assets/publications/2018_iv_robust_fusion/paper.pdf target=_blank rel="noopener noreferrer"><strong>2018 - Robust camera lidar sensor fusion via deep gated information fusion network</strong></a>
將「深度閘門機制」應用於攝影機與雷射點雲的融合流程，依據場景動態自適應調整權重，顯示在夜間、高速移動等極端情況下的抗干擾能力。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第八章像素叢林的行者>第八章：像素叢林的行者<a href=#第八章像素叢林的行者 class=hash-link aria-label="第八章：像素叢林的行者 への直接リンク" title="第八章：像素叢林的行者 への直接リンク">​</a></h2>
<blockquote>
<p><strong>從交通要道到公共場所，行人偵測在監控場景中仍是主力需求</strong></p>
</blockquote>
<ol start=36>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610075" target=_blank rel="noopener noreferrer"><strong>2019 - Fast pedestrian detection in surveillance video based on soft target training of shallow random forest</strong></a>
強調監控影片的多幀冗餘性，提出「軟標籤訓練」與淺層隨機森林，加速監控端的行人偵測並保持足夠精度。</p>
</li>
<li>
<p>[<strong>2019 - Pedestrian detection and behaviour characterization for video surveillance systems</strong>]
不只做偵測，還結合行為分析，透過局部動作特徵理解人群在監控影片中的互動模式，是監控應用的進一步延伸。</p>
</li>
<li>
<p><a href="https://books.google.com.tw/books?hl=zh-TW&lr=&id=MJn8DwAAQBAJ&oi=fnd&pg=PA163&dq=Pedestrian+detection+and+tracking+in+video+surveillance+system:+issues,+comprehensive+review,+and+challenges&ots=51-oqHj4Pa&sig=gDtUli58fCA2bLvUQx9p4fB-oQY&redir_esc=y#v=onepage&q=Pedestrian%20detection%20and%20tracking%20in%20video%20surveillance%20system%3A%20issues%2C%20comprehensive%20review%2C%20and%20challenges&f=false" target=_blank rel="noopener noreferrer"><strong>2020 - Pedestrian detection and tracking in video surveillance system: issues, comprehensive review, and challenges</strong></a>
從大量監控場景難點（擁擠、部份遮蔽、光源變化）著手，對行人偵測與追蹤方法作全面討論並列舉未來挑戰。</p>
</li>
<li>
<p><a href=https://cmst.eu/wp-content/uploads/files/10.12921_cmst.2015.21.03.005_Piniarski.pdf target=_blank rel="noopener noreferrer"><strong>2015 - Video processing algorithms for detection of pedestrians</strong></a>
較早期的監控場景方法，多以背景分割、光流與傳統機器學習特徵為主，回顧這篇能了解深度模型普及前的基礎思路。</p>
</li>
<li>
<p><a href=https://www.researchgate.net/profile/Ahmad-Jalal-9/publication/384675317_A_Smart_Surveillance_System_for_Pedestrian_Tracking_and_Counting_using_Template_Matching/links/67026403f599e0392fbc3885/A-Smart-Surveillance-System-for-Pedestrian-Tracking-and-Counting-using-Template-Matching.pdf target=_blank rel="noopener noreferrer"><strong>2021 - A smart surveillance system for pedestrian tracking and counting using template matching</strong></a>
在姿勢或深度模型資源不足的情況下，嘗試以樣板匹配與輕量級特徵實現基本行人偵測與計數功能，展現監控端對即時性和硬體限制的考量。</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=結語>結語<a href=#結語 class=hash-link aria-label="結語 への直接リンク" title="結語 への直接リンク">​</a></h2>
<p>行人偵測是電腦視覺領域持續發展的重要課題，從最初的傳統影像特徵到今日的深度學習框架，再延伸到紅外、多光譜、LiDAR 等多種感測器，都為了在真實世界中辨識並追蹤人類行為。</p>
<p>在這張地圖裡，我們看到了：</p>
<ul>
<li><strong>多樣感知技術的融合</strong>：從可見光到熱成像與點雲，充分利用不同感測器的互補性。</li>
<li><strong>跨域與泛化的強化</strong>：面對日夜差異、惡劣天候、不同場景，要做的並不只是增減資料，而是如何自動遷移與對齊特徵。</li>
<li><strong>遮蔽與群體挑戰</strong>：人群中重疊、人形被部份掩蓋，小目標在畫面中只剩下幾個像素等問題，永遠是行人偵測的難題。</li>
<li><strong>多場景應用的擴張</strong>：從自動駕駛、都市監控到機器人安全，行人偵測同時考驗即時性、穩定性與隱私安全。</li>
</ul>
<p>如果你正打算進入這個領域，以上 40 篇論文提供了從入門到進階的關鍵方向。</p>
<p>行人偵測的未來，將不只侷限於單一波段或特定場景，也不斷帶動安全、交通、零售、機器人等多元領域的研究與發展。</p>
<p>期望這份導讀能協助你快速找到合適的參考與切入點，一起為更可靠、更智慧的行人偵測技術持續努力。<section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ 1杯のコーヒーが支えになります</h3><p class=simple-cta__subtitle_ol86>AIやフルスタックの情報発信を続けるため、ご支援お願いします。<div class=simple-cta__buttonWrapper_jk1Y><img src=/ja/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-7ny38l" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-7ny38l"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-7ny38l" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/ja/img/icons/all_in.svg alt="AI・開発・運用まで一括対応 icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-7ny38l">ALL</span><h4 class=card__title_SQBY>AI・開発・運用まで一括対応</h4><p class=card__concept_Ak8F>アイデアからリリースまで、技術面はまるごとお任せください。<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>対応内容</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>技術相談 + 開発 + デプロイ<li class=card__bulletItem_wCRd>継続サポート & 拡張</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 次のプロジェクト、始めましょう！</h3><p class=simple-cta__subtitle_ol86>カスタム開発や長期支援をご希望の方は、ぜひご相談ください。</div></section><div style=margin-top:3rem> </div></div></div><button aria-label=先頭へ戻る class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button></article></article><nav class="pagination-nav docusaurus-mt-lg" aria-label=ブログ記事のナビゲーション><a class="pagination-nav__link pagination-nav__link--next" href=/ja/blog/colorful-cli-with-ansi-escape-codes><div class=pagination-nav__sublabel>過去の記事</div><div class=pagination-nav__label>ターミナルは黒と白だけではない</div></a></nav></main><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label=最近のブログ記事のナビゲーション><div class="sidebarItemTitle_pO2u margin-bottom--md">All our Posts</div><div role=group><h3 class=yearGroupHeading_rMGB>2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a aria-current=page class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href=/ja/blog/pedestrian-paper-roadmap>Pedestrian Detection 技術地圖</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/colorful-cli-with-ansi-escape-codes>ターミナルは黒と白だけではない</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/fas-paper-roadmap>Face Anti-Spoofing 技術地図</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/should-you-choose-docusaurus>Docusaurusを選ぶべきか？</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/looking-up-the-ten-steps-of-a-master>大師の十段階を仰ぎ見る</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/build-a-resume>JSを使って履歴書を書いてみよう！</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/pydantic-intro>Pydantic 入門：Python データ検証と管理</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/builds-dashboard-system>AIをやっている私が、なぜバックエンドシステムを作ったのか？</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/amazon-ses-setting-dns-on-namecheap>Amazon SES における Namecheap の DNS 設定</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/download-from-google-drive-using-python>PythonでGoogle Driveからファイルをダウンロードする</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/mount-disk-on-ubuntu>Ubuntu での外付けハードディスクのマウント</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/github-markdown-advanced-syntax>便利な GitHub Markdown の文法</a></ul></div><div role=group><h3 class=yearGroupHeading_rMGB>2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/extract-font-info-by-python>フォントファイルの情報を取得</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/flexible-video-conversion-by-python>バッチ動画変換</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/system-status-checking-by-chatgpt>Ubuntu システムの基本状態チェック自動化</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/customized-docusaurus-author-to-plugin-content-docs>DocusaurusのDocsに著者情報を追加する</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/graph-convolutional-networks>グラフ畳み込みネットワークの概要</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/fourier-transform>フーリエ変換の概要</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/fixed-pyenv-install-error>pyenvビルドエラーを修復</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/update-docusaurus-to-3-6-0>Docusaurusを3.6.0にアップデート</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/file-crawler-python-implementation>ウェブページのファイルをダウンロードするPython実装</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/customized-docusaurus-sidebars-auto-count>Docusaurusのサイドバーに記事数を自動計算させる</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/customized-docusaurus-404-page>Docusaurus の 404 ページをカスタマイズする</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/torch-layernorm-mismatch>手計算したLayerNormの値が合わない？</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/get-taiwan-all-stocks-info>TWSEの全ての株式コード情報を取得する</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/windows-python-settings>Win11 システムでの Python 環境の簡単設定</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/latex-usage>LaTeX 構文クイックリファレンス表</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/impl-normalized-levenshtein-similarity>ANLS の実装</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/python-js-basic-command-equivalents>Python と JS の基本コマンドの対応</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/vscode-settings>よく使う VScode 設定</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/setting-up-nextcloud>Nextcloud の設定記録</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/pytorch-training-out-of-memory>PyTorchのListによる罠</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/convert-pdf-to-images>Pythonを使用してPDFを画像に変換する</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/opencv-imread>PythonでHEIC画像を読み込む方法と読み込みの高速化</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/error-record>日常的なエラー排除記録</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/gosu-usage>コンテナ内のユーザー切り替えツール：gosu</a></ul></div><div role=group><h3 class=yearGroupHeading_rMGB>2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/buy-a-new-computer>パソコン購入記録</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/pyenv-installation>pyenvでPythonバージョンを管理する</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/python-env-info-collector>モデル訓練環境の問題を記録してトラブルシューティング</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/setting-up-pypiserver-on-ubuntu-with-docker>PyPiServerのセットアップ記録</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/ubuntu-install-ssh>UbuntuにSSHサーバーを設定する</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/ubuntu-github-runner-systemd>GitHub Runnerの自動実行</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/login-rtf8207w>RTF8207W ルーターにログインする</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/fail2ban-settings>Fail2ban：SSHサービスの保護</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/unicode-table>Unicode コードポイント表</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/ja/blog/mac-selective-vpn-routing>VPNに選択的なトラフィックルーティングを設定する</a></ul></div></nav></aside><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#第一章馬路上的神經網路 class="table-of-contents__link toc-highlight">第一章：馬路上的神經網路</a><li><a href=#第二章標註世界的基石 class="table-of-contents__link toc-highlight">第二章：標註世界的基石</a><li><a href=#第三章多光譜的視線 class="table-of-contents__link toc-highlight">第三章：多光譜的視線</a><li><a href=#第四章像素邊緣的旅人 class="table-of-contents__link toc-highlight">第四章：像素邊緣的旅人</a><li><a href=#第五章域之裂縫 class="table-of-contents__link toc-highlight">第五章：域之裂縫</a><li><a href=#第六章霧中之影 class="table-of-contents__link toc-highlight">第六章：霧中之影</a><li><a href=#第七章深度與雷射交響曲 class="table-of-contents__link toc-highlight">第七章：深度與雷射交響曲</a><li><a href=#第八章像素叢林的行者 class="table-of-contents__link toc-highlight">第八章：像素叢林的行者</a><li><a href=#結語 class="table-of-contents__link toc-highlight">結語</a></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/ja/docs>オープンソースプロジェクト</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/papers/intro>論文ノート</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/blog>ブログ</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/terms-of-service>利用規約</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/privacy-policy>プライバシーポリシー</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/become-an-author>著者になる</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/ja/worklog>作業日誌</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>