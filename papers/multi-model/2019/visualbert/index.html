<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multi-model/2019/visualbert" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.3.2">
<title data-rh="true">VisualBERT | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/multi-model/2019/visualbert"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="VisualBERT | DOCSAID"><meta data-rh="true" name="description" content="序幕上的凝視"><meta data-rh="true" property="og:description" content="序幕上的凝視"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/multi-model/2019/visualbert"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multi-model/2019/visualbert" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/multi-model/2019/visualbert" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multi-model/2019/visualbert" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml"><link rel="stylesheet" href="/assets/css/styles.3d275683.css">
<script src="/assets/js/runtime~main.30916279.js" defer="defer"></script>
<script src="/assets/js/main.c10e1113.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/multi-model/2019/visualbert" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/multi-model/2019/visualbert" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/暗黑模式（當前為淺色模式）" aria-label="切換淺色/暗黑模式（當前為淺色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/papers/category/multimodel">MultiModel</a><button aria-label="Collapse sidebar category &#x27;MultiModel&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/papers/category/2019">2019</a><button aria-label="Collapse sidebar category &#x27;2019&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/papers/multi-model/2019/visualbert">VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multi-model/2019/vilbert">ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multi-model/2019/vlbert">VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multi-model/2019/lxmert">LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multi-model/2019/uniter">UNITER</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/papers/category/2020">2020</a><button aria-label="Expand sidebar category &#x27;2020&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/papers/category/2021">2021</a><button aria-label="Expand sidebar category &#x27;2021&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/featurefusion">FeatureFusion</a><button aria-label="Expand sidebar category &#x27;FeatureFusion&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/objectdetection">ObjectDetection</a><button aria-label="Expand sidebar category &#x27;ObjectDetection&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/languagemodel">LanguageModel</a><button aria-label="Expand sidebar category &#x27;LanguageModel&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/multimodel"><span itemprop="name">MultiModel</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/2019"><span itemprop="name">2019</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">VisualBERT</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><h1>VisualBERT</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="序幕上的凝視">序幕上的凝視<a href="#序幕上的凝視" class="hash-link" aria-label="序幕上的凝視的直接連結" title="序幕上的凝視的直接連結">​</a></h2>
<p><strong><a href="https://arxiv.org/abs/1908.03557" target="_blank" rel="noopener noreferrer">VisualBERT: A Simple and Performant Baseline for Vision and Language (2019.08)</a></strong></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>信息</div><div class="admonitionContent_BuS1"><p>以下內容由 ChatGPT-4 彙整，並經過人工校對編輯與補充說明。</p></div></div>
<hr>
<p>稍早，看完了 CLIP 那豪氣萬千的資料集規模，相信你也和我一樣，有點無力。</p>
<p>現在稍作休息，把時間往前撥動，先回去那個承先啟後的時代看一看：2019 年。</p>
<p>在 2015 年左右，早就有很多跨模態模型嘗試，主要都是基於 LSTM 的架構來進行相關的實驗，就如本文作者所說： 「多模態的模型研究，根本就不是什麼新鮮事！」。</p>
<p>隨著時序來到 2017 年，Transformer 架構及其注意力機制在自然語言處理領域引起了廣泛的關注，並帶來了許多創新和突破。特別是 BERT，這一代表性的模型成功地預訓練了一個通用語言編碼器，能夠預測文本中被屏蔽的單詞。</p>
<p>到了 2019 年，注意力機制在多模態領域中的應用也得到了極大的發展。這促使語言與視覺的結合再次成為研究的焦點，挖掘圖像中更深層次的語義細節，涵蓋了物體、屬性、部位、空間關係、動作和意圖等語義層面。</p>
<p>作者受此啟發，希望能透過注意力機制捕捉影像中的隱式關係，並認為預先訓練能有效學習這些關係，根據前人的研究，作者總結出幾個現階段的問題：</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結">​</a></h2>
<ul>
<li>
<p><strong>視覺與語言的複雜互動：</strong></p>
<ul>
<li>當前的視覺語言任務（如物體識別、視覺字幕、視覺問答和視覺推理）都要求系統理解圖像中的詳細語義，包括對象、屬性、部分、 空間關係、</li>
<li>動作和意圖，以及如何在語言中引用和建立這些概念。</li>
</ul>
</li>
<li>
<p><strong>統一視覺與語言的模型架構：</strong></p>
<ul>
<li>目前的許多模型都是針對特定的視覺語言任務而設計的，而缺乏一個可以通用於各種任務的模型。</li>
</ul>
</li>
<li>
<p><strong>預訓練的重要性：</strong></p>
<ul>
<li>如何有效地在視覺與語言資料上預訓練模型，以提高其在下游任務的表現。</li>
</ul>
</li>
<li>
<p><strong>理解圖像語義的挑戰：</strong></p>
<ul>
<li>需要捕捉和理解圖像中描述的詳細語義，並將其與文字描述相關聯。</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="visualbert-模型設計">VisualBERT 模型設計<a href="#visualbert-模型設計" class="hash-link" aria-label="VisualBERT 模型設計的直接連結" title="VisualBERT 模型設計的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="VisualBERT 模型架構" src="/assets/images/arch_visual_bert-8032691edcd02251fc604c7557f2ea4e.jpg" width="1816" height="672" class="img_ev3q"></p>
<ol>
<li>
<p><strong>注意力機制：</strong></p>
<ul>
<li>VisualBERT 的核心想法是利用 Transformer 中的注意力機制「隱式」對輸入文字的元素和輸入影像中的區域進行對齊。</li>
</ul>
</li>
<li>
<p><strong>視覺特徵：</strong></p>
<ul>
<li>除了 BERT 的所有組件，VisualBERT 還引入了一組名為 F 的視覺特徵來對影像進行建模。</li>
<li>F 中的每一個特徵都對應於影像中的一個物件區域，這些物件區域是由物件偵測器導出的（可能是 Faster RCNN 或其他）。</li>
<li>F 中的每一個特徵 f 是透過以下三個特徵的總和來計算的：<!-- -->
<ul>
<li>(f_o)：代表 f 物件區域的視覺特徵表示，由卷積神經網路計算。</li>
<li>(f_s)：表示它是影像特徵的分段特徵到文字特徵。</li>
<li>(f_p)：位置特徵，當單字和物件區域之間的對齊作為輸入的一部分提供時使用，並設定為與對齊的單字相對應的位置特徵的總和。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>結合視覺特徵與文字特徵：</strong></p>
<ul>
<li>視覺特徵 F 與原始文字特徵集 E 一同傳遞至多層的 Transformer。這設計使模型能夠隱式地發現兩組輸入（文字和圖像）之間的有用對齊，進而建立新的聯合表示。</li>
</ul>
</li>
</ol>
<p>這種架構的設計允許 VisualBERT 在處理多模態任務時，能夠捕捉圖像和相應文字之間的豐富語義關係，並且可以利用 Transformer 的強大能力進行深入的表徵學習。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="預訓練機制">預訓練機制<a href="#預訓練機制" class="hash-link" aria-label="預訓練機制的直接連結" title="預訓練機制的直接連結">​</a></h3>
<p>VisualBERT 的預訓練過程可以細分為以下三個主要階段：</p>
<ol>
<li>
<p>與任務無關的預訓練：</p>
<ul>
<li>
<p>資料來源：</p>
<ul>
<li>假設在 COCO 資料集中，有一張照片顯示一個小男孩在公園裡和他的狗玩耍。對這張照片的五個標題可能是：<!-- -->
<ul>
<li>小男孩在公園裡玩耍。</li>
<li>一隻狗在草地上追球。</li>
<li>孩子和他的寵物在戶外度過快樂時光。</li>
<li>在陽光下，男孩和狗玩得很開心。</li>
<li>公園裡的孩子和狗互動。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>掩蔽語言建模：</p>
<ul>
<li>基於前面的例子，選取第一個標題「小男孩在公園裡玩耍」作為輸入，並隨機屏蔽「玩耍」這個詞，所以輸入變成「小男孩在公園裡[MASK]」。VisualBERT 的任務是根據上下文和與文字輸入相對應的圖像（即小男孩和狗在公園裡的照片）來預測被屏蔽的詞，也就是「玩耍」。</li>
</ul>
</li>
<li>
<p>句圖預測：</p>
<ul>
<li>再以同一張照片為例，給模型兩個標題：<!-- -->
<ul>
<li>(a) 小男孩在公園裡玩耍（描述該圖像）</li>
<li>(b) 老太太在市場購物（隨機選取的不相關的標題）</li>
</ul>
</li>
<li>VisualBERT 會接收這兩個標題和照片作為輸入，並需要確定哪個標題是與圖像相符的。在這個情境下，答案應該是標題 (a)。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>針對特定任務的預訓練：</p>
<ul>
<li>在微調 VisualBERT 至特定下游任務之前，進行這一預訓練階段是為了使模型更好地適應目標領域  。這個階段主要使用帶有圖像目標的掩蔽語言建模，在特定的任務資料上進行訓練，使模型習慣於新的目標域。</li>
</ul>
</li>
<li>
<p>微調：</p>
<ul>
<li>這一步驟與 BERT 的微調策略相似。首先，會根據特定的任務引入相對應的輸入、輸出層和目標。然後，再訓練 Transformer 使其最大化在該特定任務上的表現。</li>
</ul>
</li>
</ol>
<p>綜合以上這三階段的預訓練策略，作者希望使模型更加泛化且適應於多種視覺語言任務。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="討論的直接連結" title="討論的直接連結">​</a></h2>
<p>在這篇研究中，作者觀察到 VisualBERT 不僅在多種任務上都表現優異，更重要的是，其訓練策略和結構設計提供了獨特的洞察。尤其是如何融合圖像與文本信息，並在這兩者之間建立深度的語義連接。</p>
<p>接下來，再來探討 VisualBERT 的核心優勢、預訓練的策略選擇，以及其如何確實抓取到圖片和語言之間的細緻關聯。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型表現好嗎">模型表現好嗎？<a href="#模型表現好嗎" class="hash-link" aria-label="模型表現好嗎？的直接連結" title="模型表現好嗎？的直接連結">​</a></h3>
<ul>
<li>
<p><strong>VQA</strong></p>
<ul>
<li>
<p>任務描述：</p>
<ul>
<li>任務目的：對給定的圖像和問題提供正確的答案。</li>
<li>使用的資料集：VQA 2.0，由 Goyal 等人於 2017 年提出。</li>
<li>資料集特性：包含超過 100 萬個關於 COCO 影像的問題。</li>
</ul>
</li>
<li>
<p>模型訓練：</p>
<ul>
<li>答案的選擇：訓練模型以預測 3,129 個最常見的答案。</li>
<li>圖像特徵來源：基於 ResNeXt 的 Faster RCNN，已在 Visual Genome 上進行預訓練。</li>
</ul>
</li>
<li>
<p>第一部分：</p>
<ul>
<li>使用與本文的方法相同的視覺  特徵（這裡是指特徵維度），和物件區域建議數量（這裡是指選取影像內區域的數量）的基線模型。</li>
</ul>
</li>
<li>
<p>第二部分：</p>
<ul>
<li>展示了 VisualBERT 的模型結果。</li>
</ul>
</li>
<li>
<p>第三部分：</p>
<ul>
<li>其他不可比較方法的結果，包括使用外部問答對的方法、使用多個檢測器的方法，以及模型的集合。</li>
</ul>
</li>
<li>
<p>小結：</p>
<ul>
<li>在可以比較的基線上，效果比較好。</li>
<li>在不能直接比較的方法中，作者認為他們提出來的的方法也不差。因為這個方法「簡單且在效能上」優於其他現有的方法。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>VCR</strong></p>
<ul>
<li>
<p>任務描述：</p>
<ul>
<li>VCR 包括來自 11 萬個電影場景的 29 萬個問題。</li>
<li>這些問題主要關於視覺常識。</li>
</ul>
</li>
<li>
<p>子任務：</p>
<ul>
<li>VCR 任務被劃分為兩個多重選擇子任務。</li>
<li>分別是問題回答（Q → A）和答案論證（QA → R）。</li>
<li>對於這兩個子任務，都有獨立的模型進行訓練。</li>
</ul>
</li>
<li>
<p>影像特徵：</p>
<ul>
<li>使用 ResNet50（由 He 等人於 2016 年提出）來提取影像特徵。</li>
<li>利用資料集中所提供的「黃金」檢測物件框和分割。</li>
</ul>
</li>
<li>
<p>文字與影像對齊：</p>
<ul>
<li>VCR 資料集提供了文本中引用的單字與物件區域之間的對齊。</li>
<li>通過使用對應的位置特徵來匹配單字和區域，模型能夠利用此對齊。</li>
</ul>
</li>
<li>
<p>比較基準：</p>
<ul>
<li>研究者將他們的方法與基於 BERT (R2C) 構建的資料集發布的模型進行比較。</li>
<li>同時，還與在排行榜上表現最好的單一模型 (B2T2) 進行對比。</li>
</ul>
</li>
<li>
<p>小結：</p>
<ul>
<li>精簡版的 VisualBERT w/o COCO 預訓練與 R2C 有相同的資源配比，但其效能明顯優於 R2C。</li>
<li>使用完整版本的 VisualBERT 可  進一步提高效能。</li>
</ul>
</li>
</ul>
<p>儘管 VCR (主要涵蓋電影場景) 與 COCO 之間存在顯著的領域差異，COCO 上的預訓練對於 VCR 仍然非常有幫助。</p>
</li>
<li>
<p><strong>NLVR2</strong></p>
<ul>
<li>
<p>任務描述：</p>
<ul>
<li>NLVR2 專注於自然語言與圖像的聯合推理。</li>
<li>主要挑戰包括語義多樣性、組合性以及視覺推理。</li>
<li>資料集的任務是判定給定的自然語言描述是否正確地描述了一對影像。</li>
<li>包含超過 10 萬個與網路影像配對的英文句子範例。</li>
</ul>
</li>
<li>
<p>分段特徵調整：</p>
<ul>
<li>在 VisualBERT 中的分段特徵機制被調整。</li>
<li>用於指派來自不同影像的特徵，利用不同的分段特徵。</li>
</ul>
</li>
<li>
<p>影像特徵：</p>
<ul>
<li>利用 Detectron（由 Girshick 等人於 2018 年提出）的現成偵測器來獲取影像特徵。</li>
<li>每個影像使用 144 個提案來提供特徵。</li>
</ul>
</li>
<li>
<p>小結：</p>
<ul>
<li>VisualBERT 顯示出優越的表現。</li>
<li>其中，PhBERT w/o Early Fusion 和 VisualBERT w/o COCO 預訓練在效能上明顯超越了之前的領先模型 MaxEnt。</li>
<li>完整的 VisualBERT 更進一步擴大了其與其他模型之間的性能差距。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>FLICKR30K</strong></p>
<ul>
<li>
<p>任務描述：</p>
<ul>
<li>Flickr30K 資料集的主要目標是檢驗系統將字幕中的短語定位到圖像的特定物件區域的能力。</li>
<li>給定句子的一部分或片段，系統需要選擇對應的圖像物件區域。</li>
<li>資料集包含了 30k 個影像以及近 250k 的註釋。</li>
</ul>
</li>
<li>
<p>模型配置：</p>
<ul>
<li>基於 BAN 的設定（由 Kim et al. 在 2018 年提出）。</li>
<li>圖像特徵使用在 Visual Genome 上預先訓練過的 Faster R-CNN 來獲得。</li>
<li>微調時，加入了額外的注意力區塊，並使用注意力頭的平均權重來預測物件框和短語之間的對 齊。</li>
<li>系統預測時，會選擇短語中最後一個子詞中被關注最多的框作為結果。</li>
</ul>
</li>
<li>
<p>小結：</p>
<ul>
<li>VisualBERT 在此任務上的表現超越了目前的領先模型 BAN。</li>
<li>有趣的是，不使用早期融合的模型與完整的 VisualBERT 在性能上沒有顯著差異，這暗示對於此任務，較簡單或淺層的模型結構可能已足夠。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="在這模型設計中誰最重要">在這模型設計中，誰最重要？<a href="#在這模型設計中誰最重要" class="hash-link" aria-label="在這模型設計中，誰最重要？的直接連結" title="在這模型設計中，誰最重要？的直接連結">​</a></h3>
<p>作者探討在 VisualBERT 模型中，哪些元件或設計選擇對性能最具貢獻。</p>
<p>他們選擇了以下四個核心元件/策略進行消融研究：</p>
<ol>
<li>與任務無關的預訓練（C1）。</li>
<li>早期融合，即圖像和文字特徵之間早期的互動（C2）。</li>
<li>BERT 的初始化策略（C3）。</li>
<li>句子-影像的預測目標（C4）。</li>
</ol>
<p>實驗結果顯示：</p>
<ol>
<li>與任務無關的預訓練（C1）是非常重要的。特別是使用配對的視覺和語言資料進行預訓練對模型的性能有顯著的提升。</li>
<li>早期融合（C2）也證明是重要的。讓圖像和文字特徵在早期就進行互動，可以增強視覺和語言之間在多個互動層中的相互作用。</li>
<li>BERT 的初始化策略（C3）也有一定的重要性。雖然模型在沒有 BERT 預訓練權重的情況下性能下降，但這種下降不如預期的那麼明顯，認為模型在 COCO 預訓練期間也學到了很多有關紮根語言的知識。</li>
<li>句子-影像的預測目標（C4）有一定的影響，但相對於其他元件來說，它的影響較小。</li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>這個結論在之後 CLIP 的實驗中驗證了第一和第二個結論，只要足夠多的資料就能幹大事。至於第三點的結論，我認為這裡可以嘗試探討 BERT 的預訓練資料和 COCO 之間是否有一定的重疊性，最後一點則是依照我自己的經驗來看，這個任務可能對模型來說太簡單，沒有對模型產生應有的監督效果。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型真的有看到對的地方嗎">模型真的有看到對的地方嗎？<a href="#模型真的有看到對的地方嗎" class="hash-link" aria-label="模型真的有看到對的地方嗎？的直接連結" title="模型真的有看到對的地方嗎？的直接連結">​</a></h3>
<p>作者探討 VisualBERT 模型中的注意力頭是否能夠正確地將句子中的實體對應到圖像中的相應物件區域，此外，作者想了解 VisualBERT 模型的注意力頭是否能夠識別句子中的句法關係，特別是當這些句法關係與圖像區域之間存在明確的對應關係時？</p>
<ol>
<li>
<p>實體辨識：</p>
<ul>
<li>VisualBERT 的許多注意力頭具有很高的準確性，且沒有受到實體辨識的直接監督。</li>
<li>模型的較高層在進行辨識時的精度似乎有所提高，這意味著在模型的初級層可能對於如何進行實體辨識還不那麼確定，但在後續層中模型變得越來越確定。</li>
</ul>
</li>
<li>
<p>句法基礎：</p>
<ul>
<li>VisualBERT 的許多注意力頭似乎可以捕捉到句法關係，尤其是動詞與其對應的參數之間的關聯。</li>
<li>對於各種不同的句法依賴關係，作者發現 VisualBERT 中至少有一個注意力頭的性能是明顯優於基於猜測的基線的。</li>
<li>這意味著 VisualBERT 在無需明確句法監督的情況下，能夠隱式地識別句法結構並對其進行對應。</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="注意力分布樣態如何">注意力分布樣態如何？<a href="#注意力分布樣態如何" class="hash-link" aria-label="注意力分布樣態如何？的直接連結" title="注意力分布樣態如何？的直接連結">​</a></h3>
<p>作者探討 VisualBERT 如何在多個 Transformer 層中逐步改變其注意力分布，以更精確地對齊文字和圖像中的實體或概念。</p>
<ul>
<li>注意力的細化：VisualBERT 在其連續的 Transformer 層中逐步細化文字和圖像之間的對齊。例如：參考上圖的左下角。一開始「丈夫」和「女人」兩詞可能都強烈地專注於圖像中的「女人」區域，但在模型的後續層中，這種對齊變得更加明確和正確。</li>
<li>句法對齊：VisualBERT 不僅可以根據語義對齊實體，還可以根據句法對齊它們。例如：在圖片中，「戲弄」這個詞同時專注於男人和女人，而「被」這個詞只專注於男人。</li>
<li>共指解決：VisualBERT 似乎還能夠解決語言中的共指問題，例如：「她」這個詞在圖像中被正確地對齊到「女人」。</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="結論的直接連結" title="結論的直接連結">​</a></h2>
<p>VisualBERT 在多種視覺語言任務上都展現了卓越的表現。這些成果不僅證明了模型的效能，更重要的是，透過其內建的注意力機制，VisualBERT 提供了一個可解釋和直觀的方式來捕獲和理解資訊。</p>
<p>但有一件事情，不論如何都無法迴避：</p>
<ul>
<li>當人們嘗試結合物件偵測的模型時，模型的架構立刻變得非常複雜且難以使用。</li>
<li>這種過度複雜的設計可能會抑制模型在實際應用中的潛力，並增加了部署和調整的困難。</li>
</ul>
<p>因此，將此架構進行優化和簡化絕對應該被視為後續的重要研究方向。</p>
<p>當然，這項工作還有許多需要進一步探索和釐清的問題。例如：對於純粹的影像任務，像是場景圖解析和情境辨識，VisualBERT 是否也能展現相同的效能？此外，是否能夠進一步擴充其能力，使之在更大的字幕資料集，例如：Visual Genome 和 Conceptual Caption 上進行預訓練？</p>
<p>在本研究階段，儘管有許多值得進一步探討的問題，這項研究為後續的研究者指明了後續的方向。</p></div><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/category/2019"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">2019</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/multi-model/2019/vilbert"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">ViLBERT</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#序幕上的凝視" class="table-of-contents__link toc-highlight">序幕上的凝視</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#visualbert-模型 設計" class="table-of-contents__link toc-highlight">VisualBERT 模型設計</a></li><li><a href="#預訓練機制" class="table-of-contents__link toc-highlight">預訓練機制</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#模型表現好嗎" class="table-of-contents__link toc-highlight">模型表現好嗎？</a></li><li><a href="#在這模型設計中誰最重要" class="table-of-contents__link toc-highlight">在這模型設計中，誰最重要？</a></li><li><a href="#模型真的有看到對的地方嗎" class="table-of-contents__link toc-highlight">模型真的有看到對的地方嗎？</a></li><li><a href="#注意力分布樣態如何" class="table-of-contents__link toc-highlight">注意力分布樣態如何？</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">使用條款<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">隱私政策<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>