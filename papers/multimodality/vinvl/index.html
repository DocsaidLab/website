<!doctype html><html lang=zh-hant dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multimodality/vinvl/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>[21.01] VinVL | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/papers/multimodality/vinvl/><meta data-rh=true property=og:locale content=zh_hant><meta data-rh=true property=og:locale:alternate content=en><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=zh-hant><meta data-rh=true name=docsearch:language content=zh-hant><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[21.01] VinVL | DOCSAID"><meta data-rh=true name=description content=再訪奧斯卡><meta data-rh=true property=og:description content=再訪奧斯卡><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/papers/multimodality/vinvl/><link data-rh=true rel=alternate href=https://docsaid.org/papers/multimodality/vinvl/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/multimodality/vinvl/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/multimodality/vinvl/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/multimodality/vinvl/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/assets/css/styles.31895f42.css><script src=/assets/js/runtime~main.e4b860b9.js defer></script><script src=/assets/js/main.961a0ef1.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/img/docsaid_logo.png><link rel=preload as=image href=/img/docsaid_logo_white.png><link rel=preload as=image href=https://github.com/zephyr-sh.png><div role=region aria-label=跳至主要内容><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>跳至主要内容</a></div><nav aria-label=主導航 class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label=切換導覽列 aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><div class=navbar__logo><img src=/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/docs/>開源專案</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/papers/intro>論文筆記</a><a class="navbar__item navbar__link" href=/blog>部落格</a><a class="navbar__item navbar__link" href=/playground/intro>遊樂場</a><a class="navbar__item navbar__link" href=/aboutus>關於我們</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>繁體中文</a><ul class=dropdown__menu><li><a href=/papers/multimodality/vinvl/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=zh-hant>繁體中文</a><li><a href=/en/papers/multimodality/vinvl/ target=_self rel="noopener noreferrer" class=dropdown__link lang=en>English</a><li><a href=/ja/papers/multimodality/vinvl/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="搜尋 (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>搜尋</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-7ny38l ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label=回到頂部 class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/><img src=/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label=文件側邊欄 class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/papers/intro>論文筆記</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="展開側邊欄分類 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="展開側邊欄分類 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="展開側邊欄分類 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/face-anti-spoofing-3>Face Anti-Spoofing (3)</a><button aria-label="展開側邊欄分類 'Face Anti-Spoofing (3)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="展開側邊欄分類 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="展開側邊欄分類 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="展開側邊欄分類 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/mamba-4>Mamba (4)</a><button aria-label="展開側邊欄分類 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="展開側邊欄分類 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="收起側邊欄分類 'Multimodality (24)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/lxmert/>[19.08] LXMERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/vilbert/>[19.08] ViLBERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/visualbert/>[19.08] VisualBERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/vlbert/>[19.08] VL-BERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/uniter/>[19.09] UNITER</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/oscar/>[20.04] Oscar</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/pixelbert/>[20.04] Pixel-BERT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/ernie-vil/>[20.06] ERNIE-ViL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/villa/>[20.06] VILLA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/unimo/>[20.12] UNIMO</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/papers/multimodality/vinvl/>[21.01] VinVL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/vilt/>[21.02] ViLT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/vlt5/>[21.02] VL-T5</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/clip/>[21.03] CLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/mdetr/>[21.04] MDETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/albef/>[21.07] ALBEF</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/simvlm/>[21.08] SimVLM</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/florence/>[21.11] Florence</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/meter/>[21.11] METER</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/blip/>[22.01] BLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/flamingo/>[22.04] Flamingo</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/beit-v3/>[22.08] BEiT-3</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/flip/>[22.12] FLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/multimodality/xgen-mm/>[24.08] xGen-MM</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/normalization-1>Normalization (1)</a><button aria-label="展開側邊欄分類 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="展開側邊欄分類 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="展開側邊欄分類 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="展開側邊欄分類 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="展開側邊欄分類 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="展開側邊欄分類 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="展開側邊欄分類 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/transformers-17>Transformers (17)</a><button aria-label="展開側邊欄分類 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="展開側邊欄分類 'Vision Transformers (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/papers/intro>All Notes: 177 entries</a></ul></nav><button type=button title=收起側邊欄 aria-label=收起側邊欄 class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=頁面路徑><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label=主頁面 class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/papers/category/multimodality-24><span itemprop=name>Multimodality (24)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[21.01] VinVL</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[21.01] VinVL</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=再訪奧斯卡>再訪奧斯卡<a href=#再訪奧斯卡 class=hash-link aria-label=再訪奧斯卡的直接連結 title=再訪奧斯卡的直接連結>​</a></h2>
<p><a href=https://arxiv.org/abs/2101.00529 target=_blank rel="noopener noreferrer"><strong>VinVL: Revisiting Visual Representations in Vision-Language Models</strong></a></p>
<hr>
<p>看完了上一篇的奧斯卡，後續的研究立刻就跟了上來。</p>
<ul>
<li><a href=/papers/multimodality/oscar/><strong>Oscar</strong></a></li>
</ul>
<p>這篇論文作者認為奧斯卡雖然不錯，但還是有缺點，即：</p>
<ul>
<li>它對於視覺表示的探索不夠深入。</li>
</ul>
<p>那麼怎麼樣的作法，才能稱之為「深入」呢？</p>
<p>讓我們仔細地來看過這篇論文。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=定義問題>定義問題<a href=#定義問題 class=hash-link aria-label=定義問題的直接連結 title=定義問題的直接連結>​</a></h2>
<p><img decoding=async loading=lazy alt="VinVL demo" src=/assets/images/vinvl_1-180f3412eee38d3bcc51ccd5cc4978b5.jpg width=1224 height=416 class=img_ev3q></p>
<p>作者專注於視覺語言預訓練（VLP）領域，特別是在改進物件偵測（OD）模型的視覺表徵方面。在多數視覺語言（VL）任務中，視覺特徵的有效性和豐富性對於直接關係到模型的性能。下面是一些主要的問題點：</p>
<ol>
<li>
<p><strong>豐富的視覺表徵的必要性</strong></p>
<p>當前的 VLP 方法通常強烈依賴於物件偵測模型所提供的視覺表徵。作者指出：儘管這些模型為處理各種 VL 任務提供了有價值的視覺信息，但還有改進的空間，尤其是在處理複雜、多樣並富含語義的圖像場景方面。</p>
</li>
<li>
<p><strong>跨模態融合模型的效果</strong></p>
<p>VLP 通常由兩部分組成：預訓練的物件偵測模型和一個跨模態融合模型，後者是用於結合視覺和語言特徵。雖然大部分的研究聚焦於改進跨模態模型，但作者強調，更加強大和語義豐富的視覺表徵同樣關鍵。</p>
</li>
<li>
<p><strong>現有物件偵測模型的局限性</strong></p>
<p>作者明確指出當前廣泛使用的 OD 模型，在一些文獻中作為一個「黑盒」使用，在其訓練數據集和目標上可能存在一些局限性，這影響了其在各種 VL 任務上的效能。</p>
</li>
<li>
<p><strong>視覺物件和概念的多樣性</strong></p>
<p>在許多實際應用中，模型需要能夠辨識和理解圖像中的各種視覺物件和概念。當前的模型，尤其是在 Open Images 上訓練的模型，可能無法充分捕捉和表達這些豐富多樣的視覺內容。</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=解決問題>解決問題<a href=#解決問題 class=hash-link aria-label=解決問題的直接連結 title=解決問題的直接連結>​</a></h2>
<p><img decoding=async loading=lazy alt="VinVL 模型設計" src=/assets/images/oscar_1-6d8ac3da7dfa1389b376a2f2889266fd.jpg width=1224 height=484 class=img_ev3q></p>
<p>在這篇論文中，作者沒有提出新的結構，就僅針對之前的 Oscar 架構進行視覺特徵的優化。</p>
<p>視覺語言模型通常由兩個主要部分組成：影像理解模組（Vision）和跨模態理解模組（VL）。其中，Vision 模組將影像轉換為語義表示（q）（例如：標籤或檢測到的對象），和高維潛在空間中的分佈表示（v）（例如：由預訓練的 Faster-RCNN 模型生成）。</p>
<p>接著，這些視覺特徵與語言輸入一起用於各種 VL 任務。在本文中深入探討了影像理解模組的改進方法，尤其聚焦於物件檢測的預訓練和利用屬性信息的注入，以下是幾個要點：</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=物件檢測預訓練>物件檢測預訓練<a href=#物件檢測預訓練 class=hash-link aria-label=物件檢測預訓練的直接連結 title=物件檢測預訓練的直接連結>​</a></h3>
<ul>
<li>
<p><strong>資料處理</strong></p>
<p>使用四個主要的公共數據集（COCO、OpenImagesV5、Objects365V1 和 Visual Genome）進行目標檢測模型的預訓練，這些數據集包含多種大小和多樣性的影像和標籤資料。具體使用了多個策略來建立一個統一的大型語料庫，例如：類別感知採樣和對不平衡資料集的貢獻進行平衡。</p>
</li>
<li>
<p><strong>模型架構</strong></p>
<p>在模型架構方面，儘管特徵金字塔網路（FPN）在目標檢測任務上表現優於 C4 模型，但 C4 模型為 VL 任務提供了更有效的區域特徵。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p><strong>C4 模型是什麼？</strong><p>我剛看到的時候也感到困惑，於是跟著論文找到另外一篇<a href=https://arxiv.org/abs/2001.03615 target=_blank rel="noopener noreferrer"><strong>參考文獻</strong></a>，發現原來所謂的 C4 模型就是指從 CNN 架構中提取出來的 1/2^4（就是 1/16）的特徵圖啦！<p>在這篇參考文獻內提出了一個論述：使用 1/16 尺度的特徵圖比起 FPN 更能提升 VL 任務的表現。而這個論述被本篇論文的作者採用了，所以你會在這邊看到「C4 模型」這個名詞。</div></div>
</li>
<li>
<p><strong>預訓練策略</strong></p>
<p>作者採用了一系列經典與經驗相結合的方法以提升模型的性能。</p>
<ul>
<li>鎖定（freeze）模型的初步層級，包括第一卷積層、第一個殘差區塊以及所有的批次標準化層，以保留低級的視覺特徵並防止在初期訓練時被破壞。</li>
<li>為了擴展數據集並增強模型的泛化能力，作者也應用了一系列的資料增強技巧，其中包括水平翻轉和多尺度訓練。</li>
</ul>
<p>作者選擇使用的目標檢測模型基本架構為「X152-C4 架構」。初始模型主幹的權重是基於在 ImageNet-5K 數據集上訓練過的模型進行初始化的，並且在 16 張影像的批次大小下進行了 1.8M 次的迭代訓練。</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=屬性注入>屬性注入<a href=#屬性注入 class=hash-link aria-label=屬性注入的直接連結 title=屬性注入的直接連結>​</a></h3>
<p>在目標檢測模型（OD）的訓練與微調中，一個重要的方向是如何將額外的信息或特徵整合到模型中以提升其預測能力。</p>
<ul>
<li>
<p><strong>預訓練模型與屬性分支</strong></p>
<ul>
<li>預訓練的 OD 模型：基本上是先在一個較大的數據集上訓練物件檢測模型，以便學習到一般的視覺特徵和物件檢測的基本能力。</li>
<li>加入屬性分支：在該預訓練的 OD 模型中添加了一個新的分支來預測物件的屬性，從而使模型在進行物件檢測的同時也能辨識和預測出物件的某些屬性。</li>
</ul>
</li>
<li>
<p><strong>在 Visual Genome（VG） 數據集上的微調</strong></p>
<ul>
<li>屬性資訊：這裡的屬性指的是物件的某些特徵或者特性，比如顏色、形狀、大小等，VG 數據集提供了 524 個這樣的屬性分類。</li>
<li>微調策略：在將屬性分支加入到模型後，對其在 VG 數據集上進行微調，以便模型學習到與這些屬性相關的視覺特徵並提高在物件檢測任務中對這些屬性的預測能力。</li>
<li>屬性損失權重調整：選擇一個比先前研究更大的屬性損失權重（0.5 -> 1.25），目的是在微調過程中更加強調屬性學習的重要性，以在最終的模型中實現對物件屬性更加精準的預測。</li>
</ul>
</li>
</ul>
<p>這樣的方法能夠使模型在 VG 數據集上，不僅能夠檢測出物件，還能夠辨識和預測物件的多個屬性，並且與之前的模型相比，展現了顯著的優勢。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=提高效率>提高效率<a href=#提高效率 class=hash-link aria-label=提高效率的直接連結 title=提高效率的直接連結>​</a></h3>
<p>在視覺語言任務中，物件的視覺特徵和屬性的豐富性帶來了一定的計算挑戰，特別是在特徵提取過程中。</p>
<ul>
<li>
<p><strong>非極大值抑制 (NMS) 的挑戰</strong></p>
<p>由於視覺物件和屬性的豐富多樣性，使用類別感知非極大值抑制（NMS）在後處理階段需要消耗大量的計算資源來消除重疊的邊界框，導致特徵提取過程極其緩慢。</p>
</li>
<li>
<p><strong>提高效率的策略</strong></p>
<ul>
<li>使用類別不可知 NMS：使用只執行一次 NMS 操作的類別不可知 NMS，這不僅降低了計算的複雜度，而且維持了一個較高的操作效率。</li>
<li>調整卷積層設定：將原先設置為 dilation=2 的卷積層替換為不進行擴張的卷積層，這同樣是一個針對計算效率的優化。</li>
</ul>
</li>
</ul>
<p>這兩種主要的調整和替換使區域特徵提取過程的速度有了顯著提升，而且在 VL 下游任務的準確性方面並沒有出現下降。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p><strong>空洞卷積是什麼？</strong><p>原文是 Dilated Convolution，是一種通過在卷積核內引入間隔（即空洞）來擴大其感受域的技術。當 dilation > 1 時，卷積核對輸入的採樣更為稀疏，意味著每一個輸出特徵是由輸入特徵中更寬廣範圍的區域計算得來。它可以捕獲到較大範圍內的信息，但也帶來了更高的計算複雜度。</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=討論>討論<a href=#討論 class=hash-link aria-label=討論的直接連結 title=討論的直接連結>​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=下游任務的表現>下游任務的表現<a href=#下游任務的表現 class=hash-link aria-label=下游任務的表現的直接連結 title=下游任務的表現的直接連結>​</a></h3>
<p><img decoding=async loading=lazy alt="VinVL results" src=/assets/images/vinvl_result-d7e21928f2c9ad9d61e7e30b37ca5d3b.jpg width=2554 height=450 class=img_ev3q></p>
<ol>
<li>
<p>為了評估模型參數效率，最先進的模型 (SoTA) 被劃分為三類：</p>
<ul>
<li>SoTAS: 小型模型，在基於 Transformer 的 VLP 模型之前達到最佳表現。</li>
<li>SoTAB: 大小與 BERT 基礎類似的 VLP 模型的最佳效能。</li>
<li>SoTAL: 大小與 BERT Large 類似的 VLP 模型的最佳表現。</li>
</ul>
</li>
<li>
<p>OSCAR+ 和 VINVL 在 7 個視覺語言任務上的結果比先前的 SoTA 更好，大多數情況下都遠遠超出。</p>
</li>
<li>
<p>VQA：OSCAR+B 模型優於 VQA 排行榜上的最佳模型。</p>
</li>
<li>
<p>GQA：OSCAR+B 搭配 VINVL 是第一個超越神經狀態機 (NSM) 的 VLP 模型。</p>
</li>
<li>
<p>圖像字幕：OSCAR+B 模型在 COCO 圖像字幕線上排行榜上排名第一，超越了 263 個模型。</p>
</li>
<li>
<p>NoCaps：沒有使用任何 VLP，基於 BERT 的模型加上新的視覺特徵（VinVL）已經超越了人類的 CIDEr 效能。再加上 VIVO 預訓練後，效能進一步提高，成為新的 SoTA。</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=視覺特徵有多重要>視覺特徵有多重要？<a href=#視覺特徵有多重要 class=hash-link aria-label=視覺特徵有多重要？的直接連結 title=視覺特徵有多重要？的直接連結>​</a></h3>
<p><img decoding=async loading=lazy alt="VinVL ablation" src=/assets/images/vinvl_2-3cd4d49647d0e11187fdc28d5927e4e6.jpg width=1024 height=248 class=img_ev3q></p>
<p>作者分析了兩種不同的視覺模型（R101-C4 和 X152-C4）以及不同的視覺語言前處理（VLP）方法（無 VLP 和 OSCAR）在視覺問答（VQA）任務上的效能。X152-C4 和 R101-C4 模型分別代表著不同的視覺模型架構，其中 VinVL（一個包含 4 個資料集的預訓練模型）和其他 VLP 方法均用於預訓練。</p>
<ol>
<li>
<p><strong>模型比較與效能提升</strong></p>
<ul>
<li>OSCAR vs. OSCAR+：利用 R101-C4 特徵的 OSCAR 模型作為基線，當更換為 X152-C4 特徵的 OSCAR+ 模型時，絕對精度從 72.38 提升至 74.90。</li>
<li>貢獻分析： OSCAR+ 預訓練提供了 5% 的精度，而透過改進視覺特徵的視覺預訓練貢獻了 95% 的精度增益。</li>
</ul>
</li>
<li>
<p><strong>視覺表徵的重要性</strong></p>
<ul>
<li>經過這次模型比較，顯示視覺表徵在 VLP 和下游任務中起著關鍵的作用。視覺模型和 VLP 方法均對結果的改善做出了明顯貢獻。</li>
<li>VinVL 和 VLP 的增益顯示為相加的形式，代表視覺預訓練和 VLP 各自對視覺模型和 VL 模型做出了獨立的改善。</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=視覺概念多樣性的影響>視覺概念多樣性的影響<a href=#視覺概念多樣性的影響 class=hash-link aria-label=視覺概念多樣性的影響的直接連結 title=視覺概念多樣性的影響的直接連結>​</a></h3>
<p>文章探討了視覺概念，尤其是物件和屬性詞彙的多樣性，對視覺語言模型，特別是視覺問答（VQA）任務的影響。</p>
<ul>
<li>
<p><strong>視覺概念詞彙的豐富度</strong></p>
<p><img decoding=async loading=lazy alt="VinVL concepts" src=/assets/images/vinvl_3-bd817e374bbf85ef6720716f011889a6.jpg width=1024 height=117 class=img_ev3q></p>
<ul>
<li>更豐富的物件詞彙與 VQA 結果的正相關： 模型在物件詞彙更豐富的數據集上通常獲得更好的 VQA 結果。例如：模型在 VG w/o attr 上的表現優於在具有較少物件類別的 VG-obj 和 ImageNet 上。</li>
<li>物件詞彙與視覺概念的完整性： 某些視覺概念（例如：「天空」和「水」）被證明對 VQA 任務極具影響力，這也揭示了更全面的物件詞彙在模型表現上的優勢。</li>
<li>屬性的使用對 VQA 結果極為關鍵。當模型被訓練以辨識屬性時（如在 VG 或 4Sets→VG 數據集上），它們的表現顯著優於沒有這類訓練的模型。</li>
<li>即便在較小的視覺模型（例如：R50-C4）上，視覺預訓練也能提升 VQA 的表現，證明視覺預訓練的普遍效益。</li>
</ul>
</li>
<li>
<p><strong>豐富的視覺語義對視覺語言任務的影響</strong></p>
<p><img decoding=async loading=lazy alt="VinVL concepts" src=/assets/images/vinvl_4-886501fd32b64428c1db5ebbc844220d.jpg width=1024 height=228 class=img_ev3q></p>
<ul>
<li>COCO groundtruth（GT-Obj 和 GT-Obj&Stuff）在物件定位方面表現出色，但其詞彙有限。相對的，VG 訓練模型在物件定位方面可能不如 COCO groundtruth，但它們擁有更豐富的詞彙，因此對 VQA 任務更有利。</li>
<li>相對於典型的物件檢測（OD）任務，視覺語言（VL）中的 OD 任務更加依賴豐富的視覺語義。VL 任務要求的視覺語義需與語言模態中的豐富語義一致，強調了豐富視覺詞彙在 VL 任務中的重要性。</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=結論>結論<a href=#結論 class=hash-link aria-label=結論的直接連結 title=結論的直接連結>​</a></h2>
<p>雖然 VinVL 嘗試引入更多的視覺特徵，但總還是沒有掙脫類別的概念。當選定的物件和屬性類別也許並不足夠豐富，或者在某種程度上帶有一定的偏見，這意味著模型無法完全捕捉到所有的視覺概念，影響其深入應用於更廣泛領域的可能性。</p>
<p>再來，相信你也看出來了，這個模型規模的龐大和複雜度也是一個突出的問題。作者大概也是明白，所以特別寫了一個章節來說明提高效率的方式，來告訴使用者該怎麼改善推論速度。</p>
<p>VinVL 模型的大型架構和複雜計算過程要求較高的計算資源和儲存能力，這在一定程度上限制了其在資源有限的應用場景中的適用性和普及程度。</p>
<p>最後，屬性和視覺概念的融合難題同樣不可忽視。模型需要在多個來源的數據集上進行訓練以涵蓋廣泛的視覺概念和屬性。然而，如何融合這些異質來源的資料、確保資料一致性並充分發揮它們的優勢，在實現一個具有高度一致性和廣泛應用性的模型時仍是一大挑戰。</p>
<p>VinVL 模型透過在多個數據集上的訓練，展現出在多個 VL 任務上的強大能力和潛力，特別是在物件辨識和視覺屬性的理解上。儘管模型仍然面臨一些挑戰，但其在多個任務上的優異表現及其廣泛的應用前景彰顯出其作為視覺語言預訓練模型的價值。</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime=2025-02-11T02:49:16.000Z itemprop=dateModified>2025年2月11日</time></b> <!-- -->更新</span></div></div><div style=margin-top:3rem> </div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label=文件選項卡><a class="pagination-nav__link pagination-nav__link--prev" href=/papers/multimodality/unimo/><div class=pagination-nav__sublabel>上一頁</div><div class=pagination-nav__label>[20.12] UNIMO</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/papers/multimodality/vilt/><div class=pagination-nav__sublabel>下一頁</div><div class=pagination-nav__label>[21.02] ViLT</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#再訪奧斯卡 class="table-of-contents__link toc-highlight">再訪奧斯卡</a><li><a href=#定義問題 class="table-of-contents__link toc-highlight">定義問題</a><li><a href=#解決問題 class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href=#物件檢測預訓練 class="table-of-contents__link toc-highlight">物件檢測預訓練</a><li><a href=#屬性注入 class="table-of-contents__link toc-highlight">屬性注入</a><li><a href=#提高效率 class="table-of-contents__link toc-highlight">提高效率</a></ul><li><a href=#討論 class="table-of-contents__link toc-highlight">討論</a><ul><li><a href=#下游任務的表現 class="table-of-contents__link toc-highlight">下游任務的表現</a><li><a href=#視覺特徵有多重要 class="table-of-contents__link toc-highlight">視覺特徵有多重要？</a><li><a href=#視覺概念多樣性的影響 class="table-of-contents__link toc-highlight">視覺概念多樣性的影響</a></ul><li><a href=#結論 class="table-of-contents__link toc-highlight">結論</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/docs>開源專案</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/papers/intro>論文筆記</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/blog>部落格</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/terms-of-service>使用條款</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/privacy-policy>隱私政策</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/become-an-author>成為作者</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/worklog>工作日誌</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2025 DOCSAID.</div></div></div></footer></div>