<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multimodality/villa/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[20.06] VILLA | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/multimodality/villa/"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[20.06] VILLA | DOCSAID"><meta data-rh="true" name="description" content="別墅裡的魅影"><meta data-rh="true" property="og:description" content="別墅裡的魅影"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/multimodality/villa/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/villa/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/multimodality/villa/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/villa/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3057f3b6.css">
<script src="/assets/js/runtime~main.86a74064.js" defer="defer"></script>
<script src="/assets/js/main.d6ae87a4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/multimodality/villa/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/multimodality/villa/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">論文筆記</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/classic-cnns-8">Classic CNNs (8)</a><button aria-label="展開側邊欄分類 &#x27;Classic CNNs (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="展開側邊欄分類 &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="展開側邊欄分類 &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="展開側邊欄分類 &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="展開側邊欄分類 &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/papers/category/multimodality-18">Multimodality (18)</a><button aria-label="收起側邊欄分類 &#x27;Multimodality (18)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/papers/multimodality/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/multimodality/meter/">[21.11] METER</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/normalization-1">Normalization (1)</a><button aria-label="展開側邊欄分類 &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/object-detection-5">Object Detection (5)</a><button aria-label="展開側邊欄分類 &#x27;Object Detection (5)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="展開側邊欄分類 &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="展開側邊欄分類 &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="展開側邊欄分類 &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/transformers-13">Transformers (13)</a><button aria-label="展開側邊欄分類 &#x27;Transformers (13)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="展開側邊欄分類 &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/multimodality-18"><span itemprop="name">Multimodality (18)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[20.06] VILLA</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[20.06] VILLA</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="別墅裡的魅影">別墅裡的魅影<a href="#別墅裡的魅影" class="hash-link" aria-label="別墅裡的魅影的直接連結" title="別墅裡的魅影的直接連結">​</a></h2>
<p><a href="https://arxiv.org/abs/2006.06195" target="_blank" rel="noopener noreferrer"><strong>Large-Scale Adversarial Training for Vision-and-Language Representation Learning</strong></a></p>
<hr>
<p>這次我們來看一篇有趣的論文。</p>
<p>當我們嘗試理解如何使機器能夠更好地理解和描述圖像中  的事物時，將面臨了一個挑戰：如何使它不僅在學校的考試中表現出色，而且在真實世界中也能夠應對各種不可預見的問題？</p>
<p>想像一下，你正在訓練一名拳擊手。</p>
<p>這名拳擊手在道場或健身房中的拳擊沙袋上表現出色，但當他面對真實的對手時，情況就完全不同了。真實的對手會使用各種策略和技巧來擊敗他，而不是像拳擊沙袋那樣待在原地。</p>
<p>因此，為了使這名拳擊手在比賽中表現得更好，你需要模擬真實的對抗情境，給他一些突如其來的挑戰，讓他學會如何應對。例如：你可以製作一個「高科技的拳擊沙袋」（假設你做得出來），功能是這個「拳擊沙袋」會自動偵測拳擊手姿態動作，接著自動長出一雙手腳後，追逐毆打這位拳擊手。</p>
<p>你可能會說，這什麼鬼？</p>
<p>答對了，模型的反應也跟你一樣！</p>
<p>就是透過這種讓模型看到「不知道是什麼鬼東西」的設計，就是對抗性訓練的理念。</p>
<p>可是，這樣說來，這是不是有點像是降噪（Denoising）的任務？</p>
<p>這兩者很容易混淆，實際上他們是不一樣的概念。</p>
<p>我們從概念和目的上來比較這兩者：</p>
<ol>
<li>
<p><strong>降噪 (Denoising)</strong></p>
<ul>
<li><strong>目的</strong>：目的是清除圖片或數據中的雜訊，恢復原始的、未被雜訊干擾的數據。</li>
<li><strong>概念</strong>：當你有一張受到雜訊影響的圖片時（例如：因為低光線、壓縮等原因），降噪的過程會試圖去除這些雜訊，使圖片更接近原始、乾淨的狀態。</li>
<li><strong>方法</strong>：使用特定的算法或已經訓練好的模型，該模型已經學會了如何識別和去除多種類型的雜訊。</li>
</ul>
</li>
<li>
<p><strong>對抗性訓練 (Adversarial Training)</strong></p>
<ul>
<li><strong>目的</strong>：使模型在面對「對抗性攻擊」時仍然能夠正確 地進行預測。</li>
<li><strong>概念</strong>：在模型訓練過程中，不只提供正常的數據，還會生成並加入「對抗性範例」。這些範例是故意製造出來的，外觀上可能非常接近正常的數據，但能引導模型做出錯誤的預測。訓練模型辨識和抵抗這些範例，以提高其泛化性。</li>
<li><strong>方法</strong>：通常涉及兩個步驟：一是生成對抗性範例，二是使用這些範例來訓練模型。</li>
</ul>
</li>
</ol>
<p>降噪主要關心的是如何恢復原始的、乾淨的數據，而對抗性訓練的目的是增強模型的泛化性，使其能夠抵抗敵意的、試圖欺騙模型的輸入。另外降噪通常涉及到的是已知的雜訊模式，並試圖去除它。而對抗性訓練是故意創造新的、可能導致模型錯誤的「敵意」輸入。</p>
<p>在訓練模型時，我們不僅讓它看標準的、熟悉的圖像和文本，還要給它一些「假想的對手」，就是那些「被特意修改過，旨在迷惑模型」的輸入內容。這些「假想的對手」就像那些訓練中的突如其來的挑戰，迫使模型學會應對各種不同的情況。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結">​</a></h2>
<p>作者在這篇論文中主要專注於以下問題：</p>
<ol>
<li>
<p>預訓練模型的過度擬合問題</p>
<p>由於大規模預訓練模型的容量巨大，與下游任務中的有限標記資料不成比例，因此模型容易陷入過度擬合的情況。</p>
</li>
<li>
<p>對抗性訓練的應用問題</p>
<p>儘管對抗性訓練已在其他領域（如圖像和文本）中證明其有效性，但它如何應用於視覺和語言（V+L）問題，並是否能夠提升模型的性能，仍是一個未解的問題。</p>
</li>
<li>
<p>多模態編碼中的對抗性訓練問題</p>
<p>傳統的對抗性訓練方法專注於圖像像素和文本子詞標記層級，但作者提出在多模態的編碼層級上進行對抗性訓練可能更有利。</p>
</li>
<li>
<p>對乾淨輸入的模型性能提升</p>
<p>既然對抗性訓練的目標是提高模型對敵對攻擊的抵抗力，那麼它是否也能夠提高模型對乾淨輸入的性能呢？</p>
</li>
<li>
<p>對抗性訓練的效率問題</p>
<p>由於對抗性訓練過程非常耗時且運算成本高，如何提高大規模訓練的效率成為一個挑戰。</p>
</li>
<li>
<p>穩健性和泛化之間的平衡</p>
<p>如何在增強模型的穩健性（抵抗對抗性攻擊的能力）的同時，保持或提高其在乾淨資料上的泛化性能？</p>
</li>
<li>
<p>對抗性訓練在 V+L 任務中的普適性</p>
<p>考慮到 VILLA 是一個新的對抗性訓練策略，作者需要探索其在不同的 V+L 任務中的適用性和效果。</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結">​</a></h2>
<p>這邊會比較艱澀一點，但我們還是可以一起來讀讀看。</p>
<p>作者分為三個部分來構建整個模型。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="villa-模型設計">VILLA 模型設計<a href="#villa-模型設計" class="hash-link" aria-label="VILLA 模型設計的直接連結" title="VILLA 模型設計的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="VILLA 模型設計" src="/assets/images/villa_1-63baf9b3eda3521b902813a6ab7f0de9.jpg" width="1224" height="332" class="img_ev3q"></p>
<p>這個階段的主要目的是學習能夠在不同的下游任務中適用的通用的圖像和文字表示。</p>
<ol>
<li>
<p><strong>資料集</strong></p>
<p>使用資料集 （論文中，給定 Dp 來表示預訓練的資料集)）進行預訓練，該資料集由影像文字對（X_img，X_txt）組成。</p>
</li>
<li>
<p><strong>特徵表示</strong></p>
<ul>
<li>影像（X_img）首先通過一個自下而上的特徵提取器 (g_bu(·)) 轉換為特徵向量 。</li>
<li>文字（X_txt）則通過一個可學習的詞編碼函數 (g_emb(·)) 轉換為特徵向量。</li>
</ul>
</li>
<li>
<p><strong>多模態融合</strong></p>
<p>為了融合影像和文字的特徵，使用了多層的 Transformer 結構，這種結構在自然語言處理和其他多模態任務中非常流行。其中，[CLS] 標記在這裡有特別的意義，其編碼被用作多模態任務的聯合表示。</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="預訓練任務">預訓練任務<a href="#預訓練任務" class="hash-link" aria-label="預訓練任務的直接連結" title="預訓練任務的直接連結">​</a></h3>
<p>其中，MLM 和 ITM 在之前看的論文中有講過很多次了，這邊簡單帶過，並詳細講講 MRM 的策略。</p>
<ul>
<li>
<p><strong>掩碼語言模型 (MLM)</strong></p>
<p>在進行遮罩語言建模的過程中，模型的輸入標記中約有 15% 會被隨機選擇並遮罩。遮罩的意義是將特定標記的原始值隱藏，使模型不能直接看到它。此後，模型的工作就是嘗試恢復這些遮罩標記的真實值，它必須依賴於其他非遮罩的語言標記以及所提供的視覺標記來做出恢復的預測。</p>
</li>
<li>
<p><strong>影像文字匹配 (ITM)</strong></p>
<p>影像文字匹配（ITM）是一種複雜的策略，主要設計用於評估模型在理解圖像和相對應的文本描述之間的深層語義關聯的能力。</p>
</li>
<li>
<p><strong>掩蔽區域建模 (MRM)</strong></p>
<p>MRM 的主要目的是讓模型學習如何根據完整的圖像信息推斷出被掩蔽的部分。這樣的能力可以幫助模型在後續的任務中更好地理解和識別圖像的各個部分，並與語言特徵有效地結合。這種方法與自然語言處理中的「掩碼語言建模」（Masked Language Modeling, MLM）相似，但它是應用於圖像區域而非文字標記。</p>
<p>在影像資料 X_img​ 中，選擇一些特定的區域（可以是隨機選擇，或按某種策略選擇）並將其特徵設置為零或使用其他方式進行掩蔽。這些被掩蔽的區域可以視為模型需要「填充」的部分。</p>
<p>給定其他未被掩蔽的多模態資訊（如影像的其他部分和相關的文本描述），模型的任務是預測被掩蔽區域的正確內容。這種預測可以通過交叉熵損失、KL 散度損失或對照學習等方式來量化。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>這感覺就像是，如果你有一本幼兒著色本。在其中一頁，畫面上有一個半完整的蘋果圖案，其中一部分被擦去了或被遮蓋了。假設這個蘋果的右側 1/4 被遮住了。現在，儘管你只看到了這個蘋果的 3/4，但由於你之前已經看過很多完整的蘋果，你仍然能夠猜測那部分被遮住的蘋果看起來應該是怎樣的。</p></div></div>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="兩階段的對抗性訓練">兩階段的對抗性訓練<a href="#兩階段的對抗性訓練" class="hash-link" aria-label="兩階段的對抗性訓練的直接連結" title="兩階段的對抗性訓練的直接連結">​</a></h3>
<p>這裡的兩階段指的就是預訓練和微調。</p>
<p> 這部分的論述是探討預訓練和微調兩個階段如何進行對抗性訓練，以及它們之間的關係。</p>
<ol>
<li>
<p><strong>預訓練和微調的連接</strong></p>
<p>預訓練和微調是大多數深度學習流程中的兩個主要階段。預訓練階段是為了獲得模型的基本理解和基本的特徵提取能力。而微調階段則是對特定任務進行專業化訓練。這兩個階段有著密不可分的關係，因為預訓練為微調階段提供了必要的基礎。</p>
</li>
<li>
<p><strong>跨模態聯合理解的重要性</strong></p>
<p>模型在不同的任務上，如 MLM 或 VQA，都需要進行跨模態的理解，意即同時理解圖像和文本的內容。舉例來說，如果在圖片中看到了一隻狗，模型需要能夠將這個視覺資訊和文本「狗」相互聯結。</p>
</li>
<li>
<p><strong>對抗性訓練的假設</strong></p>
<ul>
<li>第一個假設是進行對抗性預訓練能夠增強模型的泛化能力，這將對微調階段十分有利。泛化能力是指模型在未見過的數據上的表現。</li>
<li>第二個假設是在微調階段，當有了特定任務的訓練數據，可以再次使用對抗性訓練方法，進一步提高模型的性能。</li>
</ul>
</li>
<li>
<p><strong>共享的數學公式</strong></p>
<p>預訓練和微調這兩個階段在數學上是相似的，所以它們都可以使用相同的對抗性訓練方法。</p>
</li>
</ol>
<p>這裡要強調的是：兩個階段都可以進行對抗性訓練，以提高模型的泛化能力和對抗攻擊的韌性。</p>
<p>對抗性訓練在預訓練階段有助於增強模型的基本泛化能力，而在微調階段則可以根據特定任務進一步提高模型的性能。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="微擾動perturbations">微擾動（Perturbations）<a href="#微擾動perturbations" class="hash-link" aria-label="微擾動（Perturbations）的直接連結" title="微擾動（Perturbations）的直接連結">​</a></h3>
<p>這段 主要探討了如何在影像和文字的編碼空間（embedding space）中加入對抗性擾動來進行對抗性訓練。</p>
<ol>
<li>
<p><strong>影像模態的擾動</strong></p>
<p>在最先進的 V+L 模型中，模型通常將預訓練的目標偵測器得到的影像特徵作為輸入。與傳統的在像素空間添加擾動的方法不同，這裡的方法選擇直接在這些特徵的編碼空間中添加對抗性擾動。例如：考慮一張狗的圖片，這裡不是直接對圖片的像素進行小的調整，而是對該圖片在模型中的特徵表示進行微小擾動。這種方法的主要優勢是它可以更精細地操作編碼，使得擾動更加嚴格。</p>
</li>
<li>
<p><strong>文字模態的擾動</strong></p>
<p>與影像像素的連續值不同，文字的標記是離散的，因此更難以操控。傳統上，製作保留原始語義的對抗性範例在文字模態中是困難的。然而，本文的方法選擇在單詞的編碼空間中加入擾動，而不是直接在單詞上。這樣可以避免直接改變原始的文本內容，但仍能影響模型的預測。</p>
<p>例如：考慮句子 「The dog barks.」，作者不是替換或修改其中的字，而是對「dog」 這個詞的編碼表示進行微小調整，使其在模型中對應到與 「dog」 相似但略有不同的表示。</p>
</li>
<li>
<p><strong>位置編碼</strong></p>
<p>在預訓練的 V+L 模型中，位置編碼被用來編碼影像區域和子詞標記的位置。在這裡的對抗性訓練方法中，僅修改圖像和單詞編碼，而保持其他特徵不變。</p>
</li>
<li>
<p><strong>同時擾動的考量</strong></p>
<p>作者建議一次只對一種模態（即影像或文字）加入擾動。這是基於圖像和文字的獨特特性和差異。在添加擾動後，目標是讓模型的預測不會改變，即保持原始語義。</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="多模態自由對抗訓練">多模態「自由」對抗訓練<a href="#多模  態自由對抗訓練" class="hash-link" aria-label="多模態「自由」對抗訓練的直接連結" title="多模態「自由」對抗訓練的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="多模態「自由」對抗訓練" src="/assets/images/villa_2-5cc697e3e18b6b7d83ebffb737feae57.jpg" width="1024" height="616" class="img_ev3q"></p>
<p>這一節主要闡述了 VILLA 模型中使用的多模態「自由」對抗訓練方法，其中涉及了多個數學公式與技術詳情，別怕，我們詳細地來看一下：</p>
<ul>
<li>
<p><strong>訓練目標</strong></p>
<p>在 VILLA 模型中，「對抗訓練」是一項核心技術，其目的是使模型在面對輕微的輸入擾動時仍能保持穩定的預測性能。這種訓練方式可以被理解為一種強化手段，使模型不容易受到外部噪音或微小變動的影響，從而提高其在各種情境下的泛化能力。這意味著模型不僅能夠在訓練數據上表現出色，還能夠對新的、未見過的數據產生可靠的預測。</p>
<p>為了達到這一目標，訓練過程中要考慮三個主要的損失項：</p>
<ol>
<li>
<p><strong>標準的交叉熵損失（L-std）</strong></p>
<p>這是大多數分類問題中使用的基本損失函數，它衡量模型的預測與真實標籤之間的差距。越小的交叉熵損失意味著模型的預測越接近真實情況。</p>
</li>
<li>
<p><strong>標籤保存的對抗訓練損失（R-at）</strong></p>
<p>這一項確保當輸入數據遭受輕微擾動時，模型的預測仍然保持與原始輸入相同的標籤。換句話說，即使圖像或文本有輕微的變化，模型也應該產生相同的預測結果。</p>
</li>
<li>
<p><strong>細粒度的對抗正則化項（R-kl）</strong></p>
<p>這是一個更為複雜的損失項，它不僅要求模型在擾動輸入時保持相同的標籤，還要求模型的預測信心或概率分佈也應該與未受擾動的輸入保持接近。這確保了模型不僅在標籤級別上保持泛化 性，還在預測的細節上也保持泛化性。</p>
</li>
</ol>
</li>
<li>
<p><strong>多模態的對抗擾動</strong></p>
<p>VILLA 模型在進行訓練時，不僅考慮原始的影像和文字編碼，還針對這些編碼加入了對抗性的擾動。這些擾動可以被理解為故意導入的微小變化，目的是測試和增強模型的泛化性，確保模型在遭遇未知的、微小的噪聲或變動時仍能正確工作。</p>
<p>然而，這些對抗性擾動不是隨意添加的。它們有明確的「範數限制」，意思是這些擾動的強度或大小受到了一定的控制，以確保它們不會造成模型完全無法識別的重大變動。</p>
<p>接著，模型的訓練涉及到兩個主要的優化步驟：外部的最小化和內部的最大化。外部的最小化主要是指在整體的訓練過程中，希望模型的預測錯誤（即損失）盡可能小，這一步可以通過常見的梯度下降方法如 SGD (Stochastic Gradient Descent) 來達成。而內部的最大化則是在尋找對抗性擾動時，希望這些擾動可以使模型的損失最大化，這樣找到的擾動就是最有可能干擾模型的。這一步的優化是通過一種稱為 PGD (Projected Gradient Descent) 的方法完成。</p>
<p>考慮到影像模態，每進行一次迭代更新擾動時，PGD 會先計算當前擾動對損失的影響，即計算損失對擾動的梯度。接著，它會在這一梯度的方向上進行一個小步驟，以嘗試找到一個可以最大化損失的新擾動。這個過程會反覆進行，直到達到預定的迭代次數或擾動大小的限制。</p>
<p>想像你正在訓練一個機器學習模型，它的任務是分辨照片中的物體是「狗」還是「貓」。原始的影像編碼就是從照片中提取的特徵或資訊。</p>
<ol>
<li>
<p><strong>對抗性擾動的加入</strong></p>
<p>假設在訓練的過程中，有人故意微調了一些照片的像素，使其中的貓看起來更像狗，或者使狗看起來更像貓。這些  微調就是所謂的「對抗性擾動」。</p>
</li>
<li>
<p><strong>範數限制</strong></p>
<p>但是，這些微調不是隨意的。它們有一個限制，確保變動不會太大。例如：擾動的大小不會使貓的整張臉完全變形，只是微調到足以混淆模型的程度。</p>
</li>
<li>
<p><strong>優化</strong></p>
<p>在每次訓練迭代中，模型都試圖找出這些微調是如何進行的，然後學習如何儘量忽略它們，專注於真正區分狗和貓的特徵。使用 SGD，模型會根據這些微調的影響，調整自己的參數，使得在這些被微調的圖片上的錯誤率減少。而使用 PGD，模型會在每次迭代中，嘗試找出最可能使自己出錯的那些微調，然後針對這些最壞情況進行學習和調整。</p>
</li>
</ol>
</li>
<li>
<p><strong>對抗正則化的進一步增強</strong></p>
<p>對抗正則化是機器學習中的一種技巧，旨在提高模型的泛化性，使其在面對對抗性擾動時仍能夠進行正確的預測。在某些情境中，對抗性擾動可能會使模型的預測產生很大的偏差，而這種正則化的方法則試圖限制這種偏差。</p>
<p>這種增強型的對抗正則化不只是簡單地要求模型在面對擾動時仍能正確分類，更進一步要求模型的預測機率分佈在擾動前後要保持相似。這就意味著，模型不僅要確定某個物體是「貓」或「狗」，還要確保其對此判斷的信心或機率在擾動前後都不會有太大的變化。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>Kullback-Leibler 散度是一種衡量兩個機率分佈之間差異的工具。在這個情境下，它被用來比較模型在擾動前後的預測機率分佈的相似性。如果兩個分佈很相似，則其 Kullback-Leibler 散度將接近 0；反之，如果兩者相差很大，則其值將會增加。</p></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>舉個例子：</p><p>考慮一個模型，在未受擾動的情況下，預測某照片中的動物為「貓」的機率為 90%。但當這張照片受到對抗性擾動後，模型的預測機率降至 60%。這表示擾動影響了模型的確定性，使其對於「貓」的預測不再那麼自信。這種信心的變化，即這兩個機率分佈之間的差異，會被 Kullback-Leibler 散度捕捉到。正則化項將嘗試最小化這個差異，鼓勵模型在受到擾動時仍能保持相似的預測信心。</p></div></div>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="自由的對抗訓練策略">自由的對抗訓練策略<a href="#自由的對抗訓練策略" class="hash-link" aria-label="自由的對抗訓練策略的直接連結" title="自由的對抗訓練策略的直接連結">​</a></h3>
<p>對抗訓練 (Adversarial Training, 簡稱 AT) 是一種強化模型泛化性的方法，通常使用擾動數據進行訓練以提高模型對對抗攻擊的抵抗能力。在這裡，作者提到了一種稱為 “Free” 的對抗訓練策略。（原文是 free，我也不確定要翻譯成免費或是自由，意義上感覺都可以呢？）</p>
<ul>
<li>
<p><strong>K-step PGD 的計算成本</strong></p>
<p>PGD (Projected Gradient Descent) 是進行對抗訓練時常用的方法。當使用 K-step PGD 時，需要通過模型進行 K 次的前向和反向傳播，這是非常耗時的。此外，只有在 K 步之後的最後一步中產生的擾動被用於模型訓練，這意味著先前的所有步驟都只是為了產生這最後一次的擾動。</p>
</li>
<li>
<p><strong>解決方法：FreeLB</strong></p>
<p>為了克服上述的計算問題並有效地執行大規模的訓練，作者採用了一種名為 FreeLB 的方法。FreeLB 的方法不只進行多次的 PGD 迭代以製作對抗性編碼，而且在每次迭代中都累積了「自由」參數的梯度 ∇θL。這意味著，不是在每次迭代後都更新模型參數，而是在多次迭代後一次性使用累積的梯度進行更新。</p>
<p>這種策略的優點是它可以模擬一個更大的「虛擬」小批量 (mini-batch)。換句話說，它有效地模擬了一個 K 倍大小的小批量，使得每次更新更為豐富和多樣。</p>
</li>
</ul>
<p>不免俗的，再來舉個例子：</p>
<p>想像你正在組裝一台自行車，每一步都有特定的組件需要安裝。</p>
<ul>
<li>
<p><strong>傳統的 K-step PGD 方法</strong></p>
<p>這就好像你組裝自行車的每一步都去檢查它是否能夠正常運作。例如：在安裝踏板之後，你騎了一小段，然後又裝上車鍊再騎一段，再次檢查，這樣每加一個部分都去測試。雖然這可以確保每一部分都被正確安裝，但它非常耗時。</p>
<p>然而，你發現最後只有你組裝完所有部分後的那次騎行測試是最重要的，因為它告訴你整台自行車是否完全組裝正確。</p>
</li>
<li>
<p><strong>FreeLB 策略</strong></p>
<p>現在，想象另一種組裝策略。</p>
<p>你仍然分步組裝自行車，但不是在每一步都去騎它測試。相反，你每組裝一部分都記下可能會遇到的問題或考慮的事項（這相當於累積梯度）。當所有部分都裝好之後，你再一次性地根據累積的所有問題和考慮事項去做一個全面的測試和調整。</p>
<p>這種方法允許你更高效地組裝，因為你不再浪費時間在每一步都測試，而是集中精力於最後的全面調整。</p>
</li>
</ul>
<p>當然，所有比喻都有其局限性，不可能完美地映射到目標概念，就加減看看，感覺一下就好。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="討論的直接連結" title="討論的直接連結">​</a></h2>
<p>這篇的重點其實都圍繞到對抗性訓練的設計上了。</p>
<p>不過我們還是要看一下，這樣的設計好嗎？又好多少？</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="實驗配置">實驗配置<a href="#實驗配置" class="hash-link" aria-label="實驗配置的直接連結" title="實驗配置的直接連結">​</a></h3>
<p>為了驗證 VILLA 的功能與效果，作者進行了一系列的實驗應用。這些應用主要集中在 V+L 的預訓練模型上，並進行了多種下游任務的綜合評估。這些任務包括視覺問答 (VQA)、視覺常識推理 (VCR)、引用表達 (RE) 壓縮、視覺蘊涵、圖像文字檢索以及 NLVR2。</p>
<p>主要的驗證過程分為兩個階段：首先，作者將 VILLA 納入目前領先的 UNITER 模型，進行下游任務評估和消融分析。其次，為了展示 VILLA 的廣泛適用性，作者選擇了另一 V+L 模型，名為 LXMERT，進行更全面的  測試。</p>
<p>如果這兩個模型還沒有看過，可以參考以下:</p>
<ul>
<li>
<p><strong>傳送門： <a href="/papers/multimodality/lxmert/">LXMERT</a>、<a href="/papers/multimodality/uniter/">UNITER</a></strong></p>
</li>
<li>
<p><strong>主要模型配置：UNITER 和 LXMERT</strong></p>
<ul>
<li>
<p><strong>UNITER</strong></p>
<ul>
<li>UNITER-base：這是一種單流模型，包含 12 個層，每層有 768 個隱藏單元和 12 個注意力頭。</li>
<li>UNITER-large：更大的版本，包含 24 層，每層有 1024 個隱藏單元和 16 個注意力頭。</li>
<li>與 BERT 結構相同，但其輸入結合了兩種模態（視覺和語言）的混合序列。</li>
</ul>
</li>
<li>
<p><strong>LXMERT</strong></p>
<ul>
<li>LXMERT 是一種雙流模型，先透過獨立的自我注意力在每個模態上進行多層處理（文字有 9 層，圖像有 5 層），然後融合兩個流的輸出，進一步進行 5 層的處理，其中包括交叉注意力和自注意力。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>設置細節</strong></p>
<ul>
<li>對於 UNITER 的實驗，作者選用了四個主要的大型資料集進行預訓練，分別是 COCO、Visual Genome (VG)、Conceptual Captions 以及 SBU Captions。</li>
<li>VILLA 也適用於 MLM 和 ITM 的預訓練任務，作者為 UNITER-base 和 UNITER-large 提供了不同的訓練步驟，確保公平比較且考慮到訓練時間。</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="villa-在-uniter-的效果及分析">VILLA 在 UNITER 的效果及分析<a href="#villa-在-uniter-的效果及分析" class="hash-link" aria-label="VILLA 在 UNITER 的效果及分析的直接連結" title="VILLA 在 UNITER 的效果及分析的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="VILLA 在 UNITER 的效果及分析" src="/assets/images/villa_3-06ffbecd19ded8cece9bddc9e363514c.jpg" width="982" height="1024" class="img_ev3q"></p>
<ol>
<li>
<p><strong>VILLA 對比其他預訓練 V+L 模型</strong></p>
<ul>
<li>VILLA 在所有測試基準上都達到了最新技術水準。</li>
<li>VILLA-base 模型效果提升：</li>
</ul>
</li>
<li>
<p><strong>在 VQA：比 UNITER-base 提高 +0.76</strong></p>
<ul>
<li>在 VCR 的 Q→AR：比 UNITER-base 提高 +2.4</li>
<li>在 NLVR2：比 UNITER-base 提高 +1.45</li>
<li>在 SNLI-VE：優於 UNITER-base</li>
<li>在 Flickr30k 圖像/文字擷取：比 UNITER-base 提高 +2.22/+0.70（R@1）</li>
<li>在三個 RE 資料集的平均值提升了 +0.99。</li>
</ul>
</li>
<li>
<p><strong>VILLA-large 模型效果提升</strong></p>
<ul>
<li>在整體性能上呈現相似的提升趨勢。</li>
<li>在 VCR 的 Q→AR 指標：絕對提高了 +2.9 點，這項任務特別注重對圖像中隱式編碼的複雜社會動態的理解。</li>
<li>在 VQA 基準：從 74.02 提高到 74.87。</li>
<li>通過集成策略，VILLA-large 的性能再次提升，達到 75.85。</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="深入分析-villa">深入分析 VILLA<a href="#深入分析-villa" class="hash-link" aria-label="深入分析 VILLA的直接連結" title="深入分析 VILLA的直接連結">​</a></h3>
<ol>
<li>
<p><strong>預訓練 vs. 微調</strong></p>
<p><img decoding="async" loading="lazy" alt="預訓練 vs. 微調" src="/assets/images/villa_5-1b2428a2724103cc86a2b0f7941d64c3.jpg" width="1024" height="207" class="img_ev3q"></p>
<p>以 UNITER-base 模型來研究對抗性訓練在預訓練和微調階段的效果。</p>
<ul>
<li>UNITER (reimp.)：透過標準訓練重新實現的 UNITER-base 模型。</li>
<li>VILLA-pre vs. VILLA-fine：分別只在預訓練或微調階段應用對抗性訓練的模型。</li>
<li>效果：VILLA-pre 和 VILLA-fine 在六個評估任務中平均帶來 +0.51 和 +0.82 的效能增益。兩者合併可得到 +1.15 的增益。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="訓練曲線" src="/assets/images/villa_4-66aaec9c13b31703ff12e2fd707176cc.jpg" width="1024" height="271" class="img_ev3q"></p>
<p>從上圖來看，訓練曲線顯示隨著訓練步驟的增加，加強對抗性的模型與原始 UNITER 之間的差距不斷擴大。</p>
</li>
<li>
<p><strong>影像 vs. 文字模式（表 3a）</strong></p>
<p><img decoding="async" loading="lazy" alt="影像 vs. 文字模式" src="/assets/images/villa_6-ef23c4f6a6c1600ca298110090a6dc44.jpg" width="1024" height="301" class="img_ev3q"></p>
<p>當談到對抗性範例時，指經過專門設計的輸入，這些輸入對人類來說看起來與原始輸入沒有太大區別，但它們可以導致模型產生錯誤的預測。在這裡，透過添加對抗性擾動，專注於影像和文字這兩種模態。</p>
<ul>
<li>實驗與結果<!-- -->
<ul>
<li>獨立的影像擾動：單獨對圖像特徵加擾動可以顯著提高模型的效能。這與傳統的認知可能有些相反，因為在影像領域，對抗性訓練常常會對乾淨影像的模型準確性產生負面影響。</li>
<li>直覺與實際結果：起初，可能會認為在影像和文字的兩種模式中同時添加擾動會帶來更多的好處，因為它可以增加對抗性範例的多樣性。但事實上，僅在一種模態上加入擾動已經帶來了顯著的效能提升。</li>
</ul>
</li>
</ul>
<p>VCR（視覺常識推理）任務相對更具挑戰性，需要模型理解圖像中的複雜社會動態並進行常識推理。由於其複雜性，該任務中的效能提升似乎更為顯著。這可能意味著，對於更具挑戰性的任務，對抗性訓練可以帶來更大的好處。</p>
</li>
<li>
<p><strong>FreeLB vs. VILLA（表 3b）</strong></p>
<p><img decoding="async" loading="lazy" alt="影像 vs. 文字模式" src="/assets/images/villa_6-ef23c4f6a6c1600ca298110090a6dc44.jpg" width="1024" height="301" class="img_ev3q"></p>
<p>作者比較 FreeLB 和 VILLA 在 VQA 和 VCR 這兩個具有代表性且具有挑戰性的 V+L 任務上的效果。由於額外的細粒度對抗正規化項，VILLA 在這兩個基準測試上都優於 FreeLB。</p>
<hr>
<p>不過等等，之前不就有提到 VILLA 所採用的策略是 FreeLB，為何又要拿出來比？</p>
<p>儘管 VILLA 採用了 FreeLB 的策略來執行對抗性訓練。但這不意味著 VILLA 只是 FreeLB。VILLA 是一個更為全面的框架，其中加入了一些額外的策略或正則化手段，例如：細粒度的對抗正則化項，以優化和增強模型的效能。</p>
<p>在「FreeLB vs. VILLA」這個部分，作者主要是想展示即使 VILLA 已經採用了 FreeLB 的策略，由於它還加入了其他的增強策略，所以它的效能應該比僅僅使用 FreeLB 的策略來得更好。換句話說，這部分旨在突顯 VILLA 在 FreeLB 基礎上所做的改進和它帶來的效能提升。</p>
<p>比較的目的是為了證明：儘管 VILLA 使用了 FreeLB 作為其對抗性訓練的核心，但由於其他的優化和增強，VILLA 在特定的任務上比僅使用 FreeLB 的方法更具有優勢。</p>
</li>
<li>
<p><strong>在 LXMERT 上的結果</strong></p>
<p><img decoding="async" loading="lazy" alt="在 LXMERT 上的結果" src="/assets/images/villa_7-4ebdcfd8e2c0b314502858042fc5fb95.jpg" width="1024" height="247" class="img_ev3q"></p>
<p>作者為了證明不止應用到 UNITER 上有效果，把它應用在 LXMERT 也是很不錯，從上表中看到 LXMERT 經過 VILLA 的加持後，效果直接超越 UNITER-base！</p>
<p>作者證明 VILLA 的通用性，在三個評估任務中提供了 +0.88 的平均效能提升。</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="結論的直接連結" title="結論的直接連結">​</a></h2>
<p>在探索深度學習模型的穩健性方面，對抗性攻擊是一個熱門且重要的領域。本研究特別探討了視覺和語言模型（簡稱 V+L 模型）的對抗性穩健性。儘管這是一個新興領域，目前文獻中關於如何針對這些模型進行對抗性攻擊的策略還相對稀缺。其中，如何從多模態 Transformer 模型中反向傳播梯度到 CNN 主幹以產生影像對抗，以及如何合成與視覺情境相符的文字對抗，都是目前面臨的挑戰。</p>
<p>VILLA 作為一個專為提升視覺與語言的表示學習而設計新型的對抗訓練框架。其中的獨特之處在於它不僅在預訓練階段，而且在微調階段都採用了對抗訓練。此外，透過在編碼空間中添加對抗性擾動，VILLA 還實現了在各種評估基準上的效能一致性提升。</p>
<p>這邊可以看出來，作者把 VILLA 看成是一種 plug-in 的概念，這表示當你做好了一份模型之後，大可不必更換你的模型結構，只需要在最後的時刻加入 VILLA 所指導的對抗性訓練的技法，就能提升模型 1~3 個百分點。</p>
<p>最後，作者提到了儘管 VILLA 的效果顯著，對抗訓練的耗時性仍是一大挑戰。在未來，作者也期許將尋找更有效率的對抗訓練方法，希望能夠使大規模預訓練在日常實踐中更具可行性。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">2024年9月11日</time></b> <!-- -->更新</span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/multimodality/ernie-vil/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">[20.06] ERNIE-ViL</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/multimodality/unimo/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">[20.12] UNIMO</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#別墅裡的魅影" class="table-of-contents__link toc-highlight">別墅裡的魅影</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#villa-模型設計" class="table-of-contents__link toc-highlight">VILLA 模型設計</a></li><li><a href="#預訓練任務" class="table-of-contents__link toc-highlight">預訓練任務</a></li><li><a href="#兩階段的對抗性訓練" class="table-of-contents__link toc-highlight">兩階段的對抗性訓練</a></li><li><a href="#微擾動perturbations" class="table-of-contents__link toc-highlight">微擾動（Perturbations）</a></li><li><a href="#多模態自由對抗訓練" class="table-of-contents__link toc-highlight">多模態「自由」對抗訓練</a></li><li><a href="#自由的對抗訓練策略" class="table-of-contents__link toc-highlight">自由的對抗訓練策略</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#實驗配置" class="table-of-contents__link toc-highlight">實驗配置</a></li><li><a href="#villa-在-uniter-的效果及分析" class="table-of-contents__link toc-highlight">VILLA 在 UNITER 的效果及分析</a></li><li><a href="#深入分析-villa" class="table-of-contents__link toc-highlight">深入分析 VILLA</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">使用條款<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">隱私政策<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>