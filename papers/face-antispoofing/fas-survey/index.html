<!doctype html><html lang=zh-hant dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-face-antispoofing/fas-survey/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>[22.10] FAS Survey | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/papers/face-antispoofing/fas-survey/><meta data-rh=true property=og:locale content=zh_hant><meta data-rh=true property=og:locale:alternate content=en><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=zh-hant><meta data-rh=true name=docsearch:language content=zh-hant><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[22.10] FAS Survey | DOCSAID"><meta data-rh=true name=description content=攻與防的編年史><meta data-rh=true property=og:description content=攻與防的編年史><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/papers/face-antispoofing/fas-survey/><link data-rh=true rel=alternate href=https://docsaid.org/papers/face-antispoofing/fas-survey/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/face-antispoofing/fas-survey/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/face-antispoofing/fas-survey/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/face-antispoofing/fas-survey/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/assets/css/styles.a8bd05f4.css><script src=/assets/js/runtime~main.2b99d299.js defer></script><script src=/assets/js/main.1c1f807b.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/img/docsaid_logo.png><link rel=preload as=image href=/img/docsaid_logo_white.png><link rel=preload as=image href=https://github.com/zephyr-sh.png><link rel=preload as=image href=/img/bmc-logo.svg><link rel=preload as=image href=/img/icons/all_in.svg><div role=region aria-label=跳至主要内容><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>跳至主要内容</a></div><nav aria-label=主導航 class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label=切換導覽列 aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><div class=navbar__logo><img src=/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/docs/>開源專案</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/papers/intro>論文筆記</a><a class="navbar__item navbar__link" href=/blog>部落格</a><a class="navbar__item navbar__link" href=/playground/intro>遊樂場</a><a class="navbar__item navbar__link" href=/services>技術服務</a><a class="navbar__item navbar__link" href=/aboutus>關於我們</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>繁體中文</a><ul class=dropdown__menu><li><a href=/papers/face-antispoofing/fas-survey/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=zh-hant>繁體中文</a><li><a href=/en/papers/face-antispoofing/fas-survey/ target=_self rel="noopener noreferrer" class=dropdown__link lang=en>English</a><li><a href=/ja/papers/face-antispoofing/fas-survey/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="搜尋 (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>搜尋</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-7ny38l ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label=回到頂部 class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/><img src=/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label=文件側邊欄 class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/papers/intro>論文筆記</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="展開側邊欄分類 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="展開側邊欄分類 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="展開側邊欄分類 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/papers/category/face-anti-spoofing-16>Face Anti-Spoofing (16)</a><button aria-label="收起側邊欄分類 'Face Anti-Spoofing (16)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/slrbd/>[10.09] SLRBD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/lbp/>[12.09] LBP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/three-d-mad/>[14.05] 3DMAD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/oulu-npu/>[17.06] OULU-NPU</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/wmca/>[19.09] WMCA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/cefa/>[20.03] CeFA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/ssdg/>[20.04] SSDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/celeba-spoof/>[20.07] CelebA-Spoof</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/d2am/>[21.05] D²AM</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/personalized-fas/>[22.01] Personalized-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/papers/face-antispoofing/fas-survey/>[22.10] FAS Survey</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/m2a2e/>[23.02] M²A²E</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/sa-fas/>[23.03] SA-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/mmdg/>[24.02] MMDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/shield/>[24.02] SHIELD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/face-antispoofing/cfpl-fas/>[24.03] CFPL-FAS</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="展開側邊欄分類 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="展開側邊欄分類 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="展開側邊欄分類 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/mamba-4>Mamba (4)</a><button aria-label="展開側邊欄分類 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="展開側邊欄分類 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="展開側邊欄分類 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/normalization-1>Normalization (1)</a><button aria-label="展開側邊欄分類 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="展開側邊欄分類 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="展開側邊欄分類 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="展開側邊欄分類 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="展開側邊欄分類 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="展開側邊欄分類 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="展開側邊欄分類 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/transformers-17>Transformers (17)</a><button aria-label="展開側邊欄分類 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="展開側邊欄分類 'Vision Transformers (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/papers/intro>All Notes: 190 entries</a></ul></nav><button type=button title=收起側邊欄 aria-label=收起側邊欄 class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=頁面路徑><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label=主頁面 class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/papers/category/face-anti-spoofing-16><span itemprop=name>Face Anti-Spoofing (16)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[22.10] FAS Survey</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[22.10] FAS Survey</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=攻與防的編年史>攻與防的編年史<a href=#攻與防的編年史 class=hash-link aria-label=攻與防的編年史的直接連結 title=攻與防的編年史的直接連結>​</a></h2>
<p><a href=https://ieeexplore.ieee.org/abstract/document/9925105 target=_blank rel="noopener noreferrer"><strong>Deep Learning for Face Anti-Spoofing: A Survey</strong></a></p>
<hr>
<p>看了過去幾篇論文之後，我們也大概知道了 FAS 的研究背景與發展脈絡。</p>
<p>現在，我們直接來回顧一下過去十幾年內的各種方法。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>這篇綜述性的論文有大約兩百篇的參考文獻，有興趣的讀者可以去翻出來讀一讀。</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=fas-架構>FAS 架構<a href=#fas-架構 class=hash-link aria-label="FAS 架構的直接連結" title="FAS 架構的直接連結">​</a></h2>
<p>所有攻擊的第一步，都是欺騙感測器。</p>
<p>因此，我們大致可將攻擊分為兩類：</p>
<ul>
<li><strong>數位攻擊（Digital Manipulation）</strong>：如深度偽造（deepfake）或圖像處理，直接在虛擬領域進行改造。</li>
<li><strong>實體展示攻擊（Physical Presentation Attacks）</strong>：也是本篇重點，意圖在真實世界中透過物理媒介騙過攝影鏡頭。</li>
</ul>
<p>常見的攻擊型態和 FAS 系統的建構方式如下圖所示：</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=model_arch src=/assets/images/img1-c392f98720b419bc92f8bb5bab9a71cf.jpg width=1224 height=836 class=img_ev3q></figure></div>
<p>從實體展示攻擊中，進一步根據意圖區分為：</p>
<ul>
<li><strong>模仿（Impersonation）</strong>：模仿他人，例如持照片或配戴 3D 面具，讓系統認為你是某個特定人。</li>
<li><strong>遮蔽（Obfuscation）</strong>：遮掩自己，例如化妝、墨鏡、假髮，讓系統無法辨識你是誰。</li>
</ul>
<p>根據幾何結構，又可區分為：</p>
<ul>
<li><strong>2D 攻擊</strong>：例如平面照片、螢幕重播影片、挖洞照片等。</li>
<li><strong>3D 攻擊</strong>：包含各種材質製成的立體面具，如紙質、石膏、矽膠或樹脂。這類攻擊往往更擬真，也更具挑戰性。</li>
</ul>
<p>有趣的是，除了整臉的「正攻」，也出現了不少「側擊」的<strong>局部攻擊</strong>，例如眼部戴上搞笑眼鏡、臉頰貼紙等，只針對局部區域進行欺騙，增添了防禦難度。</p>
<p>隨著深度學習方法的發展，數據集的規模與複雜度也逐步提升。</p>
<p>在本論文中歸納了三大趨勢：</p>
<ol>
<li><strong>大規模化</strong>：例如 CelebA-Spoof、HiFiMask 等，影像與影片數量均達數十萬等級，具備訓練資料飽和度。</li>
<li><strong>多樣性強化</strong>：新的數據集不再只包含常見的列印與重播攻擊，開始引入細分類型的 3D 攻擊、變化光照、跨場景錄製等情境，例如 SiW-M 中涵蓋多達 13 種攻擊類型。</li>
<li><strong>感測器升級</strong>：從單一 RGB 攝影機，拓展至深度（Depth）、近紅外（NIR）、熱感（Thermal）、短波紅外（SWIR）等模態，甚至使用光場相機進行紀錄。</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=深度學習方法總覽>深度學習方法總覽<a href=#深度學習方法總覽 class=hash-link aria-label=深度學習方法總覽的直接連結 title=深度學習方法總覽的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=method_tree src=/assets/images/img2-87fe8dfeb05f28fa7046756b309f1a1c.jpg width=1224 height=504 class=img_ev3q></figure></div>
<p>上圖中展示的是一張 <strong>FAS 方法分類樹狀結構圖</strong>，從感測器出發，逐層延伸至不同模型設計與泛化策略。</p>
<p>從感測器層級的劃分開始：</p>
<ul>
<li><strong>Commercial RGB Camera</strong>：目前最常見的設備類型，多數手機或筆電皆屬此類。針對此類裝置開發的 FAS 方法，強調在可見光單一模態下進行偽造偵測。</li>
<li><strong>Advanced Sensor</strong>：例如 NIR、Depth、Thermal、SWIR、光場相機等，提供更豐富的影像特徵。應用於這類裝置的模型，能結合跨模態資訊提升辨識力，但也提高了開發與佈署成本。</li>
</ul>
<hr>
<p>由於多數情境下，都只有 RGB 攝影機可用，因此我們將重點放在這個模態下的 FAS 方法。</p>
<ul>
<li>
<p><strong>Common Deep Learning Method</strong></p>
<p>使用最傳統的二元分類監督學習（如 Binary Cross-Entropy），或進一步進行像素級 supervision（Pixel-wise Supervision），強調模型對局部 spoof cue 的學習能力。</p>
<p>常見輔助設計如：</p>
<ul>
<li><strong>Auxiliary Supervision</strong>：引入中間特徵監督或深層特徵輔助判斷。</li>
<li><strong>Generative Model</strong>：採用 GAN 或重建方式提升對異常樣本的辨識力。</li>
</ul>
</li>
<li>
<p><strong>Hybrid Method</strong></p>
<p>將不同 supervision 結構進行融合（例如 BCE + reconstruction loss），同時兼顧分類與區域重建能力，提升模型對偽造線索的捕捉。</p>
</li>
<li>
<p><strong>Generalized Deep Learning Method</strong></p>
<p>此路線的核心目標為「<strong>跨域泛化（Domain Generalization）</strong>」，不僅在訓練場景中表現良好，更能應對未知資料分布。
主要技術分支如下：</p>
<ul>
<li><strong>Domain Adaptation</strong>：有少量目標資料時，對模型進行微調。</li>
<li><strong>Domain Generalization</strong>：完全無目標資料，仍需保持泛化能力。</li>
<li><strong>Federated Learning</strong>：將多端設備學習整合，提升隱私下的泛化效能。</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>這個年代還不流行的 Vision Language Model（VLM）方法，相關方法我們之後再談。</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=評估指標>評估指標<a href=#評估指標 class=hash-link aria-label=評估指標的直接連結 title=評估指標的直接連結>​</a></h3>
<p>FAS 任務的評估，聚焦於兩個核心面向：</p>
<ul>
<li><strong>FAR（False Acceptance Rate）</strong>：誤將攻擊樣本判為真實使用者的比率。</li>
<li><strong>FRR（False Rejection Rate）</strong>：誤將真實使用者判為攻擊樣本的比率。</li>
</ul>
<p>為兼顧這兩者的平衡表現，常見綜合指標包含：</p>
<ul>
<li>
<p><strong>HTER（Half Total Error Rate）</strong>：FAR 與 FRR 的平均值</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mtext>HTER</mtext><mo>=</mo><mfrac><mrow><mtext>FAR</mtext><mo>+</mo><mtext>FRR</mtext></mrow><mn>2</mn></mfrac></mrow><annotation encoding=application/x-tex>\text{HTER} = \frac{\text{FAR} + \text{FRR}}{2}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord text"><span class=mord>HTER</span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.0463em;vertical-align:-0.686em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3603em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>2</span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord text"><span class=mord>FAR</span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span><span class="mord text"><span class=mord>FRR</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.686em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
</li>
<li>
<p><strong>EER（Equal Error Rate）</strong>：當 FAR 與 FRR 相等時的錯誤率（即 HTER 在平衡點的值），常作為模型性能的綜合評估指標。</p>
</li>
<li>
<p><strong>AUC（Area Under Curve）</strong>：計算 ROC 曲線下的面積，反映模型將 bonafide 與 spoof 攻擊區分開的能力，越接近 1 越好。</p>
</li>
</ul>
<p>此外，依據 <strong>ISO/IEC 30107-3</strong> 標準，也逐漸引入更細化的三項錯誤指標：</p>
<ul>
<li>
<p><strong>APCER（Attack Presentation Classification Error Rate）</strong>：攻擊樣本被誤判為 bonafide 的比率。</p>
</li>
<li>
<p><strong>BPCER（Bonafide Presentation Classification Error Rate）</strong>：真實樣本被誤判為攻擊的比率。</p>
</li>
<li>
<p><strong>ACER（Average Classification Error Rate）</strong>：APCER 與 BPCER 的平均值</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mtext>ACER</mtext><mo>=</mo><mfrac><mrow><mtext>APCER</mtext><mo>+</mo><mtext>BPCER</mtext></mrow><mn>2</mn></mfrac></mrow><annotation encoding=application/x-tex>\text{ACER} = \frac{\text{APCER} + \text{BPCER}}{2}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord text"><span class=mord>ACER</span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.0463em;vertical-align:-0.686em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3603em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>2</span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord text"><span class=mord>APCER</span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span><span class="mord text"><span class=mord>BPCER</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.686em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
</li>
</ul>
<p>這些指標提供了模型在不同誤分類情境下的具體量化依據，特別適用於真實部署環境中，針對誤判風險進行細緻評估。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>HTER 與 ACER 是一樣的東西，只是命名的機構不同而已。</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=評估協議>評估協議<a href=#評估協議 class=hash-link aria-label=評估協議的直接連結 title=評估協議的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=evaluation_protocol src=/assets/images/img3-32c7164bf0e1c9a06cae881610959663.jpg width=1224 height=880 class=img_ev3q></figure><figcaption>四種主流測試協議下，深度學習 FAS 方法的表現比較。</figcaption></div>
<hr>
<p>不同的協議設定，反映著模型在面對<strong>實際部署場景時的挑戰程度</strong>。我們可將這些設定視為模型從「熟悉」邁向「未知」的成長階梯，依序可分為四種常見測試協議：</p>
<ol>
<li>
<p><strong>Intra-Dataset Intra-Type</strong></p>
<p>同一數據集、同類型攻擊。此為最基礎的評估方式，模型於訓練與測試階段皆使用來自相同分布的資料。</p>
<p>常見於如 OULU-NPU、SiW 等數據集預設子協議中。該情境下 domain gap 極小，因此為深度模型最容易獲得高準確率的設定。</p>
<p>如上圖 (a) 所示，多數方法在 OULU-NPU 的 Protocol-4 測試中表現出色，ACER 通常可低於 5%。</p>
<hr>
</li>
<li>
<p><strong>Cross-Dataset Intra-Type</strong></p>
<p>訓練與測試資料來自<strong>不同數據集</strong>，但攻擊類型相同，主要測試模型的領域泛化能力。</p>
<p>模擬實務上「開發環境」與「真實應用場景」不一致的挑戰。上圖 (b) 呈現了在 Replay-Attack 訓練、CASIA-MFSD 測試的結果，僅訓練單一資料集時（綠色柱）HTER 普遍偏高。若結合多個資料集（紫色柱）進行訓練，透過跨域學習方法（如 SSDG, SSAN）可明顯改善泛化表現。</p>
<hr>
</li>
<li>
<p><strong>Intra-Dataset Cross-Type</strong></p>
<p>採用 leave-one-type-out 設定，即某一攻擊類型<strong>只在測試階段出現</strong>，訓練資料中不包含。</p>
<p>此協議專為驗證模型面對「未知攻擊類型」時的應變能力。如上圖 (c) 展示 SiW-M 的測試結果，其中包含多達 13 類攻擊類型，各類型間的難度落差大，導致模型平均 EER 雖不高（~10%），但標準差較大。</p>
<p>透過預訓練與轉移學習，如 ViTranZFAS 即可將 EER 降至 6.7%。</p>
<hr>
</li>
<li>
<p><strong>Cross-Dataset Cross-Type</strong></p>
<p>為目前最具挑戰的設定，<strong>同時更換資料來源與攻擊類型</strong>。</p>
<p>訓練時僅使用 OULU-NPU 與 SiW（主要為 2D 攻擊），測試時則使用 HKBU-MARs 或 CASIA-SURF 3DMask 等 3D 面具資料集。上圖 (d) 顯示，目前方法如 NAS-FAS、DTN 僅能偵測某些低擬真 3D 面具，對高擬真材質仍難以區分。</p>
<p>此協議最貼近現實部署中會遇到的未知場景與複合攻擊挑戰，是未來研究的重要方向。</p>
</li>
</ol>
<hr>
<p>每一種協議的設計，都是對模型能力的逐步拉升：
從最理想的「封閉訓練測試」到真實世界的「開放未知情境」，FAS 系統該如何成長、學會辨別從未見過的攻擊，是這場防偽對抗的核心課題。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=基於-rgb-的-fas>基於 RGB 的 FAS<a href=#基於-rgb-的-fas class=hash-link aria-label="基於 RGB 的 FAS的直接連結" title="基於 RGB 的 FAS的直接連結">​</a></h2>
<p><img decoding=async loading=lazy alt=models src=/assets/images/img4-0f7a381abf330dd154090802dda936e1.jpg width=1224 height=296 class=img_ev3q></p>
<p>上圖整理了基於 RGB 相機的 FAS 技術演進脈絡。</p>
<p>從早期倚賴手工特徵的時代開始，一路走向強調多模態融合與泛化能力的深度模型設計。</p>
<p>由於 RGB 相機幾乎是所有裝置的標配，因此針對這類裝置所開發的防偽系統，也就成了 FAS 技術落地的主戰場。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=混合式方法>混合式方法<a href=#混合式方法 class=hash-link aria-label=混合式方法的直接連結 title=混合式方法的直接連結>​</a></h2>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=hybrid_method src=/assets/images/img5-6f5e4782e288e6dc94b71209347f45b3.jpg width=1076 height=1056 class=img_ev3q></figure></div>
<p>雖然深度學習已經在分類與偵測任務中橫掃千軍，但 FAS 任務的實際情況比較現實：</p>
<blockquote>
<p><strong>資料量普遍不大、樣本分布還經常失衡，導致模型訓練容易陷入過擬合。</strong></p>
</blockquote>
<p>這時候，一些研究便轉向結合傳統手工特徵與深度模型，希望透過這種「補短板」的方式，彌補模型對非紋理線索的感知盲點，像是光影變化、心跳節奏、動作異常等等。</p>
<p>這類方法大致可以分成三種套路。</p>
<p>第一種是「<strong>特徵前融合</strong>」，也就是先用傳統方法抽取靜態或動態特徵，再送進深度模型進行分類。常見的作法包括用 multi-scale LBP 或 Weber descriptors 抓出細節紋理，或是結合 LBP 與 CNN 來保留邊緣與強度等低階資訊。</p>
<p>如果目標是抓動態線索，那就會看到 dense optical flow 用來分析動作軌跡、rPPG 被轉成時序圖餵給 Vision Transformer，甚至有研究直接抓亮度變化的直方圖，對重播攻擊進行反制。</p>
<blockquote>
<p><strong>這類方法的核心邏輯是：模型你負責學，我先幫你把該看的線索準備好。</strong></p>
</blockquote>
<hr>
<p>第二種是「<strong>特徵後融合</strong>」，這邊的流程剛好相反：先讓深度模型抽特徵，接著再加上一些手工設計的描述子作為補強。像是有人用 PCA 先從 VGG-face 清理掉多餘資訊，也有人直接從卷積層擷取彩色 LBP，試圖補強統計訊號，還有在時序特徵上疊加 LBP-TOP 或 optical flow，增加時間動態的解析度。</p>
<blockquote>
<p><strong>這類方法最大的麻煩在於，CNN 特徵本身會隨著層數產生語義變化，怎麼選擇合適的融合層級，常常需要靠經驗＋實驗，沒有標準答案。</strong></p>
</blockquote>
<hr>
<p>第三種則是「<strong>雙流融合</strong>」，也就是讓手工特徵與深度模型各自跑各自的路，最後在特徵層或分數層整合起來。</p>
<p>這裡的範例就更多了：有直接把 LBP 與 VGG16 的預測分數加在一起的，也有用 HOG、LBP 這類特徵圖來引導 CNN 的底層學習方向，甚至還有人把 1D CNN 擷取的亮度與模糊線索融合起來，專門對付重播類型的攻擊。</p>
<blockquote>
<p><strong>這類方法就是把不同的模型放在一起，讓它們各自發揮專長，最後再把結果綜合起來。</strong></p>
</blockquote>
<hr>
<p>整體來說，Hybrid 方法最大的好處是：可以補上純深度模型不擅長的部分，尤其是那些微弱但有意義的非紋理特徵，像是臉部微光反射、心跳週期、鏡頭動態模糊等等。在資料量不夠、場景差異又大的情況下，這種方式確實能提供額外的穩定性與彈性。</p>
<p>不過它也有明顯限制，像是手工特徵不可學習、需要專家調參，泛化能力有限；再加上手工與深度特徵之間的語義不一致，融合時一不小心就會出現資訊打架、模型迷路的情況。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=傳統深度學習方法>傳統深度學習方法<a href=#傳統深度學習方法 class=hash-link aria-label=傳統深度學習方法的直接連結 title=傳統深度學習方法的直接連結>​</a></h2>
<p>Hybrid 方法之所以存在，是因為當時深度模型還在發育期，資料也不夠多，只好用手工特徵先撐場面。</p>
<p>但隨著 CNN 架構愈來愈成熟、大型防偽資料集陸續釋出，FAS 社群也開始擁抱「資料給多一點、網路堆厚一點、監督用狠一點」的全端學習路線。</p>
<p>這波方法強調從影像中<strong>直接學出活體與偽造之間的差異特徵</strong>，捨棄所有不可學的外掛，從 input 到 output 一條龍學習，成為目前商用 RGB FAS 的主流路線。</p>
<p>雖然形式五花八門，但這類方法大致可分成兩類：</p>
<ul>
<li>是把 FAS 當成二分類問題處理。</li>
<li>透過像素級監督加上生成式設計，讓模型學會更細膩的 spoof 模式。</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=二分類監督>二分類監督<a href=#二分類監督 class=hash-link aria-label=二分類監督的直接連結 title=二分類監督的直接連結>​</a></h3>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=binary_classification src=/assets/images/img6a-25b3280c857e428134cd48b89983ded8.jpg width=922 height=276 class=img_ev3q></figure></div>
<p>最直觀的做法，就是把 FAS 當成 Bonafide vs Spoof 的二分類問題，採用 BCE Loss 作為損失函數，從輸入影像直接預測標籤。</p>
<p>早期常見的網路如 8-layer CNN、VGG16、ResNet18，後來也出現了 MobileNet 為基礎的輕量化架構，甚至有研究結合 CNN 與 LSTM，讓模型可以看「多幀」的細微動作，比如眨眼、頭微動，試圖把時間訊號也納進判斷依據。</p>
<p>為了讓模型更不容易被誤導，損失函數也持續進化：</p>
<ul>
<li><strong>改成多分類</strong>：有人加入攻擊類型標籤（multi-class CE），讓模型學會區分重播、列印、3D 面具等不同 spoof 手法，把二分類問題轉成多分類問題。</li>
<li><strong>對比學習</strong>：也有人使用 Triplet 或 Contrastive Loss，讓類內特徵緊密、類間分離，強化表示能力</li>
</ul>
<p>甚至連 Focal Loss 和非對稱 Softmax 這些設計也被搬進來，解決樣本不平衡與 spoof 分布偏態的問題。</p>
<p>這種設計的好處在於實作容易、收斂快，但也不是沒有地雷。最大的風險是：</p>
<blockquote>
<p><strong>模型太容易學到一些不該學的東西</strong>。</p>
</blockquote>
<p>像是螢幕邊框、畫面黑邊或光線異常等「提示」，雖然短期內看起來正確率很高，實際上卻是把資料集記熟了，泛化效果堪憂，稍有不慎就變成一堆電子垃圾。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=像素級監督>像素級監督<a href=#像素級監督 class=hash-link aria-label=像素級監督的直接連結 title=像素級監督的直接連結>​</a></h3>
<p>為了解決模型太愛偷看提示這件事，部分研究開始導入像素級的監督訊號，讓模型更專注於臉部本身的物理與材質特徵，而不是背景或設備產生的雜訊。</p>
<hr>
<ul>
<li>
<p><strong>(a) 輔助任務監督（Auxiliary Task Supervision）</strong></p>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=fig8b src=/assets/images/img6b-fa238d38ca8166c71c1e06e1ae9d05a7.jpg width=1038 height=302 class=img_ev3q></figure></div>
<p>這類方法會搭配一或多個輔助標籤，舉幾個例子，例如：</p>
<ul>
<li>pseudo depth map 用來區分平面攻擊與真實臉部</li>
<li>binary mask 表示 spoof 區域範圍，或是更進階的 ternary map、rPPG、光反射圖等</li>
</ul>
<p>讓模型同時學會辨識與解釋。（輸出的圖就可以作為解釋結果）</p>
<p>像 DepthNet、CDCN 是最早用深度圖當監督的架構；FAS-SGTD 還加入了短期與長期的動作估計，試圖建立一種「你假裝得再像，也會露出破綻」的節奏感。</p>
<p>遮罩類方法則由 George & Marcel 開始推動，後來有研究加上 attention 模組，解決模型注意力偏移的問題，也有人引入 ternary mask，把背景訊號排除，讓模型更聚焦在關鍵區域。</p>
<p>這類方法的優點很明顯：</p>
<blockquote>
<p><strong>可解釋性強、有空間語意、支援多任務學習，但同時也很依賴資料品質。</strong></p>
</blockquote>
<p>大多數 pixel label 都是用外部模型或人工建構，品質一差反而會變成誤導。</p>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>(b) 生成式監督（Generative Supervision）</strong></p>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=generative src=/assets/images/img6c-cf4d0d7273ba278ec0cc293f9c77cfb6.jpg width=1224 height=298 class=img_ev3q></figure></div>
<p>除了直接貼標，還有一派研究選擇從「讓模型自己發現異常」的角度出發。</p>
<p>這類方法多半採用 autoencoder 或 encoder-decoder 結構，讓模型嘗試重建 bonafide 影像，再透過重建誤差判斷 spoof 程度。</p>
<p>也有研究定義 spoof 是一種「添加了噪聲的輸入」，試圖從 noise estimation 中挖出破綻。更進一步的像是 meta-teacher 概念，讓教師模型自動產生像素級 supervision，學生模型負責學 spoof 特徵，搞得像一套內建 QA 系統。</p>
<p>這些方法的好處是：</p>
<blockquote>
<p><strong>視覺化強、具解釋力，也特別適合資料驅動的泛化學習。</strong></p>
</blockquote>
<p>但壞處也不小：訓練容易不穩、收斂慢，有時還會學到感測器特有的噪聲，反而增加過擬合風險。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>例如模型學到 iphone 的成像特性，然後把其他品牌的手機上傳的影像全部判斷為 spoof。</div></div>
</li>
</ul>
<hr>
<p>從這一波傳統深度學習方法可以看出，FAS 社群不再滿足於單純的分類準確率，而是開始關注模型的可解釋性、泛化能力與場景適應性。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=泛化導向的深度學習方法>泛化導向的深度學習方法<a href=#泛化導向的深度學習方法 class=hash-link aria-label=泛化導向的深度學習方法的直接連結 title=泛化導向的深度學習方法的直接連結>​</a></h2>
<p>如果說傳統深度學習方法的挑戰是資料不夠、模型太信任錯誤提示，那麼泛化導向的挑戰就更現實：</p>
<blockquote>
<p><strong>模型只在它「看過」的場景下表現良好，但一換相機、一換光源，甚至只換個人臉，它就瞬間崩潰。</strong></p>
</blockquote>
<p>這種「訓練很會、實戰不靈」的問題，在 FAS 領域尤其嚴重。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>之前我們在辦公室測得很準，結果拿去部署到超商門口，模型直接去世。<p>這種情況在門禁、支付、遠端驗證等應用中顯然無法接受，因此愈來愈多研究開始關注 FAS 模型的「<strong>泛化能力</strong>」。</div></div>
<p>泛化挑戰大致可以拆成兩種：</p>
<ul>
<li>一種是來自環境的變異（unseen domains），例如光照、感測器、背景雜訊等；</li>
<li>另一種則來自攻擊類型的未知性（unknown attacks），例如你根本沒看過的 3D 面具、變形遮蔽物。</li>
</ul>
<p>這兩種問題本質不同，因此處理方式也不一樣，以下我們分別來看看：</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=面對未知環境>面對未知環境<a href=#面對未知環境 class=hash-link aria-label=面對未知環境的直接連結 title=面對未知環境的直接連結>​</a></h3>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=unseen src=/assets/images/img7-5eec04eae7884907e889feccc9db8954.jpg width=940 height=440 class=img_ev3q></figure></div>
<p>當資料分布因環境變異而產生偏移，原本訓練得再好的模型也無法保證穩定輸出。</p>
<p>這類場景下，泛化策略主要分為兩種路線：</p>
<ul>
<li><strong>Domain Adaptation</strong></li>
<li><strong>Domain Generalization</strong></li>
</ul>
<p>這兩者的差別在於「你有沒有辦法取得目標環境的資料」。</p>
<hr>
<ul>
<li>
<p><strong>Domain Adaptation：我知道你會去哪，就幫你調整一下</strong></p>
<p>這類方法假設你已經知道模型將被部署在哪個環境中，甚至可能手上有一些（不管有沒有標註的）目標資料。既然知道目的地，自然可以提前校正方向。</p>
<p>有些方法會進行無監督對齊，像是 Li 等人用 MMD（最大平均差異）最小化 source 和 target 的特徵分布差異，或是用 adversarial learning 讓模型難以分辨來源 domain，藉此逼它學出通用特徵。</p>
<p>半監督方法則進一步假設有少量 target domain 的樣本（例如每類各 3 筆），就能顯著提升模型表現。但這招也有代價：如果你只有 bonafide 沒有 spoof 樣本，那模型可能就會偏心了。</p>
<p>進階一點的設計，像是 multi-layer MMD、domain-specific normalization，或是知識蒸餾，都試圖在網路架構本身做調整。某些研究還進行 filter pruning，邊遷移邊瘦身，一舉兩得。</p>
<blockquote>
<p><strong>缺點也很明顯：你在現實場景中根本不知道 spoof 樣本會是什麼樣子，難以取得有效的資料。</strong></p>
</blockquote>
<hr>
</li>
<li>
<p><strong>Domain Generalization：我不知道你去哪，但我希望你去哪都行</strong></p>
<p>相對地，Domain Generalization 的假設比較硬核：完全沒有 target domain 的資料，那就只能靠多個 source domains 訓練出一個能「應對所有未來」的模型。這聽起來像 AI 裡的冒險者訓練營，也難怪成為近年來研究熱點。</p>
<p>方法大致可以歸為幾類：最基本的是 adversarial learning，設計 domain discriminator 強迫特徵不攜帶來源資訊；也有透過 feature disentanglement，把 identity、camera noise 等干擾因素剝離，留下真正能跨場景生存的 spoof 表徵。</p>
<p>元學習方法則更具「角色扮演」精神：將不同 source domain 當作不同任務訓練模型，讓它具備快速適應新環境的能力。甚至有研究連 domain label 都不需要，直接用 clustering 動態生成 pseudo domain，算是把泛化練成一種內功。</p>
<p>另外，還有一些簡單但有效的設計來自 normalization，例如 Liu 提出 BIAN（Batch + Instance Adaptive Normalization），結合多種正規化策略去除 domain bias，效果意外地好。</p>
<blockquote>
<p><strong>缺點就是難 Train，而且容易受到邊緣樣本影響，進而降低整體表現。</strong></p>
</blockquote>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=面對未知攻擊>面對未知攻擊<a href=#面對未知攻擊 class=hash-link aria-label=面對未知攻擊的直接連結 title=面對未知攻擊的直接連結>​</a></h3>
<p>另一種挑戰，來自攻擊手法的多樣化：</p>
<blockquote>
<p><strong>你可以事前準備不同的 spoof 類型，但無法保證攻擊者不會發明一種你沒看過的方式。</strong></p>
</blockquote>
<p>這時候，「只學已知攻擊」的模型就會出現 blind spot，進而判錯。</p>
<p>因此，對抗未知攻擊的方法，也逐漸從 closed-set 思維轉向 open-set，主要出現了兩大方向：</p>
<ol>
<li><strong>Zero / Few-Shot Learning</strong></li>
<li><strong>Anomaly Detection</strong></li>
</ol>
<hr>
<ul>
<li>
<p><strong>Zero / Few-Shot Learning：我沒看過，但我有預感</strong></p>
<p>Zero-shot 方法的概念是：從已知的 spoof 類型中學抽象特徵，當遇到一種沒看過的攻擊時，也能「用語意猜出來」。例如 Liu 等人提出 Deep Tree Network（DTN），把 spoof 分門別類建立樹狀語意結構，再用這套結構判斷未知類型的攻擊。</p>
<p>Few-shot 方法則更實際一點，允許模型看到極少數新型樣本（例如五筆），用 meta-learning 快速更新模型參數，實現快速適應。像 Qin 的方法就是結合 zero- 與 few-shot 的優點，加入動態學習率機制；而 Perez-Cabo 則設計了 continual few-shot 學習機制，模型可以隨資料成長進行更新，避免舊任務遺忘。</p>
<p>這類方法的共同優勢是反應快、可擴展，但當樣本極少甚至為零時，仍可能出現混淆。尤其當 spoof 手法幾可亂真時，模型要分辨就沒那麼簡單了。</p>
<hr>
</li>
<li>
<p><strong>異常偵測（Anomaly Detection）：我不認識你，所以我不信你</strong></p>
<p>Anomaly Detection 思維比較簡單粗暴：我只學 bonafide，任何長得不像 bonafide 的東西，我一律視為可疑。</p>
<p>這類方法通常採 one-class 訓練策略，例如用 GMM、One-Class SVM、Autoencoder 重建誤差等手段界定「正常區域」。</p>
<p>更進一步，有些設計會將特徵空間內的 bonafide 收斂成一個 hypersphere，只要落在球外的點，就可能是 spoof。還有像 George 提出的 One-Class Contrastive Loss（OCCL），結合對比學習與後驗評分，強化模型對異常點的區辨力。</p>
<p>這類方法的優點是 open-world 友善、無需攻擊樣本；但缺點也很明顯：</p>
<blockquote>
<p><strong>只要 spoof 樣本長得太像 bonafide，就容易混淆，分類邊界也難以明確定義。</strong></p>
</blockquote>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=多感測器的深度學習方法>多感測器的深度學習方法<a href=#多感測器的深度學習方法 class=hash-link aria-label=多感測器的深度學習方法的直接連結 title=多感測器的深度學習方法的直接連結>​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>資訊</div><div class=admonitionContent_BuS1><p>這篇論文中還有一段是討論使用進階感測器的相關研究方法，例如近紅外線、熱成像、3D 深度感測器等，這些方法的優勢在於能夠捕捉到更多的生物特徵與環境變化，進而提高模型的準確率與穩定性。<p>但是我們在實際生活中還是以 RGB 相機為主流，因此我們就省略這部分的內容了。<p>有興趣的讀者可以參考原始論文中的相關章節。</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=討論>討論<a href=#討論 class=hash-link aria-label=討論的直接連結 title=討論的直接連結>​</a></h2>
<p>受益於深度學習技術的快速進展，FAS 模型在過去幾年確實取得了長足進步。</p>
<p>根據多數 benchmark 測試（如 OULU、SiW、CASIA-SURF 等），現有模型在主要協議下已能穩定達到 ACER &lt; 5%、EER &lt; 10%、HTER &lt; 15% 的水準。特別是在設計精緻的架構（如 NAS-FAS、FAS-SGTD）與細緻像素級監督（pseudo depth、reflection map 等）加持下，對於 2D 與部分 3D 攻擊的辨識效果已有不錯表現。</p>
<p>然而，這些數據只是紙上談兵的一環。當模型走出實驗室、走進真實世界，才會發現它們還有很多過不去的關卡：</p>
<ul>
<li>光線一變就崩潰。</li>
<li>攻擊一換就當機。</li>
<li>資料一少就自信滿滿地猜錯。</li>
</ul>
<p>即便是泛化導向設計（如 SSDG、SSAN、FGHV）展現了跨域潛力，但整體來看，FAS 依然是一個尚未解決的任務。</p>
<p>本篇論文的作者從以下五個面向總結當前挑戰與未來可能的研究突破口。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=架構設計與模型可解釋性>架構設計與模型可解釋性<a href=#架構設計與模型可解釋性 class=hash-link aria-label=架構設計與模型可解釋性的直接連結 title=架構設計與模型可解釋性的直接連結>​</a></h3>
<p>目前主流方法仍大量依賴經典 CNN 架構（如 ResNet、MobileNet）與手工設計的監督訊號（如 depth map、mask），這類設定雖然穩，但對於資料多樣性高的實務場景往往吃力不討好。</p>
<p>未來可考慮引入自動化機制，例如：</p>
<ul>
<li>利用 <strong>AutoML</strong> 搜尋不同模態與時間條件下的最佳模型結構；</li>
<li>針對 RGB、Depth、NIR 等多模態輸入，自動設計合理融合策略，取代人為堆疊；</li>
<li>搭配輕量模型設計，讓 FAS 不再是伺服器獨享，也能下放到手機與 IoT；</li>
<li>強化解釋性：從 Grad-CAM 到 spoof map、甚至自然語言生成預測解釋，讓模型的判斷過程不再是黑盒。</li>
</ul>
<p>讓模型說得出它為什麼認定某張臉是假的，會是未來可信 FAS 系統的關鍵一步。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=表示學習>表示學習<a href=#表示學習 class=hash-link aria-label=表示學習的直接連結 title=表示學習的直接連結>​</a></h3>
<p>FAS 的本質，其實是表徵學習的高階應用。問題不在於有沒有抓到「差異」，而是抓到的到底是不是 spoof 相關的差異。</p>
<p>未來可聚焦在幾個方向：</p>
<ul>
<li>導入 <strong>遷移學習</strong> 與大型預訓模型（如 ViT、SAM）轉移通用視覺知識；</li>
<li>做好 <strong>特徵解耦</strong>：從 identity、光照、畫質中分離出 spoof 專屬訊號；</li>
<li>結合 <strong>度量學習</strong>（Triplet, Hypersphere Loss），拉開 bonafide / spoof 的特徵分布；</li>
<li>用 <strong>自監督學習</strong> 從 unlabeled patch 中學習區域對比，建立細節辨識力；</li>
<li>更進階的對抗式資料增強，像是合成反光、微動失真、極端材質，讓模型面對不熟悉攻擊時也能有判斷依據。</li>
</ul>
<p>說白了，就是讓模型學會：「我不只知道你長得不像活體，我知道你哪裡不像。」</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=真實開放場景測試>真實開放場景測試<a href=#真實開放場景測試 class=hash-link aria-label=真實開放場景測試的直接連結 title=真實開放場景測試的直接連結>​</a></h3>
<p>目前的 FAS 測試協議幾乎都偏向「小規模、單因子、封閉型」，例如只測光源變異、只測單一 spoof 類型。然而現實場景永遠不只一種變數在變，甚至是連攻擊者本身都會變。</p>
<p>因此，<strong>建立真實可用的 open-set 測試框架</strong>，將是評估新一代模型的必要條件。</p>
<p>未來可朝以下方向發展：</p>
<ul>
<li>GrandTest 或 cross-protocol 測試：讓模型面對從沒看過的 domain + spoof；</li>
<li>多模態訓練：訓練只用 RGB，推論遇到 RGB + Depth，能不能泛化？能不能預測 pseudo modality？</li>
<li>多樣混合協議：例如 RGB-NIR-D 的隨機搭配，模擬感測器不一致的場景。</li>
</ul>
<p>如果說過去的測試是在讓模型過關，那未來的測試應該是讓模型出事，因為只有在它出錯的地方，我們才能真正知道它學到了什麼。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=整合式攻擊偵測>整合式攻擊偵測<a href=#整合式攻擊偵測 class=hash-link aria-label=整合式攻擊偵測的直接連結 title=整合式攻擊偵測的直接連結>​</a></h3>
<p>隨著 Deepfake、morphing、GAN 生成等數位偽造攻擊的興起，FAS 的守備範圍也逐漸擴張。未來的防偽任務，將不再只是人臉真不真，而是：</p>
<blockquote>
<p><strong>這張影像的來源、完整性與真實性是否可信？</strong></p>
</blockquote>
<p>這意味著 FAS 模型要開始具備：</p>
<ul>
<li><strong>多任務學習能力</strong>，從文件、商品、臉部等多領域中學出共通的 spoof 模式；</li>
<li><strong>跨領域知識整合</strong>，將實體 spoof（眼鏡、化妝）與數位 spoof（StyleGAN、FaceSwap）一併建模；</li>
<li><strong>對抗式攻擊防禦</strong>，包括物理對抗眼鏡、貼紙、特殊圖紋等擾動手法的辨識與抵抗。</li>
</ul>
<p>通用 PAD（Presentation Attack Detection）將成為下一個關鍵轉捩點，也是開啟 FAS 商業化應用的最後一哩路。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=隱私保護下的訓練方式>隱私保護下的訓練方式<a href=#隱私保護下的訓練方式 class=hash-link aria-label=隱私保護下的訓練方式的直接連結 title=隱私保護下的訓練方式的直接連結>​</a></h3>
<p>GDPR 等隱私法規上路後，「把資料集中起來再訓練」這件事越來越困難。面對資料無法共享、無法標記的現實，模型訓練必須轉向「資料不動，模型動」的策略。</p>
<p>目前較被看好的方向包括：</p>
<ul>
<li><strong>聯邦學習（Federated Learning）</strong>：每個 client 端各自訓練，再將模型參數彙整，兼顧隱私與學習效能；</li>
<li><strong>Source-Free Learning</strong>：模型公開但資料封存，部署後靠 pseudo-label 自行持續學習；</li>
<li><strong>私有資料學習中的風險控制</strong>：如何避免模型記住使用者特徵本身、如何做可信的去識別化，是未來結合 FAS 與 AI 安全的新戰場。</li>
</ul>
<p>能不能「在不知道你是誰的前提下，判斷你是不是真的」，是 FAS 在隱私時代的全新命題。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=結論>結論<a href=#結論 class=hash-link aria-label=結論的直接連結 title=結論的直接連結>​</a></h2>
<p>過去的 FAS 是為了擋下紙片與手機；現在的 FAS，是在對抗伺服器端生成、跨域攻擊與偽裝式干擾。在這條進化路上，我們已經從分類模型走到生成模型，從封閉集走到 open-set，從單一監督走向多任務學習與自監督優化。</p>
<p>但還不夠。</p>
<p>FAS 是面對攻擊者的第一道防線，卻也是 AI 系統最脆弱的環節。未來的 FAS，需要的是能夠跨模態理解、跨資料學習、跨場景生存的整合式系統。它應該能自己判斷可信度、自己調整策略、自己辨識什麼是風險，並且在面對未知攻擊時，能夠自我調整與適應。</p>
<p>這個領域的探索仍在進行中，之後的路還很漫長。</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime=2025-04-06T15:28:22.000Z itemprop=dateModified>2025年4月6日</time></b> <!-- -->更新</span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ 一杯咖啡，就是我創作的燃料！</h3><p class=simple-cta__subtitle_ol86>贊助我持續分享 AI 實作、全端架構與開源經驗，讓好文章不斷更新。<div class=simple-cta__buttonWrapper_jk1Y><img src=/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-7ny38l" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-7ny38l"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-7ny38l" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/img/icons/all_in.svg alt="AI / 全端 / 客製 一次搞定 icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-7ny38l">ALL</span><h4 class=card__title_SQBY>AI / 全端 / 客製 一次搞定</h4><p class=card__concept_Ak8F>從構想到上線，涵蓋顧問、開發與部署，全方位支援你的技術實作。<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>包含內容</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>顧問服務 + 系統建置 + 客製開發<li class=card__bulletItem_wCRd>長期維運與擴充規劃</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 你的專案準備好了嗎？</h3><p class=simple-cta__subtitle_ol86>如果你需要客製服務或長期顧問，歡迎與我聯繫！</div></section><div style=margin-top:3rem> </div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label=文件選項卡><a class="pagination-nav__link pagination-nav__link--prev" href=/papers/face-antispoofing/personalized-fas/><div class=pagination-nav__sublabel>上一頁</div><div class=pagination-nav__label>[22.01] Personalized-FAS</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/papers/face-antispoofing/m2a2e/><div class=pagination-nav__sublabel>下一頁</div><div class=pagination-nav__label>[23.02] M²A²E</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#攻與防的編年史 class="table-of-contents__link toc-highlight">攻與防的編年史</a><li><a href=#fas-架構 class="table-of-contents__link toc-highlight">FAS 架構</a><ul><li><a href=#深度學習方法總覽 class="table-of-contents__link toc-highlight">深度學習方法總覽</a><li><a href=#評估指標 class="table-of-contents__link toc-highlight">評估指標</a><li><a href=#評估協議 class="table-of-contents__link toc-highlight">評估協議</a></ul><li><a href=#基於-rgb-的-fas class="table-of-contents__link toc-highlight">基於 RGB 的 FAS</a><li><a href=#混合式方法 class="table-of-contents__link toc-highlight">混合式方法</a><li><a href=#傳統深度學習方法 class="table-of-contents__link toc-highlight">傳統深度學習方法</a><ul><li><a href=#二分類監督 class="table-of-contents__link toc-highlight">二分類監督</a><li><a href=#像素級監督 class="table-of-contents__link toc-highlight">像素級監督</a></ul><li><a href=#泛化導向的深度學習方法 class="table-of-contents__link toc-highlight">泛化導向的深度學習方法</a><ul><li><a href=#面對未知環境 class="table-of-contents__link toc-highlight">面對未知環境</a><li><a href=#面對未知攻擊 class="table-of-contents__link toc-highlight">面對未知攻擊</a></ul><li><a href=#多感測器的深度學習方法 class="table-of-contents__link toc-highlight">多感測器的深度學習方法</a><li><a href=#討論 class="table-of-contents__link toc-highlight">討論</a><ul><li><a href=#架構設計與模型可解釋性 class="table-of-contents__link toc-highlight">架構設計與模型可解釋性</a><li><a href=#表示學習 class="table-of-contents__link toc-highlight">表示學習</a><li><a href=#真實開放場景測試 class="table-of-contents__link toc-highlight">真實開放場景測試</a><li><a href=#整合式攻擊偵測 class="table-of-contents__link toc-highlight">整合式攻擊偵測</a><li><a href=#隱私保護下的訓練方式 class="table-of-contents__link toc-highlight">隱私保護下的訓練方式</a></ul><li><a href=#結論 class="table-of-contents__link toc-highlight">結論</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/docs>開源專案</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/papers/intro>論文筆記</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/blog>部落格</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/terms-of-service>使用條款</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/privacy-policy>隱私政策</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/become-an-author>成為作者</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/worklog>工作日誌</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2025 DOCSAID.</div></div></div></footer></div>