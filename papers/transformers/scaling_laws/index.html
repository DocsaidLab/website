<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-transformers/scaling_laws/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[20.01] Scaling Laws | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/transformers/scaling_laws/"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[20.01] Scaling Laws | DOCSAID"><meta data-rh="true" name="description" content="模型的縮放律"><meta data-rh="true" property="og:description" content="模型的縮放律"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/transformers/scaling_laws/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/scaling_laws/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/transformers/scaling_laws/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/scaling_laws/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.64d77125.css">
<script src="/assets/js/runtime~main.16367ef6.js" defer="defer"></script>
<script src="/assets/js/main.b23db18d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/transformers/scaling_laws/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/transformers/scaling_laws/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Sponsor<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">論文筆記</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="展開側邊欄分類 &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="展開側邊欄分類 &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="展開側邊欄分類 &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="展開側邊欄分類 &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="展開側邊欄分類 &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/multimodality-19">Multimodality (19)</a><button aria-label="展開側邊欄分類 &#x27;Multimodality (19)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/normalization-1">Normalization (1)</a><button aria-label="展開側邊欄分類 &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/object-detection-7">Object Detection (7)</a><button aria-label="展開側邊欄分類 &#x27;Object Detection (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="展開側邊欄分類 &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="展開側邊欄分類 &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="展開側邊欄分類 &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/papers/category/transformers-15">Transformers (15)</a><button aria-label="收起側邊欄分類 &#x27;Transformers (15)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/roberta/">[19.07] RoBERTa</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/mqa/">[19.11] MQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/papers/transformers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/roformer/">[21.04] RoFormer</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="展開側邊欄分類 &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/transformers-15"><span itemprop="name">Transformers (15)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[20.01] Scaling Laws</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[20.01] Scaling Laws</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型的縮放律">模型的縮放律<a href="#模型的縮放律" class="hash-link" aria-label="模型的縮放律的直接連結" title="模型的縮放律的直接連結">​</a></h2>
<p><a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener noreferrer"><strong>Scaling Laws for Neural Language Models</strong></a></p>
<hr>
<p>OpenAI 在 Transformer 解碼器上走得愈來愈遠了。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結">​</a></h2>
<p>從前幾篇文章中，想必你也看出 OpenAI 的野心了：他們想建立一個「超級大」的語言模型。</p>
<p>於是就有了這篇研究：</p>
<ul>
<li>到底該怎麼把模型做大？</li>
</ul>
<p>這個問題，就是本論文想解決的核心挑戰。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結">​</a></h2>
<p>本文中，使用了以下超參數來參數化 Transformer 架構：</p>
<ul>
<li><strong>中間前饋層的維度</strong></li>
<li><strong>注意力輸出的維度</strong></li>
<li><strong>每層的注意力頭數</strong></li>
<li><strong>層數</strong>：Transformer 的層數</li>
<li><strong>Token 數量</strong>：上下文長度，通常設定為 1024</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="訓練過程">訓練過程<a href="#訓練過程" class="hash-link" aria-label="訓練過程的直接連結" title="訓練過程的直接連結">​</a></h3>
<p>除非特別註明，否則使用 Adam 優化器進行訓練，每次訓練進行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2.5</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">2.5\times10^{5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2.5</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> 步，批次大小為 512 個序列，每個序列包含 1024 個標記。由於記憶體限制，超過 10 億參數的模型使用 Adafactor 進行訓練。</p>
<p>作者試驗了各種學習率和學習率調度方法，發現收斂時的結果基本不受學習率調度的影響。除非特別說明，所有訓練均使用 3000 步的線性增溫後逐步衰減至零的學習率調度。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="數據集">數據集<a href="#數據集" class="hash-link" aria-label="數據集的直接連結" title="數據集的直接連結">​</a></h3>
<p>模型訓練在「<strong>擴展版的 WebText 數據集</strong>」上。</p>
<ul>
<li>原始 WebText 數據集來自於 Reddit 的外部鏈接，這些鏈接在 2017 年 12 月前獲得至少 3 個其他讀者認同。</li>
<li>WebText2 中，加入了 2018 年 1 月至 10 月期間的 Reddit 外部鏈接，這些鏈接也需要至少 3 個其他讀者認同。</li>
<li>使用 Newspaper3k Python 庫提取鏈接文本，最終數據集包含 2030 萬個文檔，總計 96GB 文本和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.62</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">1.62\times10^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.62</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span> 詞。</li>
<li>應用了可逆的字節對編碼（byte-pair encoding）來進行標記化，最終得到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2.29</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">2.29\times10^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2.29</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span> 個標記，並保留了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6.6</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">6.6\times10^{8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">6.6</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span> 個標記作為測試集。</li>
<li>在 Books Corpus、Common Crawl、英語維基百科和其他公共可用的網絡書籍樣本上進行測試。</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="變化因素">變化因素<a href="#變化因素" class="hash-link" aria-label="變化因素的直接連結" title="變化因素的直接連結">​</a></h3>
<p>為了全面研究語言模型的縮放特性，本論文在以下方面進行了廣泛的模型訓練：</p>
<ul>
<li><strong>模型大小</strong>：從 768 到 15 億非嵌入參數的模型</li>
<li><strong>數據集大小</strong>：從 2200 萬到 230 億標記的數據集</li>
<li><strong>模型形狀</strong>：包括深度、寬度、注意力頭數和前饋層維度</li>
<li><strong>上下文長度</strong>：大多數訓練使用 1024 長度的上下文，但我們也試驗了更短的上下文</li>
<li><strong>批次大小</strong>：大多數訓練使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">2^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span> 的批次大小，但我們也變化批次大小來測量關鍵批次大小</li>
</ul>
<p>上述的實驗設計旨在系統地研究模型在不同條件下的性能表現，並得出相關的縮放規律。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="討論的直接連結" title="討論的直接連結">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="規模大於一切">規模大於一切<a href="#規模大於一切" class="hash-link" aria-label="規模大於一切的直接連結" title="規模大於一切的直接連結">​</a></h3>
<p>模型的性能很大程度上取決於規模，而較弱地取決於模型形狀，模型性能很大程度上取決於規模，它由三個因素組成：</p>
<ol>
<li><strong>N: 模型參數的數量</strong></li>
<li><strong>D: 數據集大小</strong></li>
<li><strong>C: 使用的計算量</strong></li>
</ol>
<p>實驗指出：在合理的範圍內，效能對其他架構超參數（例如深度與寬度）的依賴性非常弱。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="冪定律">冪定律<a href="#冪定律" class="hash-link" aria-label="冪定律的直接連結" title="冪定律的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img1" src="/assets/images/img1-b5423c96cce73969bc9fe431fd8565a7.jpg" width="1696" height="544" class="img_ev3q"></p>
<p>當不受其他兩個比例因子限制時，表現與三個比例因子 N、D、C 中的每一個都存在冪律關係，趨勢跨越「六個」數量級以上，如上圖。</p>
<ul>
<li><strong>左圖</strong>：計算量愈大，模型效能愈好。</li>
<li><strong>中圖</strong>：數據集愈大，模型效能愈好。</li>
<li><strong>右圖</strong>：模型參數愈多，模型效能愈好。</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>你可能會覺得好像在說廢話？</p><p>並不是。在過去的架構中，例如 CNN 或 LSTM，這些關係並不總是成立。要不就是效能飽和，要不就是過擬合。這種橫跨六個數量級的關係，的確是一個很大的突破。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="過擬合的普遍性">過擬合的普遍性<a href="#過擬合的普遍性" class="hash-link" aria-label="過擬合的普遍性的直接連結" title="過擬合的普遍性的直接連結">​</a></h3>
<p>實驗指出：只要我們同時擴大 N 和 D，效能就可以預見地提高，但如果 N 或 D 保持固定而另一個增加，則進入收益遞減狀態。效能損失可預測地取決於比率 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msup><mi>N</mi><mn>0.74</mn></msup><mi>D</mi></mfrac></mrow><annotation encoding="application/x-tex">N^{0.74}\over D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3629em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">0.74</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，這意味著每次我們將模型大小增加 8 倍，我們只需將資料增加約 5 倍即可避免損失。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="測試表現與下游表現">測試表現與下游表現<a href="#測試表現與下游表現" class="hash-link" aria-label="測試表現與下游表現的直接連結" title="測試表現與下游表現的直接連結">​</a></h3>
<p>實驗指出：當我們在分佈與訓練不同的文本上評估模型時，結果與訓練驗證集上的結果密切相關，損失的偏移量大致恆定。</p>
<p>換句話說，轉移到不同的分佈會產生恆定的懲罰，但在其他方面會根據測試集的表現大致提高。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="樣本效率">樣本效率<a href="#樣本效率" class="hash-link" aria-label="樣本效率的直接連結" title="樣本效率的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img2" src="/assets/images/img2-62b3001dd1209df06fd9a28bdd9c3c27.jpg" width="2809" height="2030" class="img_ev3q"></p>
<p>大型模型比小型模型的樣本效率更高，可以透過更少的最佳化步驟(上圖，第一張圖表)和使用更少的數據點(上圖，第二張圖表)達到相同的效能水準。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="收斂是一件低效的事情">收斂是一件低效的事情<a href="#收斂是一件低效的事情" class="hash-link" aria-label="收斂是一件低效的事情的直接連結" title="收斂是一件低效的事情的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img3" src="/assets/images/img3-4b5347886e8f82ed827e5e8a79a0126e.jpg" width="1224" height="560" class="img_ev3q"></p>
<p>如上圖顯示，增加模型尺寸是一個更有效的方法，而不是訓練小模型直到完全收斂。</p>
<p>簡單來說，如果我們有固定的計算資源，最好訓練非常大的模型並在它們完全收斂之前停止，這樣可以在固定的計算預算內獲得最佳性能。相比之下，訓練小模型直到它們完全收斂反而效率更低。</p>
<p>此外，隨著計算資源的增加，所需的訓練數據量增加得很慢，因此我們不需要大量增加數據來匹配增加的計算資源。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="最佳批量大小">最佳批量大小<a href="#最佳批量大小" class="hash-link" aria-label="最佳批量大小的直接連結" title="最佳批量大小的直接連結">​</a></h3>
<p>訓練語言模型的最佳批次大小並不是一個固定的數值，而是與損失函數有關，可以通過測量梯度噪聲尺度來確定。</p>
<p>在本論文中，定義的最大模型為 15 億（1.5B），在訓練效果最好的時候，最佳的批次大小大約是 100 萬到 200 萬個 Tokens。 這樣的批次大小有助於在訓練過程中達到最佳性能和效果。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>這邊提到的 100 萬到 200 萬指的是 Tokens 數量，而不是 Batch size 的數量。如果模型的上下文長度是 1024 的話，那麼對應的 Batch size 大約是 1000 到 2000，就能達到最佳效果。</p></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="結論的直接連結" title="結論的直接連結">​</a></h2>
<p>這篇論文中做了大量的實驗，並對語言模型的訓練方式提出了總結。</p>
<p>這些結論在我們眼裡看到的都是白花花的鈔票，我們心懷感激地接受這些實驗結果。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">2024年9月11日</time></b> <!-- -->更新</span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/transformers/mqa/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">[19.11] MQA</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/transformers/longformer/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">[20.04] Longformer</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#模型的縮放律" class="table-of-contents__link toc-highlight">模型的縮放律</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#訓練過程" class="table-of-contents__link toc-highlight">訓練過程</a></li><li><a href="#數據集" class="table-of-contents__link toc-highlight">數據集</a></li><li><a href="#變化因素" class="table-of-contents__link toc-highlight">變化因素</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#規模大於一切" class="table-of-contents__link toc-highlight">規模大於一切</a></li><li><a href="#冪定律" class="table-of-contents__link toc-highlight">冪定律</a></li><li><a href="#過擬合的普遍性" class="table-of-contents__link toc-highlight">過擬合的普遍性</a></li><li><a href="#測試表現與下游表現" class="table-of-contents__link toc-highlight">測試表現與下游表現</a></li><li><a href="#樣本效率" class="table-of-contents__link toc-highlight">樣本效率</a></li><li><a href="#收斂是一件低效的事情" class="table-of-contents__link toc-highlight">收斂是一件低效的事情</a></li><li><a href="#最佳批量大小" class="table-of-contents__link toc-highlight">最佳批量大小</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">使用條款<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">隱私政策<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>