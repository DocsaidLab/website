<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-transformers/gpt_3/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[20.05] GPT-3 | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/transformers/gpt_3/"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[20.05] GPT-3 | DOCSAID"><meta data-rh="true" name="description" content="九十六層解碼器"><meta data-rh="true" property="og:description" content="九十六層解碼器"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/transformers/gpt_3/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/gpt_3/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/transformers/gpt_3/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/gpt_3/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.64d77125.css">
<script src="/assets/js/runtime~main.c669892d.js" defer="defer"></script>
<script src="/assets/js/main.a2041421.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/transformers/gpt_3/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/transformers/gpt_3/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Sponsor<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">論文筆記</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/classic-cnns-10">Classic CNNs (10)</a><button aria-label="展開側邊欄分類 &#x27;Classic CNNs (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="展開側邊欄分類 &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="展開側邊欄分類 &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="展開側邊欄分類 &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="展開側邊欄分類 &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/multimodality-18">Multimodality (18)</a><button aria-label="展開側邊欄分類 &#x27;Multimodality (18)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/normalization-1">Normalization (1)</a><button aria-label="展開側邊欄分類 &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/object-detection-7">Object Detection (7)</a><button aria-label="展開側邊欄分類 &#x27;Object Detection (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="展開側邊欄分類 &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="展開側邊欄分類 &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="展開側邊欄分類 &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/papers/category/transformers-15">Transformers (15)</a><button aria-label="收起側邊欄分類 &#x27;Transformers (15)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/roberta/">[19.07] RoBERTa</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/mqa/">[19.11] MQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/papers/transformers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/roformer/">[21.04] RoFormer</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/vision-transformers-10">Vision Transformers (10)</a><button aria-label="展開側邊欄分類 &#x27;Vision Transformers (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/transformers-15"><span itemprop="name">Transformers (15)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[20.05] GPT-3</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[20.05] GPT-3</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="九十六層解碼器">九十六層解碼器<a href="#九十六層解碼器" class="hash-link" aria-label="九十六層解碼器的直接連結" title="九十六層解碼器的直接連結">​</a></h2>
<p><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer"><strong>Language Models are Few-Shot Learners</strong></a></p>
<hr>
<p>第二代的 GPT 疊了四十八層 Transformer 解碼器。</p>
<p>OpenAI 覺得這樣不夠，於是他們繼續往上疊了九十六層 Transformer 解碼器，參數量達到史無前例的 175 B，名為 GPT-3。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結">​</a></h2>
<p>最近的工作已經證明，透過對大量文字進行預訓練，然後對特定任務進行微調，在許多 NLP 任務和基準測試中取得了巨大的成果。在我們的理想中的 NLP 技術應能像人類一樣，在接收少量指示或示例的情況下，快速適應和處理多種語言任務，但現階段的研究中還是有幾個問題，顯然和理想上還有很大的差距：</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="多樣化的語言任務需求">多樣化的語言任務需求<a href="#多樣化的語言任務需求" class="hash-link" aria-label="多樣化的語言任務需求的直接連結" title="多樣化的語言任務需求的直接連結">​</a></h3>
<p>當前的語言模型面臨著適應廣泛且多樣化的語言任務的需求，從語法糾正到抽象概念生成等。每個新任務通常需要大量特定的標記數據集，這限制了模型的普遍適用性。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="依賴大規模監督數據集">依賴大規模監督數據集<a href="#依賴大規模監督數據集" class="hash-link" aria-label="依賴大規模監督數據集的直接連結" title="依賴大規模監督數據集的直接連結">​</a></h3>
<p>收集和標記大型數據集對於許多語言任務來說既昂貴又耗時。每個新任務都需要重複這一數據收集過程，這增加了開發成本並延長了部署時間。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型的過度專業化和泛化問題">模型的過度專業化和泛化問題<a href="#模型的過度專業化和泛化問題" class="hash-link" aria-label="模型的過度專業化和泛化問題的直接連結" title="模型的過度專業化和泛化問題的直接連結">​</a></h3>
<p>當前模型在特定任務上進行微調可能導致過度專業化，使得模型在訓練分佈之外的數據上泛化能力差。訓練中的虛假相關性可能會誤導模型，影響其長期和廣泛的應用。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="人類的學習效率與模型的對比">人類的學習效率與模型的對比<a href="#人類的學習效率與模型的對比" class="hash-link" aria-label="人類的學習效 率與模型的對比的直接連結" title="人類的學習效率與模型的對比的直接連結">​</a></h3>
<p>相對於人類通過少量示例或直接指令就能快速適應新任務的能力，當前模型對大量標記數據的依賴顯得效率低下。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結">​</a></h2>
<p><img decoding="async" loading="lazy" alt="tuning" src="/assets/images/img2-5ebe8a874ae278f7a9930a9174ee42e9.jpg" width="1204" height="1080" class="img_ev3q"></p>
<p>在 GPT-3 中，基本預訓練方法，包括模型、資料和訓練，都與 GPT-2 中描述的過程類似，相對簡單地擴大了模型大小、資料集大小和多樣性以及訓練長度。對情境學習的使用也與 GPT-2 類似，但在這項工作中，作者們系統地探索了情境中學習的不同設定，主要的分別為：</p>
<ol>
<li>
<p><strong>Fine-tuning</strong></p>
<ul>
<li>
<p>這是最常見的方法，涉及透過對特定於所需任務的監督資料集進行訓練來更新預訓練模型的權重。通常使用數千到數十萬個標籤的範例。微調的主要優點是在許多基準測試中表現出色。</p>
</li>
<li>
<p>主要缺點是每個任務都需要一個新的大型資料集，分佈外泛化能力差的可能性，以及利用訓練資料的虛假特徵的可能性，這可能會導致與人類表現進行不公平的比較。</p>
</li>
<li>
<p>在這項工作中，作者們沒有對 GPT-3 進行微調，因為他們的重點是與任務無關的性能，但 GPT-3 原則上可以進行微調，這是未來工作的一個有希望的方向。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>這大概就是之後 ChatGPT-3.5 的伏筆了。</p></div></div>
</li>
</ul>
</li>
<li>
<p><strong>Few-shot</strong></p>
<ul>
<li>
<p>Few-shot 是指：在推理時為模型提供一些任務演示作為條件，但不允許權重更新。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>如上圖範例，假設我們需要解決的問題是「將英文句子翻譯成法文」，我們可以提供一些範例給模型，例如：</p><ul>
<li>
<p><code>Translate English to French: &quot;sea otter&quot; -&gt; &quot;loutre de mer&quot;, cheese -&gt; </code></p>
</li>
</ul><p>接著再讓主要需要翻譯的內容接續在下文，這裡就是期待模型會給出 <code>cheese</code> 的法文翻譯。</p></div></div>
</li>
<li>
<p>主要優點是：大幅減少了對特定任務資料的需求，並降低了從大而窄的微調資料集中學習過於狹窄的分佈的潛力。</p>
</li>
<li>
<p>主要缺點是：迄今為止，這種方法的結果比最先進的微調模型要差得多。此外，仍需要少量任務特定資料。</p>
</li>
</ul>
</li>
<li>
<p><strong>One-shot</strong></p>
<ul>
<li>One-shot 與 Few-shot 相同，只是只允許進行一次演示，此外還需要對任務進行自然語言描述。這種方法提供了最接近某些任務與人類溝通的方式。例如，當要求人類在人類工作者服務（例如 Mechanical Turk）上產生資料集時，通常會給出一個任務演示。相較之下，如果不給予範例，有時很難傳達任務的內容或格式。</li>
</ul>
</li>
<li>
<p><strong>Zero-shot</strong></p>
<ul>
<li>與 One-shot 相同，只是不允許進行演示，並且僅向模型提供描述任務的自然語言指令。這種方法提供了最大的便利性、穩健性的潛力，並避免了虛假相關性，但也是最具挑戰性的設定。在某些情況下，如果沒有先前的範例，人類甚至可能很難理解任務的格式，因此這種設定在某些情況下「相當困難」。</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型架構">模型架構<a href="#模型架構" class="hash-link" aria-label="模型架構的直接連結" title="模型架構的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="model_arch" src="/assets/images/img1-1f3e4c8320bb9c2d8cc890ce0a0975ec.jpg" width="1224" height="336" class="img_ev3q"></p>
<p>在本文中，作者使用與 GPT-2 相同的模型和架構，包括修改後的初始化、預歸一化和可逆標記化。</p>
<p>不同之處在於：作者在 Transformer 中使用了稀疏注意力的 Transformer，關於該技術的詳細內容，讀者可以參考我們的另外一篇文章以獲取更多資訊：</p>
<ul>
<li>
<p><a href="/papers/transformers/sparse-transformer/"><strong>[19.04] Sparse Transformer</strong></a></p>
<p><img decoding="async" loading="lazy" alt="sparse_transformer" src="/assets/images/img6-dba9e1d15b2107a0f8f0fb46d6cb84b8.jpg" width="1392" height="694" class="img_ev3q"></p>
</li>
</ul>
<p>為了研究機器學習效能對模型大小的依賴性，作者們訓練了 8 種不同大小的模型，範圍從 1.25 億個參數到 1750 億個參數三個數量級，最後一個是他們稱為 GPT-3 的模型。</p>
<p>從上表中可以看到，所有模型都使用 2048 個 Token 的上下文視窗。作者們沿著深度和寬度維度跨 GPU 劃分模型，以最大程度地減少節點之間的資料傳輸。每個模型的精確架構參數是根據運算效率和跨 GPU 模型佈局的負載平衡來選擇的。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="資料集設定">資料集設定<a href="#資料集設定" class="hash-link" aria-label="資料集設定的直接連結" title="資料集設定的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="data" src="/assets/images/img3-5443003f09defaed11a7863fe50f1974.jpg" width="1224" height="320" class="img_ev3q"></p>
<p>在本文中，作者使用包含近兆字的 Common Crawl 資料集來預先訓練模型。</p>
<p>另外，作者也發現未經篩選或僅輕度篩選的 Common Crawl 版本的資料品質通常不如精心準備的資料集。</p>
<p>為了提升資料集的平均質量，他們採取了以下三個步驟：</p>
<ol>
<li><strong>資料篩選和品質控制</strong>：篩選 Common Crawl 的數據，選擇與一系列高品質參考語料庫相似度較高的數據。</li>
<li><strong>去重處理</strong>：在文件層級進行模糊重複資料刪除，既在資料集內部進行，也跨資料集進行，以防止資料冗餘，並保持驗證集的完整性。</li>
<li><strong>增強資料多樣性</strong>：在訓練組合中加入了已知的高品質參考語料庫，以增強 Common Crawl 資料並提高其多樣性。</li>
</ol>
<p>透過這種方法，作者收集了從 2016 年至 2019 年間的 41 個 Common Crawl 每月資料片段，這些資料包含了過濾前的 45TB 壓縮明文和過濾後的 570GB，大約等同於 4000 億個位元組編碼的 Token。</p>
<p>此外，在訓練期間並非按資料大小進行採樣，而是更頻繁地採樣品質較高的資料集，例如 Common Crawl 和 Books2 資料集在訓練期間的採樣次數不足一次，而其他數據集則採樣 2 至 3 次。這種做法本質上是為了用少量的過度擬合換取更高品質的訓練資料。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>在預訓練期間無意中看到測試或開發集，可能會污染下游任務的效果。</p><p>為了減少這種污染，作者搜尋並嘗試消除所有 Benchmark 和 Training 資料的任何重疊。不幸的是，過濾中的錯誤導致作者們忽略了一些重疊，而由於訓練成本，模型不能重新訓練（因為沒錢了），只能在後面試著挽救這個錯誤。</p></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="討論的直接連結" title="討論的直接連結">​</a></h2>
<p>作者測試 GPT-3 在語言建模和相關任務中的表現，包括預測單字、完成句子或段落。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lambada">LAMBADA<a href="#lambada" class="hash-link" aria-label="LAMBADA的直接連結" title="LAMBADA的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="lambada" src="/assets/images/img4-558b8d19491337799dc4f90f438fb167.jpg" width="1224" height="772" class="img_ev3q"></p>
<p>LAMBADA 資料集要求模型預測一個句子的最後一個單字，需要閱讀整段上下文。這個基準測試特別挑戰模型對遠端依賴關係的建模能力。</p>
<ul>
<li>最近研究指出，在 LAMBADA 基準測試中，語言模型的持續擴展已經帶來收益遞減。過去的研究顯示，兩個先進結果間模型大小加倍僅提升了 1.5% 的準確率，並提出結論認為通過擴展硬體和數據規模來提升性能並不是可行之路。</li>
<li>而 OpenAI 則表示：那是你的規模不夠。在 Zero-shot 的設定中，GPT-3 在 LAMBADA 上達到了 76% 的準確率，比之前最佳技術水平「提高了 8%」。（上圖的藍線）</li>
<li>在 Few-shot 設定中，GPT-3 達到了 86.4% 的準確率，比之前最佳技術水平「提高了超過 18%」。（上圖的橘線）</li>
</ul>
<p>特別需要注意的是，當模型規模不足時，使用 Few-shot 設定會讓模型的表現大幅下降，上圖顯示大約有 20% 的差距。也就是說，模型的規模「必須」足夠大，才能在 Few-shot 設定下取得好的表現。從上圖看來，這種「頓悟」的現象，大約在參數量達到 2.6B 時開始出現，這讓後續的研究者有了一個參考點。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="closed-book-question-answering">Closed Book Question Answering<a href="#closed-book-question-answering" class="hash-link" aria-label="Closed Book Question Answering的直接連結" title="Closed Book Question Answering的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="qa" src="/assets/images/img5-e2e8027e4df1a147951d6e5b723e49d0.jpg" width="1224" height="732" class="img_ev3q"></p>
<p>這項任務用來衡量 GPT-3 回答廣泛事實知識問題的能力，通常這些問題透過資訊檢索系統查找相關文本並生成  答案。此任務被稱為「open-book」設定，即系統可以搜尋並利用相關文字回答問題。另一種設定是「閉卷」（closed-book），系統直接回答問題，不依賴外部資訊。</p>
<p>本研究測試 GPT-3 在相同 closed-book 設定中的表現，在 Zero-shot 的設定中達到 64.3%，在 One-shot 的設定中達到 68.0%，在 Few-shot 的設定中達到 71.2%。相比微調後的 T5-11B，GPT-3 在 Zero-shot 的設定中提高了 14.2%，比帶有 Q&amp;A 定制跨度預測的版本提高了 3.8%，而且用 One-shot 設定就達到 SoTA 水平。</p>
<p>在所有測試資料集上，作者發現效能隨模型大小的變化非常平滑，這可能反映了模型容量直接轉化為模型參數吸收的更多「知識」的想法。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="結論的直接連結" title="結論的直接連結">​</a></h2>
<p>該論文是一份共有 75 頁的 PDF 檔案，裡面提供的圖表遠不止上面提到的這些，但所有的圖表都表明了一個共同的趨勢：大語言模型的性能需要經過 Few-shot 設定才能發揮出來。這種趨勢在各種任務和基準測試中都是如此。這也在後續衍生出一連串的「提示工程（Prompt Engineering）」研究，這些研究主要是為了找到最佳的提示方式，以便模型能夠在 Few-shot 設定下發揮最佳性能。</p>
<p>儘管 GPT-3 在文本生成和自然語言處理任務上相較於前代模型有顯著進步，它仍然存在諸如語義重複、失去連貫性及邏輯錯誤等問題。在處理常識物理問題和上下文學習行為上，GPT-3 顯示出特定的弱點，這反映了模型在某些特定語言任務上的局限性。</p>
<p>另外就是關於偏見與公平性問題，GPT-3 和類似的大型語言模型在訓練過程中使用的大量數據來自於網際網路，這使得模型不可避免地吸收並反映了訓練數據中的偏見。主要為以下幾個方面：</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="1-性別偏見">1. 性別偏見<a href="#1-性別偏見" class="hash-link" aria-label="1. 性別偏見的直接連結" title="1. 性別偏見的直接連結">​</a></h3>
<p>GPT-3 在處理性別相關內容時展示出明顯的偏見。</p>
<p>例如，在關於職業的生成文本中，模型往往將男性與某些職業（如科技、工程等）聯繫起來，而將女性與護理、教育等領域相關聯。</p>
<p>這種偏見可能強化了社會中對性別角色的刻板印象。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="2-種族偏見">2. 種族偏見<a href="#2-種族偏見" class="hash-link" aria-label="2. 種族偏見的直接連結" title="2. 種族偏見的直接連結">​</a></h3>
<p>在種族方面，語言模型可能生成包含或暗示某種族群特徵的文本，這可能導致對某些族群的負面描述。</p>
<p>例如，當模型被引導討論特定種族的人時，生成的文本可能不成比例地反映了負面或刻板的特質。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="3-宗教偏見">3. 宗教偏見<a href="#3-宗教偏見" class="hash-link" aria-label="3. 宗教偏見的直接連結" title="3. 宗教偏見的直接連結">​</a></h3>
<p>GPT-3 在處理與宗教相關的查詢時也可能展現出偏見，例如，在描述不同宗教信徒的特性時，可能使用了過於一般化或具有偏見的描述。</p>
<hr>
<p>這些挑戰和解決策略突顯了開發大型語言模型時，平衡技術進步和倫理責任的重要性，並強調了持續研究和改進的必要性，以減少 AI 系統可能帶來的不公正現象。</p>
<p>GPT-3 及類似的語言模型在提升文本生成和自然語言處理能力方面具有顯著的潛力，但也面臨諸多挑戰。未來的研究需平衡技術進步與倫理、公平性和效率之間的關係，確保這些先進技術在為人類服務的同時，也能促進更加公平和可持續的技術發展。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">2024年9月11日</time></b> <!-- -->更新</span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/transformers/longformer/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">[20.04] Longformer</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/transformers/bigbird/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">[20.07] BigBird</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#九十六層解碼器" class="table-of-contents__link toc-highlight">九十六層解碼器</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a><ul><li><a href="#多樣化的語言任務需求" class="table-of-contents__link toc-highlight">多樣化的語言任務需求</a></li><li><a href="#依賴大規模監督數據集" class="table-of-contents__link toc-highlight">依賴大規模監督數據集</a></li><li><a href="#模型的過度專業化和泛化問題" class="table-of-contents__link toc-highlight">模型的過度專業化和泛化問題</a></li><li><a href="#人類的學習效率與模型的對比" class="table-of-contents__link toc-highlight">人類的學習效率與模型的對比</a></li></ul></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#模型架構" class="table-of-contents__link toc-highlight">模型架構</a></li><li><a href="#資料集設定" class="table-of-contents__link toc-highlight">資料集設定</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#lambada" class="table-of-contents__link toc-highlight">LAMBADA</a></li><li><a href="#closed-book-question-answering" class="table-of-contents__link toc-highlight">Closed Book Question Answering</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a><ul><li><a href="#1-性別偏見" class="table-of-contents__link toc-highlight">1. 性別偏見</a></li><li><a href="#2-種族偏見" class="table-of-contents__link toc-highlight">2. 種族偏見</a></li><li><a href="#3-宗教偏見" class="table-of-contents__link toc-highlight">3. 宗教偏見</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">使用條款<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">隱私政策<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>