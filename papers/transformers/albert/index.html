<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-transformers/albert/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[19.09] ALBERT | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/transformers/albert/"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[19.09] ALBERT | DOCSAID"><meta data-rh="true" name="description" content="愛因斯 BERT"><meta data-rh="true" property="og:description" content="愛因斯 BERT"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/transformers/albert/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/albert/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/transformers/albert/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/albert/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.ddd83665.css">
<script src="/assets/js/runtime~main.a8be0f06.js" defer="defer"></script>
<script src="/assets/js/main.bb370235.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">開源專案</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">論文筆記</a><a class="navbar__item navbar__link" href="/blog">部落格</a><a class="navbar__item navbar__link" href="/playground/intro">遊樂場</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/transformers/albert/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/transformers/albert/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">支持我們<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">論文筆記</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="展開側邊欄分類 &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="展開側邊欄分類 &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="展開側邊欄分類 &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="展開側邊欄分類 &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="展開側邊欄分類 &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/multimodality-19">Multimodality (19)</a><button aria-label="展開側邊欄分類 &#x27;Multimodality (19)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/normalization-1">Normalization (1)</a><button aria-label="展開側邊欄分類 &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/object-detection-8">Object Detection (8)</a><button aria-label="展開側邊欄分類 &#x27;Object Detection (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="展開側邊欄分類 &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="展開側邊欄分類 &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-recognition-10">Text Recognition (10)</a><button aria-label="展開側邊欄分類 &#x27;Text Recognition (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-spotting-3">Text Spotting (3)</a><button aria-label="展開側邊欄分類 &#x27;Text Spotting (3)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="展開側邊欄分類 &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/papers/category/transformers-18">Transformers (18)</a><button aria-label="收起側邊欄分類 &#x27;Transformers (18)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/xlnet/">[19.06] XLNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/roberta/">[19.07] RoBERTa</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/papers/transformers/albert/">[19.09] ALBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/mqa/">[19.11] MQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/roformer/">[21.04] RoFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/transformers/lora/">[21.06] LoRA</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="展開側邊欄分類 &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">All Notes: 121 entries</a></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/transformers-18"><span itemprop="name">Transformers (18)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[19.09] ALBERT</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[19.09] ALBERT</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="愛因斯-bert">愛因斯 BERT<a href="#愛因斯-bert" class="hash-link" aria-label="愛因斯 BERT的直接連結" title="愛因斯 BERT的直接連結">​</a></h2>
<p><a href="https://arxiv.org/abs/1909.11942" target="_blank" rel="noopener noreferrer"><strong>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</strong></a></p>
<hr>
<p>BERT 之後，還是 BERT。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結">​</a></h2>
<p>一個 BERT 大概有 340M 個參數，很難 Train，要 Train 很久。有了 BERT 作為基礎比較對象，後續研究開始加大模型規模，模型參數愈來愈多。</p>
<p>0.1B ➔ 0.3B ➔ 0.5B ➔ 1.0B ➔ 1.5B ➔ ？？？</p>
<p>這篇論文的作者說：不行，不能再更大了，我們快要 Train 不起了！</p>
<ul>
<li><strong>我們需要小的 BERT！</strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結">​</a></h2>
<p>ALBERT 的基礎架構與 BERT 一樣，但做了以下幾個改進：</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="分解的嵌入參數化">分解的嵌入參數化<a href="#分解的嵌入參數化" class="hash-link" aria-label="分解的嵌入參數化的直接連結" title="分解的嵌入參數化的直接連結">​</a></h3>
<p>在 BERT 及其後續改進模型（這裡說的是 XLNet 和 RoBERTa）中，WordPiece 嵌入大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span> 與隱藏層大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span> 被綁定，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>≡</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">E ≡ H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≡</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>。</p>
<p>這樣的設計在模型和實用性上有兩個缺點：</p>
<ul>
<li>
<p><strong>模型角度</strong>：WordPiece 嵌入主要學習上下文無關的表示，而隱藏層嵌入則專注於上下文相關的表示 。透過分離 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>，模型可以更有效地根據需求分配參數，因為隱藏層應有較大的容量（即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>&gt;</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">H &gt; E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span>）。</p>
</li>
<li>
<p><strong>實用角度</strong>：自然語言處理中，詞彙表大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span> 通常很大。如果 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>≡</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">E ≡ H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≡</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>，則增大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span> 會顯著增加嵌入矩陣的大小，該矩陣的尺寸為 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>×</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">V \times E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span>。這可能會導致模型的參數量達到數十億，且大多數參數在訓練中僅會稀疏更新。</p>
</li>
</ul>
<p>因此，ALBERT 使用了嵌入參數的分解，將它們分成兩個較小的矩陣：</p>
<ul>
<li>首先將 one-hot 向量投影到較低維度的嵌入空間，大小為 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></li>
<li>然後再將其投影到隱藏層空間</li>
</ul>
<p>此分解使嵌入參數的數量從 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>V</mi><mo>×</mo><mi>H</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(V \times H)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mclose">)</span></span></span></span> 降低至 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>V</mi><mo>×</mo><mi>E</mi><mo>+</mo><mi>E</mi><mo>×</mo><mi>H</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(V \times E + E \times H)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mclose">)</span></span></span></span>。</p>
<p>當 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>&gt;</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">H &gt; E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span> 時，參數量會顯著地減少！</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p><strong>舉個例子吧！</strong></p><p>假設詞彙表大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span> 為 30,000，隱藏層大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span> 為 1,024。在 BERT 中，因為 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>≡</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">E ≡ H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≡</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>，這意味著嵌入矩陣的大小會是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>×</mo><mi>H</mi><mo>=</mo><mn>30</mn><mo separator="true">,</mo><mn>000</mn><mo>×</mo><mn>1</mn><mo separator="true">,</mo><mn>024</mn><mo>=</mo><mn>30</mn><mo separator="true">,</mo><mn>720</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">V \times H = 30,000 \times 1,024 = 30,720,000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">30</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">000</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">024</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">30</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">720</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">000</span></span></span></span> 個參數。</p><p>然而，如果我們在 ALBERT 中將嵌入維度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span> 設定為較小的 128，那麼嵌入矩陣的大小會被分解為兩部分：</p><ul>
<li>第一部分是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>×</mo><mi>E</mi><mo>=</mo><mn>30</mn><mo separator="true">,</mo><mn>000</mn><mo>×</mo><mn>128</mn><mo>=</mo><mn>3</mn><mo separator="true">,</mo><mn>840</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">V \times E = 30,000 \times 128 = 3,840,000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">30</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">000</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">840</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">000</span></span></span></span> 個參數</li>
<li>第二部分是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>×</mo><mi>H</mi><mo>=</mo><mn>128</mn><mo>×</mo><mn>1</mn><mo separator="true">,</mo><mn>024</mn><mo>=</mo><mn>131</mn><mo separator="true">,</mo><mn>072</mn></mrow><annotation encoding="application/x-tex">E \times H = 128 \times 1,024 = 131,072</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">128</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">024</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">131</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">072</span></span></span></span> 個參數</li>
</ul><p>因此，ALBERT 的總嵌入參數數量為 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo separator="true">,</mo><mn>840</mn><mo separator="true">,</mo><mn>000</mn><mo>+</mo><mn>131</mn><mo separator="true">,</mo><mn>072</mn><mo>=</mo><mn>3</mn><mo separator="true">,</mo><mn>971</mn><mo separator="true">,</mo><mn>072</mn></mrow><annotation encoding="application/x-tex">3,840,000 + 131,072 = 3,971,072</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">840</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">000</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">131</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">072</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">971</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">072</span></span></span></span>，相比 BERT 中的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>30</mn><mo separator="true">,</mo><mn>720</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">30,720,000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">30</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">720</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">000</span></span></span></span> 個參數大幅減少。</p><p>這不僅能降低記憶體需求，還能提高計算效率。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="跨層參數共享">跨層參數共享<a href="#跨層參數共享" class="hash-link" aria-label="跨層參數共享的直接連結" title="跨層參數共享的直接連結">​</a></h3>
<p>過去有許多研究專注於提高模型的參數效率:</p>
<ul>
<li>
<p><strong>Dehghani 等人 (2018) 的 Universal Transformer (UT)</strong>：</p>
<ul>
<li><a href="https://arxiv.org/abs/1807.03819" target="_blank" rel="noopener noreferrer"><strong>[18.07] Universal transformers</strong></a></li>
</ul>
<p>他們提出了一種 Universal Transformer，並發現這種模型在性能上超越了標準的 Transformer。Universal Transformer 是一種改進版的 Transformer，它通過將多層 Transformer 的參數共享（類似 ALBERT 的做法），強化了模型的學習能力，特別是在處理不同層的時間步進時，有效提升了模型的表現。</p>
</li>
<li>
<p><strong>Bai 等人 (2019) 的 Deep Equilibrium Models (DQE)</strong>：</p>
<ul>
<li><a href="https://arxiv.org/abs/1909.01377" target="_blank" rel="noopener noreferrer"><strong>[19.09] Deep equilibrium models</strong></a></li>
</ul>
<p>他們的研究發現，Deep Equilibrium Models (DQE) 能夠達到一個「平衡點」，在該平衡點上，某一層的輸入和輸出嵌入變得相同，這意味著該層模型的表示不再發生變化。也就是說模型的表達能力在達到這個平衡點後，逐漸趨於穩定，不再有顯著的變動。</p>
</li>
</ul>
<p>除了上述兩個研究，共享參數的概念實現的方式有很多種：</p>
<ul>
<li>只在層與層之間共享前饋網路 (FFN) 的參數</li>
<li>只共享注意力機制的參數</li>
</ul>
<p>ALBERT 的預設設計是<strong>共享所有層之間的所有參數</strong>，除非另有說  明，所有實驗均基於此預設進行。如下圖，作者通過對 L2 距離和餘弦相似度的測量來進行分析，測量顯示 ALBERT 的嵌入表現出的是<strong>震盪狀態</strong>，而不是像 DQE 模型那樣收斂。</p>
<p><img decoding="async" loading="lazy" alt="albert cross-layer parameter sharing" src="/assets/images/img1-686d86f52340b4f89d2b2b345f131484.jpg" width="1438" height="484" class="img_ev3q"></p>
<p>ALBERT 的嵌入參數並不會像 DQE 那樣達到穩定的平衡點，而是保持動態變化，這與其他研究的結果形成了對比，並且這樣的動態變化或許對 ALBERT 的設計和性能具有特定的優勢。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="句序預測損失-loss">句序預測損失 Loss<a href="#句序預測損失-loss" class="hash-link" aria-label="句序預測損失 Loss的直接連結" title="句序預測損失 Loss的直接連結">​</a></h3>
<p>句序預測損失，Sentence-Order Prediction, 簡稱 SOP。</p>
<p>BERT 除了 MLM 損失外，還使用了一種額外的損失函數 NSP（Next Sentence Prediction）。在許多後續研究發現 NSP 的效果不穩定，並選擇移除 NSP 損失。</p>
<p>作者推測 NSP 的低效原因是該任務相對於 MLM 來說過於簡單。NSP 將主題預測與一致性預測混為一體，而主題預測相對於一致性預測更容易學習，並且與 MLM 損失的學習重疊較大。</p>
<p>考慮到這些因素，作者認為跨句建模是語言理解中重要的一部分，為此提出了句序預測損失：</p>
<ul>
<li>正樣本與 BERT 相同，來自同一文檔中的兩個連續片段。</li>
<li>負樣本則是相同兩個連續片段，但交換它們的順序。</li>
</ul>
<p>模型必須判斷哪個片段是先出現的，這樣能強迫模型學習更細緻的一致性區別。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="討論的直接連結" title="討論的直接連結">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bert-與-albert-的整體比較">BERT 與 ALBERT 的整體比較<a href="#bert-與-albert-的整體比較" class="hash-link" aria-label="BERT 與 ALBERT 的整體比較的直接連結" title="BERT 與 ALBERT 的整體比較的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="albert comparison" src="/assets/images/img2-5334c010a4648b4e4668f456d79fd1b9.jpg" width="1722" height="350" class="img_ev3q"></p>
<p>ALBERT 的設計選擇主要體現在參數效率的提升。</p>
<p>如上表所示：ALBERT-xxlarge 只使用 BERT-large 約 70% 的參數，卻在多個代表性下游任務上表現優異，包括 SQuAD v1.1 (+1.9%)、SQuAD v2.0 (+3.1%)、MNLI (+1.4%)、SST-2 (+2.2%)、RACE (+8.4%)。</p>
<p>此外，ALBERT 的訓練數據處理速度優於 BERT，在相同的 TPU 訓練配置下，ALBERT-large 的處理速度是 BERT-large 的 1.7 倍，而 ALBERT-xxlarge 則因其較大結構速度較慢，大約是 BERT-large 的三倍時間。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="嵌入參數分解">嵌入參數分解<a href="#嵌入參數分解" class="hash-link" aria-label="嵌入參數分解的直接連結" title="嵌入參數分解的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="albert embedding size" src="/assets/images/img3-7ea5a08de8f5d2eb05aecd6994145ca8.jpg" width="1668" height="446" class="img_ev3q"></p>
<p>上表展示了在 ALBERT-base 設定下，改變詞彙嵌入大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span> 的效果。</p>
<p>對於非共享參數的情況下（BERT 風格），較大的嵌入尺寸帶來了些許性能提升，但在全參數共享（ALBERT 風格）下，嵌入大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">E=128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span> 表現最佳。</p>
<p>基於這些結果，作者決定在未來的設置中使用嵌入大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">E=128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span> 進行模型擴展。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="跨層參數共享-1">跨層參數共享<a href="#跨層參數共享-1" class="hash-link" aria-label="跨層參數共享的直接連結" title="跨層參數共享的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="albert cross-layer parameter sharing" src="/assets/images/img4-f2f86eaaf7bfa6e42c2c96c99eacd71c.jpg" width="1612" height="388" class="img_ev3q"></p>
<p>上表展  示了不同的跨層參數共享策略實驗結果：</p>
<ul>
<li>使用 ALBERT-base 配置（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">E=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">768</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">E=128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span>）。</li>
</ul>
<p>全共享策略會在兩種條件下影響性能，但對於 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">E=128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span>，損失較小（-1.5）；對於 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">E=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">768</span></span></span></span>，損失較大（-2.5）。</p>
<p>大部分性能下降源於共享 FFN 層的參數，當 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">E=128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span> 時共享注意力層的參數對性能幾乎沒有影響 (+0.1)，而當 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">E=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">768</span></span></span></span> 時則有小幅下降 (-0.7)。</p>
<p>雖然可以考慮將 L 層劃分為 N 個大小為 M 的組，每組內共享參數，但實驗表明，分組的大小愈小，性能愈好，但參數總數也隨之增加。</p>
<p>因此，作者最終選擇了全共享策略作為預設。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="句序預測的有效性">句序預測的有效性<a href="#句序預測的有效性" class="hash-link" aria-label="句序預測的有效性的直接連結" title="句序預測的有效性的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="albert sentence order prediction" src="/assets/images/img5-b6b86bde188999fe8745a83f0472c5cf.jpg" width="1864" height="298" class="img_ev3q"></p>
<p>作者比較了三種跨句損失條件：</p>
<ul>
<li>無（XLNet 和 RoBERTa 風格）</li>
<li>NSP（BERT 風格）</li>
<li>SOP（ALBERT 風格）。</li>
</ul>
<p>結果顯示，NSP 對 SOP 任務沒有辨識力（準確率為 52.0%，接近隨機猜測），這表明 NSP 主要學習了主題轉換。而 SOP 在 NSP 任務上表現不錯（準確率為 78.9%），且在 SOP 任務上表現更佳（準確率為 86.5%）。</p>
<p>此外，SOP 對於多句編碼任務的下游性能有顯著提升：SQuAD1.1 提升 1%、SQuAD2.0 提升 2%、RACE 提升 1.7%。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="nlu-任務的當前最先進水平">NLU 任務的當前最先進水平<a href="#nlu-任務的當前最先進水平" class="hash-link" aria-label="NLU 任務的當前最先進水平的直接連結" title="NLU 任務的當前最先進水平的直接連結">​</a></h3>
<figure><p><img decoding="async" loading="lazy" alt="albert nlu" src="/assets/images/img10-64a76be7cb3f2964977fe9fa0c4e01a6.jpg" width="1426" height="522" class="img_ev3q">
</p><figcaption>ALBERT 在 GLUE benchmark 上的表現</figcaption><p></p></figure>
<hr>
<figure><p><img decoding="async" loading="lazy" alt="albert nlu" src="/assets/images/img11-0f582f0cb077fc0593372bd5a7a8f780.jpg" width="1496" height="622" class="img_ev3q">
</p><figcaption> ALBERT 在 SQuAD and RACE benchmarks 上的表現</figcaption><p></p></figure>
<hr>
<p>最後，作者在兩種微調設定下發表了 SoTA 結果：<strong>單模型</strong> 和 <strong>系集模型</strong>：</p>
<ul>
<li><strong>單模型</strong>：<!-- -->
<ul>
<li>在開發集上報告了五次運行中的中位數結果。</li>
<li>單模型的 ALBERT 配置使用了最佳表現的設置：ALBERT-xxlarge 配置，結合了 MLM 和 SOP 的損失函數，且不使用 dropout。</li>
</ul>
</li>
<li><strong>系級模型</strong>：<!-- -->
<ul>
<li>系級模型的檢查點根據開發集的性能選取，根據任務的不同，檢查點的數量範圍從 6 到 17。</li>
<li>在 GLUE 和 RACE 基準測試中，系集模型的預測結果是對來自不同訓練步驟的 12 層和 24 層架構進行微調後的模型預測進行平均。</li>
<li>在 SQuAD 中，則是對具有多個概率的 span 的預測分數進行平均，並對「無法回答」的決策分數進行平均。</li>
</ul>
</li>
</ul>
<p>無論是單模型還是系集模型，ALBERT 在以下三個基準測試中顯著提升了當前的 SoTA 水準：</p>
<ul>
<li><strong>GLUE</strong>：取得了 89.4 分</li>
<li><strong>SQuAD 2.0</strong>：測試 F1 分數達到 92.2</li>
<li><strong>RACE</strong>：測試準確率達到 89.4</li>
</ul>
<p>RACE 測試的結果特別好，相較於其他模型的提升幅度巨大：</p>
<ul>
<li>比 BERT 提升了 17.4%。</li>
<li>比 XLNet 提升了 7.6%。</li>
<li>比 RoBERTa 提升了 6.2%。</li>
<li>比 DCMI+ 提升了 5.3%。</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="結論的直接連結" title="結論的直接連結">​</a></h2>
<p>ALBERT 以創新的方式大幅提升了參數效率，並且引入了更具針對性的損失函數來提升跨句理解任務的表現。它的設計理念對於未來的大型語言模型具有深遠的影響。</p>
<p>如果你也是在資源有限的環境下開發語言模型，ALBERT 可能是一個不錯的選擇！</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime="2024-10-08T05:29:27.000Z" itemprop="dateModified">2024年10月8日</time></b> <!-- -->更新</span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/transformers/roberta/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">[19.07] RoBERTa</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/transformers/mqa/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">[19.11] MQA</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#愛因斯-bert" class="table-of-contents__link toc-highlight">愛因斯 BERT</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#分解的嵌入參數化" class="table-of-contents__link toc-highlight">分解的嵌入參數化</a></li><li><a href="#跨層參數共享" class="table-of-contents__link toc-highlight">跨層參數共享</a></li><li><a href="#句序預測損失-loss" class="table-of-contents__link toc-highlight">句序預測損失 Loss</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#bert-與-albert-的整體比較" class="table-of-contents__link toc-highlight">BERT 與 ALBERT 的整體比較</a></li><li><a href="#嵌入參數分解" class="table-of-contents__link toc-highlight">嵌入參數分解</a></li><li><a href="#跨層參數共享-1" class="table-of-contents__link toc-highlight">跨層參數共享</a></li><li><a href="#句序預測的有效性" class="table-of-contents__link toc-highlight">句序預測的有效性</a></li><li><a href="#nlu-任務的當前最先進水平" class="table-of-contents__link toc-highlight">NLU 任務的當前最先進水平</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">開源專案</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">論文筆記</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">部落格</a><span class="footer__link-separator">·</span><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="footer__link-item">支持我們<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">使用條款<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">隱私政策<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>