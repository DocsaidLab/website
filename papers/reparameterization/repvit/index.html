<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-reparameterization/repvit/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">[23.07] RepViT | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/reparameterization/repvit/"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[23.07] RepViT | DOCSAID"><meta data-rh="true" name="description" content="參考 ViT 的觀點"><meta data-rh="true" property="og:description" content="參考 ViT 的觀點"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/reparameterization/repvit/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/reparameterization/repvit/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/reparameterization/repvit/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/reparameterization/repvit/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.51ef4fe1.css">
<script src="/assets/js/runtime~main.faa18eec.js" defer="defer"></script>
<script src="/assets/js/main.0c861755.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">開源專案</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">論文筆記</a><a class="navbar__item navbar__link" href="/blog">部落格</a><a class="navbar__item navbar__link" href="/playground/intro">遊樂場</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/reparameterization/repvit/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/reparameterization/repvit/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">支持我們<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">論文筆記</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="展開側邊欄分類 &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="展開側邊欄分類 &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="展開側邊欄分類 &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="展開側邊欄分類 &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="展開側邊欄分類 &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/llm-tuning-5">LLM Tuning (5)</a><button aria-label="展開側邊欄分類 &#x27;LLM Tuning (5)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/multimodality-20">Multimodality (20)</a><button aria-label="展開側邊欄分類 &#x27;Multimodality (20)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/normalization-1">Normalization (1)</a><button aria-label="展開側邊欄分類 &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/object-detection-8">Object Detection (8)</a><button aria-label="展開側邊欄分類 &#x27;Object Detection (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="收起側邊欄分類 &#x27;Reparameterization (7)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/reparameterization/repvgg/">[21.01] RepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/reparameterization/replknet/">[22.03] RepLKNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/reparameterization/mobileone/">[22.06] MobileOne</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/reparameterization/qarepvgg/">[22.12] QARepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/reparameterization/fastvit/">[23.03] FastViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/reparameterization/vanillanet/">[23.05] VanillaNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/papers/reparameterization/repvit/">[23.07] RepViT</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="展開側邊欄分類 &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="展開側邊欄分類 &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-recognition-18">Text Recognition (18)</a><button aria-label="展開側邊欄分類 &#x27;Text Recognition (18)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-spotting-4">Text Spotting (4)</a><button aria-label="展開側邊欄分類 &#x27;Text Spotting (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/transformers-15">Transformers (15)</a><button aria-label="展開側邊欄分類 &#x27;Transformers (15)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="展開側邊欄分類 &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">All Notes: 133 entries</a></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/reparameterization-7"><span itemprop="name">Reparameterization (7)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[23.07] RepViT</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[23.07] RepViT</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="參考-vit-的觀點">參考 ViT 的觀點<a href="#參考-vit-的觀點" class="hash-link" aria-label="參考 ViT 的觀點的直接連結" title="參考 ViT 的觀點的直接連結">​</a></h2>
<p><a href="https://arxiv.org/abs/2307.09283" target="_blank" rel="noopener noreferrer"><strong>RepViT: Revisiting Mobile CNN From ViT Perspective</strong></a></p>
<hr>
<p>重新參數化的研究如火如荼，逐漸蔓延到不同的模型架構。</p>
<p>本篇論文的目標是將 MobileNet-V3 重新參數化，和過去幾篇論文不一樣的地方是：</p>
<ul>
<li>我們這次是從 ViT 的觀點開始看起。</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結">​</a></h2>
<p>輕量視覺模型的研究主要分為兩個派系：</p>
<ol>
<li><strong>基於 CNN 的研究</strong>：這裡的代表作品是 MobileNet 系列，透過深度可分離卷積來減少參數量，利用倒置殘差瓶頸結構來增加模型的表達能力。</li>
<li><strong>基於 ViT 的研究</strong>：這裡的代表作品是 MobileViT、Mobileformer 和 FastViT 等，主要透過 MetaFormer 架構來保持模型的表達能力。</li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>沒看過 MetaFormer 的讀者可以參考：</p><ul>
<li><a href="/papers/vision-transformers/poolformer/"><strong>[21.11] PoolFormer: 你需要的是框架！</strong></a></li>
<li><a href="/papers/vision-transformers/caformer/"><strong>[22.10] CAFormer: MetaFormer 使用說明書</strong></a></li>
</ul></div></div>
<hr>
<p>兩種派系之間彷彿有個不可逾越的鴻溝？</p>
<p>本篇論文的目標就是要跨越這個鴻溝，將 MobileNet-V3 重新設計，並從 ViT 的角度重新思考。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="重新設計架構">重新設計架構<a href="#重新設計架構" class="hash-link" aria-label="重新設計架構的直接連結" title="重新設計架構的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="model arch" src="/assets/images/img2-039e2f7cda7143bd23317ebdc6ffba8b.jpg" width="1224" height="996" class="img_ev3q"></p>
<p>首先，我們就拋開所有參數量和 FLOPs 的限制，因為這裡主要的目標就是 Latency！</p>
<p>要快，要更快，要非常快！</p>
<hr>
<p>一切從 MobileNet-V3 開始，參考最近輕量級的 ViT，他們大多採用 DeiT 的訓練方法。</p>
<p>這裡採用的訓練方式為：</p>
<ul>
<li>使用 AdamW 優化器</li>
<li>使用 Cosine 學習率衰減，訓練 300 epochs</li>
<li>使用 RegNetY16GF 網路進行知識蒸餾</li>
<li>影像增強方式採用：Mixup、RandAugment 和 Random Erasing</li>
</ul>
<p>所以接著所有模型都用這種訓練方式，統一標準。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>沒看過 DeiT 的讀者可以參考：</p><ul>
<li><a href="/papers/vision-transformers/deit/"><strong>[20.12] DeiT: 蒸餾後更香醇</strong></a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="重新參數化結構">重新參數化結構<a href="#重新參數化結構" class="hash-link" aria-label="重新參數化結構的直接連結" title="重新參數化結構的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="reparameterization" src="/assets/images/img3-22a730bafd3310f012365a4efb243a36.jpg" width="1224" height="704" class="img_ev3q"></p>
<p>接著調整 MobileNet-V3 的結構。</p>
<p>在 MetaFormer 中，我們知道 Transformer 的成功原因是把資訊交換分成兩個部分：</p>
<ol>
<li>Token 之間的資訊交換，在影像中，對應的是圖像的全局資訊交換。</li>
<li>特徵之間的資訊交換，在影像中，對應的是圖像的通道資訊交換。</li>
</ol>
<p>但是在 MobileNet 的結構中，這兩件事情被耦合在一起，如上圖左邊：</p>
<ul>
<li>首先用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">1x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 的卷積，這裡是「通道」資訊交換</li>
<li>接著用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>x</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">3x3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span><span class="mord mathnormal">x</span><span class="mord">3</span></span></span></span> 的深度卷積，這裡是「全局」資訊交換</li>
<li>接著經過一個 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">SE</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">SE</span></span></span></span> 層，這裡是「全局與通道」資訊交換</li>
<li>最後 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">1x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 的卷積，這裡是「通道」資訊交換</li>
</ul>
<p>於是作者先將他們分開，改成上圖右邊的結構：</p>
<ul>
<li>首先是「全局」資訊交換，這裡用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>x</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">3x3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span><span class="mord mathnormal">x</span><span class="mord">3</span></span></span></span> 的深度卷積。另外，為了提升推論速度，這裡換成重新參數化的結構。</li>
<li>接著是使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">SE</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">SE</span></span></span></span> 層，這裡是「全局與通道」資訊交換</li>
<li>最後是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">1x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 的卷積，這裡是「通道」資訊交換</li>
</ul>
<p>這樣就成功分開了「全局」和「通道」資訊交換，並且提升了 20% 的推論速度。</p>
<hr>
<p>速度快了，精度也掉了。</p>
<p>接著要想辦法補救。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="降低膨脹比並增加寬度">降低膨脹比並增加寬度<a href="#降低膨脹比並增加寬度" class="hash-link" aria-label="降低膨脹比並增加寬度的直接連結" title="降低膨脹比並增加寬度的直接連結">​</a></h3>
<p>在普通 ViT 中，通道混合器中的擴展比通常設定為 4，使得前饋網路 (FFN) 模組的隱藏維度比輸入維度寬 4 倍；在 MobileNet-V3 中還達到了 6 倍。因此，它消耗了很大一部分計算資源，從而大大增加了整體推理時間。</p>
<p>而過去許多研究也指出 FFN 中存在大量的冗餘資訊，實際上我們不需要這麼高的膨脹比。</p>
<ul>
<li><a href="https://arxiv.org/abs/2104.01136" target="_blank" rel="noopener noreferrer"><strong>[21.04] LeViT: a Vision Transformer in ConvNet&#x27;s Clothing for Faster Inference</strong></a></li>
<li><a href="https://arxiv.org/abs/2104.10858" target="_blank" rel="noopener noreferrer"><strong>[21.04] All Tokens Matter: Token Labeling for Training Better Vision Transformers</strong></a></li>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/3b11c5cc84b6da2838db348b37dbd1a2-Abstract-Conference.html" target="_blank" rel="noopener noreferrer"><strong>[22.12] SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization</strong></a></li>
<li><a href="https://openreview.net/forum?id=LzBBxCg-xpa" target="_blank" rel="noopener noreferrer"><strong>[23.10] NViT: Vision Transformer Compression and Parameter Redistribution</strong></a></li>
</ul>
<p>作者在這裡決定採用 2 倍的膨脹比，並且同時增加模型的寬度，以保持模型的表達能力。</p>
<hr>
<p>經過這個操作，精度回來了，還超越了原始的 MobileNet-V3，達到 73.5%。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="stem-結構調整">Stem 結構調整<a href="#stem-結構調整" class="hash-link" aria-label="Stem 結構調整的直接連結" title="Stem 結構調整的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="stem" src="/assets/images/img4_1-b8f0be57e3bcf036644ea39fa74d19b6.jpg" width="2360" height="1430" class="img_ev3q"></p>
<p>ViT 通常使用 patchify 操作作為主幹，將輸入影像劃分為不重疊的 Patch，這些簡單的 Stem 結構通常是一個大核 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mi>x</mi><mn>16</mn></mrow><annotation encoding="application/x-tex">16x16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span><span class="mord mathnormal">x</span><span class="mord">16</span></span></span></span> 卷積。在分層 ViT 中，也是用 patchify 的操作，不過用的是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>x</mi><mn>4</mn></mrow><annotation encoding="application/x-tex">4x4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">4</span><span class="mord mathnormal">x</span><span class="mord">4</span></span></span></span> 的卷積。</p>
<p>相比之下， MobileNet-V3 的 Stem 結構非常複雜，包含了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>x</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">3x3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span><span class="mord mathnormal">x</span><span class="mord">3</span></span></span></span> 的卷積、深度可分離卷積和倒置殘差瓶頸結構，如上圖 (a) 所示。</p>
<p>由於 Stem 結構會以最高解析度處理輸入圖像，因此複雜結構會在行動裝置上遇到嚴重的速度瓶頸，作為權衡，MobileNet-V3 的 Stem 結構被給予相當少的濾波器（只有 16 個通道），這又反過來限制了 Stem 結構的表示能力。</p>
<p>為了解決這個問題，作者採用早期卷積的方式，配置兩個 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>x</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">3x3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span><span class="mord mathnormal">x</span><span class="mord">3</span></span></span></span> 的卷積，且設定 stride 為 2，這樣可以減少計算量。同時增加濾波器的通道數量，以提升模型的表達能力，如上圖 (b) 所示。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="降採樣結構調整">降採樣結構調整<a href="#降採樣結構調整" class="hash-link" aria-label="降採樣結構調整的直接連結" title="降採樣結構調整的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="downsampling" src="/assets/images/img4_2-d5b7e3e669723f729fa1b0c2475e7502.jpg" width="2365" height="1430" class="img_ev3q"></p>
<p>MobileNet-V3 只透過一個 stride 為 2 的深度分離卷積來進行降採樣，這樣的結構在速度上有優勢，但是這種設計可能缺乏足夠的網路深度，導致資訊遺失並對模型效能產生負面影響，如上圖 (c) 所示。</p>
<p>為了解決這個問題，作者採用了更複雜的降採樣結構，如上圖 (d) 所示。這個結構包含了 stride 為 2 的深度分離卷積執行下採樣和調整通道維度，接著在前面加一個 RepViT 模塊，最後把 FFN 模組放在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">1x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 卷積之後，以記住更多的潛在資訊。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="簡單分類器">簡單分類器<a href="#簡單分類器" class="hash-link" aria-label="簡單分類器的直接連結" title="簡單分類器的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="classifier" src="/assets/images/img4_3-663d1f33d2671962921d87868268ef5c.jpg" width="2365" height="1430" class="img_ev3q"></p>
<p>在輕量級的 ViT 中，分類器通常由全域平均池化和一個線性層組成，這種架構在行動裝置上的速度很快。</p>
<p>相較之下，MobileNet-V3 的分類器包含了一個 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">1x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 的卷積和一個額外的線性層，將特徵拓展到更高維的空間，如上圖 (e) 所示，這個設計對 MobileNet-V3 而言非常重要，但是在行動裝置上會增加推理時間。</p>
<p>作者在這裡放棄 MobileNet-V3 的設計，改用一個簡單全域平均池化和一個線性層組成最後的架構。</p>
<hr>
<p>這個設計又讓速度回到 0.77 ms，但是精度又掉了！</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="階段比例調整">階段比例調整<a href="#階段比例調整" class="hash-link" aria-label="階段比例調整的直接連結" title="階段比例調整的直接連結">​</a></h3>
<p>在過去的研究中，大家都遵循著 1:1:3:1 的階段比例，這樣的設計在模型的表達能力和速度之間取得了一個平衡。</p>
<ul>
<li><a href="https://arxiv.org/abs/1905.13214" target="_blank" rel="noopener noreferrer"><strong>[19.05] On network design spaces for visual recognition</strong></a></li>
<li><a href="https://arxiv.org/abs/2003.13678" target="_blank" rel="noopener noreferrer"><strong>[20.03] Designing network design spaces</strong></a></li>
</ul>
<p>在近幾年的研究指出：更激進的比例對小模型來說可能更有利，像是 Conv2Former-T 和 Conv2Former-S 就分別採用了 1:1:4:1 和 1:1:8:1 的比例。</p>
<ul>
<li><a href="https://arxiv.org/abs/2211.11943" target="_blank" rel="noopener noreferrer"><strong>[22.11] Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition</strong></a></li>
</ul>
<p>作者這裡採用了 1:1:7:1 的比例，深度增加到 2:2:14:2，實現更深的網路佈局。</p>
<p>這個操作終於成功挽回了損失的精度，達到了 76.9%。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="內核大小的選擇">內核大小的選擇<a href="#內核大小的選擇" class="hash-link" aria-label="內核大小的選擇的直接連結" title="內核大小的選擇的直接連結">​</a></h3>
<p>在過去研究中，在小型網路中採用大核卷積（例如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi>x</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">5x5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">5</span><span class="mord mathnormal">x</span><span class="mord">5</span></span></span></span> 或 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mi>x</mi><mn>7</mn></mrow><annotation encoding="application/x-tex">7x7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">7</span><span class="mord mathnormal">x</span><span class="mord">7</span></span></span></span>），可以有效地提升模型的表達能力。</p>
<p>但是在行動裝置上，大核卷積沒辦法享受到加速的好處，一般硬體不會對大核卷積進行優化。</p>
<p>為了確保在行動裝置上的推理效率，作者在所有模組中優先考慮 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>x</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">3x3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span><span class="mord mathnormal">x</span><span class="mord">3</span></span></span></span> 卷積。</p>
<p>這個操作不會掉精度，同時可以將推理時間降到 0.89 ms。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sqeeze-and-excitation-的抉擇">Sqeeze-and-Excitation 的抉擇<a href="#sqeeze-and-excitation-的抉擇" class="hash-link" aria-label="Sqeeze-and-Excitation 的抉擇的直接連結" title="Sqeeze-and-Excitation 的抉擇的直接連結">​</a></h3>
<p>在過去的研究中，SE 模塊被廣泛應用在各種網路架構中，可以有效地提升模型的表達能力。</p>
<ul>
<li><strong>但是 SE 模塊真的很慢！</strong></li>
</ul>
<p>既  然想要享受到 SE 模塊的好處，又不想要影響速度，作者採用折衷的方式：</p>
<ul>
<li><strong>將 SE 模塊放「一點點」，只在每個階段的第 1、3、5...區塊中使用。</strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="討論的直接連結" title="討論的直接連結">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="imagenet-1k-上的性能">ImageNet-1K 上的性能<a href="#imagenet-1k-上的性能" class="hash-link" aria-label="ImageNet-1K 上的性能的直接連結" title="ImageNet-1K 上的性能的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="imagenet performance" src="/assets/images/img5-ae194471867f2a9dfd0e85170e6c7709.jpg" width="1224" height="752" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="imagenet performance 1" src="/assets/images/img1-d56a38964cb61547d5a9fa3cd09ca21e.jpg" width="1176" height="860" class="img_ev3q"></p>
<p>實驗使用了標準的 224×224 像素影像尺寸來進行訓練和測試，並且測試了多種模型的表現，包括使用了不同訓練週期數的模型（300 或 450 週期）。</p>
<p>這裡的主要實驗結果可以分為以下幾個要點：</p>
<ol>
<li>
<p><strong>模型性能比較</strong>：</p>
<ul>
<li>RepViT 模型在各種尺寸下均表現出色，與其他最先進的模型相比具有明顯的優勢。例如，在相似延遲條件下，RepViT-M0.9 的 top-1 準確率分別比 EfficientFormerV2-S0 和 FastViT-T8 高出 3.0% 和 2.0%。此外，RepViT-M1.1 也比 EfficientFormerV2-S1 高出 1.7%。</li>
</ul>
</li>
<li>
<p><strong>延遲與準確率</strong>：</p>
<ul>
<li>RepViT-M1.0 在 iPhone 12 上以 1.0 毫秒的延遲實現了超過 80% 的 top-1 準確率，這是輕量級模型的一項創新突破。</li>
<li>我們的最大模型 RepViT-M2.3 則在僅有 2.3 毫秒延遲的情況下，達到了 83.7% 的準確率，展示了優秀的性能和高效的延遲控制。</li>
</ul>
</li>
<li>
<p><strong>知識蒸餾的影響</strong>：</p>
<p>即使在沒有使用知識蒸餾的情況下（下表），RepViT 模型仍然在不同延遲水平下顯示了顯著的性能優勢。例如，在 1.0 毫秒的延遲下，RepViT-M1.0 的準確率比 MobileOne-S1 高出 2.7%。對於更大的模型，RepViT-M2.3 比 PoolFormer-S36 的準確率高出 1.1%，並且延遲減少了 34.3%（從 3.5 毫秒減少至 2.3 毫秒）。</p>
<p><img decoding="async" loading="lazy" alt="distillation" src="/assets/images/img6-4e505ad21550ff946aca6f334951df29.jpg" width="984" height="752" class="img_ev3q"></p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="消融實驗---重新參數化的結構">消融實驗 - 重新參數化的結構<a href="#消融實驗---重新參數化的結構" class="hash-link" aria-label="消融實驗 - 重新參數化的結構的直接連結" title="消融實驗 - 重新參數化的結構的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="ablation1" src="/assets/images/img7-3ef86732d8d62c6443aa4edcbf821671.jpg" width="1090" height="244" class="img_ev3q"></p>
<p>為了驗證 RepViT 區塊中重新參數化的結構的有效性，作者在訓練時刪除 SR 的多分支拓撲來對 ImageNet-1K 進行消融研究。</p>
<p>如上表所示，如果沒有重新參數化的結構，RepViT 的不同變體都會遭受一致的性能下降。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="消融實驗---se-層的影響">消融實驗 - SE 層的影響<a href="#消融實驗---se-層的影響" class="hash-link" aria-label="消融實驗 - SE 層的影響的直接連結" title="消融實驗 - SE 層的影響的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="ablation2" src="/assets/images/img8-92fb37d61c4d811a2f58b10cb0eed186.jpg" width="940" height="280" class="img_ev3q"></p>
<p>為了驗證在所有階段以跨區塊方式利用 SE 層的優勢，作者透過刪除所有 SE 層（即「w/o SE」）並在每個區塊中採用 SE 層（即，「每個區塊」）。</p>
<p>如上表所示，在區塊中交替採用 SE 層顯示了準確性和延遲之間更有利的權衡。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="結論的直接連結" title="結論的直接連結">​</a></h2>
<p>這個架構是第一個在 iPhone12 上，可以基於 1ms 內的推論速度下，在 ImageNet 1K 上精度達到 80% 以上的架構，非常有看點。</p>
<p>在這篇文章中，作者們對輕量級 CNN 的高效設計進行了深入的探索，並結合了輕量級 ViT 的創新架構，推出了全新的 RepViT 系列。</p>
<p>這款針對資源受限行動裝置設計的輕量級 CNN 模型，顯示出在各種視覺任務中的優越性能。RepViT 不僅在準確率上超越了當前最先進的輕量級 ViT 和 CNN，還在延遲方面展現了卓越的表現。</p>
<p>作者們期望 RepViT 成為輕量級模型設計的堅實基線，並激勵更多對於輕量級模型的研究和創新，推動該領域的進一步發展。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>RepViT 在使用上比較讓人卻步的地方是參數量偏大，但其實現在行動裝置的容量都很充裕，無奈仍然有大量的客戶非常在意模型 的大小，這也是我們不敢貿然使用這個架構的原因。</p><p>因此如果你的開發場景中不受模型大小的限制，只在乎精度和推論時間的話，RepViT 將是一個非常好的選擇。</p></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">2024年9月11日</time></b> <!-- -->更新</span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/reparameterization/vanillanet/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">[23.05] VanillaNet</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/category/segmentation-1"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">Segmentation (1)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#參考-vit-的觀點" class="table-of-contents__link toc-highlight">參考 ViT 的觀點</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#重新設計架構" class="table-of-contents__link toc-highlight">重新設計架構</a></li><li><a href="#重新參數化結構" class="table-of-contents__link toc-highlight">重新參數化結構</a></li><li><a href="#降低膨脹比並增加寬度" class="table-of-contents__link toc-highlight">降低膨脹比並增加寬度</a></li><li><a href="#stem-結構調整" class="table-of-contents__link toc-highlight">Stem 結構調整</a></li><li><a href="#降採樣結構調整" class="table-of-contents__link toc-highlight">降採樣結構調整</a></li><li><a href="#簡單分類器" class="table-of-contents__link toc-highlight">簡單分類器</a></li><li><a href="#階段比例調整" class="table-of-contents__link toc-highlight">階段比例調整</a></li><li><a href="#內核大小的選擇" class="table-of-contents__link toc-highlight">內核大小的選擇</a></li><li><a href="#sqeeze-and-excitation-的抉擇" class="table-of-contents__link toc-highlight">Sqeeze-and-Excitation 的抉擇</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#imagenet-1k-上的性能" class="table-of-contents__link toc-highlight">ImageNet-1K 上的性能</a></li><li><a href="#消融實驗---重新參數化的結構" class="table-of-contents__link toc-highlight">消融實驗 - 重新參數化的結構</a></li><li><a href="#消融實驗---se-層的影響" class="table-of-contents__link toc-highlight">消融實驗 - SE 層的影響</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">開源專案</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">論文筆記</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">部落格</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/terms-of-service">使用條款</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/privacy-policy">隱私政策</a><span class="footer__link-separator">·</span><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="footer__link-item">支持我們<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>