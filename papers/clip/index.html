<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-clip/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">[21.03] CLIP | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/clip/"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[21.03] CLIP | DOCSAID"><meta data-rh="true" name="description" content="打碎次元的屏障"><meta data-rh="true" property="og:description" content="打碎次元的屏障"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/clip/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/clip/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/clip/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/clip/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.1fe4c5ae.css">
<script src="/assets/js/runtime~main.df884d8a.js" defer="defer"></script>
<script src="/assets/js/main.0ac67d54.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" href="/papers/clip/"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/clip/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/clip/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/暗黑模式（當前為淺色模式）" aria-label="切換淺色/暗黑模式（當前為淺色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/alexnet/">[12.09] AlexNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/hourglass/">[16.03] Hourglass</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/densenet/">[16.08] DenseNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/fpn/">[16.12] FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/mobilenet-v1/">[17.04] MobileNet-V1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/nasnet/">[17.07] NASNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/shufflenet/">[17.07] ShuffleNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/senet/">[17.09] SENet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/mobilenet-v2/">[18.01] MobileNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/panet/">[18.03] PANet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/nasfpn/">[19.04] NAS-FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/mobilenet-v3/">[19.05] MobileNet-V3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/unetpp/">[19.12] UNet++</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/detr/">[20.05] DETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/vit/">[20.10] ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/deit/">[20.12] DeiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/papers/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/pp-lcnet/">[21.09] PP-LCNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/meter/">[21.11] METER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/convnext/">[22.01] ConvNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/frvt-distinguishing-twins/">[22.09] FRVT-Twins</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/tivc/">[23.09] TIVC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/mobilenet-v4/">[24.04] MobileNet-V4</a></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[21.03] CLIP</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><h1>[21.03] CLIP</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="打碎次元的屏障">打碎次元的屏障<a class="hash-link" aria-label="打碎次元的屏障的直接連結" title="打碎次元的屏障的直接連結" href="/papers/clip/#打碎次元的屏障">​</a></h2>
<p><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer"><strong>Learning Transferable Visual Models From Natural Language Supervision</strong></a></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>信息</div><div class="admonitionContent_BuS1"><p>以下內容由 ChatGPT-4 彙整，並經過人工校對編輯與補充說明。</p></div></div>
<hr>
<p>在我們日常生活中，訊息無所不在，它們有的以文字出現，有的以圖像呈現，還有的以聲音或影片等形式展現在我們面前。你或許曾在解釋一幅畫時，用文字描述它的美，或者在講述一個故事時，輔以圖片讓聽者更為生動感受。</p>
<p>這次我們要討論的主題是多模態學習（Multi-Model Machine Learning, MMML）。</p>
<p>多模態學習是一種利用多種資訊來源或模態（例如：文字、圖像和音訊）的機器學習方法。透過結合不同種類的資訊，多模態學習能夠捕捉更豐富的資料特徵，以期提高模型的準確性  和效能。這是一個相對較新但迅速發展的研究領域，並且已在多種應用如視覺問答、情感分析和自動字幕生成等方面取得了顯著的進步。</p>
<p>視覺語言（Vision-Language）是經常會和多模態學習一起出現的的名詞，它是多模態學習的一個子領域，它主要關注於視覺（如圖像或影片）和語言（如文字或語音）之間的交互作用。例如：視覺語言模型可以接受一個圖像和一個問題，然後生成一個關於該圖像的回答，或者給定一個圖像，生成描述它的文字。</p>
<p>真的要說的話，使用多模態學習能夠描述更廣泛的概念。</p>
<p>話說回來，多模態學習不是什麼新東西，它早在過去十多年就已經受到廣泛的關注和研究，只是近幾年隨著注意力機制所帶來的強勢效能提升，使得這個領域獲得了更多關愛的目光。</p>
<p>現在，我們來談談 CLIP，它的全名是 「Contrastive Language–Image Pre-training」，它的核心理念很簡單：就是透過大量的文字和圖像數據，讓模型學會如何透過文字理解圖像，或透過圖像來理解文字。</p>
<p>這種「雙向理解」不僅增強了模型的理解能力，也為多模態學習的發展打開了新的可能。通過在大量的資料上預訓練，CLIP 模型學會了將文字和圖像緊密關聯起來，使得它可以在看到一張圖片後，生成描述它的文字，或者在看到一段描述後，找到匹配的圖片。</p>
<p>在開始詳細介紹整個模型之前，我們先來看看 CLIP 發現了什麼問題，然後看看它規劃了什麼樣的解決方式。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結" href="/papers/clip/#定義問題">​</a></h2>
<p>現在試著描述以下這個圖：</p>
<p><img decoding="async" loading="lazy" alt="Coffee" src="/assets/images/coffee-740001a9c0f56f99363cdd405c10b4f7.jpg" width="1024" height="1024" class="img_ev3q"></p>
<ul>
<li>有杯咖啡…呃，杯子底下有一本書…它們都放在桌子上？</li>
<li>這是早晨的咖啡店一個角落？</li>
<li>咖啡色的桌椅，還有咖啡色的咖啡？（<del>喂！認真點</del>）</li>
<li>……</li>
</ul>
<p>事實上，這張圖是使用這一段描述生成的影像：</p>
<blockquote>
<p><em>在一個寧靜的清晨，陽光透過窗簾的縫隙溫柔地灑在簡約木質的桌上。 桌上放著一杯剛剛沖好的咖啡，咖啡的香氣與陽光交織在空氣中，讓人感受到了新的一天的溫暖和希望。 杯子的影子在桌上拉出一道長長的倒影，與窗邊的綠植形成了美麗的畫面。 咖啡的表面輕輕盪著，似乎在訴說著早晨的寧靜和生活的美好。 在桌旁，一本敞開的書靜靜地躺著，等待著主人的翻閱。 在這個寧靜的早晨，咖啡、陽光、綠植和書籍共同建構了一個溫馨而寧靜的場景，它們似乎在訴說著生活的簡單和美麗。</em></p>
</blockquote>
<p>如果你也在試著描述這張圖，就應該可以很清楚地感覺到現在，此時此刻，在學術界及工程界所使用的 ImageNet 資料集，我們引以為傲的兩萬多個類別，上千萬張的圖像，有多麽地蒼白無力。</p>
<ul>
<li>因為存在太多的方式，可以用來描述同一張圖像。</li>
</ul>
<p>反觀 ImageNet，一張圖片可能只有一組層級式的類別，或者有限的標籤來描述它的內容。然而，人類的語言和感知能力卻可以從無限多的角度來解釋和理解這張圖像。比如，透過色彩、形狀、情感和故事情節等，都能呈現出豐富多彩的描述。這就是單模態學習和多模態學習的最大區別：前者通常只能從單一角度解析資訊，而後者能夠將不同類型的資訊融合在一起，提供更為豐富和多元的解釋。</p>
<p>在 ImageNet 的單標籤類別中，經常會忽略掉很多其他有意義的資訊，例如：物件之間的交互關係、情境背景、或是圖像所引起的感受等。而 CLIP 這篇論文，目的就是試圖打破這些限制，通過糅合來自不同來源、不同類型的資訊，如文字、圖像等，來豐富模型的理解和表述能力，使其更接近人類的感知和理解水平。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結" href="/papers/clip/#解決問題">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="clip-模型架構">CLIP 模型架構<a class="hash-link" aria-label="CLIP 模型架構的直接連結" title="CLIP 模型架構的直接連結" href="/papers/clip/#clip-模型架構">​</a></h3>
<p><img decoding="async" loading="lazy" alt="CLIP Architecture" src="/assets/images/arch_clip-ef4a34c3ec1c19ee45e598ca12ff5459.jpg" width="1726" height="622" class="img_ev3q"></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="預訓練過程">預訓練過程<a class="hash-link" aria-label="預訓練過程的直接連結" title="預訓練過程的直接連結" href="/papers/clip/#預訓練過程">​</a></h3>
<p>假設有一組圖像-文字對，其中一對可能是一張狗的圖片和文字「一隻可愛的小狗」。 在一個訓練批次中，CLIP 會接收到多個這樣的對。 圖像編碼器可能透過 ResNet 或 ViT 來處理圖像，獲取圖像特徵，而文本編碼器可能透過 Transformer 來處理文本，獲取文字特徵。</p>
<p>然後，模型會比較這些特徵，以確保正確配對的圖像和文字（例如：狗的圖像和「一隻可愛的小狗」的文字）之間的餘弦相似度最大，而錯誤配對的圖像和文字（ 例如：狗的圖像和「一個蘋果」的文字）之間的餘弦相似度最小。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="零樣本推論zero-shot">零樣本推論（Zero-Shot）<a class="hash-link" aria-label="零樣本推論（Zero-Shot）的直接連結" title="零樣本推論（Zero-Shot）的直接連結" href="/papers/clip/#零樣本推論zero-shot">​</a></h3>
<p>假設需要用 CLIP 來完成一個關於果物分類的任務，但沒有額外的標籤資料。我們可以創造一組自然語言提示，例如： “A photo of an apple”, “A photo of a banana”, “A photo of a cherry” 等。當我們有一張待分類的果物圖片時，我們可以將它和這些提示的文本特徵進行比較，找出餘弦相似度最高的文本，從而推斷出圖像的類別。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="自然語言監督">自然語言監督<a class="hash-link" aria-label="自然語言監督的直接連結" title="自然語言監督的直接連結" href="/papers/clip/#自然語言監督">​</a></h3>
<p>利用自然語言作為監督訊號，CLIP 能夠輕鬆擴展其訓練資料集。 傳統的監督學習需要大量的標籤工作，而透過利用自然語言，人們只需要收集大量的圖像和文字配對。 這種方法不僅能學習到多模態的表示，還能將這些表示與自然語言連結起來，進而實現靈活的零樣本遷移學習。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="大規模資料集wit">大規模資料集：WIT<a class="hash-link" aria-label="大規模資料集：WIT的直接連結" title="大規模資料集：WIT的直接連結" href="/papers/clip/#大規模資料集wit">​</a></h3>
<p>CLIP 模型能夠做出強勁的表現，很大一部分是因為它使用了大規模的資料集。</p>
<p>剛開始時，研究者主要使用三個資料集：MS-COCO、Visual Genome 和 YFCC100M。 但很快就發現，這些資料集的規模對現代的要求來說太小了。 例如：MS-COCO 和 Visual Genome 雖然是高品質的資料集，但只有大約 100,000 張訓練照片，與需要處理高達 35 億張 Instagram 照片的其他電腦視覺系統相比，簡直是杯水車薪。 而 YFCC100M 雖然擁有 1 億張照片，但許多圖片的標題和描述資訊品質非常低，如自動產生的檔案名稱或相機曝光設定的描述，這對於模型的訓練幫助不大。</p>
<p>因此，研究者決定創建一個新的大規模資料集，命名為 WIT（WebImageText）。 這個資料集包括 4 億對圖像和文本，所有的資料都是從網路的公開資源中收集而來。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>以一個簡單的例子來說，假設我們在網路上找到了一張美麗的夕陽圖片，圖片下方的文字描述說「紅色的夕陽映照在寧靜的湖面上」，這樣的圖像和文字對就會被收集到 WIT 資料集中。</p></div></div>
<p>研究者透過 500,000 個不同的搜尋查詢，廣泛收集了不同主題和類別的圖像和文字對，每個查詢最多有 20,000 個對應的圖像和文字對，以確保資料集的多樣性。</p>
<p>創建 WIT 資料集的目的，不僅是為了解決原有資料集規模小、品質低的問題，也想讓 CLIP 模型能夠從中學到更多豐富多彩的視覺和語言知識。 WIT 資料集的總字數與用於訓練 GPT-2 模型的 WebText 資料集相似，顯示它的規模非常龐大。透過這個豐富的資料集，CLIP 模型可以在大量的實例中學習到圖像和文字之間的相關性，從而在圖像和文  字的理解上達到了新的高度。</p>
<p>最後不得不感嘆，OpenAI 的雄厚資金實在是讓人羨慕，有了充足的資源，真是可以大膽地推進研究，創造出許多先進和實用的技術和模型。</p>
<p>這邊附上資料集下載位置：<a href="https://github.com/google-research-datasets/wit#wit--wikipedia-based-image-text-dataset" target="_blank" rel="noopener noreferrer">WIT : Wikipedia-based Image Text Dataset</a></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="訓練模型細節">訓練模型細節<a class="hash-link" aria-label="訓練模型細節的直接連結" title="訓練模型細節的直接連結" href="/papers/clip/#訓練模型細節">​</a></h3>
<p>在模型訓練的細節方面，作者們選擇了一系列的 5 個 ResNet 和 3 個 Vision Transformer 模型進行訓練。 在 ResNet 系列中，他們訓練了一個 ResNet-50 和一個 ResNet-101，隨後又以 ResNet-50 為基礎，按照 EfficientNet 的風格進行模型縮放，創建了三個不同規模的模型，分別標識為 RN50x4、RN50x16 和 RN50x64。 這三個模型的計算量是 ResNet-50 的 4 倍、16 倍和 64 倍。</p>
<p>對於 ViT 模型，作者們訓練了 ViT-B/32、ViT-B/16 和 ViT-L/14 三種不同規模的模型。 每個模型都訓練了 32 個 epoch，使用的最佳化器是 Adam，並且應用了解耦權重衰減正則化和餘弦退火學習率策略，以確保訓練過程的穩定性和效率。</p>
<p>初始超參數的設定是透過在基線 ResNet-50 模型上結合網格搜尋、隨機搜尋和手動調整的方式進行的。 隨後，由於計算資源的限制，這些超參數被啟發式地調整以適應更大的模型。</p>
<p>為了確保訓練的穩定，可學習的溫度參數 τ 被初始化為 0.07，並透過剪裁措施防止 logits 的縮放超過 100。 為了加速訓練和節省內存，他們採用了非常大的小批量大小 32,768，同時應用了混合精度、梯度檢查點、半精度 Adam 統計和半精度隨機舍入文本編碼器權重等技術。</p>
<p>在硬體方面，訓練最大的 ResNet 模型 RN50x64 需要在 592 個 V100 GPU 上運行 18 天，而訓練最大的 Vision Transformer 模型則需要在 256 個 V100 GPU 上運行 12 天。 特別地，為了提升 ViT-L/14 模型的效能，他們在更高的 336 像素解析度上額外預訓練了一個 epoch，將此模型標識為 ViT-L/14@336px。</p>
<p>下面的表格是一個簡單的範例，用來說明作者在訓練不同模型時的配置和需求：</p>
<table><thead><tr><th>模型名稱</th><th>基礎模型</th><th>計算量倍數</th><th>GPU 數量</th><th>訓練天數</th><th>特別配置</th></tr></thead><tbody><tr><td>ResNet-50</td><td>ResNet-50</td><td>1x</td><td>(未說明)</td><td>(未說明)</td><td>無</td></tr><tr><td>ResNet-101</td><td>ResNet-101</td><td>1x</td><td>(未說明)</td><td>(未說明)</td><td>無</td></tr><tr><td>RN50x4</td><td>ResNet-50</td><td>4x</td><td>(未說明)</td><td>(未說明)</td><td>無</td></tr><tr><td>RN50x16</td><td>ResNet-50</td><td>16x</td><td>(未說明)</td><td>(未說明)</td><td>無</td></tr><tr><td>RN50x64</td><td>ResNet-50</td><td>64x</td><td>592</td><td>18</td><td>無</td></tr><tr><td>ViT-B/32</td><td>ViT-B</td><td>(未說明)</td><td>(未說明)</td><td>(未說明)</td><td>無</td></tr><tr><td>ViT-B/16</td><td>ViT-B</td><td>(未說明)</td><td>(未說明)</td><td>(未說明)</td><td>無</td></tr><tr><td>ViT-L/14</td><td>ViT-L</td><td>(未說明)</td><td>256</td><td>12</td><td>額外預訓練 1 個 epoch@336px</td></tr></tbody></table>
<p>在這個表格中列出了模型名稱、基礎模型、計算量倍數、使用的 GPU 數量、訓練天數和特別配置。 需要注意的是，「未說明」是指原論文中沒有明確提供具體的資訊。</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>備註</div><div class="admonitionContent_BuS1"><p>那 592 個 V100 的這種規模簡直不可理諭，讓人非常羨慕。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型推論方式">模型推論方式<a class="hash-link" aria-label="模型推論方式的直接連結" title="模型推論方式的直接連結" href="/papers/clip/#模型推論方式">​</a></h3>
<p>在影像分類的領域中，資料集的標籤設計往往反映了一種事後思考（afterthought）的模式。在大多數標準影像分類資料集中，類別的命名或描述常被視為次要考慮。例如：有許多資料集僅使用數位 ID 來標注影像，並附帶一個檔案以將這些 ID 對映回其英文名稱。然而，有些資料集，如：Flowers102 和 GTSRB，它們的發布版本中似乎並未包含這樣的映射，這在某程度上嚴重限制了零樣本轉移的可能性。</p>
<p>作者指出，在許多資料集中，類別標籤的選擇可能相當隨意，並未考慮到零樣本轉移成功實施所需的描述資訊。舉例來說，當只有類別名稱提供給 CLIP 文本編碼器時，由於缺乏上下文，編碼器無法區分單字的不同含義。在某些情況下，同一單字的不同含義可能被分類為不同類別，例如：在 ImageNet 資料集中，「起重機」一詞，可能指的是建築起重機或飛行起重機。另一個例子是在 Oxford-IIIT Pet 資料集中，「boxer」一詞，在此上下文中明顯指的是一種狗品種，但對於缺乏  上下文的文本編碼器來說，可能會解讀為指某種運動員。</p>
<p>為了解決這些問題，作者嘗試利用提示模板，例如：「『標籤』的照片」，來幫助提供有關圖像內容的文本信息。通過這種方式，相比於僅使用標籤文本，他們在 ImageNet 資料集上的準確率得到了 1.3% 的提升，顯示出這種方法的有效性。</p>
<p>此外，作者的研究也揭示了透過客製化提示文本為每個任務，可以顯著提升零樣本表現。他們提供了一些具體的範例來證明這一點。例如：在 Oxford-IIIT Pets 資料集上，使用「『標籤』的照片，一種寵物」的提示能夠顯著提高效能。同樣地，在 Food101 資料集上指定一種食物，或在 FGVC Aircraft 資料集上指定一架飛機，也都證明了提示的效用。</p>
<p>最後，透過集成多個零樣本分類器，作者進一步探索了提高分類效能的可能。他們在嵌入空間而不是機率空間上建立系集預測，並通過使用不同的上下文提示，例如：「大『標籤』的照片」和「小『標籤』的照片」，來計算分類。這種系集預測方法不僅提高了性能，而且通過在 ImageNet 資料集上集合 80 個不同的上下文提示，實現了額外的 3.5% 的效能提升，顯示了提示工程和系集預測方法在提高 ImageNet 資料集上的準確性方面具有近 5% 的潛力。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a class="hash-link" aria-label="討論的直接連結" title="討論的直接連結" href="/papers/clip/#討論">​</a></h2>
<p>本章節中，作者將針對 Zero-Shot 學習的效能與表現提出一系列令人思考的問題。Zero-Shot 學習是一種無需事先標記資料的學習方法，它利用已有的知識，通過抽象和推理來理解未曾遇到的情況。</p>
<p>作者首先擬探討的是 Zero-Shot 學習是否真的達到了預期的效果？是否能在沒有先前示例的情況下，準確辨識新的類別？緊接著，他們會考察模型的抽象表徵能力，看它是否能妥善捕捉和表述資料的核心特質。</p>
<p>在數據日益豐富的今天，一個模型是否能跨資料域的運作，也成為了作者關切的焦點。比如，一個主要訓練在動物圖像上的模型，能否轉而妥善處理機械零件的圖像識別？同時，他們也會討論模型的表現是否能媲美人類的判斷？</p>
<p>自建資料集的準確性和完整性是另一個要討論的重點。作者擔心是否在創建資料集的過程中，有些「偷看答案」的情況發生，比如某些類別的標籤是否被不當地標注或遺漏。</p>
<p>最後，資料中潛藏的惡意也是不容忽視的。如果資料中包含了不正確或有害的資訊，那對模型的影響又會是怎樣的呢？是不是會引導模型走向一個錯誤的方向？</p>
<p>通過以上的問題設定，作者希望能對 Zero-Shot 學習進行一番全面而深入的討論。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>附帶一提，由於原論文在本章節中，有非常大量的圖表。考量到在文章中穿插大量圖表會讓讀者看了頭暈，因此我大多只講結論，圖表僅截取部分，其他未擷取的圖表，我們會特別標注在論文中的位置。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="zero-shot-有符合預期嗎">Zero-Shot 有符合預期嗎？<a class="hash-link" aria-label="Zero-Shot 有符合預期嗎？的直接連結" title="Zero-Shot 有符合預期嗎？的直接連結" href="/papers/clip/#zero-shot-有符合預期嗎">​</a></h3>
<p>CLIP 在某些範疇確實滿足了預期，但也顯露出一些需要改進的地方：</p>
<ol>
<li>
<p>特定資料集的優異表現：</p>
<p>在一些特定資料集上，例如：那些底層特徵表達相對明確的資料集，零樣本 CLIP 的性能可以匹敵於或甚至超越完全監督分類器。這突顯了在某些情況下，零樣本學習可能提供了一個高效的替代方案。</p>
</li>
<li>
<p>性能匹配：</p>
<p>CLIP 與少樣本邏輯回歸分類器的對比，能突顯了 CLIP 的強大性能。例如：若在一個涵蓋多種動物的圖像分類任務中，零樣本 CLIP 能夠通過文字描述來識別未見過的類別（例如：未在訓練數據中見過的「斑馬」），而少樣本分類器可能需要更多的訓練數據來達到相似的性能。</p>
</li>
<li>
<p>零樣本轉換效率：</p>
<p>不同資料集的零樣本轉換效率差異反映了資料集的困難程度和多樣性。例如：在一個簡單的資料集中，零樣本轉換可能只需少量的樣本，而在一個複雜多變的資料集中，它可能需要更多的樣本來達到相同的性能。</p>
</li>
<li>
<p>與完全監督分類器比較：</p>
<p>在許多情況下，完全監督分類器的表現超越了零樣本 CLIP，顯示在某些傳統的分類任務中，完全監督學習可能仍然是一個更穩定的選擇。</p>
</li>
<li>
<p>效能預測的線性趨勢：</p>
<p>完全監督與零樣本表現之間的正相關性可能指示了有潛力進一步改善零樣本學習的性能。例如：通過增加完全監督的資料量或改進底層特徵的學習，可能會帶來零樣本學習性能的提升。</p>
</li>
<li>
<p>縮放模式的探討：</p>
<p>CLIP 的零樣  本效能的對數線性縮放趨勢可能受到多種因素的影響，例如：模型大小、訓練數據的多樣性等。通過進一步的分析，可能能夠找到影響這些趨勢的具體因素，並提供改善零樣本學習性能的方法。</p>
</li>
</ol>
<p>零樣本 CLIP 的表現在一定程度上符合預期，尤其是在與少樣本邏輯迴歸的比較中表現相匹配，並且在某些資料集上能夠達到完全監督的效能。 然而，與完全監督分類器的比較以及不同資料集間的表現差異表明，CLIP 的零樣本遷移能力仍有很大的提升空間，也可能存在一些未被充分探討的影響性能的因素。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="表徵能力好嗎">表徵能力好嗎？<a class="hash-link" aria-label="表徵能力好嗎？的直接連結" title="表徵能力好嗎？的直接連結" href="/papers/clip/#表徵能力好嗎">​</a></h3>
<p>從各方面來看，CLIP 模型展示了它在表徵學習和泛化能力上的強大潛力。通過深入分析以下幾個方面，可以更明確地理解 CLIP 模型的優勢和可能的局限性：</p>
<ol>
<li>
<p>表徵質量的評估：</p>
<p>CLIP 模型能夠上擬合線性分類器，並在多個資料集上測試它的性能，有效地展示了其表徵學習的能力。然而，該模型是否真的能在未見過的資料集或極端情況下保持相同的性能，仍需進一步探索。</p>
</li>
<li>
<p>線性分類器的效用：</p>
<p>通過使用線性分類器進行評估，模型在預訓練階段獲得的通用和泛化表徵得到了突顯。這提供了有用的反饋，但也暴露出可能的缺陷，例如：如果模型的表徵過於依賴某些特定的特徵，可能會影響其在不同任務上的適應性。</p>
</li>
<li>
<p>模型的規模和效能：</p>
<p>CLIP 的不同模型規模，如 ResNet-50×64 和 CLIP ViT，表明了模型規模與表徵能力和計算效率之間的關聯。然而，大規模模型可能需要更多的計算  資源，這可能會限制它在低資源設備上的應用。</p>
</li>
<li>
<p>任務多樣性：</p>
<p>CLIP 在多個任務如地理定位、光學字符識別（OCR）、臉部情感識別和動作識別等方面都有不錯的表現。例如：在 OCR 任務上，CLIP 能夠有效地識別不同字型和颜色的文本。但是，是否有更專業的模型在特定任務上表現更佳，值得深究。</p>
</li>
<li>
<p>更廣泛的資料集評估：</p>
<p>27 個不同資料集上的評估展示了 CLIP 在表徵通用性和計算效率方面的強大，但可能還存在數據偏差的問題，需要進一步驗證其在多元和不同分布的數據集上的表現。</p>
</li>
<li>
<p>自我監督系統的表現：</p>
<p>與 SimCLRv2 等自我監督系統相比，CLIP 展現出了相對更好的表現。然而，自我監督學習的效率和效果是否能夠持續提高，尚需時間和多方面的測試來證實。</p>
</li>
<li>
<p>與先進模型的比較：</p>
<p>與 Noisy Student EfficientNet-L2 等先進模型相比，CLIP 在多數資料集上展現出優勢，但在某些特定任務或數據集上可能仍存在不足。</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="能夠跨資料域嗎">能夠跨資料域嗎？<a class="hash-link" aria-label="能夠跨資料域嗎？的直接連結" title="能夠跨資料域嗎？的直接連結" href="/papers/clip/#能夠跨資料域嗎">​</a></h3>
<p><img decoding="async" loading="lazy" alt="CLIP 跨資料域表現" src="/assets/images/clip_demo-22093a2af3b3c9b43d2dcf4dde81e7a0.jpg" width="1770" height="698" class="img_ev3q"></p>
<p>在探索深度學習模型的性能與穩健性時，跨資料域（cross-domain）的能力成為一個重要的衡量標準。理想的情況下，一個模型應該能夠在不同的資料集和分佈上維持良好的性能。然而，如上文所述，很多基於 ImageNet 訓練的模型在面對新的或未見過的資料分佈時，性能往往會大幅下降。</p>
<p>例如：經  過對不同的自然分佈變化資料集（ImageNetV2, ImageNet Sketch, Youtube-BB 等）的評估後，發現傳統的 ImageNet 模型（如 ResNet-101）在這些新資料集上的準確率遠低於在原始 ImageNet 驗證集上的準確率。這表明了這些模型的跨資料域性能存在很大的缺陷，並且容易受到資料分佈偏移的影響。</p>
<p>零樣本 CLIP 模型在這些新的資料集上的表現明顯優於傳統的 ImageNet 模型，並且有效地縮小了 ImageNet 準確率與分佈偏移下準確率之間的差距。根據分析，零樣本分類器在適應 ImageNet 分佈後展現了顯著的效能提升，準確率從原始的水平提升了 9.2%。 然而，一旦面臨分佈偏移的情境，其準確率的表現卻出現了輕微的下滑。</p>
<p>這引出了一個重要的問題：如何解釋 CLIP 在 ImageNet 分佈上的準確率提升，以及其在面臨分佈偏移時準確率未見明顯提升的現象？ 是否這些收益主要來自於「利用虛假相關性」？ 此外，這種現像是否是 CLIP、ImageNet 資料集和分佈變化的某種特定組合所獨有的，還是它反映了更普遍的現象？ 它是否同樣適用於端對端微調和線性分類器？</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="跟人類能比嗎">跟人類能比嗎？<a class="hash-link" aria-label="跟人類能比嗎？的直接連結" title="跟人類能比嗎？的直接連結" href="/papers/clip/#跟人類能比嗎">​</a></h3>
<p>為了了解 CLIP 模型與人類表現的差異，研究人員在特定的任務上進行了比較。他們選擇了 Oxford IIT Pets 資料集，要求 5 位參與者在不看到任何品種示例的情況下，對 3669 張圖像中的每一張進行貓或狗品種的標籤分配。在另一次實驗中，參與者得到每個品種的一個或兩個圖像示例作為參考。通過這種方式，研究人員意圖理解人類如何在給定一些示例後改善他們的分類準確度。</p>
<p>結果顯示，在另一個  資料集 STL-10 中，人類能夠達到高達 94% 的準確率，而在某些子集中準確率甚至能達到 97-100%，展示了人類在這些任務上的強大能力。當每個類別僅有一個示例時，人類的平均表現能從 54% 提升到 76%，額外的示例並沒有帶來太多的幫助，但它們對於那些令人困惑的圖像卻非常有幫助。這指出人類很擅長於識別他們的不確定性，並能根據新的信息來調整他們的判斷。</p>
<p>然而，儘管 CLIP 模型在零樣本學習方面展現出潛力，並在自然分布偏移測試中表現不錯，但它與人類在少量示例學習方面的表現仍存在顯著的差異。研究者指出，人類能夠有效地利用少量的示例來改善他們的分類準確度，而 CLIP 模型在這方面的表現不足。尤其是在利用先驗知識方面，人類明顯優於 CLIP 模型。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型有沒有偷看答案">模型有沒有偷看答案？<a class="hash-link" aria-label="模型有沒有偷看答案？的直接連結" title="模型有沒有偷看答案？的直接連結" href="/papers/clip/#模型有沒有偷看答案">​</a></h3>
<p>在對大規模網路資料集進行預訓練的過程中，一個不可忽視的問題是與下游評估資料集的無意重疊。這樣的重疊可能會導致模型在評估時表現異常良好，但這實際上並不反映其真正的泛化能力。</p>
<p>如同在研究中呈現的，有些資料集由於其獨特性或合成的特點，並未顯示出重疊的情況，例如：MNIST、CLEVR 和 GTSRB 等。然而，也有資料集顯示出明顯的重疊情況，特別是那些由公共資料集如 YFCC100M 構建的資料集，如 Country211，它的重疊率高達 21.5%。</p>
<p>分析揭示，即便如此，這種重疊對準確率的提高效果十分有限，因為在 Country211 的準確率僅提高了 0.2%。這可能是因為與下游任務有關的特定信息並未被重複項目的訓練文本所覆 蓋。此外，在某些情況下，例如：在 Kinetics-700 資料集上，重疊項目實際上是無關緊要的黑色過渡幀，導致重疊子集的準確度下降了 20%。</p>
<p>重疊分析是一個微妙而複雜的問題，不僅需要考慮資料的重複程度，還需注意底層資料分布的變化和其他潛在的混淆因素。例如：在 CIFAR-100 資料集上，由於影像解析度很低，一些重礇的影像實際上是對小物件如鳥類或飛機的誤報，這可能會導致類別分布或重疊難度的變化，進而影響準確度的變動。</p>
<p>這些觀察與先前的大規模預訓練工作中的重疊分析結果一致。例如：Mahajan 等人（2018）和 Kolesnikov 等人（2019）的研究都指出了類似的重疊率和整體性能變化的微小性。重要的是，這些研究還比較了不同的重疊資料刪除策略，並發現作者選擇的方法與替代策略之間幾乎沒有差異。這再次強調了重複分析在瞭解和改善模型泛化性能中的重要性，以及在訓練前識別和處理重疊的重要性。</p>
<p>作者在論文中指出，雖然探測器在代理訓練任務上表現出接近 100% 的準確率，且通過手動檢查和閾值調整達到非常高的精度，但由於無法輕易檢查 4 億個示例的召回率，其完美程度仍有所限制。此外，底層資料分佈在重疊子集和乾淨子集間的變化也是一個可能影響分析的混雜因素。例如：Kinetics-700 資料集中的許多「重疊」實際上是黑色過渡幀，這導致了在重疊部分的準確度下降了 20%。</p>
<p>作者還提到，可能存在更為微妙的分佈變化，例如：在 CIFAR-100 資料集上，由於影像解析度非常低，許多重複影像可能是鳥類或飛機等小物體的誤報。準確度的變化可能由類別分佈或重複難度的變化所導致，但不幸的是，這些變化也可能掩蓋了過度擬合的影響。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="有什麼限制"> 有什麼限制？<a class="hash-link" aria-label="有什麼限制？的直接連結" title="有什麼限制？的直接連結" href="/papers/clip/#有什麼限制">​</a></h3>
<p>CLIP 模型雖然展現了一些令人鼓舞的能力，但仍存在許多嚴重的限制：</p>
<ol>
<li>
<p>效能限制</p>
<p>在一些監督訓練分割的資料集上，零樣本 CLIP 的表現能與基於 ResNet-50 特徵的線性分類器持平。然而，在大多數資料集上，其效能仍然遠遠低於現有的技術水準。即便通過擴展的方式有所改善，但要達到最先進的效能，CLIP 的計算量需要增加約 1000 倍，而現有的硬體條件使得這種擴展變得不切實際。</p>
</li>
<li>
<p>任務學習與遷移能力</p>
<p>CLIP 在某些特定任務上的表現仍然不佳，例如：在細粒度分類（如汽車型號、花卉種類和飛機變體）以及處理更抽象和系統化的任務（如計算圖像中的對象數量）上表現不足。</p>
</li>
<li>
<p>泛化能力</p>
<p>儘管 CLIP 在多種自然影像分佈上表現良好，但當面臨真正不符合分佈的數據時，其零樣本性能會大幅度下降。例如：CLIP 在 MNIST 手寫數字識別上的準確率僅達 88%，而在原始像素上的邏輯回歸的基線效能卻優於零樣本 CLIP。這不僅突顯出 CLIP 在解決深度學習模型泛化脆弱的根本問題上幾乎無能為力，也顯示了僅依賴龐大多樣化的資料集進行訓練，試圖使所有資料都能有效地分佈，是一個天真的假設，正如 MNIST 所證明的那樣，這個假設很容易被違反。</p>
</li>
<li>
<p>數據與計算效率</p>
<p>CLIP 模型需要大量的資料和運算資源，其訓練過程對於現有硬體來說過於龐大，如需數百年才能完成全部訓練週期。雖然現有的自我監督和自我訓練方法已顯示出對於提高資料效率的潛力，結合這些方法可能會為 CLIP 的進一步發展提供方向。</p>
</li>
<li>
<p>評估與最佳化</p>
<ul>
<li>本文的方法有幾個明顯的限制。 儘管專注於零樣本模型，但研究者還是反覆詢問完整驗證集的效能，以指導 CLIP 的開發。這些驗證集通常有數千個範例，這對於真正的零樣本場景來說是不切實際的。 半監督學習領域也提出了類似的擔憂（Oliver 等人，2018）。</li>
<li>當從零樣本設定過渡到少樣本設定時，性能反直覺地下降，顯示 CLIP 在少樣本學習方面的最佳化仍需改進。</li>
</ul>
</li>
<li>
<p>社會偏見與道德考量</p>
<p>CLIP 模型透過網路圖像與文字配對進行訓練，由於這些資料未經過濾和策劃，可能導致模型學習到社會偏見。</p>
</li>
</ol>
<p>未來的研究需要探索如何結合 CLIP 強大的零樣本性能與高效的少樣本學習方法，以實現更為全面和可靠的視覺和語言理解模型。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="來自資料的惡意">來自資料的惡意<a class="hash-link" aria-label="來自資料的惡意的直接連結" title="來自資料的惡意的直接連結" href="/papers/clip/#來自資料的惡意">​</a></h3>
<ul>
<li>
<p>偏見（Bias）</p>
<p>在這篇文章中，作者深入探討了資料可能帶來的偏見和惡意，並藉由多方面的分析和例證，展現了這些問題可能對社會和個人造成的影響。透過探討不同來源的資料以及其潛在的偏見，作者呼籲讀者和相關的政策制定者更加關注和理解資料的重要性和可能潛藏的風險。</p>
<p>首先，作者指出由於資料的收集和處理過程可能受到不同程度的主觀性影響，從而使得這些資料在一定程度上失去了客觀性。例如：資料可能因為收集者的主觀偏見而被歪曲，或是因為特定的資料處理方式而忽略了某些重要的信息。</p>
<p>接著，作者通過一系列的例子，說明了這些資料偏見是如何在各種不同的情境下產生負面效應的。他提到了種族偏見、性別歧視和經濟不平等等問題，並解釋了這些問題是如何與資料的偏見相互影響，從而造成了更加嚴重的社會問題。</p>
<p>作者也不忘提醒讀者，即使資料本身是無辜的，但如果處理不當，還是可能會帶來嚴重的後果。他強調，要想避免這些問題，就必須建立完善的資料治理機制，並且需要資料的使用者和政策制定者具有足夠的資料素養，以便能夠理解和應對資料可能帶來的風險。</p>
<p>最後，他呼籲所有的資料相關人士和機構，應該對資料的處理和使用持更加負責任的態度，並努力創建一個公平、透明並且可持續的資料生態系統，以促進社會的公正和進步。</p>
</li>
<li>
<p>監視（Surveillance）</p>
<p>在這篇文章中，作者深入探討了日益通用化的電腦視覺模型在監視領域的應用與影響，尤其關注了模型在閉路電視攝影機影像分類和零樣本名人識別方面的表現。 文章的目的在於，透過分析模型的效能和限制，為研究界提供有關這些模型未來潛在影響的更多認識，以協助制定圍繞此類系統的規範和檢查。</p>
<p>作者首先測試了模型在監控攝影機（例如：閉路電視攝影機）捕獲的低解析度影像上的效能。 利用了 VIRAT 數據集和 Varadarajan &amp; Odobez 提取的數據，作者在自建的粗粒度和細粒度分類上測試了 515 個由 12 個不同視頻序列捕獲的監控圖像。 粗分類主要是識別圖像的主要主題，例如：確定圖像是空的停車場、學校校園等，而細粒度分類則需模型在兩個選項中選擇，以確定是否可以識別圖像中的較小特徵，例如：站在角落的人。 在粗分類的任務中，模型表現出了 91.8% 的 Top-1 精確率，然而，在細粒度檢測和「壓力測試」中，模型的表現顯著下降，準確率降至 51.1%，錯誤選擇「接近 」答案的機率為 40.7%。</p>
<p>進一步地，作者也探討了零樣本名人辨識的效  能。 透過使用 CelebA 資料集測試 CLIP 的零樣本效能，作者旨在僅使用預先訓練的公開資料來評估模型的身份檢測效能。 結果顯示，在「野外」8k 名人圖像的任務中，模型在 100 個可能的類別中具有 59.2% 的 Top-1 準確率。 但當類別規模增加到 1,000 個名人時，準確率下降至 43.3%。 雖然與 Google 的名人辨識等生產級模型相比，這種性能並不具有競爭力，但仍然顯示了在沒有任何額外特定於任務的數據集的情況下，基於零樣本識別功能完成的分析的可行性和模型的相對強大性能。</p>
<p>最終，作者指出，儘管 CLIP 在零樣本功能方面具有顯著優勢，使其對於數據相對較少的任務具有吸引力，但對於許多按需監視任務（例如：人臉辨識），存在大型數據集和高效能監督模型，因此，CLIP 對此類用途的相對吸引力較低。 同時，由於 CLIP 不適用於常見的監視相關任務，例如：物件偵測和語義分割，其在某些監視任務中的應用是有限的。 然而，由於 CLIP 減輕了對訓練資料的需求，它確實解鎖了某些可用性方面，使得在沒有專門定制的模型或資料集的情況下，能夠實現定制的利基監視用例，降低了構建此類應用 程序的技能要求。</p>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a class="hash-link" aria-label="結論的直接連結" title="結論的直接連結" href="/papers/clip/#結論">​</a></h2>
<p>本研究通過探討將自然語言處理（NLP）中的大規模預訓練成功轉移至電腦視覺領域（CV），展示了 CLIP 模型在多任務學習和零樣本轉移上的強大能力。</p>
<p>最後，整理一下這篇論文提到的的貢獻和未來展望：</p>
<ol>
<li>
<p>貢獻：</p>
<ul>
<li>多模態學習：CLIP 模型的創新之處在於其能夠同時處理圖像和文本信息，通過大規模預訓練和自然語言提示，使得模型能夠  實現對多種不同任務的學習和適應。</li>
<li>零樣本轉移學習：通過自然語言提示，CLIP 模型顯示了強大的零樣本轉移能力，即使在沒有特定任務訓練數據的情況下，也能實現良好的性能。</li>
<li>可擴展性：在足夠的規模下，CLIP 模型的性能可以與特定任務的監督模型相媲美，證明了其訓練和運用的可擴展性。</li>
</ul>
</li>
<li>
<p>未來展望：</p>
<ul>
<li>解釋性不足：雖然 CLIP 模型能夠融合文本和圖像信息，並能通過自然語言描述生成某種解釋，但仍存在一定的解釋性局限。例如：模型可能在高級抽象層次上提供解釋，但缺乏低級或詳細的決策過程說明。此外，由於模型內部的多層神經網絡結構的複雜性，以及可能存在的訓練數據偏見，其解釋可能不總是準確或可靠的。</li>
<li>性能改進空間：儘管 CLIP 在多種任務上展現出了強大的性能，但仍存在改進空間，尤其是與特定任務的監督模型相比。</li>
</ul>
</li>
</ol>
<p>此外，作者也對可能出現的社會影響進行了深入探討，並感謝了所有參與此項目的人員和提供技術支持的軟體套件開發者。本研究不僅為跨領域的技術探索提供了有價值的見解，也為未來的多模態學習的進一步發展打開了新的可能。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime="2024-07-02T04:10:41.000Z" itemprop="dateModified">2024年7月2日</time></b> <!-- -->更新</span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/vlt5/"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">[21.02] VL-T5</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/mdetr/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">[21.04] MDETR</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#打碎次元的屏障">打碎次元的屏障</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#定義問題">定義問題</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#解決問題">解決問題</a><ul><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#clip-模型架構">CLIP 模型架構</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#預訓練過程">預訓練過程</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#零樣本推論zero-shot">零樣本推論（Zero-Shot）</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#自然語言監督">自然語言監督</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#大規模資料集wit">大規模資料集：WIT</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#訓練模型細節">訓練模型細節</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#模型推論方式">模型推論方式</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#討論">討論</a><ul><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#zero-shot-有符合預期嗎">Zero-Shot 有符合預期嗎？</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#表徵能力好嗎">表徵能力好嗎？</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#能夠跨資料域嗎">能夠跨資料域嗎？</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#跟人類能比嗎">跟人類能比嗎？</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#模型有沒有偷看答案">模型有沒有偷看答案？</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#有什麼限制">有什麼限制？</a></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#來自資料的惡意">來自資料的惡意</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/papers/clip/#結論">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">使用條款<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">隱私政策<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>