<!doctype html><html lang=zh-hant dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-model-tuning/blip2/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>[23.01] BLIP-2 | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/papers/model-tuning/blip2/><meta data-rh=true property=og:locale content=zh_hant><meta data-rh=true property=og:locale:alternate content=en><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=zh-hant><meta data-rh=true name=docsearch:language content=zh-hant><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[23.01] BLIP-2 | DOCSAID"><meta data-rh=true name=description content="Q-Former 初登場"><meta data-rh=true property=og:description content="Q-Former 初登場"><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/papers/model-tuning/blip2/><link data-rh=true rel=alternate href=https://docsaid.org/papers/model-tuning/blip2/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/model-tuning/blip2/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/model-tuning/blip2/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/model-tuning/blip2/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin><link rel=alternate type=application/rss+xml href=/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin><link rel=stylesheet href=/assets/css/styles.9d26d9d1.css><script src=/assets/js/main.265b171a.js defer></script><script src=/assets/js/runtime~main.48aac838.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label=跳至主要内容><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>跳至主要内容</a></div><nav aria-label=主導航 class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class=navbar__inner><div class=navbar__items><button aria-label=切換導覽列 aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><div class=navbar__logo><img src=/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/docs/>開源專案</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/papers/intro>論文筆記</a><a class="navbar__item navbar__link" href=/blog>部落格</a><a class="navbar__item navbar__link" href=/playground/intro>遊樂場</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>繁體中文</a><ul class=dropdown__menu><li><a href=/papers/model-tuning/blip2/ rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=zh-hant>繁體中文</a><li><a href=/en/papers/model-tuning/blip2/ rel="noopener noreferrer" class=dropdown__link lang=en>English</a><li><a href=/ja/papers/model-tuning/blip2/ rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><a href=https://buymeacoffee.com/zephyr_docsaid target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">支持我們<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a><a class="navbar__item navbar__link" href=/aboutus>關於我們</a><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="搜尋 (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>搜尋</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label=回到頂部 class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/><img src=/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label=文件側邊欄 class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/papers/intro>論文筆記</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="展開側邊欄分類 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/face-anti-spoofing-1>Face Anti-Spoofing (1)</a><button aria-label="展開側邊欄分類 'Face Anti-Spoofing (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="展開側邊欄分類 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/feature-fusion-7>Feature Fusion (7)</a><button aria-label="展開側邊欄分類 'Feature Fusion (7)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="展開側邊欄分類 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/mamba-1>Mamba (1)</a><button aria-label="展開側邊欄分類 'Mamba (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="收起側邊欄分類 'Model Tuning (8)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/model-tuning/adapter/>[19.02] Adapter</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/model-tuning/autoprompt/>[20.10] AutoPrompt</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/model-tuning/prefix-tuning/>[21.01] Prefix-Tuning</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/model-tuning/soft-prompts/>[21.04] Soft Prompts</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/model-tuning/lora/>[21.06] LoRA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/model-tuning/coop/>[21.09] CoOp</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/papers/model-tuning/cocoop/>[22.03] CoCoOp</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/papers/model-tuning/blip2/>[23.01] BLIP-2</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/multimodality-22>Multimodality (22)</a><button aria-label="展開側邊欄分類 'Multimodality (22)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/normalization-1>Normalization (1)</a><button aria-label="展開側邊欄分類 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="展開側邊欄分類 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/reparameterization-7>Reparameterization (7)</a><button aria-label="展開側邊欄分類 'Reparameterization (7)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="展開側邊欄分類 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-detection-12>Text Detection (12)</a><button aria-label="展開側邊欄分類 'Text Detection (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="展開側邊欄分類 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="展開側邊欄分類 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/transformers-15>Transformers (15)</a><button aria-label="展開側邊欄分類 'Transformers (15)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/papers/category/vision-transformers-11>Vision Transformers (11)</a><button aria-label="展開側邊欄分類 'Vision Transformers (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/papers/intro>All Notes: 143 entries</a></ul></nav><button type=button title=收起側邊欄 aria-label=收起側邊欄 class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=頁面路徑><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label=主頁面 class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/papers/category/model-tuning-8><span itemprop=name>Model Tuning (8)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[23.01] BLIP-2</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[23.01] BLIP-2</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=q-former-初登場>Q-Former 初登場<a href=#q-former-初登場 class=hash-link aria-label="Q-Former 初登場的直接連結" title="Q-Former 初登場的直接連結">​</a></h2>
<p><a href=https://arxiv.org/abs/2301.12597 target=_blank rel="noopener noreferrer"><strong>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</strong></a></p>
<hr>
<p>這是以 BLIP 命名的第二代架構。</p>
<p>有別於第一代 BLIP 是多模態的工作項目，第二代 BLIP 則是專注使用少量的參數來帶動大型的預訓練模型走向我們想要的方向。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>如果沒看過 BLIP 的讀者，可以參考我們之前的文章：<ul>
<li><a href=/papers/multimodality/blip/><strong>[22.01] BLIP: 合成文本技術</strong></a></li>
</ul></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=定義問題>定義問題<a href=#定義問題 class=hash-link aria-label=定義問題的直接連結 title=定義問題的直接連結>​</a></h2>
<p>我們還是聚焦在 VLM 的領域上。</p>
<p>現階段的預訓練架構多採用大規模訓練的方式，除了需要耗費大量資源之外，應對千奇百怪的下游任務也不是每個任務都能遷移的很好。</p>
<p>如果我們可以直接利用訓練好的單模態 LLM，在凍結參數的前提下，進行多模態的特徵對齊，那我們就不必費這個大的力氣來重新訓練一個多模態的模型了，不是嗎！</p>
<p>基於這個思路，作者設計了一個輕量化的查詢式轉換器，稱為 Q-Former，作為橋樑來提取影像編碼器中的關鍵特徵，並將其轉換為語言模型可以理解的輸出。</p>
<p>有這種事？！讓我們來學習一下。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=解決問題>解決問題<a href=#解決問題 class=hash-link aria-label=解決問題的直接連結 title=解決問題的直接連結>​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=模型架構>模型架構<a href=#模型架構 class=hash-link aria-label=模型架構的直接連結 title=模型架構的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=q-former src=/assets/images/img1-791ac547f2c8f03927aaf2f7ff4dbc8c.jpg width=1358 height=502 class=img_ev3q></figure></div>
<p>這個架構圖看起來有點複雜，我們依序來看看：</p>
<p>首先從最左邊開始，這裡輸入一張影像，經過一個預訓練的影像編碼器。這個編碼器的參數是凍結的，只負責抽取特徵，不參與訓練。</p>
<p>接著，右邊一整坨都是「Q-Former」模組，這是一個雙子架構，這個架構的輸入有兩個部分：</p>
<ol>
<li><strong>可學習的查詢嵌入（Learned Query）</strong>：這組參數隨機初始化，長度在論文中設為 32 個 token。</li>
<li><strong>對影像描述的文字輸入</strong>：這跟常見的語言模型一樣，就是自然語言的描述。</li>
</ol>
<p>上圖中的橘色，也就是 Self-Attention 的部分，直接使用預訓練的語言模型權重作為初始化參數。Q-Former 的部分經過 Self-Attention 的運算之後，會對影像編碼器輸出的特徵進行查詢，也就是說這裡的「Learned Query」，主要的工作就是對齊影像特徵與語言描述。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>也就是說，我們隨機產一組 token，先拿去對文字特徵查詢一下，再接著拿去對影像特徵查詢一下，這樣就得到了一個「混合」特徵。<p>最後把這個「混合」特徵拿去和文字特徵對齊一下，結束這回合。</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第一階段預訓練>第一階段預訓練<a href=#第一階段預訓練 class=hash-link aria-label=第一階段預訓練的直接連結 title=第一階段預訓練的直接連結>​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=objectives src=/assets/images/img2-9b927fe9752cdc3aad2a2d235c4838ab.jpg width=760 height=412 class=img_ev3q></figure></div>
<p>上圖中的 Q 指的是 Query token，T 指的是 Text token，掩碼的部分會用深色標示。</p>
<p>在第一階段，Q-Former 與凍結的影像編碼器結合，進行基於影像-文本對的預訓練，主要優化以下三個目標函數：</p>
<ol>
<li>
<p><strong>影像-文本匹配（Image-Text Matching, ITM）</strong></p>
<p>這個是用來訓練「Learned Query」的目標函數，也就是上圖中的 Q 部分。</p>
<p>注意力掩碼的設計如上圖所示，查詢與文本可互相觀看，並使用線性分類器進行二分類，匹配分數由所有查詢嵌入的分類結果平均計算。 訓練時會使用硬負樣本挖掘策略（hard negative mining）以生成更具挑戰性的負樣本。</p>
</li>
<li>
<p><strong>影像文本生成（Image-grounded Text Generation, ITG）</strong></p>
<p>這是用來訓練 Text token 的目標函數，也就是上圖中的 T 部分。</p>
<p>這裡使用多模態因果自注意力掩碼，Q 部分的 token 可以互相觀看，但 T 部分的文本 token 只可以看 Q 部分的查詢和 T 部分的「前序文本」。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>這裡以 <code>[DEC]</code> token 替代 <code>[CLS]</code> token，作為解碼任務的起始標誌。</div></div>
</li>
<li>
<p><strong>影像-文本對比學習（Image-Text Contrastive Learning, ITC）</strong></p>
<p>截至剛才的兩個目標函數，一個訓練 Q 部分，一個訓練 T 部分。</p>
<p>但這兩個部分所描述的是同一間是，所以我們必須把這兩個特徵也進行對齊，這個目標函數就是用來訓練這個對齊的部分。</p>
<p>這裡的設計是比較正向影像-文本對與負向對的相似性，並調整查詢輸出 <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>Z</mi></mrow><annotation encoding=application/x-tex>Z</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>Z</span></span></span></span> 與文本特徵 <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=application/x-tex>t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6151em></span><span class="mord mathnormal">t</span></span></span></span> 的對齊程度，其中 <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>t</mi></mrow><annotation encoding=application/x-tex>t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6151em></span><span class="mord mathnormal">t</span></span></span></span> 是 <code>[CLS]</code> token 的輸出嵌入。</p>
<p>為了避免資訊洩漏，採用單模態自注意力掩碼，查詢與文本無法直接互相觀看。</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=第二階段預訓練>第二階段預訓練<a href=#第二階段預訓練 class=hash-link aria-label=第二階段預訓練的直接連結 title=第二階段預訓練的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=pretraining src=/assets/images/img3-c0d545fc49a52f7a1d8c2393de68e58b.jpg width=1606 height=482 class=img_ev3q></figure></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>第一階段做的事情是對齊影像特徵與語言描述，第二階段是「生成文字」的預訓練。</div></div>
<p>如上圖所示，Q-Former 被接入一個凍結的大語言模型（LLM），以利用 LLM 的語言生成能力，在生成語言時，可以採用以下兩種不同的架構：</p>
<ul>
<li><strong>解碼器型</strong>：搭配語言建模損失進行預訓練，要求 LLM 在視覺特徵的條件下生成文本。</li>
<li><strong>編碼器-解碼器型</strong>：使用前綴語言建模損失，將文本切分為前綴部分和後綴部分。前綴文本與視覺特徵一同作為輸入送至 LLM 編碼器，後綴文本作為 LLM 解碼器的生成目標。</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>注意到這裡的 Image Encoder 是凍結的；LLM 也是凍結的，這裡的目的是利用 Q-Former 提取的視覺提示，來引導 LLM 生成文本。</div></div>
<p>這裡的設計是將 Q-Former 的輸出查詢嵌入 <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>Z</mi></mrow><annotation encoding=application/x-tex>Z</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>Z</span></span></span></span> 線性投影到與 LLM 文本嵌入相同的維度，並將投影後的查詢嵌入加到輸入文本嵌入的前面，作為視覺提示（soft visual prompts）。</p>
<p>由於 Q-Former 已被預訓練為萃取與語言相關的視覺特徵，它作為資訊瓶頸，有效濾除不相關的視覺資訊，減輕 LLM 的視覺-語言對齊負擔。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>我們之前讀過「Soft Prompts」的概念，感興趣的讀者，可以參考我們之前的文章：<ul>
<li><a href=/papers/model-tuning/soft-prompts/><strong>[21.04] Soft Prompts: 小弦切切如私語</strong></a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=預訓練數據集>預訓練數據集<a href=#預訓練數據集 class=hash-link aria-label=預訓練數據集的直接連結 title=預訓練數據集的直接連結>​</a></h3>
<ul>
<li>使用 BLIP 的預訓練數據集，共計 1.29 億張影像，包含以下來源：<!-- -->
<ul>
<li>COCO、Visual Genome、CC3M、CC12M、SBU，以及 LAION400M 數據集中篩選出的 1.15 億張影像。</li>
</ul>
</li>
<li>利用 <strong>CapFilt 方法</strong> 生成網路影像的合成描述：<!-- -->
<ul>
<li>使用 BLIPlarge 生成 10 條影像描述。</li>
<li>利用 CLIP ViT-L/14 模型計算影像-文本相似性，保留相似性排名前兩的描述作為訓練數據，每次預訓練隨機選擇一條描述。</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>CapFilt 方法是 BLIP 論文中提出的一種方法，用來生成影像的描述，這裡不再贅述。</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=預訓練設置>預訓練設置<a href=#預訓練設置 class=hash-link aria-label=預訓練設置的直接連結 title=預訓練設置的直接連結>​</a></h3>
<ul>
<li><strong>預訓練步數</strong>：<!-- -->
<ul>
<li>特徵學習階段：250,000 步。</li>
<li>生成學習階段：80,000 步。</li>
</ul>
</li>
<li><strong>批量大小</strong>：<!-- -->
<ul>
<li>特徵學習：2320 (ViT-L) 或 1680 (ViT-g)。</li>
<li>生成學習：1920 (OPT) 或 1520 (FlanT5)。</li>
</ul>
</li>
<li><strong>計算效率</strong>：<!-- -->
<ul>
<li>使用單台 16 × A100 (40G) 訓練：<!-- -->
<ul>
<li>特徵學習階段（最大模型）：少於 6 天。</li>
<li>生成學習階段（最大模型）：少於 3 天。</li>
</ul>
</li>
</ul>
</li>
<li><strong>優化器與學習率</strong>：<!-- -->
<ul>
<li>優化器：AdamW（<span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=application/x-tex>\beta_1=0.9</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8889em;vertical-align:-0.1944em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.05278em>β</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0528em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.9</span></span></span></span>，<span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.98</mn></mrow><annotation encoding=application/x-tex>\beta_2=0.98</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8889em;vertical-align:-0.1944em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.05278em>β</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0528em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.98</span></span></span></span>，權重衰減 <span class=katex><span class=katex-mathml><math><semantics><mrow><mn>0.05</mn></mrow><annotation encoding=application/x-tex>0.05</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.05</span></span></span></span>）。</li>
<li>學習率策略：餘弦衰減，峰值學習率 <span class=katex><span class=katex-mathml><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=application/x-tex>1 \times 10^{-4}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7278em;vertical-align:-0.0833em></span><span class=mord>1</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8141em></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span>，線性暖啟動 2000 步；第二階段的最小學習率為 <span class=katex><span class=katex-mathml><math><semantics><mrow><mn>5</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=application/x-tex>5 \times 10^{-5}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7278em;vertical-align:-0.0833em></span><span class=mord>5</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8141em></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span>。</li>
</ul>
</li>
<li><strong>影像處理</strong>：<!-- -->
<ul>
<li>影像尺寸：<span class=katex><span class=katex-mathml><math><semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation encoding=application/x-tex>224 \times 224</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7278em;vertical-align:-0.0833em></span><span class=mord>224</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>224</span></span></span></span>。</li>
<li>增強方式：隨機裁剪與水平翻轉。</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=討論>討論<a href=#討論 class=hash-link aria-label=討論的直接連結 title=討論的直接連結>​</a></h2>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=table1 src=/assets/images/img4-6068c788ca50dfdad61a9d797981d985.jpg width=1728 height=376 class=img_ev3q></figure></div>
<p>什麼都不說，先看看效果。</p>
<p>和先前最先進的模型相比，BLIP-2 實現了更高的效能，在 VLM 預訓練期間所需的可訓練參數數量大幅減少。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=zero-shot-vqa>Zero-shot VQA<a href=#zero-shot-vqa class=hash-link aria-label="Zero-shot VQA的直接連結" title="Zero-shot VQA的直接連結">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=table2 src=/assets/images/img5-691e5f0dcd924f2dd83c8083a6f92fc6.jpg width=1224 height=584 class=img_ev3q></figure></div>
<p>作者首先測試 BLIP-2 的 Zero-shot 能力，上表是 BLIP-2 在不同的 VQA 任務上的表現，包括 VQAv2、GQA 和 OK-VQA。</p>
<p>評估方式：</p>
<ul>
<li>使用 OPT 模型的提示為「<code>Question: {}</code> Answer:」</li>
<li>使用 FlanT5 模型的提示為「<code>Question: {}</code> Short answer:」</li>
<li>採用 beam search 生成答案，束寬為 5，並將長度懲罰設為 <span class=katex><span class=katex-mathml><math><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding=application/x-tex>-1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7278em;vertical-align:-0.0833em></span><span class=mord>−</span><span class=mord>1</span></span></span></span> 以鼓勵簡短答案。</li>
</ul>
<p>在 VQAv2 和 GQA 上，BLIP-2 比 Flamingo80B 表現更佳，尤其在 VQAv2 上超越 8.7%，儘管參數量僅為 Flamingo80B 的 <span class=katex><span class=katex-mathml><math><semantics><mrow><mn>1</mn><mi mathvariant=normal>/</mi><mn>54</mn></mrow><annotation encoding=application/x-tex>1/54</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mord>1/54</span></span></span></span>。</p>
<p>在 OK-VQA 上，BLIP-2 次於 Flamingo80B，原因可能是 OK-VQA 更側重於開放世界知識，而 Flamingo80B 的 Chinchilla 語言模型 (70B) 比 BLIP-2 使用的 FlanT5XXL (11B) 擁有更多知識。</p>
<p>對於模型的 Zero-shot 能力，經過實驗測試，在第一階段的多模態預訓練的步驟非常關鍵。由於 Q-Former 在第一階段預訓練學習了文本相關的視覺特徵，減少 LLM 需要進行視覺語言對齊的負擔，如果移除這個步驟，那模型會出現災難性遺忘，性能隨訓練進行而大幅下降。（在 OPT 模型上有觀察到 15% 的性能下降）</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=finetune-vqa>Finetune VQA<a href=#finetune-vqa class=hash-link aria-label="Finetune VQA的直接連結" title="Finetune VQA的直接連結">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=finetune src=/assets/images/img8-c05c9de74e09daf454442bad8083ea6c.jpg width=1024 height=720 class=img_ev3q></figure></div>
<p>作者在具標註的 VQA 資料上進行微調，固定 LLM 的參數，僅微調 Q-Former 與影像編碼器的參數。</p>
<p>在這個任務中，BLIP-2 在開放式生成模型中達成視覺問答任務的最新水平，顯示出其在專注提取與問題相關視覺特徵的能力。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=圖像描述生成>圖像描述生成<a href=#圖像描述生成 class=hash-link aria-label=圖像描述生成的直接連結 title=圖像描述生成的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=table2 src=/assets/images/img7-683d2f3ffb9173763f5f13a2dcd7e72d.jpg width=1614 height=504 class=img_ev3q></figure></div>
<p>作者評估了 BLIP-2 在圖像描述生成任務上的性能，並與其他方法進行了比較。</p>
<p>針對圖像描述的任務，初始提示為「<code>a photo of</code>」，並使用語言建模損失進行訓練。在微調過程中，固定 LLM 的參數，僅更新 Q-Former 和影像編碼器的參數。</p>
<p>在 COCO 數據集上，BLIP-2 的性能達到最新水平，並在 NoCaps 驗證集上展現出對零樣本遷移的強泛化能力。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=圖像文本檢索>圖像文本檢索<a href=#圖像文本檢索 class=hash-link aria-label=圖像文本檢索的直接連結 title=圖像文本檢索的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=retrieval src=/assets/images/img9-a38965b0cbfb5c4e5923469623276fe3.jpg width=1556 height=634 class=img_ev3q></figure></div>
<p>作者在 COCO 和 Flickr30K 數據集上進行圖像-文本檢索任務的評估，並比較了不同損失對性能的影響。</p>
<p>BLIP-2 在零樣本圖像-文本檢索任務中實現最新水平，顯著優於現有方法。</p>
<p>最後作者探討了每個損失函數對結果的影響如下表：</p>
<p>ITC 和 ITM 損失直接學習圖像與文本的相似性，ITG 損失強化了查詢提取與文本相關的視覺特徵的能力，進一步提升視覺-語言的對齊效果。</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=retrieval src=/assets/images/img10-80f7afc69444ebd24d8bc43c950cb31f.jpg width=912 height=264 class=img_ev3q></figure></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=文字圖像生成展示>文字圖像生成展示<a href=#文字圖像生成展示 class=hash-link aria-label=文字圖像生成展示的直接連結 title=文字圖像生成展示的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=demo src=/assets/images/img6-5ee13454ac94d648b16587f80dc6b5d9.jpg width=1224 height=788 class=img_ev3q></figure></div>
<p>作者展示各種零樣本圖像到文字功能的範例，包括視覺知識推理、視覺常識推理、視覺對話、個性化圖像到文字生成等。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>論文中的圖片實在太占空間，我們沒有截取整張圖片，有興趣的讀者可以參考原文。</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=失敗案例>失敗案例<a href=#失敗案例 class=hash-link aria-label=失敗案例的直接連結 title=失敗案例的直接連結>​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="fail demo" src=/assets/images/img11-510651f6df184913be9ea17904716283.jpg width=1588 height=526 class=img_ev3q></figure></div>
<p>和過去的 LLM 一樣，模型對於之前沒看過的物品或概念，可能會出現一些奇怪的結果。此外，模型也容易將不正確的人物和概念進行不正確的關聯。這些都是未來需要改進的地方。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=結論>結論<a href=#結論 class=hash-link aria-label=結論的直接連結 title=結論的直接連結>​</a></h2>
<p>Q-Former 的概念讓我們只需要用一點點參數，幾個 token 就可以引導大型的語言模型進行多模態的任務。</p>
<p>如果我們想要解決某個領域的問題，就可以用這種方式將大型預訓練模型的能力遷移到我們想要的方向。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>提示</div><div class=admonitionContent_BuS1><p>近年來，活體辨識（FAS）的研究領域，隨處可見 Q-Former 呢！</div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime=2024-12-03T03:47:26.000Z itemprop=dateModified>2024年12月3日</time></b> <!-- -->更新</span></div></div></footer><div style=margin-top:3rem> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label=文件選項卡><a class="pagination-nav__link pagination-nav__link--prev" href=/papers/model-tuning/cocoop/><div class=pagination-nav__sublabel>上一頁</div><div class=pagination-nav__label>[22.03] CoCoOp</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/papers/category/multimodality-22><div class=pagination-nav__sublabel>下一頁</div><div class=pagination-nav__label>Multimodality (22)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#q-former-初登場 class="table-of-contents__link toc-highlight">Q-Former 初登場</a><li><a href=#定義問題 class="table-of-contents__link toc-highlight">定義問題</a><li><a href=#解決問題 class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href=#模型架構 class="table-of-contents__link toc-highlight">模型架構</a><li><a href=#第一階段預訓練 class="table-of-contents__link toc-highlight">第一階段預訓練</a><li><a href=#第二階段預訓練 class="table-of-contents__link toc-highlight">第二階段預訓練</a><li><a href=#預訓練數據集 class="table-of-contents__link toc-highlight">預訓練數據集</a><li><a href=#預訓練設置 class="table-of-contents__link toc-highlight">預訓練設置</a></ul><li><a href=#討論 class="table-of-contents__link toc-highlight">討論</a><ul><li><a href=#zero-shot-vqa class="table-of-contents__link toc-highlight">Zero-shot VQA</a><li><a href=#finetune-vqa class="table-of-contents__link toc-highlight">Finetune VQA</a><li><a href=#圖像描述生成 class="table-of-contents__link toc-highlight">圖像描述生成</a><li><a href=#圖像文本檢索 class="table-of-contents__link toc-highlight">圖像文本檢索</a><li><a href=#文字圖像生成展示 class="table-of-contents__link toc-highlight">文字圖像生成展示</a><li><a href=#失敗案例 class="table-of-contents__link toc-highlight">失敗案例</a></ul><li><a href=#結論 class="table-of-contents__link toc-highlight">結論</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/docs>開源專案</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/papers/intro>論文筆記</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/blog>部落格</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/terms-of-service>使用條款</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/privacy-policy>隱私政策</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/worklog>工作日誌</a><span class=footer__link-separator>·</span><a href=https://buymeacoffee.com/zephyr_docsaid target=_blank rel="noopener noreferrer" class=footer__link-item>支持我們<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>