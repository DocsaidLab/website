<!doctype html>
<html lang="zh-hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-vision-transformers/vit/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[20.10] ViT | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/papers/vision-transformers/vit/"><meta data-rh="true" property="og:locale" content="zh_hant"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-hant"><meta data-rh="true" name="docsearch:language" content="zh-hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[20.10] ViT | DOCSAID"><meta data-rh="true" name="description" content="新世界拓荒者"><meta data-rh="true" property="og:description" content="新世界拓荒者"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/papers/vision-transformers/vit/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/vision-transformers/vit/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/vision-transformers/vit/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/vision-transformers/vit/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.3057f3b6.css">
<script src="/assets/js/runtime~main.f1b846cd.js" defer="defer"></script>
<script src="/assets/js/main.7a8c55b6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>繁體中文</a><ul class="dropdown__menu"><li><a href="/papers/vision-transformers/vit/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/vision-transformers/vit/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜尋"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜尋</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/papers/intro">論文筆記</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/classic-cnns-8">Classic CNNs (8)</a><button aria-label="展開側邊欄分類 &#x27;Classic CNNs (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="展開側邊欄分類 &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="展開側邊欄分類 &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="展開側邊欄分類 &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="展開側邊欄分類 &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/multimodality-18">Multimodality (18)</a><button aria-label="展開側邊欄分類 &#x27;Multimodality (18)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/normalization-1">Normalization (1)</a><button aria-label="展開側邊欄分類 &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/object-detection-5">Object Detection (5)</a><button aria-label="展開側邊欄分類 &#x27;Object Detection (5)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="展開側邊欄分類 &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="展開側邊欄分類 &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="展開側邊欄分類 &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/papers/category/transformers-13">Transformers (13)</a><button aria-label="展開側邊欄分類 &#x27;Transformers (13)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="收起側邊欄分類 &#x27;Vision Transformers (11)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/papers/vision-transformers/vit/">[20.10] ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/deit/">[20.12] DeiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/pvt/">[21.02] PVT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/swin-transformer/">[21.03] Swin Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/mlp-mixer/">[21.05] MLP-Mixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/beit/">[21.06] BEiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/mae/">[21.11] MAE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/poolformer/">[21.11] PoolFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/convmixer/">[22.01] ConvMixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/convnext/">[22.01] ConvNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/papers/vision-transformers/caformer/">[22.10] CAFormer</a></li></ul></li></ul></nav><button type="button" title="收起側邊欄" aria-label="收起側邊欄" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/papers/category/vision-transformers-11"><span itemprop="name">Vision Transformers (11)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[20.10] ViT</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>[20.10] ViT</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="新世界拓荒者">新世界拓荒者<a href="#新世界拓荒者" class="hash-link" aria-label="新世界拓荒者的直接連結" title="新世界拓荒者的直接連結">​</a></h2>
<p><a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer"><strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</strong></a></p>
<hr>
<p>在 2017 年，Transformer 模型提出後，在 NLP 領域內掀起一陣狂潮，霎時間便烽火連天。</p>
<p>這陣風吹了三年，終於把硝煙也吹進了電腦視覺領域。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="定義問題的直接連結" title="定義問題的直接連結">​</a></h2>
<p>在前幾年的嘗試中，許多研究嘗試將注意力機制與卷積網路結合使用。</p>
<p>或是在保持卷積網路的結構不變的前提下，替換掉某些部分。</p>
<p>這些研究都暗示著 Transformer 架構沒有辦法直接生搬硬套到圖像領域。</p>
<p>本篇論文的作者認為：</p>
<ul>
<li><strong>那只是你們的方法不對！</strong></li>
</ul>
<p>我們完全可以捨棄卷積，直接用 Transformer 來處理圖像。</p>
<p>圖像這件事，不過就是 16 x 16 的文字集合罷了！</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="解決問題的直接連結" title="解決問題的直接連結">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="模型架構">模型架構<a href="#模型架構" class="hash-link" aria-label="模型架構的直接連結" title="模型架構的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="arch" src="/assets/images/img1-e4df6921f628ba6819ec9fc778d6150e.jpg" width="1472" height="772" class="img_ev3q"></p>
<p>說到 Transformer，我們都很熟悉。</p>
<p>把文字經過 Embedding 後，排成一個序列，然後丟進 Encoder，再經過 Decoder，最後輸出結果。</p>
<p>當我們想在圖像上套用這個架構，首先要思考的問題是：</p>
<ul>
<li><strong>該怎麼把圖像轉換成文字序列？</strong></li>
</ul>
<p>在這裡，作者提出的方法是：切塊吧！</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="patchify">Patchify<a href="#patchify" class="hash-link" aria-label="Patchify的直接連結" title="Patchify的直接連結">​</a></h3>
<p>給定一張圖像，假設其尺寸為 224 x 224，試想該如何把他們切成一塊一塊的區域呢？</p>
<p>手動切嗎？當然不是！</p>
<p>這裡作者引入一個 Conv2d 的操作，就完成了這個任務。</p>
<p>直接來實作一下：</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">nn </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> nn</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 假設圖像尺寸為 224 x 224</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dummy_img </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">randn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">224</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 切塊</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">patch_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">16</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 編碼維度</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">embed_dim </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">768</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Patchify</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">emb_layer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Conv2d</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> embed_dim</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> kernel_size</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">patch_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> stride</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">patch_size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 切塊後的結果：</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># input.shape = (1, 3, 224, 224)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># tokens.shape = (1, 768, 14, 14)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> emb_layer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">dummy_img</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="複製程式碼至剪貼簿" title="複製" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>這裡我們設定切塊圖像的尺寸為 16 x 16，並且設定編碼維度為 768。</p>
<p>透過卷積的 stride 進行不重疊滑動視窗，就可以將 224 x 224 的圖像切成 14 x 14 塊。</p>
<p>在原本的 Transformer 中，我們會將文字序列的每個 token 進行 Embedding，這裡也是一樣的。將每個切塊的圖像進行 Embedding，意思就是把每個 16 x 16 x 3 的區域轉換經過線性轉換，投影到 768 維的向量。</p>
<p>最後我們把這些切塊的結果展平，變成一個 序列：</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tokens</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">flatten</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># (1, 768, 14, 14) -&gt; (1, 768, 196)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tokens</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">permute</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic"># (1, 768, 196) -&gt; (196, 1, 768)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="複製程式碼至剪貼簿" title="複製" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>在 Transformer encoder 中，輸入的第一個維度是序列的長度，第二個維度是 batch size，第三個維度是特徵編碼長度。</p>
<p>經過上面的操作，我們就得到可以丟進 Transformer 的輸入序列了。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="然後呢">然後呢？<a href="#然後呢" class="hash-link" aria-label="然後呢？的直接連結" title="然後呢？的直接連結">​</a></h3>
<p>然後就結束了。</p>
<p>之後就跟 NLP 的 Transformer 一樣，想怎樣就怎樣。</p>
<p>卡！不對，不能這麼早結束！</p>
<p>＊</p>
<p>下表為 ViT 的參數設定：</p>
<p><img decoding="async" loading="lazy" alt="params" src="/assets/images/img3-c3e287a1c047ee24e5f254dec9bff6b7.jpg" width="1224" height="332" class="img_ev3q"></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="沒有歸納偏差">沒有歸納偏差<a href="#沒有歸納偏差" class="hash-link" aria-label="沒有歸納偏差的直接連結" title="沒有歸納偏差的直接連結">​</a></h3>
<p>在 Transformer 的架構中，並沒有對於圖片的歸納偏差（inductive bias）。</p>
<p>在 ViT 中，只有 MLP 層是局部和平移不變的，同時自注意力機制是全局的，二維鄰域的結構關係非常少。</p>
<p>因此模型必須重頭開始學習並理解：什麼是圖像？所謂圖像的特徵又是什麼？</p>
<p>這也是為什麼從 Transformer 到 ViT 的時間需要這麼長的原因，大部分的早期研究都沒有做出比卷積網路 更好的結果，故而不了了之。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p><strong>卷積網路的歸納偏差是什麼？</strong></p><p>卷積網路的歸納偏差是指，卷積網路在設計上，對於圖像的平移不變性和局部性有著很強的偏好。這種偏好是通過卷積核的設計實現的，卷積核的共享權重和局部感受野，使得卷積網路能夠捕捉到圖像的局部特徵，並且對於圖像的平移不變性有著很好的性能，並且容易泛化到其他圖像辨識任務上。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="訓練資料必須大">訓練資料必須大<a href="#訓練資料必須大" class="hash-link" aria-label="訓練資料必須大的直接連結" title="訓練資料必須大的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="data" src="/assets/images/img2-c1d80181e8e6a9afb5e7c4b1ed9f0941.jpg" width="1224" height="748" class="img_ev3q"></p>
<p>從實驗中可以看到，如果訓練資料的規模不夠大，ViT 的效果會比卷積網路差很多。</p>
<p>上圖中，灰色的線表示 ResNet50x1 (BiT) 和 ResNet152x2 (BiT) 的結果，而其他顏色的線則是 ViT 的結果。底下橫軸是訓練資料量，當資料量來到 300M 時，ViT 的效果終於超越卷積網路。</p>
<p>作者認為：</p>
<ul>
<li><strong>在小資料集上，卷積網路的歸納偏差是很重要的。</strong></li>
<li><strong>在大資料集上，直接從資料中學習相關的模式就足夠了！</strong></li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>ViT 模型的閱讀方式：</p><ul>
<li>ViT-L/16：Large 模型，16 x 16 的切塊</li>
<li>ViT-L/32：Large 模型，32 x 32 的切塊</li>
</ul><p>切塊的大小愈小，編碼的解析度愈高，模型的效果愈好，但是計算量也愈大，呈平方關係增長。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="還能再更大">還能再更大<a href="#還能再更大" class="hash-link" aria-label="還能再更大的直接連結" title="還能再更大的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="scale" src="/assets/images/img4-c6486156c2adc483b4526e2ec9f41f29.jpg" width="1224" height="524" class="img_ev3q"></p>
<p>如果一直 Train 下去，會怎樣？</p>
<p>在這個實驗中，作者使用了 3 個不同的模型：</p>
<ul>
<li>ViT</li>
<li>ResNet</li>
<li>Hybrid Model</li>
</ul>
<p>實驗結果顯示，當訓練資料量足夠大時，ViT 的效果會超越 ResNet。</p>
<p>同時 Hybrid Model 在小模型上的效果會比 ViT 稍微好一點，但是當模型變大時，這種差異就消失了。</p>
<p>最  後，ViT 在嘗試的範圍內並未飽和，這其中顯然還有很多潛力可以挖掘。</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>資訊</div><div class="admonitionContent_BuS1"><p>人們可能會期望卷積局部特徵處理能夠幫助任何大小的 ViT，但是沒有。</p></div></div>
<p><img decoding="async" loading="lazy" alt="result" src="/assets/images/img5-e3e59b7769f43255b07a34b40a54aa2c.jpg" width="1224" height="608" class="img_ev3q"></p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="討論的直接連結" title="討論的直接連結">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="vit-看到了什麼">ViT 看到了什麼？<a href="#vit-看到了什麼" class="hash-link" aria-label="ViT 看到了什麼？的直接連結" title="ViT 看到了什麼？的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="what" src="/assets/images/img6-51c223d1ccba5bf64a36cb3c07e9fdd2.jpg" width="1654" height="500" class="img_ev3q"></p>
<p>作者將第一層將影像片段（patches）投影到低維空間，取出前 28 個主成份。</p>
<ul>
<li>
<p><strong>Self-attention（上圖左）</strong></p>
<p>ViT 通過自我注意力機制，能夠整合影像的整體信息，即使在最低層也是如此。</p>
<p>注意力權重用於計算影像空間中信息整合的平均距離，這類似於 CNN 中的感受野大小。</p>
<p>模型展示了在最低層中對大部分影像的關注，說明其全局整合信息的能力。而其他注意力頭在低層的關注則更局部化。</p>
</li>
<li>
<p><strong>Position Embeddings（上圖中）</strong></p>
<p>空間上較近的區塊具有相似的 embeddings，表示這些 embeddings 能夠編碼影像內部各區塊之間的距離關係。</p>
<p>從 embeddings 中看到看到行列結構，對於較大的網格，有時觀察到顯著的正弦結構。</p>
</li>
<li>
<p><strong>注意力距離（上圖右）</strong></p>
<p>這種「注意力距離」類似於 CNN 中的感受野大小。</p>
<p>較低層中各個頭部的平均注意力距離變化很大，有些頭部關注影像的大部分，而其他頭部則專注於查詢位置或附近的小區域。</p>
<p>隨著深度的增加，所有頭部的注意力距離都會增加。 在網路的後半部分，大多數頭部廣泛帶有長注意力距離，這表明了模型在這些層中更多地關注全局信息。</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="用-cls-預測還是用-gap-預測">用 [CLS] 預測還是用 GAP 預測？<a href="#用-cls-預測還是用-gap-預測" class="hash-link" aria-label="用 [CLS] 預測還是用 GAP 預測？的直接連結" title="用 [CLS] 預測還是用 GAP 預測？的直接連結">​</a></h3>
<p><img decoding="async" loading="lazy" alt="cls" src="/assets/images/img7-493ec8fcb301917eb825b68df3dd31bb.jpg" width="1224" height="598" class="img_ev3q"></p>
<p>在本篇論文中，作者使用了兩種不同的方法來進行分類任務：</p>
<ul>
<li>
<p><strong>[CLS] 預測</strong></p>
<p>這是 NLP 中常見的做法，將序列的第一個 token 作為整個序列的表示。</p>
<p>這種方法在圖像領域中也取得了不錯的效果。</p>
</li>
<li>
<p><strong>GAP 預測</strong></p>
<p>GAP（Global Average Pooling）是一種常見的特徵提取方法，將特徵圖的每個通道進行平均，得到一個向量。</p>
<p>一開始作者是用這種方式，但發現效果非常差！</p>
<p>仔細分析後發現，問題不是 GAP 的問題，而是「學習率」設太大了！</p>
</li>
</ul>
<p>經過調整後，兩種預測方式都取得了不錯的效果。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>提示</div><div class="admonitionContent_BuS1"><p>在我們的經驗中，Transformer 架構對學習率非常敏感，這在 ViT 中也是如此。</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="自監督訓練">自監督訓練<a href="#自監督訓練" class="hash-link" aria-label="自監督訓練的直接連結" title="自監督訓練的直接連結">​</a></h3>
<p>作者嘗試使用 MLM 的訓練方式：將 50% 的影像 token 進行破壞，方法包括：將嵌入替換為一個可學習的 [MASK]（80%）、隨機替換為其他 token（10%）、或者保留原樣（10%）。</p>
<p>接著使用 JFT 資料集進行訓練，模型跑了 100 萬步（相當於 14 個 epoch），每個批次的大小是 4096。訓練過程中使用 Adam 優化器，基礎學習率設為 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2 \times 10^{-4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span>，前 1 萬步進行熱身，並採用餘弦學習率衰減的策略。</p>
<p>在這個過程中，作者嘗試了不同的預測目標，包括：</p>
<ol>
<li>預測平均的 3 位元顏色</li>
<li>預測 4×4 縮小版的 token</li>
<li>預測 16×16 完整的 token</li>
</ol>
<p>實驗發現，上述這些方法都取得了不錯的效果（L2 稍微遜色）。</p>
<p>最後，作者選擇了效果最好的第一種方法，因為它在少樣本學習上表現最佳。此外，作者也還嘗試了 15%遮罩率，但發現這樣的設置的效果稍差。</p>
<p>但不管是哪一個方式，效果都比監督式的學習要差(落後 4% 的準確度)。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="其他注意事項">其他注意事項<a href="#其他注意事項" class="hash-link" aria-label="其他注意事項的直接連結" title="其他注意事項的直接連結">​</a></h3>
<p>除了主要論述的內容之外，還有一些訓練的技巧和注意事項：</p>
<ol>
<li>使用 0.1 的 Weight Decay 進行訓練，作者發現這對於後續下游任務很有幫助。</li>
<li>當輸入影像解析度有變動時，會對應地改變輸入序列的長度（因為 Patch 的尺寸固定），這時候必須將學習好的位置編碼進行線性插值。</li>
<li>用 Adam 會比 SGD 更好，作者認為這是因為 Adam 能夠更 好地處理學習率的問題。（現在大多改用 AdamW）</li>
<li>使用 1-D 的可學習的位置編碼，或是 2-D 的可學習的位置編碼，或是相對位置邊編碼，沒有太大的差別，但一定要選一個，如果都不選，效果會很差。</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="結論的直接連結" title="結論的直接連結">​</a></h2>
<p>這篇論文探索了 Transformer 在圖像領域的應用，提出了一種全新的架構 ViT。</p>
<p>ViT 在大資料集上的效果超越了傳統的卷積網路，並且在實驗中展現了更大的潛力。</p>
<p>這篇論文的發表，標誌著 Transformer 架構在圖像領域的成功應用，也為未來的研究開啟了新的方向。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最後<!-- -->由 <b>zephyr-sh</b> <!-- -->於 <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">2024年9月11日</time></b> <!-- -->更新</span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/papers/category/vision-transformers-11"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">Vision Transformers (11)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/papers/vision-transformers/deit/"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">[20.12] DeiT</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#新世界拓荒者" class="table-of-contents__link toc-highlight">新世界拓荒者</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#模型架構" class="table-of-contents__link toc-highlight">模型架構</a></li><li><a href="#patchify" class="table-of-contents__link toc-highlight">Patchify</a></li><li><a href="#然後呢" class="table-of-contents__link toc-highlight">然後呢？</a></li><li><a href="#沒有歸納偏差" class="table-of-contents__link toc-highlight">沒有歸納偏差</a></li><li><a href="#訓練資料必須大" class="table-of-contents__link toc-highlight">訓練資料必須大</a></li><li><a href="#還能再更大" class="table-of-contents__link toc-highlight">還能再更大</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#vit-看到了什麼" class="table-of-contents__link toc-highlight">ViT 看到了什麼？</a></li><li><a href="#用-cls-預測還是用-gap-預測" class="table-of-contents__link toc-highlight">用 [CLS] 預測還是用 GAP 預測？</a></li><li><a href="#自監督訓練" class="table-of-contents__link toc-highlight">自監督訓練</a></li><li><a href="#其他注意事項" class="table-of-contents__link toc-highlight">其他注意事項</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">使用條款<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">隱私政策<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>