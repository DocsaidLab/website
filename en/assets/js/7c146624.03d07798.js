"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7368],{90542:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>t,metadata:()=>r,toc:()=>h});var i=n(74848),a=n(28453);const t={},l="[17.04] MobileNet-V1",r={id:"cnns/lightweight/mobilenet-v1/index",title:"[17.04] MobileNet-V1",description:"Pioneers of Depthwise Separable Convolutions",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/cnns/lightweight/1704-mobilenet-v1/index.md",sourceDirName:"cnns/lightweight/1704-mobilenet-v1",slug:"/cnns/lightweight/mobilenet-v1/",permalink:"/en/papers/cnns/lightweight/mobilenet-v1/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1725965282e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"Lightweight",permalink:"/en/papers/category/lightweight"},next:{title:"[17.07] ShuffleNet",permalink:"/en/papers/cnns/lightweight/shufflenet/"}},c={},h=[{value:"Pioneers of Depthwise Separable Convolutions",id:"pioneers-of-depthwise-separable-convolutions",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Depthwise Separable Convolutions",id:"depthwise-separable-convolutions",level:3},{value:"Network Architecture",id:"network-architecture",level:2},{value:"Training Techniques",id:"training-techniques",level:2},{value:"Discussion",id:"discussion",level:2},{value:"Performance of Depthwise Separable Convolutions",id:"performance-of-depthwise-separable-convolutions",level:3},{value:"Depth vs. Width?",id:"depth-vs-width",level:3},{value:"Comparison with Other Models",id:"comparison-with-other-models",level:3},{value:"Conclusion",id:"conclusion",level:2}];function o(e){const s={a:"a",admonition:"admonition",annotation:"annotation",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"1704-mobilenet-v1",children:"[17.04] MobileNet-V1"})}),"\n",(0,i.jsx)(s.h2,{id:"pioneers-of-depthwise-separable-convolutions",children:"Pioneers of Depthwise Separable Convolutions"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1704.04861",children:(0,i.jsx)(s.strong,{children:"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"})})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"Depthwise separable convolution is a convolutional neural network structure that decomposes standard convolution into two separate layers: depthwise convolution and pointwise convolution."}),"\n",(0,i.jsx)(s.p,{children:"This structure reduces the number of model parameters and computation, making the model more lightweight, which is particularly useful for resource-constrained scenarios like mobile devices."}),"\n",(0,i.jsx)(s.p,{children:"Although this paper extensively discusses depthwise separable convolution, the originator of this structure is actually:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1403.1687",children:(0,i.jsx)(s.strong,{children:"Rigid-motion scattering for image classification"})})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The authors of MobileNets mention that their main contribution is the systematic application of depthwise separable convolutions in convolutional neural networks, achieving good results on ImageNet."}),"\n",(0,i.jsxs)(s.p,{children:["Thus, the well-known ",(0,i.jsx)(s.strong,{children:"MobileNet-V1"})," was born."]}),"\n",(0,i.jsx)(s.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,i.jsx)(s.p,{children:"In recent years, there has been growing interest in building small and efficient neural networks."}),"\n",(0,i.jsx)(s.p,{children:'However, many papers on small networks "focus only on size without considering efficiency."'}),"\n",(0,i.jsx)(s.p,{children:"Clearly, we need network architectures that are both small and fast."}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsxs)(s.p,{children:["Although not explicitly mentioned in the paper, we speculate that this statement refers to ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1602.07360",children:(0,i.jsx)(s.strong,{children:"SqueezeNet"})}),"."]})}),"\n",(0,i.jsx)(s.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,i.jsx)(s.h3,{id:"depthwise-separable-convolutions",children:"Depthwise Separable Convolutions"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"Depthwise Separable Convolution",src:n(66511).A+"",width:"1064",height:"1080"})}),"\n",(0,i.jsx)(s.p,{children:"Traditional convolution layers filter and combine the input feature maps through multi-channel convolutional kernels, generating new feature maps. This process considers all input channel information for each output channel, making it computationally expensive. Depthwise separable convolutions break this process into two stages: depthwise convolution, which applies a filter to each input channel independently, and pointwise convolution, which combines these outputs linearly."}),"\n",(0,i.jsx)(s.p,{children:"Specifically, depthwise convolution does not change the number of input feature map channels, only performing spatial filtering on each channel independently. If there are M input channels, there will be M depthwise convolution filters. Pointwise convolution uses a 1\xd71 kernel to linearly combine these filtered channels, producing the final output channels. This decomposition significantly reduces computational cost."}),"\n",(0,i.jsxs)(s.p,{children:["For each depthwise convolution, the computational cost is ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"K"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"K"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"M"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"F"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"F"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"DK \\times DK \\times M \\times DF \\times DF"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"DK"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"DK"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"})]})]})]}),", where ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"K"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"DK"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"DK"})]})})]})," is the kernel size, ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"F"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"DF"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"})]})})]})," is the input feature map size, and ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"M"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"M"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"})]})})]})," is the number of input channels."]}),"\n",(0,i.jsxs)(s.p,{children:["For pointwise convolution, the computational cost is ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"M"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"N"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"F"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"F"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"M \\times N \\times DF \\times DF"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"})]})]})]}),", where ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"N"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]})," is the number of output channels."]}),"\n",(0,i.jsxs)(s.p,{children:["Adding these two steps still results in significantly lower computational costs than traditional convolution, which is ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"K"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"K"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"M"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"N"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"F"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"D"}),(0,i.jsx)(s.mi,{children:"F"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"DK \\times DK \\times M \\times N \\times DF \\times DF"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"DK"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"DK"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"D"}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"})]})]})]}),"."]}),"\n",(0,i.jsx)(s.p,{children:"Thus, using depthwise separable convolutions can significantly reduce model computation and parameter requirements while maintaining relatively high accuracy, making it ideal for applications running on devices with limited computational resources."}),"\n",(0,i.jsxs)(s.admonition,{type:"info",children:[(0,i.jsx)(s.p,{children:"We can implement depthwise separable convolutions in PyTorch as follows:"}),(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DepthwiseSeparableConv(nn.Module):\n\n    def __init__(self, n_in, n_out, kernel_size, padding=0, stride=1):\n        super(DepthwiseSeparableConv, self).__init__()\n        self.depthwise = nn.Conv2d(n_in, n_in, kernel_size=kernel_size, padding=padding, stride=stride, groups=n_in)\n        self.pointwise = nn.Conv2d(n_in, n_out, kernel_size=1)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out\n"})}),(0,i.jsx)(s.p,{children:"In the implementation mentioned in the paper, each convolution layer is followed by Batch Normalization and a ReLU activation function, as shown below:"}),(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"Depthwise Separable Convolution with BN and ReLU",src:n(52896).A+"",width:"972",height:"504"})}),(0,i.jsx)(s.p,{children:"So, the implementation can be modified as follows:"}),(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DepthwiseSeparableConv(nn.Module):\n\n    def __init__(self, n_in, n_out, kernel_size, padding=0, stride=1):\n        super(DepthwiseSeparableConv, self).__init__()\n        self.depthwise = nn.Conv2d(n_in, n_in, kernel_size=kernel_size, padding=padding, stride=stride, groups=n_in)\n        self.pointwise = nn.Conv2d(n_in, n_out, kernel_size=1)\n        self.depth_bn = nn.BatchNorm2d(n_in)\n        self.depth_relu = nn.ReLU()\n        self.point_bn = nn.BatchNorm2d(n_out)\n        self.point_relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.depth_bn(x)\n        x = self.depth_relu(x)\n        x = self.pointwise(x)\n        x = self.point_bn(x)\n        x = self.point_relu(x)\n        return x\n"})})]}),"\n",(0,i.jsx)(s.h2,{id:"network-architecture",children:"Network Architecture"}),"\n",(0,i.jsx)(s.p,{children:'According to the paper, the MobileNet-V1 network architecture consists of "depthwise separable convolutions except for the first layer, which is a full convolution."'}),"\n",(0,i.jsxs)(s.blockquote,{children:["\n",(0,i.jsx)(s.p,{children:"The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The overall structure is as follows:"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"MobileNet-V1 Architecture",src:n(92125).A+"",width:"932",height:"1080"})}),"\n",(0,i.jsx)(s.h2,{id:"training-techniques",children:"Training Techniques"}),"\n",(0,i.jsx)(s.p,{children:"The authors of MobileNet-V1 used several techniques during training to enhance the model's performance:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Use less regularization and data augmentation since small models are less prone to overfitting."}),"\n",(0,i.jsx)(s.li,{children:"Place little or no weight decay (L2 regularization) on the depthwise convolution filters."}),"\n",(0,i.jsx)(s.li,{children:"Use RMSprop as the optimizer with a smaller learning rate."}),"\n",(0,i.jsx)(s.li,{children:"Employ asynchronous gradient descent similar to Inception-V3."}),"\n"]}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsx)(s.p,{children:"Asynchronous Gradient Descent is an optimization technique used in deep learning training, particularly suitable for distributed computing environments. In traditional synchronous gradient descent, all computation nodes (or workers) must complete their gradient calculations before the model is updated. This means the training process speed is limited by the slowest node. In contrast, asynchronous gradient descent allows each computation node to independently compute gradients and update the shared model without waiting for other nodes' results."})}),"\n",(0,i.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(s.p,{children:"In the overall experimental design, the above network architecture served as the baseline. The authors introduced two constant hyperparameters:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.strong,{children:["Width Multiplier ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"\u03b1"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\alpha"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03b1"})]})})]})]}),": Controls the width of the network, i.e., the number of output channels in each layer."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.strong,{children:["Resolution Multiplier ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"\u03c1"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\rho"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"\u03c1"})]})})]})]}),": Controls the resolution of the input image."]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"performance-of-depthwise-separable-convolutions",children:"Performance of Depthwise Separable Convolutions"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"Depthwise Separable Convolution Performance",src:n(60662).A+"",width:"988",height:"292"})}),"\n",(0,i.jsx)(s.p,{children:"Compared to full convolutions, using depthwise separable convolutions on ImageNet only reduced accuracy by 1% while significantly saving on multiplications and parameters."}),"\n",(0,i.jsx)(s.h3,{id:"depth-vs-width",children:"Depth vs. Width?"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"Depth vs Width",src:n(23827).A+"",width:"972",height:"284"})}),"\n",(0,i.jsx)(s.p,{children:"Reducing the model width decreased accuracy by 2.2%; reducing the model depth decreased accuracy by 5.3%."}),"\n",(0,i.jsx)(s.p,{children:"This indicates that when computational power is limited, it's better to reduce model width rather than depth."}),"\n",(0,i.jsx)(s.h3,{id:"comparison-with-other-models",children:"Comparison with Other Models"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"Comparison with Other Models",src:n(72196).A+"",width:"1012",height:"684"})}),"\n",(0,i.jsx)(s.p,{children:"The full MobileNet was compared to the original GoogleNet and VGG16."}),"\n",(0,i.jsx)(s.p,{children:"MobileNet achieved nearly the same accuracy as VGG16 while being 32 times smaller and reducing computational intensity by 27 times."}),"\n",(0,i.jsx)(s.p,{children:"MobileNet was more accurate than GoogleNet while being smaller and reducing computations by over 2.5 times."}),"\n",(0,i.jsx)(s.p,{children:"With approximately the same size and 22 times fewer computations, MobileNet also outperformed Squeezenet by about 4%."}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(s.p,{children:"The introduction of MobileNet-V1 brought depthwise separable convolution into the spotlight. This structure significantly reduces model size and computational needs while maintaining relatively high accuracy, addressing the challenge of running complex deep learning models in resource-limited environments."}),"\n",(0,i.jsx)(s.p,{children:"This paper provides an effective solution for deploying deep learning models in resource-constrained environments and has inspired the development of subsequent lightweight deep learning architectures, further advancing the application of deep learning in mobile devices and edge computing."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"MobileNet has since released V2, V3, and V4 versions, further improving model performance and efficiency."}),"\n",(0,i.jsx)(s.p,{children:"These research outcomes have achieved widespread success in practical applications, becoming classics in lightweight deep learning models."})]})}function m(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},66511:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img1-941e9b2943c5e476b4e1e16350316c45.jpg"},52896:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img2-c077021c8181d71db5ff38e4f4be81c9.jpg"},92125:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img3-95022b238fac2fc64881e419a592720d.jpg"},60662:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img4-64ab7c428a236ad9a7cf007c1c6db99e.jpg"},23827:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img5-239af6ce9fe53c42ab903303ead6be88.jpg"},72196:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img6-9856e8ff02664221debb20bcbf03a27d.jpg"},28453:(e,s,n)=>{n.d(s,{R:()=>l,x:()=>r});var i=n(96540);const a={},t=i.createContext(a);function l(e){const s=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);