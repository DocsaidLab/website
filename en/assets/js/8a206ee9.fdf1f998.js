"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[36865],{89393:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"text-recognition/union14m/index","title":"[23.07] Union14M","description":"Chess Pieces Falling Like Stars","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/text-recognition/2307-union14m/index.md","sourceDirName":"text-recognition/2307-union14m","slug":"/text-recognition/union14m/","permalink":"/en/papers/text-recognition/union14m/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1730706876000,"frontMatter":{},"sidebar":"papersSidebar","previous":{"title":"[23.06] DiffusionSTR","permalink":"/en/papers/text-recognition/diffusionstr/"},"next":{"title":"Text Spotting (3)","permalink":"/en/papers/category/text-spotting-3"}}');var t=s(74848),r=s(28453);const o={},a="[23.07] Union14M",l={},d=[{value:"Chess Pieces Falling Like Stars",id:"chess-pieces-falling-like-stars",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Problem Solution",id:"problem-solution",level:2},{value:"Union14M-L",id:"union14m-l",level:3},{value:"Union14M-U",id:"union14m-u",level:3},{value:"Diverse Text Styles",id:"diverse-text-styles",level:3},{value:"Rich Vocabulary",id:"rich-vocabulary",level:3},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Pretraining and Fine-Tuning",id:"pretraining-and-fine-tuning",level:3},{value:"Discussion",id:"discussion",level:2},{value:"The Harsh Reality",id:"the-harsh-reality",level:3},{value:"Uncovering the Challenges",id:"uncovering-the-challenges",level:3},{value:"The Benchmark Dataset",id:"the-benchmark-dataset",level:3},{value:"Experimental Results",id:"experimental-results",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"2307-union14m",children:"[23.07] Union14M"})}),"\n",(0,t.jsx)(n.h2,{id:"chess-pieces-falling-like-stars",children:"Chess Pieces Falling Like Stars"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2307.08723",children:(0,t.jsx)(n.strong,{children:"Revisiting Scene Text Recognition: A Data Perspective"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"We\u2019ve explored dozens of papers on scene text recognition (STR), and the validation datasets in use tend to revolve around the same few: ICDAR, IIIT5K, SVT, SVTP, and CUTE80."}),"\n",(0,t.jsx)(n.p,{children:"Truth be told, performance on these datasets is already near saturation."}),"\n",(0,t.jsx)(n.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"model performance",src:s(49333).A+"",width:"1224",height:"572"})}),"\n",(0,t.jsx)(n.p,{children:"As shown above, this graph represents the average performance across six widely-used STR datasets."}),"\n",(0,t.jsxs)(n.p,{children:["Starting with familiar models like CRNN, advancing through SAR, SATRN, and ABINet, the field achieved more than ",(0,t.jsx)(n.strong,{children:"92% accuracy by 2022."})," However, progress has since plateaued, leaving little room for further improvements."]}),"\n",(0,t.jsxs)(n.p,{children:["Does this imply that STR is a ",(0,t.jsx)(n.strong,{children:"\u201csolved\u201d problem"}),"? Or is it possible that the validation datasets we rely on are too limited, preventing us from exposing deeper challenges within the field?"]}),"\n",(0,t.jsx)(n.p,{children:"If existing benchmarks cannot help us move forward, perhaps it\u2019s time to overturn the chessboard\u2014upending the status quo to create new possibilities. And with the pieces now falling freely, it\u2019s time to reassemble them into something entirely new."}),"\n",(0,t.jsx)(n.h2,{id:"problem-solution",children:"Problem Solution"}),"\n",(0,t.jsxs)(n.p,{children:["Since current datasets no longer capture the full scope of real-world challenges in STR, the authors propose ",(0,t.jsx)(n.strong,{children:"Union14M"}),", a large-scale, unified dataset."]}),"\n",(0,t.jsxs)(n.p,{children:["Union14M brings together ",(0,t.jsx)(n.strong,{children:"4 million labeled images (Union14M-L)"})," and ",(0,t.jsx)(n.strong,{children:"10 million unlabeled images (Union14M-U)"}),", consolidated from ",(0,t.jsx)(n.strong,{children:"17 publicly available datasets."})," The structure is shown below:"]}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"union14m",src:s(98346).A+"",width:"1116",height:"782"})})})}),"\n",(0,t.jsx)(n.p,{children:"By aggregating existing datasets and categorizing diverse challenges, Union14M aims to better represent the variability of real-world text, offering new insights into the limitations of current STR models and pushing the field forward."}),"\n",(0,t.jsx)(n.h3,{id:"union14m-l",children:"Union14M-L"}),"\n",(0,t.jsxs)(n.p,{children:["Union14M-L consists of 4 million labeled images gathered from ",(0,t.jsx)(n.strong,{children:"14 public datasets"})," with the goal of representing diverse real-world scenarios. Some of these datasets include:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ArT"}),": Focuses on curved text images."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ReCTS, RCTW, LSVT, KAIST, NEOCR, and IIIT-ILST"}),": Provide street-view text from various countries."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MTWI"}),": Extracts text images from web pages."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"COCOTextV2"}),": Includes low-resolution and vertical text images."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"IntelOCR, TextOCR, and HierText"}),": Collected from OpenImages, covering around 9 million images of various real-world scenarios."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Simply merging these datasets, however, would not suffice. Challenges such as inconsistent annotations, duplicate samples, non-Latin characters, or corrupted images must be addressed. The authors applied several optimization strategies to ensure high-quality consolidation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Cropping Text Instances"}),": Instead of using polygon annotations, the authors applied ",(0,t.jsx)(n.strong,{children:"minimum rotated rectangles"})," to crop text regions. This method introduces more background noise, making the model more robust against distractions and reducing dependency on precise detectors. This setup also helps analyze the pure recognition performance of models."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Removing Duplicates"}),": To eliminate redundancy, the authors first filtered out overlaps between Union14M-L and other common benchmarks. They also removed duplicate samples within the 14 datasets. For example, ",(0,t.jsx)(n.strong,{children:"HierText, TextOCR, and IntelOCR"})," overlap, as they all derive from OpenImages. The authors kept HierText as the main reference, removing the redundant samples from the other two datasets."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Excluding Non-Latin Characters and Ignored Samples"}),': To focus on Latin-based text, only samples containing letters, numbers, or symbols were kept, while images marked as "ignored" were excluded.']}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"union14m-u",children:"Union14M-U"}),"\n",(0,t.jsxs)(n.p,{children:["Union14M-U comprises ",(0,t.jsx)(n.strong,{children:"10 million unlabeled images"}),", leveraging self-supervised learning\u2014a growing trend in computer vision and STR. Since manually labeling text images is both time-consuming and requires linguistic expertise, the authors explored how unlabeled data can enhance STR models."]}),"\n",(0,t.jsxs)(n.p,{children:["They sourced these unlabeled images from ",(0,t.jsx)(n.strong,{children:"Book32, OpenImages, and Conceptual Captions (CC)"}),". To select high-quality text instances, the authors employed ",(0,t.jsx)(n.strong,{children:"three different text detectors"})," and adopted an ",(0,t.jsx)(n.strong,{children:"IoU voting mechanism"})," to ensure consistency. Additionally, they removed duplicate samples from OpenImages to avoid redundancy with the labeled portion of Union14M."]}),"\n",(0,t.jsx)(n.h3,{id:"diverse-text-styles",children:"Diverse Text Styles"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"text style",src:s(20999).A+"",width:"1080",height:"860"})})})}),"\n",(0,t.jsxs)(n.p,{children:["As depicted in the above image, Union14M includes a wide variety of text styles. These styles range from ",(0,t.jsx)(n.strong,{children:"curved, slanted, and vertical text"})," to challenging scenarios with ",(0,t.jsx)(n.strong,{children:"blur, complex backgrounds, and occlusion."})," The dataset also captures text from diverse real-world environments, such as ",(0,t.jsx)(n.strong,{children:"street signs and brand logos."})]}),"\n",(0,t.jsxs)(n.p,{children:["One important aspect is the large presence of ",(0,t.jsx)(n.strong,{children:"vertical text"}),", which is quite common in real-world scenarios but often underrepresented in synthetic datasets."]}),"\n",(0,t.jsx)(n.h3,{id:"rich-vocabulary",children:"Rich Vocabulary"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"vocabulary",src:s(13564).A+"",width:"1104",height:"234"})})})}),"\n",(0,t.jsxs)(n.p,{children:["The vocabulary used in synthetic datasets is often derived from common corpora, but real-world text can vary significantly, including combinations not typically found in standard datasets\u2014such as ",(0,t.jsx)(n.strong,{children:"license plate numbers"})," or ",(0,t.jsx)(n.strong,{children:"mixed Chinese-English pinyin."})," As shown above, the vocabulary size of Union14M-L is nearly double that of synthetic datasets, providing a richer and more representative collection of real-world scenarios."]}),"\n",(0,t.jsx)(n.p,{children:"This expanded vocabulary enhances the depth and breadth of analysis available for STR models."}),"\n",(0,t.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"model architecture",src:s(26352).A+"",width:"1152",height:"500"})})})}),"\n",(0,t.jsxs)(n.p,{children:["After constructing the Union14M dataset, the authors proposed a ",(0,t.jsx)(n.strong,{children:"self-supervised learning-based solution: MAERec"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["This model takes advantage of ",(0,t.jsx)(n.strong,{children:"self-supervised pretraining"})," to leverage the ",(0,t.jsx)(n.strong,{children:"10 million unlabeled images"})," from Union14M-U."]}),"\n",(0,t.jsxs)(n.p,{children:["The core of MAERec is based on ",(0,t.jsx)(n.strong,{children:"Vision Transformers (ViT)"}),", which excel at ",(0,t.jsx)(n.strong,{children:"masked image modeling"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/en/papers/vision-transformers/mae/",children:(0,t.jsx)(n.strong,{children:"[21.11] MAE: A Quarter of the Clue"})})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The input image is divided into ",(0,t.jsx)(n.strong,{children:"4 \xd7 4 image patches"})," and passed through the ",(0,t.jsx)(n.strong,{children:"ViT backbone network"}),". The output sequence is then processed by an ",(0,t.jsx)(n.strong,{children:"autoregressive decoder"})," (using the ",(0,t.jsx)(n.strong,{children:"Transformer decoder"})," from SATRN) to generate the final text prediction."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/en/papers/text-recognition/satrn/",children:(0,t.jsx)(n.strong,{children:"[19.10] SATRN: Transformer Reaches the Battlefield"})})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["To ensure fair comparisons, the ",(0,t.jsx)(n.strong,{children:"number of character classes"})," is standardized to ",(0,t.jsx)(n.strong,{children:"91 classes"})," (including numbers, uppercase and lowercase letters, symbols, and spaces), while other hyperparameters remain consistent with the original model configurations."]}),"\n",(0,t.jsx)(n.h3,{id:"pretraining-and-fine-tuning",children:"Pretraining and Fine-Tuning"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"model pretrained",src:s(53453).A+"",width:"1614",height:"528"})})})}),"\n",(0,t.jsxs)(n.p,{children:["To fully utilize the large number of unlabeled images from Union14M-U, the authors adopted the ",(0,t.jsx)(n.strong,{children:"MAE framework"})," for pretraining, with some modifications."]}),"\n",(0,t.jsxs)(n.p,{children:["As shown above, ",(0,t.jsx)(n.strong,{children:"even with a 75% masking ratio"}),", the ",(0,t.jsx)(n.strong,{children:"ViT backbone"})," can still generate high-quality reconstructed text images. This demonstrates that ",(0,t.jsx)(n.strong,{children:"MAERec"})," effectively learns both the structural and semantic representations of text, capturing useful features even from highly incomplete images."]}),"\n",(0,t.jsxs)(n.p,{children:["After pretraining, the pretrained ViT weights are used to ",(0,t.jsx)(n.strong,{children:"initialize MAERec"}),", followed by ",(0,t.jsx)(n.strong,{children:"fine-tuning on Union14M-L"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(n.h3,{id:"the-harsh-reality",children:"The Harsh Reality"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"model performance",src:s(10937).A+"",width:"1104",height:"710"})})})}),"\n",(0,t.jsxs)(n.p,{children:["The authors evaluated ",(0,t.jsx)(n.strong,{children:"13 representative STR models"}),", all trained on synthetic datasets, and tested them on Union14M-L. As shown in the table above, these models ",(0,t.jsx)(n.strong,{children:"suffered a significant performance drop"})," on Union14M-L, with the average accuracy falling by:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"20.50%!"})}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Who still dares to claim that STR is a solved problem?"})}),"\n",(0,t.jsx)(n.h3,{id:"uncovering-the-challenges",children:"Uncovering the Challenges"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"challenges",src:s(54798).A+"",width:"2232",height:"1024"})}),"\n",(0,t.jsxs)(n.p,{children:["To identify the types of errors common to these 13 models, the authors assigned a ",(0,t.jsx)(n.strong,{children:"difficulty score"})," to each sample in Union14M-L, focusing on those that most models failed to recognize correctly. From these, they identified ",(0,t.jsx)(n.strong,{children:"four unresolved challenges"})," and ",(0,t.jsx)(n.strong,{children:"three additional real-world scenarios"})," that have been underexplored in previous research."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Curve Text"})}),"\n",(0,t.jsxs)(n.p,{children:["As shown in image (a), recognizing ",(0,t.jsx)(n.strong,{children:"curved text"})," has been a focus in recent years, with two main approaches:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Correction-based models"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Models leveraging 2D attention mechanisms"})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["While these methods have performed well on datasets like ",(0,t.jsx)(n.strong,{children:"CUTE"}),", the proportion of curved text in these datasets is small, and the curvature tends to be moderate. When faced with highly curved text, existing models still struggle."]}),"\n",(0,t.jsx)(n.hr,{}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Multi-Oriented Text"})}),"\n",(0,t.jsxs)(n.p,{children:["In image (b), text appears in ",(0,t.jsx)(n.strong,{children:"vertical, slanted, or mirrored orientations"})," on various surfaces, such as vertical text on signboards or slanted text due to camera angles. Most STR models assume near-horizontal text alignment, neglecting multi-oriented text challenges."]}),"\n",(0,t.jsxs)(n.p,{children:["These models often resize images to a fixed height (e.g., 32 pixels) while maintaining the aspect ratio. However, such resizing can ",(0,t.jsx)(n.strong,{children:"compress vertical or slanted text"}),", making it harder to recognize."]}),"\n",(0,t.jsx)(n.hr,{}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Artistic Text"})}),"\n",(0,t.jsxs)(n.p,{children:["Image (e) showcases ",(0,t.jsx)(n.strong,{children:"artistic text"}),", which is crafted with unique fonts, effects, and layouts, often embedded within complex backgrounds. Each instance of artistic text can be unique, making it a ",(0,t.jsx)(n.strong,{children:"zero-shot or few-shot problem"}),". Models require specialized networks to handle this. Due to the lack of artistic text samples in synthetic datasets, current models struggle to maintain robustness when encountering these texts in real-world scenarios."]}),"\n",(0,t.jsx)(n.hr,{}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Contextless Text"})}),"\n",(0,t.jsxs)(n.p,{children:["As shown in image (f), ",(0,t.jsx)(n.strong,{children:"contextless text"})," refers to text that carries no inherent meaning or is not found in dictionaries, such as abbreviations or random combinations of letters, numbers, and symbols. Even with clear backgrounds and minimal distortion, models may misinterpret these texts due to over-reliance on semantic knowledge from training corpora."]}),"\n",(0,t.jsxs)(n.p,{children:["For example, a model might mistakenly recognize \u201cYQJ\u201d as \u201cyou,\u201d which could be dangerous in applications like license plate recognition, invoice processing, or ID verification, where ",(0,t.jsx)(n.strong,{children:"misrecognition can lead to security risks or financial losses"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Salient Text"})}),"\n",(0,t.jsxs)(n.p,{children:["Image (c) depicts ",(0,t.jsx)(n.strong,{children:"salient text"}),", where irrelevant characters coexist with the primary text. When multiple texts of different sizes are adjacent or overlapping, these distractors may be included in the recognition, reducing model accuracy."]}),"\n",(0,t.jsxs)(n.p,{children:["In the detection phase, ",(0,t.jsx)(n.strong,{children:"ROI masking strategies"})," (as proposed by Liao et al.) can help remove such distracting characters. However, when detection models fail to locate text regions accurately, it becomes crucial for recognition models to ",(0,t.jsx)(n.strong,{children:"quickly identify key regions"})," for robust performance."]}),"\n",(0,t.jsx)(n.hr,{}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Multi-Words Text"})}),"\n",(0,t.jsxs)(n.p,{children:["As seen in image (d), ",(0,t.jsx)(n.strong,{children:"multi-word text"})," appears in contexts like ",(0,t.jsx)(n.strong,{children:"logos or phrases"}),", where a single word is insufficient to convey complete meaning. Most STR models are trained on synthetic datasets with individual word-level annotations, making them prone to ",(0,t.jsx)(n.strong,{children:"ignoring spaces between words"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["The authors observed that models often concatenate multiple words into one or modify some visible characters to match grammatical rules. For instance, ",(0,t.jsx)(n.strong,{children:'"Live to Evolve"'})," might be incorrectly recognized as ",(0,t.jsx)(n.strong,{children:'"liveroee"'}),", as the model interprets it as a single word."]}),"\n",(0,t.jsx)(n.hr,{}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Incomplete Text"})}),"\n",(0,t.jsxs)(n.p,{children:["As shown in image (g), text can become incomplete due to ",(0,t.jsx)(n.strong,{children:"occlusion or inaccurate detection bounding boxes"}),". When the beginning or end of a word is cropped, models might automatically fill in the missing parts to produce a complete prediction, even though the missing characters are not visible."]}),"\n",(0,t.jsxs)(n.p,{children:["This behavior is especially prominent in ",(0,t.jsx)(n.strong,{children:"systems relying on language models"}),", where predictions are heavily influenced by prior linguistic knowledge. However, this ",(0,t.jsx)(n.strong,{children:"auto-completion feature"})," can reduce reliability in certain applications. For example, if an image only shows \u201cight,\u201d the model might complete it as ",(0,t.jsx)(n.strong,{children:'"might"'})," or ",(0,t.jsx)(n.strong,{children:'"light"'}),", but the ideal output would be just ",(0,t.jsx)(n.strong,{children:'"ight,"'})," leaving the decision to the downstream system for further anomaly detection."]}),"\n",(0,t.jsx)(n.p,{children:"Therefore, it is essential to thoroughly evaluate the auto-completion feature and carefully assess its potential impact on downstream applications."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"the-benchmark-dataset",children:"The Benchmark Dataset"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"union14m",src:s(81259).A+"",width:"856",height:"548"})})})}),"\n",(0,t.jsxs)(n.p,{children:["To thoroughly evaluate the performance of STR models in real-world scenarios and facilitate further research on the ",(0,t.jsx)(n.strong,{children:"seven key challenges"}),", the authors developed a ",(0,t.jsx)(n.strong,{children:"challenge-oriented benchmark dataset"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Union14M-Benchmark"})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This benchmark is divided into ",(0,t.jsx)(n.strong,{children:"eight subsets"}),", containing a total of ",(0,t.jsx)(n.strong,{children:"409,393 images"}),", representing a wide range of textual complexities and diversities."]}),"\n",(0,t.jsx)(n.h3,{id:"experimental-results",children:"Experimental Results"}),"\n",(0,t.jsxs)(n.p,{children:["To comprehensively compare the models, the authors first reported results using models trained on ",(0,t.jsx)(n.strong,{children:"synthetic datasets (MJ + ST)"}),":"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"synthetic data",src:s(31004).A+"",width:"2876",height:"890"})}),"\n",(0,t.jsxs)(n.p,{children:["Next, they re-trained all models using ",(0,t.jsx)(n.strong,{children:"Union14M"}),":"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"union14m data",src:s(44729).A+"",width:"2962",height:"1138"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"Compared to traditional benchmark datasets, models trained on synthetic data showed a significant 48.5% drop in average accuracy on the Union14M-Benchmark. However, when models were trained on Union14M-L, the accuracy drop was mitigated to 33.0%, indicating that real-world text images are far more complex than those in the six common benchmarks."}),"\n",(0,t.jsx)(n.p,{children:"Moreover, models trained on Union14M-L achieved accuracy improvements of 3.9% on standard benchmarks and 19.6% on the Union14M-Benchmark. This demonstrates the inadequacy of synthetic data for addressing real-world complexities and highlights the generalization benefits of training with real-world datasets."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"The relatively smaller improvement on traditional benchmarks suggests that these benchmarks have reached a saturation point."})}),"\n",(0,t.jsx)(n.p,{children:"When models were trained only on Union14M-L, the highest average accuracy achieved on the Union14M-Benchmark was 74.6%, reinforcing that STR remains an unsolved problem. While large-scale real-world data provides some performance gains, more research is needed to overcome existing challenges."}),"\n",(0,t.jsx)(n.p,{children:"Performance on the incomplete text subset showed a significant decline for all models trained with synthetic data. The drop was particularly severe for language models:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language models"}),": 10.2% drop"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CTC models"}),": 5.6% drop"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Attention models"}),": 5.9% drop"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The authors attribute this drop to the ",(0,t.jsx)(n.strong,{children:"error-correction behavior"})," of language models, where the models ",(0,t.jsx)(n.strong,{children:"attempt to auto-complete text"})," perceived as missing characters. This issue was somewhat alleviated when models were trained on Union14M-L, possibly due to the larger vocabulary, which reduced overfitting to training corpora. However, this auto-completion problem persists and requires further investigation."]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsxs)(n.p,{children:["This paper presents a wealth of insights, addressing both ",(0,t.jsx)(n.strong,{children:"current model limitations"})," and ",(0,t.jsx)(n.strong,{children:"dataset challenges"}),". The creation of Union14M was a significant undertaking, providing a more comprehensive real-world text recognition dataset that allows for more thorough model evaluation."]}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:"We highly recommend reading the original paper and visiting their GitHub repository."}),(0,t.jsxs)(n.p,{children:["Project link: ",(0,t.jsx)(n.a,{href:"https://github.com/Mountchicken/Union14M",children:(0,t.jsx)(n.strong,{children:"Union14M GitHub"})})]})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},49333:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img1-940308a62a9159aa05c7e3b82a2649f1.jpg"},44729:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img10-108322397cf6a73c91d0efc257935beb.jpg"},31004:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img11-9b5086cf6a9ac4b1151f52a0401a85ea.jpg"},98346:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img2-4a09d592cb8313973fec2572c2da5e3f.jpg"},20999:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img3-b5b4f8356e86599c2bbab2f148773f86.jpg"},13564:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img4-ddcf7ba385f82e48e003d75c6603360c.jpg"},10937:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img5-1381eb388f69abb912c66afa5fb8f478.jpg"},54798:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img6-a21225140133b9a0856ba68f2e8c1a4c.jpg"},81259:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img7-a9a400b4ebca55f5512661a1d4ed5e51.jpg"},26352:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img8-042c9bcd41a558cb0a876ccaea4fbb33.jpg"},53453:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/img9-192761ec704b18ad056b7340b3c9fd29.jpg"},28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(96540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);