"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5042],{85138:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var t=i(74848),s=i(28453);const a={},r="[19.08] ViLBERT",o={id:"multimodality/vilbert/index",title:"[19.08] ViLBERT",description:"Interweaving in the Prologue",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/1908-vilbert/index.md",sourceDirName:"multimodality/1908-vilbert",slug:"/multimodality/vilbert/",permalink:"/en/papers/multimodality/vilbert/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1722483445e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[19.08] LXMERT",permalink:"/en/papers/multimodality/lxmert/"},next:{title:"[19.08] VisualBERT",permalink:"/en/papers/multimodality/visualbert/"}},l={},d=[{value:"Interweaving in the Prologue",id:"interweaving-in-the-prologue",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Integrating Vision and Language",id:"integrating-vision-and-language",level:3},{value:"Effective Visual Grounding",id:"effective-visual-grounding",level:3},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"ViLBERT Model Design",id:"vilbert-model-design",level:3},{value:"Pre-Training Mechanism",id:"pre-training-mechanism",level:3},{value:"Discussion",id:"discussion",level:2},{value:"How Well Does ViLBERT Perform?",id:"how-well-does-vilbert-perform",level:3},{value:"How Does the Depth of ViLBERT&#39;s Visual Stream Affect Performance?",id:"how-does-the-depth-of-vilberts-visual-stream-affect-performance",level:3},{value:"Impact of Pre-Training Dataset Size",id:"impact-of-pre-training-dataset-size",level:3},{value:"What Did ViLBERT Learn?",id:"what-did-vilbert-learn",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"1908-vilbert",children:"[19.08] ViLBERT"}),"\n",(0,t.jsx)(n.h2,{id:"interweaving-in-the-prologue",children:"Interweaving in the Prologue"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1908.02265",children:(0,t.jsx)(n.strong,{children:"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"The following content has been compiled by ChatGPT-4 and manually proofread, edited, and supplemented."})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"This article introduces ViLBERT, which emerged around the same time as VisualBERT. 2019 was a particularly eventful year in the academic world, and we are far from the end of this journey."}),"\n",(0,t.jsx)(n.p,{children:'We all know that it is no easy task to make machines truly "understand" an image and describe it in words. In the past, most research methods separated image understanding from language processing. However, when attempting to combine these two domains, the results often fell short. It\'s like a machine can recognize that an image shows a Shiba Inu but struggles to identify the breed when asked, "What breed of dog is this?"'}),"\n",(0,t.jsx)(n.p,{children:'This paper proposes a new approach: ViLBERT. Unlike traditional methods, ViLBERT aims to simultaneously teach machines about images and language from the start. For example, when given a photo of a red apple along with the text "This is a red apple," ViLBERT tries to help the machine understand the connection between the word "red" and the apple\'s color.'}),"\n",(0,t.jsx)(n.p,{children:'Preliminary research shows that ViLBERT performs exceptionally well in various tests, significantly outperforming other methods. Specifically, in question-answering tasks, ViLBERT not only "sees" the images but also provides more accurate answers.'}),"\n",(0,t.jsx)(n.p,{children:"Let's explore how the authors define the problem and the solutions they propose in this paper."}),"\n",(0,t.jsx)(n.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(n.h3,{id:"integrating-vision-and-language",children:"Integrating Vision and Language"}),"\n",(0,t.jsx)(n.p,{children:"Although both fields have made significant progress individually, combining them remains challenging. Most current methods train vision and language models separately and then attempt to merge them. This approach often yields suboptimal results, especially when visual and language data are limited or biased, leading to poor generalization."}),"\n",(0,t.jsx)(n.h3,{id:"effective-visual-grounding",children:"Effective Visual Grounding"}),"\n",(0,t.jsx)(n.p,{children:'Even if computers can recognize objects in images or understand language, linking the two remains a significant challenge. For instance, a computer might recognize a dog in an image but fail to associate it with the concept of a "Shiba Inu" or "Shepherd Dog."'}),"\n",(0,t.jsx)(n.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,t.jsx)(n.h3,{id:"vilbert-model-design",children:"ViLBERT Model Design"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ViLBERT Model Architecture",src:i(66248).A+"",width:"1024",height:"813"})}),"\n",(0,t.jsx)(n.p,{children:"When designing the ViLBERT model, the primary goal was to merge vision and language to create a model that can learn from paired image and text data. Inspired by BERT's success in language modeling, the authors aimed to develop a similar model that can learn joint representations of language and vision from paired data."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Basic Concept"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'The authors mention an intuitive design approach: treating visual input as "tokens" similar to text input. However, this method has limitations as it may lose visual details and overlook the need for different processing levels for the two modalities.'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Dual-Stream Architecture"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The authors proposed a dual-stream architecture with one stream dedicated to vision and the other to language."}),"\n",(0,t.jsx)(n.li,{children:"These two streams interact through co-attention Transformer layers, allowing variable network depth in each modality and enabling cross-modal connections at different depths."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Co-Attention Transformer Layer"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"This is the core mechanism introduced by the authors, allowing the model to exchange information between keys and values in each modality."}),"\n",(0,t.jsx)(n.li,{children:"This design enables features in one modality to be integrated into the representations of the other modality."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Image Representation"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The authors chose to extract bounding boxes and corresponding visual features from a pre-trained object detection network."}),"\n",(0,t.jsx)(n.li,{children:"Since image regions do not have a fixed sequence, the authors encoded spatial positions based on region locations and combined them with visual features."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Training Tasks and Objectives"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The authors proposed two primary pre-training tasks: MLM (Masked Language Modeling) and multimodal alignment prediction."}),"\n",(0,t.jsx)(n.li,{children:"The MLM task aims to enable the model to reconstruct masked input."}),"\n",(0,t.jsx)(n.li,{children:"Multimodal alignment prediction requires the model to determine whether the image and text are aligned, i.e., if the text describes the image."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"pre-training-mechanism",children:"Pre-Training Mechanism"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Training ViLBERT"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Dataset"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The authors used the Conceptual Captions dataset for training, consisting of 3.3 million image-caption pairs automatically harvested from web images."}),"\n",(0,t.jsx)(n.li,{children:"Although this dataset contains some noise and incomplete captions, it provides a highly diverse set of visual content."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Implementation Details"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The language stream was initialized using the BERT-BASE model, pre-trained on BookCorpus and English Wikipedia."}),"\n",(0,t.jsx)(n.li,{children:"Visual features were extracted using Faster R-CNN, pre-trained on the Visual Genome dataset."}),"\n",(0,t.jsx)(n.li,{children:"Both the Transformer and co-attention Transformer blocks in the visual stream had 1024 neurons and 8 attention heads."}),"\n",(0,t.jsx)(n.li,{children:"The entire model was trained for 10 epochs on 8 TitanX GPUs."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Fine-Tuning on Vision and Language Tasks"})}),"\n",(0,t.jsx)(n.p,{children:"The authors applied the pre-trained ViLBERT model to a set of vision and language tasks and fine-tuned it. Below is a detailed overview of each task:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Visual Question Answering (VQA)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset"}),": Trained and evaluated on the VQA 2.0 dataset, which contains 1.1 million questions related to COCO images."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fine-Tuning Method"}),": Fine-tuned using two layers of MLP on top of the element-wise product of image and text elements, mapping the representation to 3,129 possible answers. Multilabel classification was performed using binary cross-entropy loss."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Visual Commonsense Reasoning (VCR)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset"}),": VCR dataset, sourced from 110,000 multiple-choice QA questions from movie scenes."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fine-Tuning Method"}),": Fine-tuned with each possible answer and question combination as four separate text inputs, then fine-tuned each text input with the image. Finally, predicted scores for each pair were calculated."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Grounding Referring Expressions"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset"}),": RefCOCO+ dataset."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fine-Tuning Method"}),": Used proposed bounding boxes generated by Mask R-CNN, re-ranked each set of image region proposals, and trained using binary cross-entropy loss."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Caption-Based Image Retrieval"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset"}),": Contains 31,000 images from Flickr, each with five unique captions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fine-Tuning Method"}),": Trained in a multiple-choice setting, randomly selecting three distractors by replacing a random caption, a random image, or a hard negative from the target image's 100 nearest neighbors. Calculated similarity scores for each pair using alignment prediction and applied the softmax function to determine the most likely match."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Zero-shot Caption-Based Image Retrieval"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset"}),": Used the Flickr30k dataset for zero-shot image retrieval."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Method"}),": Aimed to demonstrate ViLBERT's ability to localize based on text from pre-training on the Conceptual Captions dataset, generalizing to unseen visual and language variations without task-specific fine-tuning. The authors directly used the pre-trained ViLBERT model, leveraging the alignment prediction objective as a scoring function to evaluate image-caption alignment."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(n.h3,{id:"how-well-does-vilbert-perform",children:"How Well Does ViLBERT Perform?"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Table 1",src:i(29552).A+"",width:"1024",height:"264"})}),"\n",(0,t.jsx)(n.p,{children:"Referring to Table 1, the ViLBERT model demonstrated excellent performance across various vision-language tasks:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Comparison with Baselines"})}),"\n",(0,t.jsx)(n.p,{children:"ViLBERT outperformed both single-stream models and ViLBERT in both pre-training and non-pre-training scenarios. Particularly in VQA and RefCOCO+, ViLBERT achieved the most significant gains."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Benefits of Pre-Training Tasks"})}),"\n",(0,t.jsx)(n.p,{children:"By using ViLBERT in pre-training tasks, performance improved by 2% to 13% across different tasks (comparing ViLBERT with ViLBERT\u2020). This indicates that both ViLBERT and single-stream models can benefit from these pre-training tasks."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Comparison with Task-Specific Benchmarks"})}),"\n",(0,t.jsx)(n.p,{children:"Fine-tuning ViLBERT surpassed state-of-the-art task-specific models in all four established tasks. Notably, in VCR, RefCOCO+, and image retrieval, ViLBERT set new technical standards with a 7-10 percentage point increase. Additionally, extending these tasks was relatively straightforward, requiring only the addition of a classifier for each task."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-does-the-depth-of-vilberts-visual-stream-affect-performance",children:"How Does the Depth of ViLBERT's Visual Stream Affect Performance?"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Table 2",src:i(96927).A+"",width:"1024",height:"194"})}),"\n",(0,t.jsx)(n.p,{children:"From Table 2, it's evident that the depth of ViLBERT's visual stream impacts its performance across various tasks:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"VQA and Image Retrieval"})}),"\n",(0,t.jsx)(n.p,{children:"Both tasks benefited from deeper ViLBERT models, with performance monotonically increasing as layer depth increased. Performance peaked at a depth of 6 layers."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Zero-shot Image Retrieval"})}),"\n",(0,t.jsx)(n.p,{children:"For this task, deeper models showed significant gains, indicating that deeper models might be more suitable for this task."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"VCR and RefCOCO+"})}),"\n",(0,t.jsx)(n.p,{children:"Contrary to the above tasks, these tasks seemed to favor shallower models, suggesting that different tasks might require different model depths for optimal performance."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"From the perspective of modern models with up to 100 layers (yes, GPT-4, we're talking about you!), whether it\u2019s 2 or 8 layers, it's still within the scale of small models."})}),"\n",(0,t.jsx)(n.h3,{id:"impact-of-pre-training-dataset-size",children:"Impact of Pre-Training Dataset Size"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Table 3",src:i(5218).A+"",width:"1024",height:"196"})}),"\n",(0,t.jsx)(n.p,{children:"The experiments show that the size of the pre-training dataset significantly impacts ViLBERT's performance."}),"\n",(0,t.jsx)(n.p,{children:"When randomly sampling 25% and 50% subsets of the Conceptual Captions dataset for pre-training, performance increased monotonically with more data used."}),"\n",(0,t.jsx)(n.p,{children:"This finding confirms the common notion in deep learning that more training data provides better performance, as the model has more opportunities to learn and extract features and patterns from numerous samples."}),"\n",(0,t.jsx)(n.p,{children:"Moreover, it suggests that ViLBERT has the potential to benefit from even larger datasets during pre-training, which could be a direction for future research and applications to further optimize and enhance model performance."}),"\n",(0,t.jsx)(n.h3,{id:"what-did-vilbert-learn",children:"What Did ViLBERT Learn?"}),"\n",(0,t.jsx)(n.p,{children:"During pre-training, ViLBERT learned semantically meaningful alignments between visual and language elements from the Conceptual Captions dataset. This is evident from its performance in zero-shot caption-based image retrieval: although zero-shot performance is noticeably lower than the fine-tuned models, it performs reasonably well on the Flickr30k images and captions without any exposure to them. This demonstrates that ViLBERT successfully learned and mastered alignment techniques during pre-training."}),"\n",(0,t.jsx)(n.p,{children:"Additionally, by inputting images and generating conditional text descriptions (essentially image captions), we can further understand what ViLBERT learned during pre-training. While the generated descriptions are unlikely to be high quality without fine-tuning on clean, human-annotated captions, they still provide insight into the pre-trained model's learning outcomes."}),"\n",(0,t.jsx)(n.p,{children:"However, it's important to note that generating text from BERT-style models remains a challenge. In ViLBERT's case, the generated \"captions\" often describe the primary content of the image but may sometimes include edited or non-visual concepts from Conceptual Captions, leading to discrepancies between descriptions and actual image content."}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"In this article, the researchers delve into the application of self-supervised learning in the vision and language domain. ViLBERT represents a milestone in this field, attempting to deeply integrate image content with text, tightly combining the two. It can be seen as a computer simultaneously reading images and text, trying to understand their relationship."}),"\n",(0,t.jsx)(n.p,{children:"Compared to the earlier VisualBERT, ViLBERT's dual-stream architecture stands out, providing a more detailed perspective to observe and learn the nuanced connections between images and text. For example, when describing a cooking process, ViLBERT can more accurately align textual descriptions with visual content, rather than simply identifying key objects or actions."}),"\n",(0,t.jsx)(n.p,{children:"While ViLBERT has already surpassed some previous techniques in certain applications, further validation and optimization are needed in more scenarios and tasks. Compared to earlier VisualBERT, ViLBERT shows more potential and depth, but each method has its unique aspects worth further research and comparison."})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},66248:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/arch_vilbert-16638bbbd4cdd4384236b9bb29fcfc8c.jpg"},29552:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vil_bert_table1-4bccb527b41c66e7cda171e364f53d1b.jpg"},96927:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vil_bert_table2-662499c8a2ff88bd94a328e1e4893697.jpg"},5218:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vil_bert_table3-087a820ee7c2681bf93473ca37e16ab2.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(96540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);