"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["36871"],{18822:function(e,s,n){n.r(s),n.d(s,{default:()=>d,frontMatter:()=>r,metadata:()=>a,assets:()=>c,toc:()=>h,contentTitle:()=>l});var a=JSON.parse('{"id":"vision-transformers/pvt/index","title":"[21.02] PVT","description":"Spatial Reduction Attention Mechanism","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/vision-transformers/2102-pvt/index.md","sourceDirName":"vision-transformers/2102-pvt","slug":"/vision-transformers/pvt/","permalink":"/en/papers/vision-transformers/pvt/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1733839479000,"frontMatter":{"title":"[21.02] PVT","authors":"Zephyr"},"sidebar":"papersSidebar","previous":{"title":"[21.02] CPVT","permalink":"/en/papers/vision-transformers/cpvt/"},"next":{"title":"[21.03] Swin Transformer","permalink":"/en/papers/vision-transformers/swin-transformer/"}}'),i=n("85893"),t=n("50065");let r={title:"[21.02] PVT",authors:"Zephyr"},l=void 0,c={},h=[{value:"Spatial Reduction Attention Mechanism",id:"spatial-reduction-attention-mechanism",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Hierarchical Structure",id:"hierarchical-structure",level:3},{value:"Spatial Reduction Attention (SRA)",id:"spatial-reduction-attention-sra",level:3},{value:"Model Configuration",id:"model-configuration",level:3},{value:"Discussion",id:"discussion",level:2},{value:"ImageNet Performance",id:"imagenet-performance",level:3},{value:"Object Detection Performance",id:"object-detection-performance",level:3},{value:"Contribution of the Pyramid Structure",id:"contribution-of-the-pyramid-structure",level:3},{value:"Depth vs. Width Trade-off",id:"depth-vs-width-trade-off",level:3},{value:"Addressing but Not Solving the Core Issue",id:"addressing-but-not-solving-the-core-issue",level:3},{value:"Conclusion",id:"conclusion",level:2}];function m(e){let s={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",msup:"msup",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h2,{id:"spatial-reduction-attention-mechanism",children:"Spatial Reduction Attention Mechanism"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2102.12122",children:(0,i.jsx)(s.strong,{children:"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"})})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"ViT has achieved remarkable results in image classification, officially marking the entry of the Transformer architecture into the realm of computer vision."}),"\n",(0,i.jsx)(s.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,i.jsx)(s.p,{children:"The ViT architecture begins by using 16 x 16 large kernel convolutions for patchifying images. For a 224 x 224 image, this process results in a 14 x 14 image. This resolution is sufficient for image classification tasks that rely on highly abstract global features. However, for dense prediction tasks like image segmentation or object detection, this patchifying method loses local details."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"Because the details are lost in the 16 x 16 feature compression process."})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Obviously, we need more refined features for dense prediction tasks. So, what if we change the 16 x 16 convolution to a 2 x 2 convolution?"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"Of course not!"})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Let's take a 224 x 224 image as an example:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Using a 16 x 16 patch size, we get 14 x 14, totaling 196 input tokens."}),"\n",(0,i.jsx)(s.li,{children:"Using a 2 x 2 patch size, we get 112 x 112, totaling 12,544 input tokens."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Imagine calculating a 12,544 x 12,544 self-attention matrix..."})}),"\n",(0,i.jsx)(s.p,{children:"Clearly, no one would consider this a good idea."}),"\n",(0,i.jsx)(s.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,i.jsx)(s.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"model architecture",src:n(96024).Z+"",width:"1368",height:"676"})}),"\n",(0,i.jsx)(s.p,{children:"The above image shows the design of the PVT architecture. While it may look complex, it essentially follows the same principles as convolutional neural network (CNN) architectures."}),"\n",(0,i.jsx)(s.h3,{id:"hierarchical-structure",children:"Hierarchical Structure"}),"\n",(0,i.jsx)(s.p,{children:"First, we notice the hierarchical structure, similar to the common 1/2 downsampling in ConvNets, split into five stages. Therefore, there are 1/2 size feature maps, 1/4 size feature maps, 1/8 size feature maps, and so on. In the PVT architecture, it starts from a 1/4 size feature map and goes down to a 1/32 size feature map."}),"\n",(0,i.jsx)(s.p,{children:"The downsampling process is implemented using convolutions with specified strides. For example, an input image of 3 x 224 x 224 with a stride of 4 convolution, producing 64 output channels, will result in a feature map of 64 x 56 x 56."}),"\n",(0,i.jsx)(s.p,{children:"So, in the first stage, the input to the Transformer encoder is:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Sequence Length"}),": 3136 (all image patches, i.e., 56 x 56)"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Feature Number"}),": 64 (features per patch, i.e., the number of channels)"]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"spatial-reduction-attention-sra",children:"Spatial Reduction Attention (SRA)"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"spatial reduction attention",src:n(57980).Z+"",width:"882",height:"432"})}),"\n",(0,i.jsx)(s.p,{children:"After obtaining the feature maps, we realize that a (56 x 56) x (56 x 56) self-attention matrix is still enormous and needs serious handling."}),"\n",(0,i.jsx)(s.p,{children:'The authors propose the concept of "Spatial Reduction Attention" (SRA), which retains the size of the queries (Q) but reduces the size of the keys (K) and values (V). For example, if the original self-attention map size is (56 x 56) x (56 x 56), reducing the key and value sizes to (7 x 7) will change the attention map size to 3136 x 49.'}),"\n",(0,i.jsx)(s.p,{children:"What seemed like an overly large self-attention matrix is now manageable."}),"\n",(0,i.jsxs)(s.p,{children:["Here's a closer look at the core concept of the paper. The key part is highlighted, and the ",(0,i.jsx)(s.code,{children:"sr_ratio"})," setting in this paper is ",(0,i.jsx)(s.strong,{children:"[8, 4, 2, 1]"}),", indicating the spatial reduction ratio for each stage."]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-python",metastring:"{31-34,40-44}",children:"# Reference:\n#   - https://github.com/whai362/PVT/blob/v2/classification/pvt.py\n\nimport torch\nimport torch.nn as nn\n\nclass SpatialReductionAttention(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.,\n        proj_drop=0.,\n        sr_ratio=1\n    ):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        if self.sr_ratio > 1:\n            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n            x_ = self.norm(x_)\n            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        else:\n            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        k, v = kv[0], kv[1]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n"})}),"\n",(0,i.jsx)(s.h3,{id:"model-configuration",children:"Model Configuration"}),"\n",(0,i.jsx)(s.p,{children:"Finally, let's look at the PVT model configuration."}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsxs)(s.msub,{children:[(0,i.jsx)(s.mi,{children:"P"}),(0,i.jsx)(s.mi,{children:"i"})]})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"P_i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"P"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.3117em"},children:(0,i.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.1389em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"})})]})}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(s.span,{})})})]})})]})]})})]}),": Patch size at stage ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"i"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6595em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"i"})]})})]})]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsxs)(s.msub,{children:[(0,i.jsx)(s.mi,{children:"C"}),(0,i.jsx)(s.mi,{children:"i"})]})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"C_i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"C"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.3117em"},children:(0,i.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0715em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"})})]})}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(s.span,{})})})]})})]})]})})]}),": Number of output channels at stage ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"i"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6595em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"i"})]})})]})]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsxs)(s.msub,{children:[(0,i.jsx)(s.mi,{children:"L"}),(0,i.jsx)(s.mi,{children:"i"})]})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"L_i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"L"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.3117em"},children:(0,i.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"})})]})}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(s.span,{})})})]})})]})]})})]}),": Number of encoder layers at stage ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"i"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6595em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"i"})]})})]})]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsxs)(s.msub,{children:[(0,i.jsx)(s.mi,{children:"R"}),(0,i.jsx)(s.mi,{children:"i"})]})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"R_i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.3117em"},children:(0,i.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0077em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"})})]})}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(s.span,{})})})]})})]})]})})]}),": Reduction ratio of SRA at stage ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"i"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6595em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"i"})]})})]})]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsxs)(s.msub,{children:[(0,i.jsx)(s.mi,{children:"N"}),(0,i.jsx)(s.mi,{children:"i"})]})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"N_i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.3117em"},children:(0,i.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.109em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"})})]})}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(s.span,{})})})]})})]})]})})]}),": Number of heads in SRA at stage ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"i"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6595em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"i"})]})})]})]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsxs)(s.msub,{children:[(0,i.jsx)(s.mi,{children:"E"}),(0,i.jsx)(s.mi,{children:"i"})]})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"E_i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"E"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.3117em"},children:(0,i.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0576em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"})})]})}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(s.span,{})})})]})})]})]})})]}),": Expansion ratio in the feed-forward layer at stage ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"i"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"i"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6595em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"i"})]})})]})]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The model design follows the design rules of ResNet:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"Use smaller output channels in the shallow stages."}),"\n",(0,i.jsx)(s.li,{children:"Concentrate most computational resources in the middle stages."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"For discussion purposes, the table below shows a series of PVT models of different scales: PVT-Tiny, PVT-Small, PVT-Medium, and PVT-Large. Their parameter counts are comparable to ResNet18, ResNet50, ResNet101, and ResNet152, respectively."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"model configuration",src:n(1074).Z+"",width:"1626",height:"728"})}),"\n",(0,i.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(s.h3,{id:"imagenet-performance",children:"ImageNet Performance"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"imagenet performance",src:n(76438).Z+"",width:"1098",height:"1196"})}),"\n",(0,i.jsx)(s.p,{children:"The authors compare PVT with two of the most representative CNN backbones, ResNet and ResNeXt, which are widely used in many downstream task benchmarks."}),"\n",(0,i.jsx)(s.p,{children:"In the table above, PVT models outperform traditional CNN backbones with similar parameter counts and computational budgets."}),"\n",(0,i.jsxs)(s.p,{children:["For instance, with a similar GFLOP, PVT-Small achieves a top-1 error rate of ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"20.2"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"%"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"20.2\\%"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8056em",verticalAlign:"-0.0556em"}}),(0,i.jsx)(s.span,{className:"mord",children:"20.2%"})]})})]}),", outperforming ResNet50's ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"21.5"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"%"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"21.5\\%"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8056em",verticalAlign:"-0.0556em"}}),(0,i.jsx)(s.span,{className:"mord",children:"21.5%"})]})})]})," by ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mn,{children:"1.3"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1.3"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1.3"})]})})]})," percentage points (",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"20.2"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"%"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"20.2\\%"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8056em",verticalAlign:"-0.0556em"}}),(0,i.jsx)(s.span,{className:"mord",children:"20.2%"})]})})]})," vs. ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"21.5"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"%"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"21.5\\%"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8056em",verticalAlign:"-0.0556em"}}),(0,i.jsx)(s.span,{className:"mord",children:"21.5%"})]})})]}),")."]}),"\n",(0,i.jsx)(s.p,{children:"With similar or lower complexity, PVT models achieve comparable performance to recently proposed Transformer-based models like ViT and DeiT."}),"\n",(0,i.jsxs)(s.p,{children:["PVT-Large achieves a top-1 error rate of ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"18.3"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"%"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"18.3\\%"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8056em",verticalAlign:"-0.0556em"}}),(0,i.jsx)(s.span,{className:"mord",children:"18.3%"})]})})]}),", on par with ViT (DeiT)-Base/16's ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"18.3"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"%"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"18.3\\%"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8056em",verticalAlign:"-0.0556em"}}),(0,i.jsx)(s.span,{className:"mord",children:"18.3%"})]})})]}),"."]}),"\n",(0,i.jsx)(s.h3,{id:"object-detection-performance",children:"Object Detection Performance"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"dense prediction performance-retinanet",src:n(59426).Z+"",width:"1428",height:"516"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"dense prediction performance-maskrcnn",src:n(78735).Z+"",width:"1416",height:"476"})}),"\n",(0,i.jsx)(s.p,{children:"Since this architecture targets dense prediction tasks from the outset, let's focus on PVT's performance in dense prediction tasks."}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Datasets"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Using the COCO benchmark"}),"\n",(0,i.jsx)(s.li,{children:"Training set: COCO train2017 (118k images)"}),"\n",(0,i.jsx)(s.li,{children:"Validation set: COCO val2017 (5k images)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Models and Initialization"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Standard detectors: RetinaNet and Mask R-CNN"}),"\n",(0,i.jsx)(s.li,{children:"Backbone initialized with ImageNet pretrained weights"}),"\n",(0,i.jsx)(s.li,{children:"New layers initialized with Xavier initialization"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Training Settings"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Batch size: 16"}),"\n",(0,i.jsx)(s.li,{children:"Hardware: 8 V100 GPUs"}),"\n",(0,i.jsx)(s.li,{children:"Optimizer: AdamW"}),"\n",(0,i.jsxs)(s.li,{children:["Initial learning rate: ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mn,{children:"10"}),(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mo,{children:"\u2212"}),(0,i.jsx)(s.mn,{children:"4"})]})]})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1 \\times 10^{-4}"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8141em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord",children:"0"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.8141em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(s.span,{className:"mord mtight",children:[(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2212"}),(0,i.jsx)(s.span,{className:"mord mtight",children:"4"})]})})]})})})})})]})]})]})]})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"With comparable parameter counts, PVT models significantly outperform traditional models. With RetinaNet, PVT-Tiny's AP is 4.9 points higher than ResNet18 (36.7 vs. 31.8)."}),"\n",(0,i.jsx)(s.p,{children:"With Mask R-CNN, PVT-Tiny's mask AP (APm) is 35.1, 3.9 points higher than ResNet18 (35.1 vs. 31.2), and even 0.7 points higher than ResNet50 (35.1 vs. 34.4)."}),"\n",(0,i.jsx)(s.p,{children:"These results suggest that PVT can serve as a good alternative to CNN backbones for object detection and instance segmentation tasks."}),"\n",(0,i.jsx)(s.p,{children:"The image below shows PVT's results on the COCO validation set."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"coco validation results",src:n(84018).Z+"",width:"1024",height:"784"})}),"\n",(0,i.jsx)(s.h3,{id:"contribution-of-the-pyramid-structure",children:"Contribution of the Pyramid Structure"}),"\n",(0,i.jsx)(s.p,{children:"The authors conducted several ablation studies to verify the impact of different parts of PVT on performance."}),"\n",(0,i.jsx)(s.p,{children:"First, the contribution analysis of the pyramid structure is shown in the table below:"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"pyramid structure contribution",src:n(47691).Z+"",width:"1080",height:"260"})}),"\n",(0,i.jsx)(s.p,{children:"Compared to the original ViT structure, PVT's pyramid structure improved the AP score by 8.7 percentage points."}),"\n",(0,i.jsx)(s.p,{children:"This indicates that the pyramid structure design helps improve performance in dense prediction tasks."}),"\n",(0,i.jsx)(s.h3,{id:"depth-vs-width-trade-off",children:"Depth vs. Width Trade-off"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"depth vs width",src:n(89780).Z+"",width:"994",height:"196"})}),"\n",(0,i.jsx)(s.p,{children:"The authors further explore whether PVT should be deeper or wider, as well as the impact of feature map sizes at different stages on performance."}),"\n",(0,i.jsx)(s.p,{children:"By multiplying PVT-Small's hidden dimension by 1.4, making it comparable to PVT-Medium in parameter count, the experiments show that deeper models perform better under similar parameter counts."}),"\n",(0,i.jsx)(s.h3,{id:"addressing-but-not-solving-the-core-issue",children:"Addressing but Not Solving the Core Issue"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"limitations",src:n(97877).Z+"",width:"1218",height:"840"})}),"\n",(0,i.jsx)(s.p,{children:"Lastly, the authors discussed performance limitations."}),"\n",(0,i.jsx)(s.p,{children:"As the input size increases, the GFLOPs of PVT grow faster than ResNet but slower than ViT. When the input size does not exceed 640\xd7640 pixels, PVT-Small and ResNet50 have similar GFLOPs."}),"\n",(0,i.jsx)(s.p,{children:"Additionally, with a fixed input image size of 800 pixels on the shorter side, the inference speed of RetinaNet based on PVT-Small is slower."}),"\n",(0,i.jsx)(s.p,{children:"In practical scenarios, convolutional network architectures are better suited for large input sizes, a key direction for performance improvement."}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsx)(s.p,{children:"The root cause is that the SRA mechanism merely reduces the size of the self-attention matrix without addressing the computational complexity issue of the self-attention matrix at its core."})}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(s.p,{children:"Although in 2021, comparing against ResNet and ResNeXt might seem conservative, this study introduces the pyramid structure into Transformers to provide a pure Transformer backbone for dense prediction tasks, rather than focusing on task-specific heads or image classification models, offering an important direction for future research."}),"\n",(0,i.jsx)(s.p,{children:"At this point, Transformer-based models in computer vision are still in their early stages, and many potential technologies and applications remain to be explored."})]})}function d(e={}){let{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},96024:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img1-fc2045bdeaa56a12a3e69d5e8d908c08.jpg"},97877:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img10-32e9701f3cb06f1821a22d3907b33e66.jpg"},57980:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img2-3d0b1b4353ec53868f89a464ff17d3e5.jpg"},1074:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img3-02585ce188d13dc5a82ad837d2604d7b.jpg"},76438:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img4-15fa3d4bff76fa6641ca93058364be37.jpg"},59426:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img5-ac354d02e9d4777d893ad9cdddcfa0af.jpg"},78735:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img6-a19bf9f68e70c920f53b1505cfb4fcd0.jpg"},84018:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img7-77b4feb4dea9efc0417f5e7a32867b85.jpg"},47691:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img8-01237fed10029bfad5ceee0d9a91776a.jpg"},89780:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img9-5b41322f17249faebaf42c76113c6fe4.jpg"},50065:function(e,s,n){n.d(s,{Z:function(){return l},a:function(){return r}});var a=n(67294);let i={},t=a.createContext(i);function r(e){let s=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:s},e.children)}}}]);