"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["75471"],{64268:function(e,n,t){t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>l,metadata:()=>a,toc:()=>u});var a=t(95668),i=t(85893),r=t(50065);let l={slug:"file-crawler-python-implementation",title:"Python Implementation of a Web File Downloader",authors:"Z. Yuan",image:"/en/img/2024/0923.webp",tags:["Python","File Crawler"],description:"Implement a simple web file downloader."},s=void 0,o={authorsImageUrls:[void 0]},u=[{value:"Install Required Packages",id:"install-required-packages",level:2},{value:"The Code",id:"the-code",level:2},{value:"Running the Script",id:"running-the-script",level:2}];function d(e){let n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,r.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"We came across a webpage containing hundreds of PDF file links."}),"\n",(0,i.jsx)(n.p,{children:"As engineers, if we were to download them manually, it would be highly inefficient, right?"}),"\n",(0,i.jsx)(n.p,{children:"So, what we need here is a small script that will help us download all the files."}),"\n",(0,i.jsx)(n.h2,{id:"install-required-packages",children:"Install Required Packages"}),"\n",(0,i.jsx)(n.p,{children:"First, you need to install the necessary packages. If you haven't installed them yet, you can do so using the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install requests beautifulsoup4 urllib3\n"})}),"\n",(0,i.jsx)(n.h2,{id:"the-code",children:"The Code"}),"\n",(0,i.jsx)(n.p,{children:"Without further ado, since the script is already written, let's dive straight into the code!"}),"\n",(0,i.jsx)(n.p,{children:"The parts highlighted are the ones you\u2019ll need to modify yourself. Adjust the script according to your needs."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'{13,16} title="file_crawler.py"',children:'import os\nfrom urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Simulating a browser\'s headers\nheaders = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"\n}\n\n# Web page URL\nurl = "put_your_url_here"\n\n# Target file format\ntarget_format = ".pdf"\n\n# Send an HTTP GET request with headers\nresponse = requests.get(url, headers=headers)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Use BeautifulSoup to parse the HTML\n    soup = BeautifulSoup(response.text, "html.parser")\n\n    # Find all <a> tags and filter those with href attributes matching the target format\n    target_links = []\n    for link in soup.find_all("a"):\n        href = link.get("href")\n        if href and href.endswith(target_format):  # Specify the file format you want to download\n            target_links.append(urljoin(url, href))\n\n    # Create a folder to save the files\n    os.makedirs("downloads", exist_ok=True)\n\n    # Download each file\n    for url in target_links:\n        file_name = url.split("/")[-1]  # Extract the filename from the URL\n        file_path = os.path.join("downloads", file_name)\n\n        # Send a request to download the file\n        response = requests.get(url, headers=headers)  # Add headers again\n        if response.status_code == 200:\n            with open(file_path, "wb") as f:\n                f.write(response.content)\n            print(f"Downloaded: {file_name}")\n        else:\n            print(f"Failed to download: {url}")\nelse:\n    print(f"Unable to access the webpage, status code: {response.status_code}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"running-the-script",children:"Running the Script"}),"\n",(0,i.jsx)(n.p,{children:"Once you\u2019re done, you can simply run the script to download all the files matching the target format."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python file_crawler.py\n"})})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},50065:function(e,n,t){t.d(n,{Z:()=>s,a:()=>l});var a=t(67294);let i={},r=a.createContext(i);function l(e){let n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),a.createElement(r.Provider,{value:n},e.children)}},95668:function(e){e.exports=JSON.parse('{"permalink":"/en/blog/file-crawler-python-implementation","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/09-23-file-crawler/index.md","title":"Python Implementation of a Web File Downloader","description":"Implement a simple web file downloader.","date":"2024-09-23T00:00:00.000Z","tags":[{"inline":true,"label":"Python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"File Crawler","permalink":"/en/blog/tags/file-crawler"}],"readingTime":1.67,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"file-crawler-python-implementation","title":"Python Implementation of a Web File Downloader","authors":"Z. Yuan","image":"/en/img/2024/0923.webp","tags":["Python","File Crawler"],"description":"Implement a simple web file downloader."},"unlisted":false,"prevItem":{"title":"Update Docusaurus to 3.6.0","permalink":"/en/blog/update-docusaurus-to-3-6-0"},"nextItem":{"title":"Automatically Count Articles in Docusaurus Sidebar","permalink":"/en/blog/customized-docusaurus-sidebars-auto-count"}}')}}]);