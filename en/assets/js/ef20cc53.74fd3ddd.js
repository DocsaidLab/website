"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([["51804"],{43926:function(e,n,i){i.r(n),i.d(n,{metadata:()=>t,contentTitle:()=>o,default:()=>h,assets:()=>l,toc:()=>c,frontMatter:()=>a});var t=JSON.parse('{"id":"multimodality/vlbert/index","title":"[19.08] VL-BERT","description":"Watching from the Sidelines","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/1908-vlbert/index.md","sourceDirName":"multimodality/1908-vlbert","slug":"/multimodality/vlbert/","permalink":"/en/papers/multimodality/vlbert/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1733839479000,"frontMatter":{"title":"[19.08] VL-BERT","authors":"Zephyr"},"sidebar":"papersSidebar","previous":{"title":"[19.08] VisualBERT","permalink":"/en/papers/multimodality/visualbert/"},"next":{"title":"[19.09] UNITER","permalink":"/en/papers/multimodality/uniter/"}}'),s=i("85893"),r=i("50065");let a={title:"[19.08] VL-BERT",authors:"Zephyr"},o=void 0,l={},c=[{value:"Watching from the Sidelines",id:"watching-from-the-sidelines",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Approach",id:"approach",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Pre-training Mechanism",id:"pre-training-mechanism",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Comparison with Other Models",id:"comparison-with-other-models",level:3},{value:"Can the Model Understand Natural Language?",id:"can-the-model-understand-natural-language",level:3},{value:"Ablation Study",id:"ablation-study",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){let n={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"watching-from-the-sidelines",children:"Watching from the Sidelines"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1908.08530",children:(0,s.jsx)(n.strong,{children:"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"})})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"Previously, we discussed VisualBERT, a One-Tower Encoder structure, which primarily supervises the text portion without direct visual supervision. We also covered ViLBERT, a Two-Tower Encoder where visual and text encoders interact through cross-attention, though this adds complexity."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Wait, what are One-Tower and Two-Tower architectures?"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We haven\u2019t emphasized these terms earlier, as they might seem confusing without further context. But with the emergence of diverse model architectures, it\u2019s a good time to introduce them."}),"\n",(0,s.jsx)(n.p,{children:"In vision-and-language learning, One-Tower and Two-Tower are two main architectures describing how multimodal information is integrated and exchanged."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Single-Tower Architecture (One-Tower)"})}),"\n",(0,s.jsx)(n.p,{children:"In this architecture, a single Transformer encoder operates on concatenated visual and language inputs. This approach enables unconstrained and direct interaction between visual and text tokens and generally requires fewer parameters than Two-Tower models."}),"\n",(0,s.jsx)(n.p,{children:"Examples include ViLT, VL-BERT, UNITER, and OSCAR. Many models like VisualBERT and VL-BERT adapt BERT for this purpose."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Two-Tower Architecture (Dual-Tower)"})}),"\n",(0,s.jsx)(n.p,{children:"Here, visual and text inputs are processed separately in distinct Transformer stacks. Cross-modal interactions are introduced through cross-attention layers."}),"\n",(0,s.jsx)(n.p,{children:"This architecture allows more explicit and structured interactions, with a separate encoder for each modality, as seen in ViLBERT, LXMERT, and BridgeTower."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,s.jsx)(n.p,{children:"When choosing between these architectures, there are trade-offs to consider."}),"\n",(0,s.jsx)(n.p,{children:"On one hand, One-Tower Encoder provides a straightforward training approach but may not capture deep associations between images and text. On the other, Two-Tower Encoder captures richer connections but adds complexity and demands more computational resources."}),"\n",(0,s.jsx)(n.p,{children:"This paper\u2019s authors aim to integrate the strengths of both architectures, creating a truly versatile visual-linguistic representation that performs well across multiple vision-language tasks."}),"\n",(0,s.jsx)(n.h2,{id:"approach",children:"Approach"}),"\n",(0,s.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"VL-BERT Model Architecture",src:i(50366).Z+"",width:"1576",height:"688"})})})}),"\n",(0,s.jsx)(n.p,{children:"Inspired by BERT, the VL-BERT model builds on the original BERT framework, a multi-layer bidirectional Transformer encoder designed to capture dependencies between input elements."}),"\n",(0,s.jsxs)(n.p,{children:["To accommodate both visual and language inputs, the model input sequence begins with a special ",(0,s.jsx)(n.code,{children:"[CLS]"})," token, followed by language elements, visual elements, and ends with an ",(0,s.jsx)(n.code,{children:"[END]"})," token. A ",(0,s.jsx)(n.code,{children:"[SEP]"})," token is inserted between language and visual elements to differentiate them clearly."]}),"\n",(0,s.jsx)(n.p,{children:"Each input element in VL-BERT is represented by four encoding types: token encoding, visual feature encoding, segment encoding, and sequence position encoding. Visual feature encoding is especially important for capturing visual clues, while the other encodings derive from the original BERT design."}),"\n",(0,s.jsx)(n.p,{children:"The visual feature encoding includes appearance features extracted by Faster R-CNN from RoIs (regions of interest) and geometric features describing each element's position within the image."}),"\n",(0,s.jsx)(n.h3,{id:"pre-training-mechanism",children:"Pre-training Mechanism"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"VL-BERT Pre-training Process",src:i(19622).Z+"",width:"1224",height:"980"})})})}),"\n",(0,s.jsx)(n.p,{children:"The authors design specific pre-training tasks to help VL-BERT capture relationships between visual and language elements."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Task #1: Masked Language Modeling (MLM) with Visual Clues"})}),"\n",(0,s.jsx)(n.p,{children:"This task adapts BERT\u2019s MLM task to use visual information. For example, given the sentence \u201CThe puppy is playing in the [MASK],\u201D and an image showing a puppy playing in a pool, the model predicts that the masked word is \u201Cpool\u201D using unmasked portions and visual clues."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Task #2: Masked RoI Classification with Language Clues"})}),"\n",(0,s.jsx)(n.p,{children:"Here, a part of an image (e.g., a bird) is masked. Given a description like \u201CThe bird is flying in the clear [MASK],\u201D the model must predict that the masked RoI corresponds to \u201Csky\u201D using other image parts and contextual cues from the text."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(n.h3,{id:"comparison-with-other-models",children:"Comparison with Other Models"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"VL-BERT Performance on VQA Task",src:i(24078).Z+"",width:"1024",height:"325"})}),"\n",(0,s.jsx)(n.p,{children:"The dataset used is VQA v2.0, based on COCO images, with training, validation, and test splits comprising 83k, 41k, and 81k images, respectively, and 444k, 214k, and 448k questions."}),"\n",(0,s.jsx)(n.p,{children:"The pre-trained VL-BERT shows a 1.6% improvement on the VQA task. Compared to BUTD, a popular model tailored for this task, VL-BERT achieves over 5% higher accuracy. Among contemporary models, VL-BERT performs well, ranking just behind LXMERT, which was pre-trained on a larger dataset."}),"\n",(0,s.jsx)(n.p,{children:"This experiment highlights the importance of pre-training, especially for tasks like visual question answering. Despite training on a smaller dataset than LXMERT, VL-BERT achieves comparable performance to state-of-the-art models."}),"\n",(0,s.jsx)(n.h3,{id:"can-the-model-understand-natural-language",children:"Can the Model Understand Natural Language?"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"VL-BERT Performance on RefCOCO+",src:i(58377).Z+"",width:"1024",height:"288"})}),"\n",(0,s.jsx)(n.p,{children:"To investigate this, the authors use the RefCOCO+ dataset."}),"\n",(0,s.jsx)(n.p,{children:"RefCOCO+ is a referential object retrieval dataset focusing on visual grounding tasks where a system receives an image and a natural language description and must locate or identify the specific object mentioned in the description."}),"\n",(0,s.jsx)(n.p,{children:"Results show that pre-trained VL-BERT significantly improves reference understanding performance, confirming the effectiveness of its pre-training strategy."}),"\n",(0,s.jsx)(n.p,{children:"When compared with other models like MAttNet, VL-BERT demonstrates strong and efficient performance. Although VL-BERT has a simpler architecture without task-specific modules, it performs comparably to advanced models like ViLBERT."}),"\n",(0,s.jsx)(n.h3,{id:"ablation-study",children:"Ablation Study"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Ablation Study Results",src:i(81850).Z+"",width:"2502",height:"532"})}),"\n",(0,s.jsx)(n.p,{children:"The authors conducted extensive experiments to assess the impact of different design choices on model performance."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Importance of Pre-training"})}),"\n",(0,s.jsx)(n.p,{children:"Comparing \u201Cno pre-training\u201D with the VL-BERT-BASE setting, pre-training shows clear improvements across all three downstream tasks, underscoring its central role in the model\u2019s design."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Task-Specific Benefits"})}),"\n",(0,s.jsx)(n.p,{children:"Different pre-training tasks benefit different downstream tasks. For example, the Masked RoI Classification task with language cues particularly benefits RefCOCO+ but may not be optimal for other tasks."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Sentence-Image Relation Prediction"})}),"\n",(0,s.jsx)(n.p,{children:"Although sentence-image relation prediction was considered beneficial, it negatively affected performance across all three downstream tasks, highlighting the need to carefully choose pre-training tasks."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Addition of Pure Text Corpus"})}),"\n",(0,s.jsx)(n.p,{children:"Adding a pure text corpus enhances performance across all downstream tasks, particularly in the VCR task, which involves complex sentences. This underscores the importance of language information for vision-language models."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"End-to-End Training"})}),"\n",(0,s.jsx)(n.p,{children:"Fine-tuning the entire network, including the Fast R-CNN portion generating visual features, further improves performance across all downstream tasks. This emphasizes the importance of integrating and aligning the visual and language components."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The results confirm that VL-BERT\u2019s pre-training strategy is crucial, not only enhancing task-specific performance but also improving its generalization across multiple downstream tasks. The model design also considers how different pre-training tasks affect downstream performance, optimizing results through end-to-end training."}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"VL-BERT\u2019s major strength is its Transformer-based architecture, which avoids task-specific modules, achieving simplicity and efficiency. Pre-training on both the Conceptual Captions dataset and pure text corpus reinforces its ability to align visual and language cues effectively."}),"\n",(0,s.jsx)(n.p,{children:"The authors\u2019 exploration of additional pre-training tasks offers valuable insights for future researchers and suggests directions for further optimization."})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},50366:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/arch_vlbert-0463a7b69a3740da7cab71514d1075d6.jpg"},24078:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/vl_bert_table2-3361ee90c34a4b4124b96df1e913e237.jpg"},58377:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/vl_bert_table3-f4fb9c39c2675f6407aa40b93ae3ab5b.jpg"},81850:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/vlbert_ablation-f760e827306b29d804c74183515a0589.jpg"},19622:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/vlbert_finetune-5378255fb3cb43407971009120f93651.jpg"},50065:function(e,n,i){i.d(n,{Z:function(){return o},a:function(){return a}});var t=i(67294);let s={},r=t.createContext(s);function a(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);