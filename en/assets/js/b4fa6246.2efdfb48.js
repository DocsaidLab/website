"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[692],{53340:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>d});var t=i(74848),o=i(28453);const r={sidebar_position:4},s="Advanced",a={id:"docaligner/advance",title:"Advanced",description:"When invoking the DocAligner model, you can make advanced settings by passing parameters.",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/docaligner/advance.md",sourceDirName:"docaligner",slug:"/docaligner/advance",permalink:"/en/docs/docaligner/advance",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:17230934e5,sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"QuickStart",permalink:"/en/docs/docaligner/quickstart"},next:{title:"Model Design",permalink:"/en/docs/docaligner/model_arch"}},c={},d=[{value:"Initialization",id:"initialization",level:2},{value:"1. Backend",id:"1-backend",level:3},{value:"2. ModelType",id:"2-modeltype",level:3},{value:"3. ModelCfg",id:"3-modelcfg",level:3},{value:"Inference",id:"inference",level:2},{value:"CenterCrop",id:"centercrop",level:3},{value:"Return <code>Document</code> Object",id:"return-document-object",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"advanced",children:"Advanced"}),"\n",(0,t.jsxs)(n.p,{children:["When invoking the ",(0,t.jsx)(n.code,{children:"DocAligner"})," model, you can make advanced settings by passing parameters."]}),"\n",(0,t.jsx)(n.h2,{id:"initialization",children:"Initialization"}),"\n",(0,t.jsx)(n.p,{children:"Here are the advanced options available during the initialization phase:"}),"\n",(0,t.jsx)(n.h3,{id:"1-backend",children:"1. Backend"}),"\n",(0,t.jsxs)(n.p,{children:["Backend is an enumeration type used to specify the computational backend for ",(0,t.jsx)(n.code,{children:"DocAligner"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"It includes the following options:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"cpu"}),": Use the CPU for computation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"cuda"}),": Use the GPU for computation (requires appropriate hardware support)."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from docsaidkit import Backend\n\nmodel = DocAligner(backend=Backend.cuda) # Using CUDA backend\n#\n# Or\n#\nmodel = DocAligner(backend=Backend.cpu) # Using CPU backend\n"})}),"\n",(0,t.jsx)(n.p,{children:"We use ONNXRuntime as the inference engine for the model. Although ONNXRuntime supports various backend engines (including CPU, CUDA, OpenCL, DirectX, TensorRT, etc.), due to common usage environments, we've encapsulated it slightly, currently only offering CPU and CUDA backend engines. Additionally, using the CUDA computation not only requires appropriate hardware but also the installation of corresponding CUDA drivers and CUDA Toolkit."}),"\n",(0,t.jsx)(n.p,{children:"If CUDA is not installed in your system, or if the installed version is incorrect, the CUDA computation backend cannot be used."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["If you have other requirements, please refer to the ",(0,t.jsx)(n.a,{href:"https://onnxruntime.ai/docs/execution-providers/index.html",children:(0,t.jsx)(n.strong,{children:"ONNXRuntime Official Documentation"})})," for customization."]}),"\n",(0,t.jsxs)(n.li,{children:["For issues related to dependency installation, please refer to the ",(0,t.jsx)(n.a,{href:"https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements",children:(0,t.jsx)(n.strong,{children:"ONNXRuntime Release Notes"})})]}),"\n"]})}),"\n",(0,t.jsx)(n.h3,{id:"2-modeltype",children:"2. ModelType"}),"\n",(0,t.jsxs)(n.p,{children:["ModelType is an enumeration type used to specify the type of model used by ",(0,t.jsx)(n.code,{children:"DocAligner"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"It includes the following options:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"heatmap"}),": Use the heatmap model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"point"}),": Use the point regression model."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'We offer two different models: "heatmap model" and "point regression model."'}),"\n",(0,t.jsxs)(n.p,{children:["You can specify the model to use via the ",(0,t.jsx)(n.code,{children:"model_type"})," parameter."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from docaligner import ModelType\n\nmodel = DocAligner(model_type=ModelType.heatmap) # Using the heatmap model\n#\n# Or\n#\nmodel = DocAligner(model_type=ModelType.point) # Using the point regression model\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:'However, it\'s advised not to use the "point regression" model, as its performance is not very satisfactory; this is purely for research purposes.'})}),"\n",(0,t.jsx)(n.h3,{id:"3-modelcfg",children:"3. ModelCfg"}),"\n",(0,t.jsx)(n.p,{children:"We have trained many models and named them,"}),"\n",(0,t.jsxs)(n.p,{children:["You can use ",(0,t.jsx)(n.code,{children:"list_models"})," to see all available models."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from docaligner import DocAligner\n\nprint(DocAligner().list_models())\n# >>> [\n#     'lcnet050',\n#     'lcnet050_fpn',\n#     'lcnet100',\n#     'lcnet100_fpn',\n#     'mobilenetv2_140',\n#     'fastvit_t8',\n#     'fastvit_sa24',       <-- Default\n#     ...\n# ]\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You can specify the model configuration using the ",(0,t.jsx)(n.code,{children:"model_cfg"})," parameter."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"model = DocAligner(model_cfg='mobilenetv2_140') # Using 'mobilenetv2_140' configuration\n"})}),"\n",(0,t.jsx)(n.h2,{id:"inference",children:"Inference"}),"\n",(0,t.jsx)(n.p,{children:"Here are the advanced settings options during the inference phase:"}),"\n",(0,t.jsx)(n.h3,{id:"centercrop",children:"CenterCrop"}),"\n",(0,t.jsx)(n.p,{children:"Setting appropriate advanced options during the inference phase can significantly affect model performance and effectiveness."}),"\n",(0,t.jsxs)(n.p,{children:["Among them, ",(0,t.jsx)(n.code,{children:"do_center_crop"})," is a key parameter that determines whether to perform center cropping during inference."]}),"\n",(0,t.jsx)(n.p,{children:"This setting is particularly important because in real-world applications, the images we encounter are often not in standard square sizes."}),"\n",(0,t.jsx)(n.p,{children:"In reality, image dimensions and proportions vary greatly, such as:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Photos taken by mobile phones commonly adopt a 9:16 aspect ratio;"}),"\n",(0,t.jsx)(n.li,{children:"Scanned documents often appear in A4 paper ratios;"}),"\n",(0,t.jsx)(n.li,{children:"Screenshots are mostly in a 16:9 aspect ratio;"}),"\n",(0,t.jsx)(n.li,{children:"Images captured through webcams are usually in a 4:3 ratio."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These non-square images, when used directly for inference without proper processing, often contain large areas of irrelevant regions or whitespace, which adversely affect the model's inference effectiveness. Performing center cropping can effectively reduce these irrelevant areas, focusing on the central region of the image, thereby improving the accuracy and efficiency of inference."}),"\n",(0,t.jsx)(n.p,{children:"Usage is as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import docsaidkit as D\nfrom docaligner import DocAligner\n\nmodel = DocAligner()\n\nimg = D.imread('path/to/image.jpg')\nresult = model(img, do_center_crop=True) # Using center cropping\n"})}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"When to use"}),': Use center cropping when "not cutting the image" and when the image ratio is not square.']}),(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"Center cropping is just one step in the computation process and does not modify the original image. The final results will be mapped back to the original image size, so users need not worry about image distortion or loss of quality."})})]}),"\n",(0,t.jsxs)(n.h3,{id:"return-document-object",children:["Return ",(0,t.jsx)(n.code,{children:"Document"})," Object"]}),"\n",(0,t.jsxs)(n.p,{children:["Using the ",(0,t.jsx)(n.code,{children:"return_document_obj"})," parameter, you can specify whether to return a ",(0,t.jsx)(n.a,{href:"../docsaidkit/funcs/objects/document",children:(0,t.jsx)(n.strong,{children:"Document"})})," object."]}),"\n",(0,t.jsx)(n.p,{children:"Often, you might only need the polygon information of the document and not the other attributes."}),"\n",(0,t.jsxs)(n.p,{children:["In such cases, you can set ",(0,t.jsx)(n.code,{children:"return_document_obj=False"}),", which will only return the polygon information."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"result = model(img)\nprint(type(result))\n# >>> <class 'docsaidkit.funcs.objects.document.Document'>\n\n# Or\n\nresult = model(img, return_document_obj=False) # Only return polygon information\nprint(type(result))\n# >>> <class 'numpy.ndarray'>\n\nprint(result)\n# >>> array([[ 48.151894, 223.47687 ],\n#            [387.1344  , 198.09961 ],\n#            [423.0362  , 345.51334 ],\n#            [ 40.148613, 361.38782 ]], dtype=float32)\n"})}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsxs)(n.p,{children:["When you obtain a ",(0,t.jsx)(n.code,{children:"numpy.ndarray"}),", you can call the ",(0,t.jsx)(n.a,{href:"../docsaidkit/funcs/vision/geometric/imwarp_quadrangle",children:(0,t.jsx)(n.strong,{children:"Docsaidkit.imwarp_quadrangle"})})," function for further post-processing, see the example:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import docsaidkit as D\n\nresult = model(img, return_document_obj=False)\nflat_doc_img = D.imwarp_quadrangle(img, result)\n"})}),(0,t.jsx)(n.p,{children:"Output results as follows:"}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"flat_doc_img",src:i(820).A+"",width:"383",height:"148"})}),(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": The function ",(0,t.jsx)(n.a,{href:"../docsaidkit/funcs/vision/geometric/imwarp_quadrangle",children:(0,t.jsx)(n.strong,{children:"Docsaidkit.imwarp_quadrangle"})}),' does not support specifying document size, so the output image size will be determined by the "minimum rotated bounding rectangle" of the polygon.']})})]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},820:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/flat_result_1-718383b7dfb1628127a66957fd9192a0.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(96540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);