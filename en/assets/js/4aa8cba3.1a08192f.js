"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5378],{35346:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>h});var i=n(74848),a=n(28453);const t={},l="[21.09] PP-LCNet",r={id:"pp-lcnet/index",title:"[21.09] PP-LCNet",description:"Exploring the Boundaries of Speed",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/2109-pp-lcnet/index.md",sourceDirName:"2109-pp-lcnet",slug:"/pp-lcnet/",permalink:"/en/papers/pp-lcnet/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1724335925e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[21.06] BEiT",permalink:"/en/papers/beit/"},next:{title:"[21.11] PoolFormer",permalink:"/en/papers/poolformer/"}},c={},h=[{value:"Exploring the Boundaries of Speed",id:"exploring-the-boundaries-of-speed",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Experimental Results",id:"experimental-results",level:3},{value:"Ablation Studies",id:"ablation-studies",level:3},{value:"Conclusion",id:"conclusion",level:2}];function o(e){const s={a:"a",admonition:"admonition",annotation:"annotation",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",mtext:"mtext",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"2109-pp-lcnet",children:"[21.09] PP-LCNet"})}),"\n",(0,i.jsx)(s.h2,{id:"exploring-the-boundaries-of-speed",children:"Exploring the Boundaries of Speed"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2109.15099",children:(0,i.jsx)(s.strong,{children:"PP-LCNet: A Lightweight CPU Convolutional Neural Network"})})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"In the race for lightweight models, several main directions have emerged:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Optimizing parameter count"})," to reduce model size."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Optimizing computation (FLOPs)"})," to lower computational requirements and improve model speed."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Optimizing inference time"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Wait, what?"}),"\n",(0,i.jsx)(s.p,{children:"Isn't optimizing parameter count and computation the same as optimizing inference time?"}),"\n",(0,i.jsx)(s.p,{children:"Most people would think that reducing parameter count and computation would naturally lead to faster inference times."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"The answer is: Not necessarily!"})}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"Here's a simple example:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Residual connections in ResNet."}),"\n",(0,i.jsx)(s.li,{children:"Inception modules in GoogleNet."}),"\n",(0,i.jsx)(s.li,{children:"Shared parameters between different modules."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"These operations can increase inference time even with the same parameter count or computation due to memory read operations and inter-branch waiting times."}),"\n",(0,i.jsx)(s.p,{children:"However, this paper doesn't delve deeply into this issue. We will explore it further when we come across other related papers."}),"\n",(0,i.jsx)(s.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,i.jsx)(s.p,{children:"This paper aims to address a few key issues:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"How to enhance feature representation without increasing latency?"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"What are the key factors in improving the accuracy of lightweight models on CPUs?"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"How to effectively combine different strategies to design lightweight models on CPUs?"})}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"solution",children:"Solution"}),"\n",(0,i.jsx)(s.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"PP-LCNet arch",src:n(91325).A+"",width:"1224",height:"708"})}),"\n",(0,i.jsx)(s.p,{children:"The authors propose several strategies to address the above issues."}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"DepthSepConv"})}),"\n",(0,i.jsxs)(s.p,{children:["The authors use DepthSepConv from ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1704.04861",children:(0,i.jsx)(s.strong,{children:"MobileNetV1"})})," as the basic block. This approach avoids using shortcuts and concatenations, thereby improving computational efficiency."]}),"\n",(0,i.jsx)(s.p,{children:"Previous studies have shown that using shortcuts in small models does not significantly enhance performance."}),"\n",(0,i.jsx)(s.p,{children:"Moreover, Intel CPUs have optimizations for Inverted Block or ShuffleNet Block, resulting in better inference performance."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"HSwish"})}),"\n",(0,i.jsx)(s.p,{children:"Using the ReLU activation function is undoubtedly the fastest option!"}),"\n",(0,i.jsx)(s.p,{children:"But it\u2019s usually not the best."}),"\n",(0,i.jsx)(s.p,{children:"Many improved activation functions, such as Swish, Mish, and GELU, can enhance model performance."}),"\n",(0,i.jsxs)(s.p,{children:["Here, the authors reference HSwish from ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1905.02244",children:(0,i.jsx)(s.strong,{children:"MobileNetV3"})}),", an activation function that maintains speed while improving model performance."]}),"\n",(0,i.jsxs)(s.admonition,{type:"tip",children:[(0,i.jsx)(s.p,{children:"The HSwish function is expressed as follows:"}),(0,i.jsx)(s.p,{children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mtext,{children:"Hswish"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"x"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mi,{children:"x"}),(0,i.jsxs)(s.mfrac,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mtext,{children:"ReLU6"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"x"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mn,{children:"3"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.mn,{children:"6"})]}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mtext,{children:"ReLU6"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"x"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mi,{children:"min"}),(0,i.jsx)(s.mo,{children:"\u2061"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"max"}),(0,i.jsx)(s.mo,{children:"\u2061"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"x"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mn,{children:"0"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mn,{children:"6"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\text{Hswish}(x) = x \\frac{\\text{ReLU6}(x+3)}{6}, \\text{ReLU6}(x) = \\min(\\max(x, 0), 6)"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord text",children:(0,i.jsx)(s.span,{className:"mord",children:"Hswish"})}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"x"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.355em",verticalAlign:"-0.345em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"x"}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mopen nulldelimiter"}),(0,i.jsx)(s.span,{className:"mfrac",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsxs)(s.span,{className:"vlist",style:{height:"1.01em"},children:[(0,i.jsxs)(s.span,{style:{top:"-2.655em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"6"})})})]}),(0,i.jsxs)(s.span,{style:{top:"-3.23em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(s.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,i.jsxs)(s.span,{style:{top:"-3.485em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(s.span,{className:"mord mtight",children:[(0,i.jsx)(s.span,{className:"mord text mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"ReLU6"})}),(0,i.jsx)(s.span,{className:"mopen mtight",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"x"}),(0,i.jsx)(s.span,{className:"mbin mtight",children:"+"}),(0,i.jsx)(s.span,{className:"mord mtight",children:"3"}),(0,i.jsx)(s.span,{className:"mclose mtight",children:")"})]})})]})]}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.345em"},children:(0,i.jsx)(s.span,{})})})]})}),(0,i.jsx)(s.span,{className:"mclose nulldelimiter"})]}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord text",children:(0,i.jsx)(s.span,{className:"mord",children:"ReLU6"})}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"x"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mop",children:"min"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mop",children:"max"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"x"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord",children:"0"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord",children:"6"}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})]})]})}),(0,i.jsxs)(s.p,{children:["where ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mtext,{children:"ReLU6"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"x"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\text{ReLU6}(x)"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord text",children:(0,i.jsx)(s.span,{className:"mord",children:"ReLU6"})}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"x"}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})})]})," is a variant of the ReLU function that restricts the input ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"x"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"x"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"x"})]})})]})," to the range of 0 to 6."]}),(0,i.jsx)(s.p,{children:"The characteristics of this function include:"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Nonlinearity"}),": It provides the nonlinear processing capability necessary for deep learning models to learn complex data patterns."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Boundedness and Smoothness"}),": With the ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mtext,{children:"ReLU6"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\text{ReLU6}"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord text",children:(0,i.jsx)(s.span,{className:"mord",children:"ReLU6"})})]})})]})," restriction, the HSwish function is bounded in the range of ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mo,{children:"\u2212"}),(0,i.jsx)(s.mn,{children:"3"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"-3"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"\u2212"}),(0,i.jsx)(s.span,{className:"mord",children:"3"})]})})]})," to infinity and relatively smooth."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Computational Efficiency"}),": Its relatively simple structure makes it computationally efficient, making it particularly suitable for resource-constrained devices."]}),"\n"]})]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"SEBlock"})}),"\n",(0,i.jsx)(s.p,{children:"This module comes from the following paper:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1709.01507",children:(0,i.jsx)(s.strong,{children:"Squeeze-and-excitation networks"})})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"This module also helped SENet win the 2017 ImageNet challenge."}),"\n",(0,i.jsx)(s.p,{children:"However, on Intel CPUs, this module increases inference time, so the authors simplified it by placing it only at the network's end."}),"\n",(0,i.jsx)(s.p,{children:"Experiments showed that this design improved accuracy without compromising inference speed."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Large Kernel Convolutions"})}),"\n",(0,i.jsxs)(s.p,{children:["In ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1907.09595",children:(0,i.jsx)(s.strong,{children:"MixNet"})}),", the authors analyzed the impact of different kernel sizes on network performance and ultimately mixed different kernel sizes within the same network layer."]}),"\n",(0,i.jsx)(s.p,{children:"However, this arbitrary mixing reduces model inference speed, so the authors simplified it by placing it only at the network's end."}),"\n",(0,i.jsx)(s.p,{children:"Replacing 3x3 convolutions with 5x5 convolutions at the network's end improved model accuracy."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Additional Fully Connected Layer"})}),"\n",(0,i.jsx)(s.p,{children:"Due to the model's small size, its information capacity is insufficient. Therefore, the authors added an extra 1280-dimensional fully connected layer at the network's end."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(s.h3,{id:"experimental-results",children:"Experimental Results"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"PP-LCNet results",src:n(81426).A+"",width:"944",height:"780"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"PP-LCNet results2",src:n(49359).A+"",width:"1224",height:"516"})}),"\n",(0,i.jsx)(s.p,{children:"The authors conducted experiments on the ImageNet-1k dataset, which includes 1.28 million training images and 50,000 validation images across 1,000 categories."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Training Configuration"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Used SGD (stochastic gradient descent) as the optimizer, with a momentum of 0.9 and weight decay of 3e-5 (4e-5 for larger models)."}),"\n",(0,i.jsx)(s.li,{children:"Batch size of 2048, with an initial learning rate of 0.8, adjusted using a cosine annealing schedule."}),"\n",(0,i.jsx)(s.li,{children:"The entire training process includes 360 epochs with 5 linear warm-up epochs."}),"\n",(0,i.jsx)(s.li,{children:"Image preprocessing involved random cropping to 224\xd7224 pixels and random horizontal flipping."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Evaluation Phase"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"During evaluation, images were first resized along the short edge to 256 pixels, followed by center cropping to 224\xd7224 pixels."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Accuracy Enhancement Techniques"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Using the SSLD distillation method significantly improved PP-LCNet's accuracy, as shown in Table 3, comparing PP-LCNet with other state-of-the-art models."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"From the above charts, it is evident that PP-LCNet offers over 1.5 times faster inference speed than other models with similar computational and parameter counts and comparable accuracy."}),"\n",(0,i.jsx)(s.h3,{id:"ablation-studies",children:"Ablation Studies"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"ablation",src:n(71396).A+"",width:"2992",height:"1054"})}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"SE Module (Squeeze-and-Excitation)"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"The SE module enhances the model's attention to relationships between channels, thereby improving model accuracy."}),"\n",(0,i.jsx)(s.li,{children:"Adding SE modules at the end of the network proved more effective than adding them elsewhere."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Therefore, PP-LCNet prioritizes adding SE modules in the last two blocks to balance inference speed and accuracy."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Use of Large Kernels"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"While large kernels (such as 5\xd75) can improve accuracy, they are not suitable for adding to all positions in the network."}),"\n",(0,i.jsx)(s.li,{children:"Similar to the SE module, placing large kernels at the network's end proved more effective."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Thus, specific layers use 5\xd75 depthwise separable convolutions, while other layers retain 3\xd73 kernels."}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Combined Effect of Different Techniques"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"H-Swish and large kernels can enhance performance with minimal impact on inference time."}),"\n",(0,i.jsx)(s.li,{children:"Adding SE modules appropriately further enhances performance."}),"\n",(0,i.jsx)(s.li,{children:"Using a larger fully connected layer after global average pooling (GAP) significantly improves accuracy."}),"\n",(0,i.jsx)(s.li,{children:"Applying dropout strategies also helps improve model accuracy."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(s.p,{children:"In this study, the authors focused on developing methods for lightweight Intel CPU networks, aiming to improve model accuracy without increasing inference time."}),"\n",(0,i.jsx)(s.p,{children:"Through extensive experiments and method optimization, they proposed a network architecture that performs well on various visual tasks, particularly excelling in the balance of accuracy and speed."}),"\n",(0,i.jsxs)(s.admonition,{type:"info",children:[(0,i.jsx)(s.p,{children:"This model is indeed small, accurate, and has fast inference speed."}),(0,i.jsx)(s.p,{children:"For developing mobile applications or scenarios with limited model size, this would typically be our first choice."})]})]})}function d(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},91325:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img1-6beb01162ab94395de4fc852e93a2ce9.jpg"},81426:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img2-a13572e872b1d1dd5cc3a64bf19db520.jpg"},49359:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img3-54e164913c1181d902302e4de36bd8fc.jpg"},71396:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img4-64d2d49e9bdb117c07b3328a6b5fe215.jpg"},28453:(e,s,n)=>{n.d(s,{R:()=>l,x:()=>r});var i=n(96540);const a={},t=i.createContext(a);function l(e){const s=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);