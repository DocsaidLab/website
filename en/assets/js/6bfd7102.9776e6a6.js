"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([["55663"],{9763:function(e,n,s){s.r(n),s.d(n,{metadata:()=>t,contentTitle:()=>o,default:()=>d,assets:()=>l,toc:()=>c,frontMatter:()=>a});var t=JSON.parse('{"id":"mrzscanner/model_arch","title":"Model Design","description":"The world is simple, yet we complicate it ourselves.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/mrzscanner/model_arch.md","sourceDirName":"mrzscanner","slug":"/mrzscanner/model_arch","permalink":"/en/docs/mrzscanner/model_arch","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1732782069000,"sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Advanced","permalink":"/en/docs/mrzscanner/advance"},"next":{"title":"Dataset","permalink":"/en/docs/mrzscanner/dataset"}}'),i=s("85893"),r=s("50065");let a={sidebar_position:5},o="Model Design",l={},c=[{value:"Two-Stage Recognition Model",id:"two-stage-recognition-model",level:2},{value:"Localization Model",id:"localization-model",level:3},{value:"Recognition Model",id:"recognition-model",level:3},{value:"Error Propagation",id:"error-propagation",level:3},{value:"Single-Stage Recognition Model",id:"single-stage-recognition-model",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Backbone",id:"backbone",level:3},{value:"Neck",id:"neck",level:3},{value:"Patchify",id:"patchify",level:3},{value:"Cross-Attention",id:"cross-attention",level:3},{value:"Additional Notes",id:"additional-notes",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let n={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",math:"math",mn:"mn",mo:"mo",mrow:"mrow",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"model-design",children:"Model Design"})}),"\n",(0,i.jsx)(n.p,{children:"The world is simple, yet we complicate it ourselves."}),"\n",(0,i.jsx)(n.p,{children:"Yes, we're the ones causing our own headaches\u2014both annoyed and intrigued at the same time."}),"\n",(0,i.jsx)(n.h2,{id:"two-stage-recognition-model",children:"Two-Stage Recognition Model"}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsx)(n.p,{children:"During the testing phase, we are initially releasing an end-to-end single-stage model."}),(0,i.jsx)(n.p,{children:"The two-stage model that separates localization and recognition is not yet available, but we plan to release it in the V1.0 stable version."}),(0,i.jsx)(n.p,{children:"However, that won't stop us from discussing the detailed approach here."})]}),"\n",(0,i.jsx)(n.p,{children:"The two-stage model splits MRZ recognition into two phases: localization and recognition."}),"\n",(0,i.jsx)(n.p,{children:"Based on this idea, we can start designing the model. Let\u2019s first take a look at the localization model."}),"\n",(0,i.jsx)(n.h3,{id:"localization-model",children:"Localization Model"}),"\n",(0,i.jsx)(n.p,{children:"MRZ localization can be approached in two ways:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Locating MRZ corner points:"})}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"50%"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Locating MRZ corner points",src:s(80937).Z+"",width:"1237",height:"1384"}),"\n",(0,i.jsxs)("figcaption",{children:["Image source: ",(0,i.jsx)(n.a,{href:"http://l3i-share.univ-lr.fr/MIDV2020/midv2020.html",children:(0,i.jsx)(n.strong,{children:"MIDV-2020 Synthetic Dataset"})})]})]})})}),"\n",(0,i.jsx)(n.p,{children:"This is similar to previous document localization projects we've worked on, except here we\u2019re localizing the MRZ area instead of the whole document."}),"\n",(0,i.jsx)(n.p,{children:'The difference is that document corners "physically" exist on the image, and the model doesn\u2019t need to \u201Cimagine\u201D a corner. For the MRZ area, however, the model needs to "guess" where these corners are.'}),"\n",(0,i.jsx)(n.p,{children:"It turns out that using this method leads to an unstable model. If the passport is slightly moved, the predicted corners of the MRZ area tend to jump around."}),"\n",(0,i.jsx)(n.hr,{}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Segmenting the MRZ area:"})}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"50%"},children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"Segmenting the MRZ area",src:s(37198).Z+"",width:"1237",height:"1384"}),"\n",(0,i.jsxs)("figcaption",{children:["Image source: ",(0,i.jsx)(n.a,{href:"http://l3i-share.univ-lr.fr/MIDV2020/midv2020.html",children:(0,i.jsx)(n.strong,{children:"MIDV-2020 Synthetic Dataset"})})]})]})})}),"\n",(0,i.jsx)(n.p,{children:'This approach is much more stable, as we can directly predict the MRZ region using a segmentation model. The text within the MRZ area physically exists in the image, so the model doesn\u2019t have to "imagine" anything extra. This allows us to segment the MRZ region directly without worrying about corner points.'}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"We\u2019ve chosen the segmentation method."}),"\n",(0,i.jsx)(n.p,{children:"In real-world scenarios, passports are often held at slight angles. Therefore, we need to correct the MRZ region to transform it into a proper rectangle."}),"\n",(0,i.jsx)(n.p,{children:"Regarding the loss function, we referred to a survey paper:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2006.14822",children:(0,i.jsx)(n.strong,{children:"[20.06] A Survey of Loss Functions for Semantic Segmentation"})})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["This paper offers a unified comparison and introduction to various loss functions for segmentation proposed over recent years. It proposes a solution to existing issues: ",(0,i.jsx)(n.strong,{children:"Log-Cosh Dice Loss"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"For those interested, you can refer to the paper; we won\u2019t go into detail here."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Log-Cosh Dice Loss",src:s(35611).Z+"",width:"1224",height:"720"})}),"\n",(0,i.jsx)(n.h3,{id:"recognition-model",children:"Recognition Model"}),"\n",(0,i.jsx)(n.p,{children:"The recognition model is much simpler. Since we\u2019ve already segmented the MRZ region, we just need to pass that region to a text recognition model to get the final result."}),"\n",(0,i.jsx)(n.p,{children:"At this stage, there are a couple of design options:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Segment the text and recognize it piece by piece:"})}),"\n",(0,i.jsx)(n.p,{children:"Some MRZs have two lines of text, such as in the TD2 and TD3 formats; others have three lines, like the TD1 format. We can segment these lines of text and recognize them one by one."}),"\n",(0,i.jsx)(n.p,{children:"The recognition model\u2019s job is to convert a string of text from an image into output. There are many methods available, such as the classic CRNN+CTC or more modern approaches like CLIP4STR."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Recognize the entire cropped MRZ region at once:"})}),"\n",(0,i.jsx)(n.p,{children:"Since the aspect ratio of the MRZ region doesn\u2019t vary much, we can crop the whole MRZ area and recognize it in one go. In this case, transformer-based models are especially suitable for solving this problem."}),"\n",(0,i.jsx)(n.p,{children:"For example, if you use a Transformer Encoder structure, the model design could look like this:"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"50%"},children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Transformer Encoder",src:s(26052).Z+"",width:"2108",height:"2374"})})})}),"\n",(0,i.jsx)(n.p,{children:"Due to the self-attention mechanism, there could be multiple tokens pointing to the same character. Using a typical decoding method could confuse the model\u2014why should it decode one character's image into another character?"}),"\n",(0,i.jsx)(n.p,{children:"Using CTC for text decoding works better here because each token corresponds to a part of the image related to a specific character. In the final stage, we just need to merge the outputs to get the final text result."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"If you don\u2019t like CTC and find it cumbersome, you can use an Encoder-Decoder architecture instead, where the model design would look like this:"}),"\n",(0,i.jsx)("figure",{children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Transformer Encoder-Decoder",src:s(17965).Z+"",width:"3281",height:"2300"})})}),"\n",(0,i.jsx)(n.p,{children:"This approach allows direct string decoding without needing CTC since the tokens fed into the Decoder are text queries, and each token is responsible for finding the corresponding character in sequence."}),"\n",(0,i.jsxs)(n.admonition,{type:"tip",children:[(0,i.jsx)(n.p,{children:"The Decoder here can output in parallel; autoregressive decoding is unnecessary. Autoregressive decoding is typically used when each prediction depends on the previous one."}),(0,i.jsx)(n.p,{children:"In this case, the character predicted at the first position doesn\u2019t affect the second position. They\u2019re independent, as all relevant information is already present in the Encoder output. The Decoder\u2019s job is simply to query and retrieve it."})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"error-propagation",children:"Error Propagation"}),"\n",(0,i.jsx)(n.p,{children:"This brings us back to the issue of corner points."}),"\n",(0,i.jsxs)(n.p,{children:["All two-stage models face a common challenge: ",(0,i.jsx)(n.strong,{children:"error propagation"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"No model is 100% accurate. Since we can never fully model the statistical population, every rule has exceptions, and every model has errors. Regardless of the method chosen, the final challenge is the same:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Inaccurate corner estimation"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Inaccurate corner estimation leads to an inaccurate MRZ region after correction. An inaccurate MRZ region then leads to inaccurate text recognition. This becomes a classic example of error propagation."}),"\n",(0,i.jsx)(n.h2,{id:"single-stage-recognition-model",children:"Single-Stage Recognition Model"}),"\n",(0,i.jsx)(n.p,{children:"The primary challenge of a single-stage model is dealing with multi-scale features."}),"\n",(0,i.jsx)(n.p,{children:"The MRZ region can vary with the user\u2019s capture angle, meaning that before detecting text, we must handle the image at different scales."}),"\n",(0,i.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"single-stage",src:s(19183).Z+"",width:"8396",height:"4221"})}),"\n",(0,i.jsx)(n.h3,{id:"backbone",children:"Backbone"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"backbone",src:s(41191).Z+"",width:"1324",height:"504"})}),"\n",(0,i.jsxs)(n.p,{children:["Recently, Google released a new paper: ",(0,i.jsx)(n.strong,{children:"MobileNet-V4"}),", which is optimized for mobile devices. This is great news for us, and we\u2019ll use it as our Backbone, using the pretrained weights from ",(0,i.jsx)(n.code,{children:"timm"}),", with an input image size of 512 x 512 in RGB.\n",(0,i.jsx)(n.a,{href:"https://docsaid.org/papers/lightweight/mobilenet-v4/",children:"https://docsaid.org/papers/lightweight/mobilenet-v4/"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docsaid.org/en/papers/lightweight/mobilenet-v4/",children:(0,i.jsx)(n.strong,{children:"[24.04] MobileNet-V4: The Legacy After Five Years"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/huggingface/pytorch-image-models",children:(0,i.jsx)(n.strong,{children:"huggingface/pytorch-image-models"})})}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"After testing, we found that at an input resolution of 512 x 512, the text size in the MRZ area is around 4 to 8 pixels. Reducing the resolution further leads to blurry text in the MRZ area, degrading recognition performance."})}),"\n",(0,i.jsx)(n.h3,{id:"neck",children:"Neck"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"neck",src:s(36055).Z+"",width:"1322",height:"500"})}),"\n",(0,i.jsx)(n.p,{children:"To better fuse multi-scale features, we introduced BiFPN. By allowing bidirectional flow of contextual information, BiFPN enhances feature representation. It produces a set of rich, scale-aware feature maps that effectively capture objects at different scales, positively impacting prediction accuracy."}),"\n",(0,i.jsx)(n.p,{children:"In our ablation study, we tried removing this component and directly using the Backbone output feature maps, but training failed."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docsaid.org/papers/feature-fusion/bifpn/",children:(0,i.jsx)(n.strong,{children:"[19.11] EfficientDet: BiFPN Is the Key"})})}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"patchify",children:"Patchify"}),"\n",(0,i.jsx)(n.p,{children:"Now for some unconventional experimentation."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"We need to convert feature maps from each stage into a format that Transformers can process. Here, we use standard convolution operations to turn feature maps into patches."}),"\n",(0,i.jsx)(n.p,{children:"Here are some of our settings:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Patch Size: 4 x 4."})}),"\n",(0,i.jsx)(n.p,{children:"We measured the text size in the MRZ area and found that small characters are around 4 to 8 pixels. Given this, we set the patch size to 4 x 4."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Each feature map has a corresponding Patch Embedding and Position Embedding."})}),"\n",(0,i.jsx)(n.p,{children:"Since each feature map has a different scale, they can\u2019t share the same embedding, or information exchange across scales won\u2019t work properly. We considered designing a shared embedding, but it was too complex, so we abandoned that idea."}),"\n",(0,i.jsx)(n.p,{children:"We also tested shared weights for Patch Embedding, where all feature maps share the same Conv2d for embedding, but the results were poor."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"cross-attention",children:"Cross-Attention"}),"\n",(0,i.jsx)(n.p,{children:"Finally, we used Cross-Attention for text recognition."}),"\n",(0,i.jsx)(n.p,{children:"We randomly initialized 93 tokens."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Why 93 tokens?"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'We based this on the longest MRZ format, TD1, which has 90 characters. TD1 has three lines, so we need 2 "separator" characters. Additionally, we need one "end" character, making a total of 93 tokens.'}),"\n",(0,i.jsxs)(n.p,{children:["The separator character is ",(0,i.jsx)(n.code,{children:"&"}),", and the end character is ",(0,i.jsx)(n.code,{children:"[EOS]"}),". If there are extra positions, we mark them with ",(0,i.jsx)(n.code,{children:"[EOS]"}),", and the model can predict whatever it wants beyond that point, but we won\u2019t supervise it."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"For the Transformer decoder, here are the basic settings:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Dimension: 256"}),"\n",(0,i.jsx)(n.li,{children:"Layers: 6"}),"\n",(0,i.jsx)(n.li,{children:"Attention heads: 4"}),"\n",(0,i.jsx)(n.li,{children:"Dropout: 0"}),"\n",(0,i.jsx)(n.li,{children:"Normalization: Post-LN"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'The main design philosophy of this architecture is to provide the Decoder with a "multi-scale" feature space so it can freely choose the appropriate scale for text recognition. We don\'t need to worry about the position of the text in the image\u2014this is up to the model to handle.'}),"\n",(0,i.jsx)(n.h3,{id:"additional-notes",children:"Additional Notes"}),"\n",(0,i.jsx)(n.p,{children:"During the experiment, we kept some records, which may be helpful:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Models with dimensions 64 and 128 can converge, but reducing the dimension doubles the convergence time."})}),"\n",(0,i.jsx)(n.p,{children:"On an RTX4090, training a model with 256 dimensions takes around 50 hours. For 128 dimensions, it takes around 100 hours; for 64 dimensions, about 200 hours."}),"\n",(0,i.jsx)(n.p,{children:"Why not try 512 dimensions? Because it makes the model too large, exceeding 100 MB, which isn't what we want."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Adding extra branches, like Polygon or text center points, can speed up convergence."})}),"\n",(0,i.jsx)(n.p,{children:"But it\u2019s impractical! Collecting data is hard enough; adding MRZ area annotation makes it even more challenging. In the end, the benefits don\u2019t justify the effort."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Removing the Neck."})}),"\n",(0,i.jsx)(n.p,{children:"It can still converge, but the time triples, so think carefully."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Removing position encoding."})}),"\n",(0,i.jsx)(n.p,{children:"No convergence."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ol,{start:"5",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Adjusting weight decay from ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mn,{children:"1"}),(0,i.jsxs)(n.msup,{children:[(0,i.jsx)(n.mn,{children:"0"}),(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mo,{children:"\u2212"}),(0,i.jsx)(n.mn,{children:"5"})]})]})]}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"10^{-5}"})]})})}),(0,i.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.8141em"}}),(0,i.jsx)(n.span,{className:"mord",children:"1"}),(0,i.jsxs)(n.span,{className:"mord",children:[(0,i.jsx)(n.span,{className:"mord",children:"0"}),(0,i.jsx)(n.span,{className:"msupsub",children:(0,i.jsx)(n.span,{className:"vlist-t",children:(0,i.jsx)(n.span,{className:"vlist-r",children:(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.8141em"},children:(0,i.jsxs)(n.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(n.span,{className:"mord mtight",children:[(0,i.jsx)(n.span,{className:"mord mtight",children:"\u2212"}),(0,i.jsx)(n.span,{className:"mord mtight",children:"5"})]})})]})})})})})]})]})})]})," to ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mn,{children:"1"}),(0,i.jsxs)(n.msup,{children:[(0,i.jsx)(n.mn,{children:"0"}),(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mo,{children:"\u2212"}),(0,i.jsx)(n.mn,{children:"2"})]})]})]}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"10^{-2}"})]})})}),(0,i.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.8141em"}}),(0,i.jsx)(n.span,{className:"mord",children:"1"}),(0,i.jsxs)(n.span,{className:"mord",children:[(0,i.jsx)(n.span,{className:"mord",children:"0"}),(0,i.jsx)(n.span,{className:"msupsub",children:(0,i.jsx)(n.span,{className:"vlist-t",children:(0,i.jsx)(n.span,{className:"vlist-r",children:(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.8141em"},children:(0,i.jsxs)(n.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(n.span,{className:"mord mtight",children:[(0,i.jsx)(n.span,{className:"mord mtight",children:"\u2212"}),(0,i.jsx)(n.span,{className:"mord mtight",children:"2"})]})})]})})})})})]})]})})]}),"."]})}),"\n",(0,i.jsx)(n.p,{children:"Faster convergence but reduced generalization."}),"\n",(0,i.jsx)(n.p,{children:"Small models inherently have some regularization, so they don\u2019t need strong weight decay."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ol,{start:"6",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Using Pre-LN."})}),"\n",(0,i.jsx)(n.p,{children:"Faster convergence but reduced generalization."}),"\n",(0,i.jsx)(n.p,{children:"Pre-LN reduces model depth to some extent, which isn\u2019t ideal for small models."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.ol,{start:"7",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Increasing data augmentation."})}),"\n",(0,i.jsx)(n.p,{children:"To speed up experiments, we limited the rotation of MRZ images to within \xb145 degrees."}),"\n",(0,i.jsx)(n.p,{children:"We tried using full rotation and more augmentations, but the model couldn\u2019t handle such a heavy load and failed to converge."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"We believe the current single-stage model design is still missing some key components. We plan to continue reading more literature and conducting further experiments."}),"\n",(0,i.jsx)(n.p,{children:'Scaling up the model size is probably the most effective solution. The challenge lies in how to meet all these requirements while maintaining a "lightweight" parameter size, which is our next focus.'}),"\n",(0,i.jsxs)(n.p,{children:["As mentioned earlier, this problem can already be solved reliably in almost all cases with a ",(0,i.jsx)(n.strong,{children:"two-stage"})," solution. If you're serious about this, we still recommend going back to developing a two-stage model, which will save you from many unnecessary headaches."]})]})}function d(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},36055:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img10-34e38a23a3eaa918e094bcba71530b38.jpg"},80937:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img2-4c688adc6a2092ef627a98e50b3f3751.jpg"},37198:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img3-1e876a97bdc10e5af658d51578f1ca29.jpg"},35611:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img4-e16824cd8031318fd44188fabe8737b6.jpg"},26052:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img6-28f672ed73db01e2240821ea41a65085.jpg"},17965:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img7-9c8575fcb7e63fceaf879b803032fd90.jpg"},41191:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img8-3d70aa43813018cb370af597d75d7001.jpg"},19183:function(e,n,s){s.d(n,{Z:function(){return t}});let t=s.p+"assets/images/img9-b0fc93a05449de8d10a858a1664eb683.jpg"},50065:function(e,n,s){s.d(n,{Z:function(){return o},a:function(){return a}});var t=s(67294);let i={},r=t.createContext(i);function a(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);