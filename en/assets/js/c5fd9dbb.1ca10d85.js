"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["78709"],{41979:function(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"fas-paper-roadmap","metadata":{"permalink":"/en/blog/fas-paper-roadmap","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/04-01-fas-paper-roadmap/index.md","title":"Face Anti-Spoofing Technology Map","description":"A guide to 40 papers from traditional to future advancements.","date":"2025-04-01T00:00:00.000Z","tags":[{"inline":true,"label":"face-anti-spoofing","permalink":"/en/blog/tags/face-anti-spoofing"},{"inline":true,"label":"liveness-detection","permalink":"/en/blog/tags/liveness-detection"}],"readingTime":14.32,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"fas-paper-roadmap","title":"Face Anti-Spoofing Technology Map","authors":"Z. Yuan","image":"/en/img/2025/0401.jpg","tags":["face-anti-spoofing","liveness-detection"],"description":"A guide to 40 papers from traditional to future advancements."},"unlisted":false,"nextItem":{"title":"Should You Choose Docusaurus?","permalink":"/en/blog/should-you-choose-docusaurus"}},"content":"What is Face Anti-Spoofing? Why is it important? How do I get started?\\n\\nThis article is a comprehensive roadmap I\u2019ve put together after reading a substantial amount of literature, designed for those who are learning, researching, or developing FAS systems.\\n\\nI have selected the 40 most representative papers, divided into eight major themes based on time and technological advancements. Each paper includes reasons to read, key contributions, and the appropriate positioning. From traditional LBP, rPPG, and CNN to Transformer, CLIP, and Vision-Language Models, you will get the full scope.\\n\\nLater, I will share the details of each paper in the \\"Paper Notes\\" section. Let\u2019s first get a grasp of the overall context.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Chapter 1: The Dawn of Low-Resolution Light\\n\\n> **From traditional feature engineering to the first glimmer of deep learning**\\n\\nEarly research on Face Anti-Spoofing primarily relied on traditional image processing techniques. Researchers used handcrafted features such as texture, contrast, and frequency to describe the authenticity of faces, performing binary classification with classic classifiers.\\n\\n1. [**[10.09] Face Liveness Detection from a Single Image with Sparse Low Rank Bilinear Discriminative Model**](https://parnec.nuaa.edu.cn/_upload/article/files/4d/43/8a227f2c46bda4c20da97715f010/db1eef47-b25f-4af9-88d4-a8afeccda889.pdf)\\n   Using the Lambertian model and sparse low-rank representation to construct feature space, effectively separating real faces from photos, providing theoretical and practical basis for early single-image liveness detection.\\n\\n2. [**[12.09] On the Effectiveness of Local Binary Patterns in Face Anti-Spoofing**](https://ieeexplore.ieee.org/document/6313548)\\n   Utilizing LBP and its variants, this paper recognizes flat photos and screen replay attacks and establishes the REPLAY-ATTACK dataset, one of the earliest publicly available datasets and classic baselines.\\n\\n3. [**[14.05] Spoofing Face Recognition with 3D Masks**](https://ieeexplore.ieee.org/document/6810829)\\n   A systematic analysis of the attack effects of 3D masks on different face recognition systems (2D/2.5D/3D), pointing out that the traditional assumption of flat fake faces is no longer valid with 3D printing technologies.\\n\\n4. [**[19.09] Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network**](https://arxiv.org/abs/1909.08848)\\n   Proposing a multi-channel CNN architecture that combines RGB, depth, infrared, and thermal signals for recognition, and releasing the WMCA dataset to enhance detection of advanced fake faces (e.g., silicone masks).\\n\\n5. [**[22.10] Deep Learning for Face Anti-Spoofing: A Survey**](https://ieeexplore.ieee.org/abstract/document/9925105)\\n   The first systematic survey in the FAS field focusing on deep learning, covering pixel-wise supervision, multi-modal sensors, and domain generalization trends, establishing a comprehensive knowledge base.\\n\\n---\\n\\nAlthough these methods are simple, they laid the foundation for recognizing flat fake faces (e.g., photos and screen replays) and set the conceptual framework for the later introduction of deep learning techniques.\\n\\n## Chapter 2: The Real-World Stage\\n\\n> **A milestone for FAS technology moving from the lab to real-world scenarios**\\n\\nDatasets and benchmarks determine whether a field can grow steadily.\\n\\nFAS technology expanded from a single scene to multiple devices, lighting conditions, and attack methods, driven by these representative public datasets.\\n\\n6. [**[17.06] OULU-NPU: A Mobile Face Presentation Attack Database with Real-World Variations**](https://ieeexplore.ieee.org/document/7961798)\\n   A mobile-specific FAS dataset designed for real-world factors such as device, environmental lighting, and attack methods, with four testing protocols, becoming a milestone in \\"generalization ability\\" evaluation.\\n\\n7. [**[20.03] CASIA-SURF CeFA: A Benchmark for Multi-modal Cross-ethnicity Face Anti-Spoofing**](https://arxiv.org/abs/2003.05136)\\n   The world\u2019s first large-scale multi-modal FAS dataset with \\"ethnicity annotations,\\" covering RGB, Depth, IR, and multiple attack types, specifically used to study ethnic bias and modality fusion strategies.\\n\\n8. [**[20.07] CelebASpoof: Large-scale Face Anti-Spoofing Dataset with Rich Annotations**](https://arxiv.org/abs/2007.12342)\\n   The largest FAS dataset currently, with over 620,000 images and 10 types of spoof annotations, along with 40 attributes from the original CelebA, enabling multi-task and spoof trace learning.\\n\\n9. [**[22.01] A Personalized Benchmark for Face Anti-Spoofing**](https://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Belli_A_Personalized_Benchmark_for_Face_Anti-Spoofing_WACVW_2022_paper.html)\\n   Advocating for including liveness images from user registration in the recognition process, proposing two new test configurations, CelebA-Spoof-Enroll and SiW-Enroll, exploring the possibility of personalized FAS systems.\\n\\n10. [**[24.02] SHIELD: An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models**](https://arxiv.org/abs/2402.04178)\\n    Combining LLM and multi-modal inputs, proposing a QA task format to evaluate the reasoning ability of MLLMs in spoof/forgery detection, opening a new field of \\"understanding attacks with language modeling.\\"\\n\\n## Chapter 3: The Cross-Domain Battleground\\n\\n> **From single-domain learning to core technologies for multi-scene deployment**\\n\\nOne of the most challenging problems in Face Anti-Spoofing is generalization\u2014how to make models not only effective on training data but also capable of handling new devices, environments, and attacks.\\n\\n11. [**[20.04] Single-Side Domain Generalization for Face Anti-Spoofing**](https://arxiv.org/abs/2004.14043)\\n    Proposing a one-sided adversarial learning strategy, aligning only real faces across domains, allowing fake face features to naturally scatter across domains, and preventing over-compression of erroneous information. This is an enlightening direction for DG design.\\n\\n12. [**[21.05] Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing**](https://arxiv.org/abs/2105.02453)\\n    Not assuming known domain labels, but using instance normalization and MMD for unsupervised clustering and alignment, achieving a generalization training process that does not rely on manual grouping.\\n\\n13. [**[23.03] Rethinking Domain Generalization for Face Anti-Spoofing: Separability and Alignment**](https://arxiv.org/abs/2303.13662)\\n    Proposing the SA-FAS framework, emphasizing maintaining feature separability across different domains while ensuring that the live-to-spoof transition path is consistent across domains, a deep application of IRM theory in FAS.\\n\\n14. [**[24.02] Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing**](https://arxiv.org/abs/2402.19298)\\n    A deep analysis of the multi-modal DG problem, using U-Adapter to suppress unstable modal interference, paired with ReGrad to dynamically adjust the convergence speed of each modality, providing a complete solution for modality imbalance and reliability issues.\\n\\n15. [**[24.04] VL-FAS: Domain Generalization via Vision-Language Model for Face Anti-Spoofing**](https://ieeexplore.ieee.org/document/10448156)\\n    Introducing Vision-Language mechanisms for the first time, guiding attention to face regions via semantic guidance, combined with image-text contrastive learning (SLVT) for semantic layer generalization, significantly improving ViT\'s cross-domain stability.\\n\\n---\\n\\nThese five papers form the core technical axis under the current Domain Generalization (DG) theme, from one-sided adversarial, label-free clustering, separability analysis, to supervisory methods that integrate language, presenting a complete strategy to address cross-domain challenges.\\n\\n## Chapter 4: The Rise of a New World\\n\\n> **From CNN to ViT, the architectural innovation path of FAS models**\\n\\nThe rise of Vision Transformers (ViT) has ushered in an era of global modeling for image tasks, shifting away from local convolutions. Face Anti-Spoofing (FAS) is no exception.\\n\\n16. [**[23.02] Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing**](https://arxiv.org/abs/2302.05744)\\n    A comprehensive review of the core issues of ViT in multimodal FAS, including input design, pre-training strategies, and fine-tuning processes. The paper proposes the AMA adapter and M2A2E pre-training architecture to construct cross-modal, label-free self-supervised workflows.\\n\\n17. [**[23.04] Ma-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing**](https://arxiv.org/abs/2304.07549)\\n    Using a single-branch early fusion architecture, this paper implements modality-agnostic recognition ability through Modal-Disentangle Attention and Cross-Modal Attention, balancing memory efficiency and flexible deployment, marking an important step in ViT\'s practicality.\\n\\n18. [**[23.05] FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing**](https://arxiv.org/abs/2305.03277)\\n    To solve the issues of modality loss and high-fidelity attacks, the paper introduces a cross-modal attention design (MMA + MFA), which strengthens the focus on spoof patches while preserving the characteristics of each modality, serving as a model for deployment flexibility.\\n\\n19. [**[23.09] Sadapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens**](https://arxiv.org/abs/2309.04038)\\n    Using an Efficient Parameter Transfer Learning architecture, this approach inserts statistical adapters into ViT while fixing the main network parameters. Token Style Regularization helps suppress style differences, providing a lightweight solution for cross-domain FAS.\\n\\n20. [**[23.10] LDCFormer: Incorporating Learnable Descriptive Convolution to Vision Transformer for Face Anti-Spoofing**](https://ieeexplore.ieee.org/document/10222330)\\n    Combining learnable descriptive convolution (LDC) with ViT to enhance local detail representation, the paper introduces a decoupled optimization version (LDCformerD), achieving state-of-the-art performance across multiple benchmarks.\\n\\n---\\n\\nThese five papers demonstrate how the Transformer architecture handles critical challenges in multimodal input, modality loss, cross-domain style, and local patch representations, representing a comprehensive shift in the logic of FAS model design.\\n\\n## Chapter 5: The Battle of Styles\\n\\n> **When spoofing comes from different worlds, how can we build style-invariant models?**\\n\\nThe generalization of FAS models is not only challenged by domain shifts but also by the interference caused by asymmetric information between different styles.\\n\\nThis chapter focuses on style decoupling, adversarial learning, test-time adaptation, and instance-aware designs. These approaches attempt to enable models to maintain stable recognition performance even under unknown styles and sample distributions.\\n\\n21. [**[21.07] Unified Unsupervised and Semi-Supervised Domain Adaptation Network for Cross-Scenario Face Anti-Spoofing**](https://www.sciencedirect.com/science/article/abs/pii/S0031320321000753)\\n    Proposing the USDAN framework, which supports both unsupervised and semi-supervised settings, and learns generalized representations compatible with different task configurations through marginal and conditional alignment modules, along with adversarial training.\\n\\n22. [**[22.03] Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing**](https://arxiv.org/abs/2203.05340)\\n    Adopting content and style separation strategies, this paper reshuffles the style space to simulate style shifts, emphasizing live-related styles through contrastive learning. It represents a significant breakthrough in style-aware domain generalization (DG) design.\\n\\n23. [**[23.03] Adversarial Learning Domain-Invariant Conditional Features for Robust Face Anti-Spoofing**](https://link.springer.com/article/10.1007/s11263-023-01778-x)\\n    Not only aligning marginal distributions, but also introducing adversarial structures for conditional alignment, learning distinguishable cross-domain shared representations at the class level, effectively solving misalignment issues.\\n\\n24. [**[23.03] Style Selective Normalization with Meta Learning for Test-Time Adaptive Face Anti-Spoofing**](https://www.sciencedirect.com/science/article/abs/pii/S0957417422021248)\\n    Utilizing statistical information to estimate the style of input images, this method dynamically selects normalization parameters for test-time adaptation, and combines meta-learning to pre-simulate the transfer process for unknown domains.\\n\\n25. [**[23.04] Instance-Aware Domain Generalization for Face Anti-Spoofing**](https://arxiv.org/abs/2304.05640)\\n    Discarding coarse domain labels, this paper adopts an instance-level style alignment strategy, refining style-invariant recognition features through asymmetric whitening, style enhancement, and dynamic kernel designs.\\n\\n---\\n\\nThese five papers challenge the \\"style generalization\\" theme from different angles, particularly with attempts at instance-based and test-time adaptation, gradually approaching the demands of real-world applications.\\n\\n## Chapter 6: The Summoning of Multimodality\\n\\n> **When images are no longer the only modality, sound and physiological signals come into play**\\n\\nWhen traditional RGB models face bottlenecks in high-fidelity attacks and cross-domain challenges, the FAS community began exploring non-visual signals, such as **rPPG, physiological signals, and acoustic echoes**, to establish recognition bases that are harder to forge, starting from \\"human-centered signals.\\"\\n\\nThis chapter features five representative papers spanning physiological signals, 3D geometry, and acoustic perception, showcasing the potential and future of multimodal FAS technology.\\n\\n26. [**[18.09] Remote Photoplethysmography Correspondence Feature for 3D Mask Face Presentation Attack Detection**](https://dl.acm.org/doi/10.1007/978-3-030-01270-0_34)\\n    Introducing CFrPPG (Correspondence rPPG) features to enhance liveness signal acquisition, ensuring accurate heart rate tracking even under low light or camera shake, showing strong performance against 3D mask attacks.\\n\\n27. [**[19.05] Multi-Modal Face Authentication Using Deep Visual and Acoustic Features**](https://ieeexplore.ieee.org/document/8761776)\\n    Using the built-in speakers and microphones of smartphones, this method emits ultrasound and analyzes facial echoes, combined with CNN-extracted image features, creating a dual-modal authentication system that requires no additional hardware.\\n\\n28. [**[21.04] Contrastive Context-Aware Learning for 3D High-Fidelity Mask Face Presentation Attack Detection**](https://arxiv.org/abs/2104.06148)\\n    To address the challenge of high-fidelity 3D masks, the HiFiMask dataset is introduced, along with a Contrastive Context-Aware Learning method, using context information (person, material, lighting) to enhance attack detection capability.\\n\\n29. [**[22.08] Beyond the Pixel World: A Novel Acoustic-Based Face Anti-Spoofing System for Smartphones**](https://ieeexplore.ieee.org/document/9868051)\\n    Creating the Echo-Spoof acoustic FAS dataset and designing the Echo-FAS framework, which uses sound waves to reconstruct 3D geometry and material information, entirely independent of cameras, showcasing a low-cost and high-resilience mobile device application.\\n\\n30. [**[24.03] AFace: Range-Flexible Anti-Spoofing Face Authentication via Smartphone Acoustic Sensing**](https://dl.acm.org/doi/10.1145/3643510)\\n    Extending the Echo-FAS concept, incorporating an iso-depth model and distance-adaptive algorithm to combat 3D printed masks, and adjusting based on user distance, this is a crucial design in the practical implementation of acoustic-based liveness verification.\\n\\n---\\n\\nThese five papers mark the beginning of the significant role non-image modalities play in FAS, and if you wish to bypass the limitations of traditional cameras, this is a promising direction worth exploring.\\n\\n## Chapter 7: Decoding the Trace of Deception\\n\\n> **Deeply modeling the structure and semantics of spoofing to enhance model discriminability**\\n\\nAs FAS models face dual challenges of interpretability and generalization, researchers have begun to focus on the concept of \\"spoof trace\\": the subtle patterns left by fake faces in images, such as color biases, edge contours, or frequency anomalies.\\n\\nThe five papers in this chapter all approach this from the perspective of **representation disentanglement**, attempting to separate spoof features from facial content, then reconstruct, analyze, or even synthesize spoof samples, allowing models to truly \\"see through the disguise.\\"\\n\\n31. [**[20.07] On Disentangling Spoof Trace for Generic Face Anti-Spoofing**](https://arxiv.org/abs/2007.09273)\\n    Proposes a multi-scale spoof trace separation model, treating spoof signals as multi-layered patterns. Through adversarial learning, it reconstructs real faces and spoof masks, applicable for synthesizing new attack samples. It is a representative work in spoof-aware representation learning.\\n\\n32. [**[20.08] Face Anti-Spoofing via Disentangled Representation Learning**](https://arxiv.org/abs/2008.08250)\\n    Decomposes facial features into two subspaces: liveness and identity. Through a CNN structure, it separates low- and high-level signals to build a more transferable liveness classifier, improving stability across different attack types.\\n\\n33. [**[22.03] Spoof Trace Disentanglement for Generic Face Anti-Spoofing**](https://ieeexplore.ieee.org/document/9779478)\\n    Models spoof traces as additive and repairable patterns, proposing a two-stage disentanglement framework that incorporates frequency domain information to strengthen low-level spoof detection, also useful for spoof data augmentation to enhance long-tail attack generalization.\\n\\n34. [**[22.07] Learning to Augment Face Presentation Attack Dataset via Disentangled Feature Learning from Limited Spoof Data**](https://ieeexplore.ieee.org/document/9859657)\\n    Proposes a disentangled remix strategy for limited spoof samples, generating in the separated liveness and identity feature spaces, and using contrastive learning to maintain discriminability, significantly improving recognition performance in small-sample scenarios.\\n\\n35. [**[22.12] Learning Polysemantic Spoof Trace: A Multi-Modal Disentanglement Network for Face Anti-Spoofing**](https://arxiv.org/abs/2212.03943)\\n    Extends the spoof trace disentanglement framework to multimodal settings, designing an RGB/Depth dual-network to capture complementary spoof clues and integrating cross-modality fusion to combine their semantics, offering a forward-looking solution for universal FAS models.\\n\\n---\\n\\nThis chapter marks a key turning point: from recognizing liveness \u2192 analyzing disguises \u2192 simulating attacks, Face Anti-Spoofing research is gradually moving toward the next stage of \\"generative, interpretable, and controllable\\" models. These methods not only improve model accuracy but may also inspire the future evolution of offense and defense strategies.\\n\\n## Chapter 8: The Chaotic Landscape of the Future\\n\\n> **From CLIP to human perception, the next frontier of FAS**\\n\\nAs single-modal and single-attack-type solutions fail to meet real-world needs, FAS is stepping into higher-level challenges: **physical + digital dual attacks, semantic-driven recognition, and zero-shot generalization in diverse environments**.\\n\\nThese five representative works are the three major development axes for the future of FAS: **fusion recognition, language modeling, and human-centered perception**.\\n\\n36. [**[20.07] Face Anti-Spoofing with Human Material Perception**](https://arxiv.org/abs/2007.02157)\\n    Integrates material perception into FAS model design, with the BCN architecture simulating human perception at macro and micro levels to judge material differences (skin, paper, silicone), enhancing the model\'s semantic interpretability and cross-material recognition ability.\\n\\n37. [**[23.09] FLIP: Cross-domain Face Anti-Spoofing with Language Guidance**](https://arxiv.org/abs/2309.16649)\\n    Applies the CLIP model to the FAS task, guiding visual representation spaces through natural language descriptions to improve cross-domain generalization. The paper proposes semantic alignment and multimodal contrastive learning strategies, achieving true zero-shot FAS under language guidance.\\n\\n38. [**[24.04] Joint Physical-Digital Facial Attack Detection via Simulating Spoofing Clues**](https://arxiv.org/abs/2404.08450)\\n    Proposes SPSC and SDSC data augmentation strategies to simulate both physical and digital attack clues, enabling a single model to learn to recognize both types of attacks. This won the CVPR 2024 competition, setting a new paradigm for fusion models.\\n\\n39. [**[24.04] Unified Physical-Digital Attack Detection Challenge**](https://arxiv.org/abs/2404.06211)\\n    Launched the first unified attack detection challenge, releasing the 28,000-entry UniAttackData complex attack dataset and analyzing model architectures, catalyzing the research community toward Unified Attack Detection.\\n\\n40. [**[24.08] La-SoftMoE CLIP for Unified Physical-Digital Face Attack Detection**](https://arxiv.org/abs/2408.12793)\\n    Combines CLIP with the Mixture of Experts architecture, introducing a soft-adaptive mechanism to dynamically assign sub-models for complex decision boundaries, providing an efficient parameter selection solution for physical and digital attack fusion handling.\\n\\n---\\n\\nThis chapter signifies the future trend in the FAS field: **from recognizing fake faces \u2192 inferring attack types \u2192 understanding semantics \u2192 combining multimodal language logic reasoning**. Research is evolving from \\"visual understanding\\" to \\"semantic cognition,\\" and attacks are shifting from single-mode to complex hybrid models.\\n\\n## Conclusion\\n\\nThe real world is never short of malice. As long as there is a demand for face recognition, the need for anti-spoofing will never stop.\\n\\nFrom the initial texture analysis and light-shadow modeling to the advent of convolutional networks, and now to the introduction of ViT, CLIP, sound waves, and human perception, FAS technology continues to expand its boundaries. These papers are not only a collection of classics and trends but also a map that spans decades of technological evolution, connecting the past, present, and future.\\n\\nOn this map, we see:\\n\\n- **From single-modal to multimodal**: Not just seeing the image but sensing depth, sound, pulse, and material.\\n- **From classification to disentanglement**: Not just determining real or fake, but attempting to understand the structure of each disguise.\\n- **From recognition to reasoning**: Not just distinguishing liveness, but starting to understand the semantics, materials, and language descriptions behind the truth.\\n- **From defense to generation**: Not just passive defense, but starting to simulate, reconstruct, and intervene proactively.\\n\\nIf you\'re planning to enter this field, this technical guide won\'t give you \\"a one-size-fits-all solution,\\" but it will help you find your starting point: are you fascinated by the visualization of spoof traces? Or do you want to explore how CLIP can assist in secure recognition? Or perhaps you\'re interested in sound waves and material recognition?\\n\\nNo matter what your background is, FAS is an intersection of image recognition, biometrics, human perception, semantic reasoning, and cross-modal fusion.\\n\\nThis battle is far from over."},{"id":"should-you-choose-docusaurus","metadata":{"permalink":"/en/blog/should-you-choose-docusaurus","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/03-28-should-you-choose-docusaurus/index.md","title":"Should You Choose Docusaurus?","description":"A year of practical experience shared.","date":"2025-03-28T00:00:00.000Z","tags":[{"inline":true,"label":"docusaurus","permalink":"/en/blog/tags/docusaurus"},{"inline":true,"label":"static-site","permalink":"/en/blog/tags/static-site"},{"inline":true,"label":"blog","permalink":"/en/blog/tags/blog"}],"readingTime":10.9,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"should-you-choose-docusaurus","title":"Should You Choose Docusaurus?","authors":"Z. Yuan","image":"/en/img/2025/0328.jpg","tags":["docusaurus","static-site","blog"],"description":"A year of practical experience shared."},"unlisted":false,"prevItem":{"title":"Face Anti-Spoofing Technology Map","permalink":"/en/blog/fas-paper-roadmap"},"nextItem":{"title":"Looking Up the Ten Steps of a Master","permalink":"/en/blog/looking-up-the-ten-steps-of-a-master"}},"content":"Is Docusaurus easy to use?\\n\\nThis is the most common question I get asked after running Docsaid for over a year.\\n\\nThe answer is yes, but the reasons it\u2019s easy to use might not be what you expect.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why did I initially choose Docusaurus?\\n\\nActually, my first choice was WordPress.org. (Surprised?)\\n\\nIt has a large user base, is widely used, and has a well-established ecosystem. Many features come with ready-made plugins, such as SEO, social sharing, sitemaps, and so on. Plus, WordPress has a very user-friendly interface, which makes it easy for those who are not familiar with code to get started.\\n\\nBut hey, I can write code!\\n\\nSometimes, being able to write code is a curse. When you don\u2019t know much, it\u2019s fine to follow tutorials online without questioning much, as there aren\u2019t many choices.\\n\\nBut once you realize there are other options, you begin to question whether this choice is the right one: is this really the best approach?\\n\\nEspecially when you know you \\"can\\" do certain things, like:\\n\\n- **Why can\u2019t I write articles using Markdown?**\\n\\n  Actually, you can, but it requires installing extra plugins. Writing with Markdown doesn\u2019t have the developer tools like syntax highlighting, version control, or easy metadata integration.\\n\\n  ***\\n\\n- **Why can\u2019t I manage versions with Git?**\\n\\n  You can, but most modifications in WordPress are done via the backend, making it hard to sync with Git. Plus, much of the content is stored in the database, not as files.\\n\\n  ***\\n\\n- **Why can\u2019t I call external APIs?**\\n\\n  You can, but you have to bypass CORS, write custom JS, and figure out how to inject frontend code without affecting the theme.\\n\\n  ***\\n\\nYou think you\u2019re saving time, but in reality, you end up wasting more time dealing with these issues.\\n\\nThe solutions to these problems aren\u2019t simple plugins but require you to write your own code.\\n\\nIn the end, if you still have to write your own code, then why bother with WordPress?\\n\\nAlso, the WordPress plugin ecosystem isn\u2019t perfect. Many plugins are developed by third parties, and their quality can vary widely. Some even affect website performance or security.\\n\\nSo, was it really worth it?\\n\\n:::tip\\nPersonally, I think it wasn\u2019t.\\n:::\\n\\n## What other options are there?\\n\\nAfter carefully considering my needs, my most basic requirement is: writing articles.\\n\\nWriting articles itself is already quite a task, and if I had to spend energy on formatting tools, it wouldn\'t take long before I\'d be mentally drained.\\n\\nSo, I don\'t just want to \\"write articles,\\" I also want to \\"write Markdown articles,\\" so I can focus on the content rather than the formatting.\\n\\n:::tip\\n**So, does this mean formatting is ignored?**\\n\\nDo you have a misconception about Markdown? It already has good layouts that can help organize content quickly.\\n\\nFurthermore, formatting can be handled with CSS and JS, which I can write myself. This is much simpler than using drag-and-drop editors.\\n:::\\n\\nThere are many static site generators based on Markdown, each with its own strengths.\\n\\nWe gathered some information online and here\u2019s a quick comparison:\\n\\n<div style={{\\n  whiteSpace: \'nowrap\',\\n  overflowX: \'auto\',\\n  fontSize: \'0.8rem\',\\n  lineHeight: \'0.8\',\\n  justifyContent: \'center\',\\n  display: \'flex\',\\n}}>\\n\\n| Feature                | Docusaurus       | Hugo         | Jekyll            | Hexo          | Astro            |\\n| ---------------------- | ---------------- | ------------ | ----------------- | ------------- | ---------------- |\\n| Document-oriented      | \u2705 Strong        | \u26A0\uFE0F Fair      | \u26A0\uFE0F Acceptable     | \u26A0\uFE0F Acceptable | \u274C Weak          |\\n| Multilingual Support   | \u2705 Strong        | \u274C Poor      | \u26A0\uFE0F Acceptable     | \u26A0\uFE0F Acceptable | \u26A0\uFE0F Acceptable    |\\n| Deployment Ease        | \u2705 Strong        | \u2705 Strong    | \u2705 Strong         | \u2705 Strong     | \u2705 Strong        |\\n| Frontend Interactivity | \u26A0\uFE0F Acceptable    | \u274C Poor      | \u274C Poor           | \u274C Poor       | \u2705 Strong        |\\n| Markdown Support       | \u2705 Strong        | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u26A0\uFE0F Acceptable | \u2705 Strong        |\\n| Git/Version Control    | \u2705 Strong        | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u26A0\uFE0F Acceptable | \u2705 Strong        |\\n| Themes & Community     | \u26A0\uFE0F Weak          | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u2705 Strong     | \u26A0\uFE0F Weak          |\\n| Arch Flexibility       | \u26A0\uFE0F Acceptable    | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u26A0\uFE0F Acceptable | \u2705 Strong        |\\n| Learning Curve         | \u274C Difficult     | \u26A0\uFE0F Moderate  | \u2705 Easy           | \u2705 Easy       | \u26A0\uFE0F Moderate      |\\n| Build Speed            | \u26A0\uFE0F Acceptable    | \u2705 Very Fast | \u26A0\uFE0F Average        | \u26A0\uFE0F Average    | \u2705 Fast          |\\n| SEO Friendliness       | \u2705 Strong        | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u26A0\uFE0F Acceptable | \u2705 Strong        |\\n| Plugins & Ecosystem    | \u26A0\uFE0F Weak          | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u26A0\uFE0F Acceptable | \u26A0\uFE0F Weak          |\\n| Data Integration       | \u26A0\uFE0F Acceptable    | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u274C Weak       | \u2705 Strong        |\\n| Programming Language   | \u269B\uFE0F React         | \uD83D\uDC39 Go        | \uD83D\uDC8E Ruby           | \uD83D\uDFE2 Node.js    | \uD83D\uDCDC JS/TS         |\\n| Maintenance Frequency  | \u2705 Strong        | \u2705 Strong    | \u26A0\uFE0F Acceptable     | \u274C Weak       | \u2705 Strong        |\\n| Suitable for           | \uD83D\uDC68\u200D\uD83D\uDCBB Technical Doc | \u270D\uFE0F Blogs     | \uD83D\uDCDD Personal Pages | \uD83C\uDF0F Chinese    | \uD83D\uDEA7 Customization |\\n\\n</div>\\n\\n### Docusaurus\\n\\n1. **Best for technical documentation and multilingual sites**: Excels in \\"documentation-oriented\\" and \\"multilingual support,\\" solidifying its position as the top choice for development documentation websites.\\n2. **Frontend interaction is flexible, but requires React experience**: Supports the React framework, providing decent interactivity but also a steeper \\"learning curve,\\" making it better suited for experienced frontend developers.\\n3. **Frequent updates and excellent version control integration, ideal for team collaboration**: Well-suited for team development scenarios requiring Git workflows, with an active update frequency.\\n\\n### Hugo\\n\\n1. **Static site compilation is incredibly fast, perfect for speed and efficiency seekers**: Hugo is built using Go, with the fastest compile time among all frameworks, enabling quick deployment.\\n2. **Strong flexibility in structure and plugin integration**: Supports highly customizable features and data source integration, making it a popular choice for large content websites.\\n3. **Weak multilingual and frontend interaction support, not suitable for internationalization or dynamic applications**: These weaknesses make Hugo more suitable for single-language content websites or blogs.\\n\\n### Jekyll\\n\\n1. **Most beginner-friendly learning curve, ideal for newcomers to static sites**: Jekyll\'s simple syntax makes it easy for non-technical users to get started with GitHub Pages deployment.\\n2. **Most features are average, making it unsuitable for complex applications**: Although deployment and SEO are good, it lacks standout strengths, which has led to its gradual replacement by other frameworks.\\n3. **Ecosystem and maintenance momentum are declining, with limited future scalability**: Compared to other frameworks, its update frequency and community activity are notably declining, making it more suitable for low-maintenance personal pages.\\n\\n### Hexo\\n\\n1. **Active Chinese community, ideal for Chinese-speaking users**: While international support is limited, there is abundant Chinese-language resources, making it easy to find tutorials and theme packages.\\n2. **Easy to learn and deploy, ideal for writing-focused users**: Built on Node.js, Hexo is very friendly for those familiar with JavaScript, allowing for rapid website creation without much hassle.\\n3. **Weak plugin and multilingual support, unsuitable for long-term large-scale maintenance projects**: More suitable for simple blogs or personal websites, it struggles with handling multiple languages and enterprise-level documentation.\\n\\n### Astro\\n\\n1. **Strongest frontend interaction and modern technology integration capabilities**: Supports modern frameworks like React, Vue, and Svelte, making it ideal for building rich interactive and static hybrid websites.\\n2. **Highly flexible architecture with excellent data integration capabilities, perfect for highly customized projects**: Can pull from multiple data sources and supports complex architectures, making it useful for content-driven or design-oriented websites.\\n3. **Multilingual and theme ecosystems are still developing, with a moderate learning curve**: For users needing internationalization and ready-to-use templates, it\u2019s not yet mature enough.\\n\\n## What Has It Done for Me?\\n\\nI didn\'t have enough time to try everything individually, so based on the available information, I chose Docusaurus.\\n\\n- Document-oriented design with automatic generation of sidebars and breadcrumbs\\n- Built-in multilingual support, with my site currently having Chinese, English, and Japanese versions\\n- Easy Markdown / MDX writing, with the ability to insert React components\\n- The default theme is good enough without needing a lot of custom CSS\\n- One-click deployment to GitHub Pages, suitable for self-hosted sites\\n\\nIf you ask me which feature I\u2019m most satisfied with, I would say it\u2019s \\"MDX writing.\\"\\n\\nFor React developers, the ability to insert React components with MDX greatly enhances the flexibility of content presentation. If something is missing, just create a custom React component.\\n\\nAdditionally, I almost never have to deal with frontend bundling, optimization, routing configuration, or multilingual setup details.\\n\\nOverall, it has handled about 80% of my website needs, and the remaining 20% consists of React components and articles that I wrote myself.\\n\\n## What Are the Limitations?\\n\\nAlthough Docusaurus is very convenient for setting up documentation websites or knowledge bases, there are some obvious limitations and downsides in the process:\\n\\n1. **Not suitable for highly customized dynamic websites**\\n\\n   Docusaurus is designed for static content generation, so it\'s not suitable for websites that require extensive dynamic interaction or real-time data updates, such as e-commerce sites, forums, or membership systems. For websites that need dynamic rendering (SSR), frequent data interactions, or backend integration, frameworks like Next.js, Remix, or Nuxt are more appropriate.\\n\\n   ***\\n\\n2. **React is the only built-in supported frontend framework**\\n\\n   Docusaurus uses React and MDX technology, and does not natively support other frontend frameworks (such as Vue, Svelte, Angular). If your project team is familiar with other frameworks or wants to switch frameworks in the future without making significant changes to the content, Docusaurus may feel limiting.\\n\\n   ***\\n\\n3. **Limited customization for styles and UI**\\n\\n   While Docusaurus offers a built-in theme and allows custom CSS, most UI adjustments require overriding built-in components or redefining the theme. If you need extensive UI customization or a complete redesign of the visual style, it might lead to significant additional work.\\n\\n   ***\\n\\n4. **Limited plugin ecosystem**\\n\\n   Although Docusaurus\u2019s plugin ecosystem is relatively active, it is still less extensive compared to more mature ecosystems like Gatsby, Next.js, or Hugo. If an official or community plugin is not available for certain needs, you\u2019ll have to develop your own, potentially increasing the maintenance cost.\\n\\n   ***\\n\\n5. **Performance issues with large-scale websites**\\n\\n   When the content scale becomes very large (over thousands of pages), Docusaurus\u2019s build time may significantly increase. Every time new content is added or adjusted, the entire website content needs to be rebuilt. If the website content is frequently updated, this could pose significant efficiency issues. In such cases, a more mature and optimized framework like Hugo might be a better fit.\\n\\n   :::tip\\n   This issue has been improved in the recent V3.6.0 release, with significant build speed improvements. The detailed user experience still needs to be verified by the community.\\n   :::\\n\\n   ***\\n\\n6. **Some features need to be implemented through third-party services**\\n\\n   Docusaurus focuses on static site generation, so features like search (e.g., Algolia), comment systems (e.g., Disqus or GitHub Issues), user login, or database integration must be implemented through third-party services. The internal system doesn\u2019t offer built-in solutions.\\n\\n   ***\\n\\n7. **Learning curve depends on familiarity with React**\\n\\n   While Docusaurus is very friendly for Markdown writers, if you want to make full use of MDX, add React components, or customize features, users must be familiar with React and JSX. For non-frontend developers or beginners, this might increase the difficulty of getting started.\\n\\n---\\n\\nOverall, Docusaurus is best suited for quickly building knowledge bases, documentation, or technical content websites. If your needs exceed this scope, the limitations mentioned above could affect the user experience and development efficiency.\\n\\n## My Usage Status\\n\\n:::info\\nUpdated in March 2025.\\n:::\\n\\nMy website, Docsaid, has been using the Docusaurus framework for over a year now:\\n\\n- Accumulated over 170 paper notes\\n- Supports Chinese, English, and Japanese language switching\\n- Includes modules such as blog, docs, papers, playground, etc.\\n- I built my own backend for user registration, login, and API token issuance\\n- Developed over 20 React components to support different presentation needs for articles\\n\\nAlthough Docusaurus is not perfect, for content-heavy websites like mine, it remains the most convenient and stable solution.\\n\\n## Conclusion: Who Is Docusaurus Suitable For?\\n\\nI recommend it to the following types of users:\\n\\n- Those who want to build content-oriented websites (technical documentation, learning notes, open-source manuals)\\n- Those who want multilingual support but don\'t want to maintain a complex website structure\\n- Those who want to set up a website with low maintenance cost quickly\\n- Those who are willing to embrace the React ecosystem (MDX/JS config)\\n\\nI will continue using it and will keep improving my knowledge of Docusaurus\u2019s usage techniques and component extensions. If you\u2019re considering a site-building tool, it\u2019s worth trying out Docusaurus first to see if it suits your needs.\\n\\nIf you have any specific topics you\'d like me to cover about Docusaurus, feel free to leave a comment below, and I\'ll prioritize writing about it.\\n\\nGood luck with your website setup!"},{"id":"looking-up-the-ten-steps-of-a-master","metadata":{"permalink":"/en/blog/looking-up-the-ten-steps-of-a-master","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/03-27-looking-up-the-ten-steps-of-a-master/index.md","title":"Looking Up the Ten Steps of a Master","description":"Perhaps I am just a lost traveler in the mountains.","date":"2025-03-27T00:00:00.000Z","tags":[{"inline":true,"label":"Essays","permalink":"/en/blog/tags/essays"}],"readingTime":5.865,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"looking-up-the-ten-steps-of-a-master","title":"Looking Up the Ten Steps of a Master","authors":"Z. Yuan","image":"/en/img/2025/0327.jpg","tags":["Essays"],"description":"Perhaps I am just a lost traveler in the mountains."},"unlisted":false,"prevItem":{"title":"Should You Choose Docusaurus?","permalink":"/en/blog/should-you-choose-docusaurus"},"nextItem":{"title":"Write a Resume with JS!","permalink":"/en/blog/build-a-resume"}},"content":"import { UserCard, ChatGPTCard } from \'@site/src/components/cards.js\'\\n\\nReading papers can broaden one\'s horizons, but it inevitably comes with side effects.\\n\\nBecause we don\'t know where this path ends, it makes us feel like a mere speck in the vast ocean.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Dialogue\\n\\nWhy not play an RPG?\\n\\nIf we were to divide the workers in the field of deep learning into ten levels and turn them into an RPG experience system, what would the difference between these ten levels look like?\\n\\nI played this game with ChatGPT, and here is the record of our conversation:\\n\\n<UserCard>\\nLet\'s imagine something.\\nSuppose we divide scientists and engineers in the field of deep learning into ten levels.\\nIf each level had to be expressed in one sentence describing their work status,\\nBased on your past knowledge and understanding of the world, what would each of these ten levels be like?\\n</UserCard>\\n\\n<ChatGPTCard>\\nHere is a possible imaginative development, from beginner to master level, with a vivid one-sentence description of each stage of a deep learning scientist or engineer\u2019s work:\\n\\n1. **Beginner**\\n\\"I need to understand what these math symbols and codes are doing.\\"\\n\\n2. **Early Exploration**\\n\\"I want to successfully train my first neural network model.\\"\\n\\n3. **Basic Engineer**\\n\\"I need to learn how to optimize parameters and solve convergence problems with the model.\\"\\n\\n4. **Skilled Engineer**\\n\\"I hope to improve the model\u2019s performance and find better training techniques and processes.\\"\\n\\n5. **Advanced Engineer**\\n\\"I\u2019m thinking about how to systematically improve the model architecture and deploy it effectively into production.\\"\\n\\n6. **Senior Researcher**\\n\\"I\'m researching how to integrate methods from different fields and propose new techniques.\\"\\n\\n7. **Domain Expert**\\n\\"I want to propose an innovative idea or theory that has the potential to influence the entire research community.\\"\\n\\n8. **Technical Leader**\\n\\"I am leading a team to try and solve key problems that have long been unresolved in the industry or academia.\\"\\n\\n9. **Academic Authority**\\n\\"I hope to establish a new research paradigm and redefine the research direction of the entire field.\\"\\n\\n10. **Thought Leader (Master Level Guru)**\\n\\"I want to explore and clarify the essence of intelligence and learning, laying a new theoretical foundation for the next generation of intelligent systems.\\"\\n</ChatGPTCard>\\n\\n<UserCard>\\nIf we use quantified experience values as a metaphor, what would the gap between these ten levels be approximately?\\n</UserCard>\\n\\n<ChatGPTCard>\\nIf we consider the differences between these ten levels in terms of quantified experience values, taking into account that the learning curve in deep learning shows \\"exponential\\" or \\"nonlinear\\" growth, here is an approximate distribution:\\n\\n| Level | Role Description           | Experience Value (Quantified Metaphor) |\\n|-------|----------------------------|---------------------------------------|\\n| 1     | Beginner                   | 1                                     |\\n| 2     | Early Exploration           | 5                                     |\\n| 3     | Basic Engineer              | 20                                    |\\n| 4     | Skilled Engineer            | 100                                   |\\n| 5     | Advanced Engineer           | 500                                   |\\n| 6     | Senior Researcher           | 2,500                                 |\\n| 7     | Domain Expert               | 12,500                                |\\n| 8     | Technical Leader            | 62,500                                |\\n| 9     | Academic Authority          | 312,500                               |\\n| 10    | Thought Leader (Master Guru)| 1,500,000+                            |\\n\\n#### \uD83D\uDD11 Design Logic:\\n- The difference between each level is roughly 5 times that of the previous level, forming exponential growth.\\n- The gap between the top and bottom levels exceeds a million times, fully illustrating the vast divide between top thinkers and beginners.\\n</ChatGPTCard>\\n\\n<UserCard>\\nWith AI tools prevailing, they should be able to narrow the gap for lower-level developers or researchers. If we consider AI tools, what would the corresponding experience values be like?\\n</UserCard>\\n\\n<ChatGPTCard>\\nYou\u2019ve made a very good observation.\\n\\nAI tools (such as ChatGPT, Copilot, AutoML) do indeed significantly lower the threshold for beginners and mid-level developers, boosting their work efficiency and even allowing them to approach engineers several levels higher than themselves.\\n\\nConsidering the additive effect of AI tools, here are the adjusted experience values that reflect reality more accurately:\\n\\n| Level | Role Description          | Without AI Tools | With AI Tools |\\n|-------|---------------------------|------------------|---------------|\\n| 1     | Beginner                  | 1                | 20            |\\n| 2     | Early Exploration          | 5                | 50            |\\n| 3     | Basic Engineer             | 20               | 100           |\\n| 4     | Skilled Engineer           | 100              | 400           |\\n| 5     | Advanced Engineer          | 500              | 1,500         |\\n| 6     | Senior Researcher          | 2,500            | 6,000         |\\n| 7     | Domain Expert              | 12,500           | 20,000        |\\n| 8     | Technical Leader           | 62,500           | 90,000        |\\n| 9     | Academic Authority         | 312,500          | 400,000       |\\n| 10    | Thought Leader (Master Guru)| 1,500,000+      | 1,600,000+    |\\n\\n#### \uD83D\uDD11 Main Logic:\\n- **Low-level Developers (Levels 1-4)**: AI tools have the biggest impact, boosting experience by about 20 times (helping them learn faster, solve difficulties, and accomplish tasks they couldn\u2019t previously).\\n- **Mid and High-level (Levels 5-7)**: AI tools still make a significant difference, but the gap gradually narrows, increasing experience by about 1.5 to 3 times (improving productivity, overcoming bottlenecks, but still requiring solid experience and understanding).\\n- **Top Researchers (Levels 8-10)**: The influence of AI tools is relatively small, as the decisive factors at this stage come from innovation, deep thinking, intuition, experience, and breakthroughs in theory. AI can only assist.\\n\\nThis adjustment more accurately reflects the actual help provided by AI tools and their impact at different stages. It also clearly explains that the most important gap between top talents in the field still lies in higher-level insight and innovation ability, rather than mere execution efficiency.\\n</ChatGPTCard>\\n\\n## Beyond Technology\\n\\nAs for whether the real-world gap truly follows this five-fold pattern, I can\u2019t say for certain.\\n\\nHowever, that feeling of looking up at the top power is, from my perspective, quite close to reality. And within the final few levels, there can often be huge differences in power even among individuals at the same level. This also matches what we observe in real life.\\n\\nThe reality is that the gap between each level is not something that can be bridged by sheer effort alone, and we should be mentally prepared for that. But effort can definitely improve one\u2019s experience, especially when taking on higher-level challenges\u2014experience gains are faster in such cases.\\n\\nLastly, the current AI tools are truly powerful, and boosting combat power by 20 times at the beginner level seems like a very reasonable estimate. But as we level up, the help from AI tools becomes smaller, and at times, they can even become a burden.\\n\\nSo, the conclusion might be: \\"I won\u2019t try too hard! I\u2019ll let AI support me!\\"\\n\\nHaha, just kidding.\\n\\nIn this conversation, I\u2019ve roughly found my own capability range. I have a sense of where I stand and know how much further I still have to go\u2014some paths, though, might never be reached. (Got to accept it \uD83D\uDE05)\\n\\nAlthough AI can make you play any role, you will always be yourself.\\n\\nOnly by enhancing our own capabilities can we truly go further.\\n\\nAt least, that\u2019s what I believe."},{"id":"build-a-resume","metadata":{"permalink":"/en/blog/build-a-resume","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/03-24-build-a-resume/index.md","title":"Write a Resume with JS!","description":"No need to rely on resume templates, just write the code yourself!","date":"2025-03-24T00:00:00.000Z","tags":[{"inline":true,"label":"JavaScript","permalink":"/en/blog/tags/java-script"},{"inline":true,"label":"React","permalink":"/en/blog/tags/react"},{"inline":true,"label":"Resume","permalink":"/en/blog/tags/resume"}],"readingTime":8.6,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"build-a-resume","title":"Write a Resume with JS!","authors":"Z. Yuan","image":"/en/img/2025/0324.webp","tags":["JavaScript","React","Resume"],"description":"No need to rely on resume templates, just write the code yourself!"},"unlisted":false,"prevItem":{"title":"Looking Up the Ten Steps of a Master","permalink":"/en/blog/looking-up-the-ten-steps-of-a-master"},"nextItem":{"title":"Pydantic Introduction: Python Data Validation and Management","permalink":"/en/blog/pydantic-intro"}},"content":"This time, I needed to write a resume.\\n\\nSo, as usual, I started looking for some online tools, which mostly turned out to be the same old options: either the templates were too basic or you had to pay to download them.\\n\\nAfter taking a look around, one thought kept popping into my mind:\\n\\n> **Why does everything have to be behind a paywall?**\\n\\n\x3c!-- truncate --\x3e\\n\\n## Then I\u2019ll Do It Myself!\\n\\nAnyone who\u2019s written a resume before knows that writing good content is one thing, but having a beautiful layout is another skill altogether.\\n\\nThe text has to be right, the layout has to be neat, and the style should be clean and professional. With all these requirements, it\u2019s basically no different from creating a webpage.\\n\\nSince I already know how to build webpages, why not just use JS to write the resume directly?\\n\\nNot only does this allow for full customization of the layout and style, but it also gives me the freedom to assemble modules as I like. It\u2019s also great for version control. Plus, with React\u2019s component-based structure, maintaining it becomes even clearer.\\n\\n## Configuration File\\n\\nI keep all the data for the resume in a single configuration file called `resumeData.js`.\\n\\nThis way, I can clearly separate the data from the presentation logic. If I need to change the data, switch languages, or even add API integration, it becomes much more flexible.\\n\\nThis data mainly includes personal information, a list of skills, about me, work experience, personal achievements, and education.\\n\\nHere is an example with some placeholder data I filled in:\\n\\n```javascript title=\\"src/data/resumeData.js\\"\\nconst resumeData = {\\n  name: \\"Z. Yuan\\",\\n  title: \\"Senior CV/ML Engineer\\",\\n  contact: {\\n    phone: \\"09xx-xxx-xxx\\",\\n    email: \\"xxx@gmail.com\\",\\n    location: \\"XXX City, Taiwan\\",\\n    linkedin: \\"https://www.linkedin.com/in/ze-yuan-sh7/\\",\\n    github: \\"https://github.com/zephyr-sh\\",\\n    website: \\"https://docsaid.org\\"\\n  },\\n  skills: [\\n    { skillName: \\"Python\\", levelLabel: \\"Expert\\", levelWidth: \\"95%\\" },\\n    { skillName: \\"PyTorch\\", levelLabel: \\"Expert\\", levelWidth: \\"95%\\" },\\n    { skillName: \\"Deep Learning\\", levelLabel: \\"Expert\\", levelWidth: \\"95%\\" },\\n    { skillName: \\"Computer Vision\\", levelLabel: \\"Expert\\", levelWidth: \\"95%\\" },\\n    { skillName: \\"ONNX Runtime\\", levelLabel: \\"Proficient\\", levelWidth: \\"85%\\" }\\n  ],\\n  aboutMe: `\\n    Senior CV/ML Engineer with strong expertise in deep learning, MLOps, and document processing.\\n  `,\\n  workExperience: [\\n    {\\n      role: \\"Senior AI Engineer\\",\\n      company: \\"CompanyA, Taipei\\",\\n      date: \\"Aug 2020 - Present\\",\\n      highlights: [\\n        \\"Developed OCR and facial recognition solutions.\\",\\n        \\"Optimized deployment using Docker and ONNX Runtime.\\"\\n      ]\\n    },\\n    {\\n      role: \\"ML Engineer\\",\\n      company: \\"CompanyB, Taipei\\",\\n      date: \\"Feb 2020 - Jun 2020\\",\\n      highlights: [\\n        \\"Built threat detection models.\\",\\n        \\"Improved data pipeline efficiency.\\"\\n      ]\\n    }\\n  ],\\n  personalAchievements: [\\n    {\\n      title: \\"Web Design\\",\\n      description: \\"Created a multilingual technical blog.\\"\\n    },\\n    {\\n      title: \\"Open Source\\",\\n      description: \\"Contributed to deep learning projects.\\"\\n    }\\n  ],\\n  education: [\\n    {\\n      degree: \\"Master\'s Degree, XXXX\\",\\n      date: \\"XXXX - XXXX\\",\\n      desc: \\"Dept. of XXXX Engineering\\"\\n    }\\n  ]\\n};\\n\\nexport default resumeData;\\n```\\n\\n## Two-Column Layout\\n\\nThis time, we chose a two-column layout to present the resume.\\n\\nThis design is considered a classic format for resumes. It clearly categorizes the information and doesn\u2019t feel too overwhelming visually.\\n\\n- The left column mainly contains static basic information, such as name, title, contact details, and skills.\\n- The right column is responsible for dynamic content related to experiences, such as the self-introduction, work experience, achievements, and education.\\n\\n### Contact Information\\n\\nFor the contact information, we use icons paired with text to list various contact methods, including clickable links (such as LinkedIn, GitHub, and personal website), allowing potential employers to find you with just a click:\\n\\n```javascript title=\\"src/components/Resume/ContactInfo.js\\"\\nimport React from \\"react\\";\\nimport resumeData from \\"../../data/resumeData\\";\\n\\nfunction ContactInfo() {\\n  const { phone, email, location, linkedin, github, website } = resumeData.contact;\\n  return (\\n    <div className=\\"section\\">\\n      <h3>\\n        <i className=\\"fa-solid fa-address-book\\"></i> Contact Information\\n      </h3>\\n      <ul className=\\"contact-list\\">\\n        <li>\\n          <i className=\\"fa-solid fa-phone\\"></i> {phone}\\n        </li>\\n        <li>\\n          <i className=\\"fa-solid fa-envelope\\"></i> {email}\\n        </li>\\n        <li>\\n          <i className=\\"fa-solid fa-location-dot\\"></i> {location}\\n        </li>\\n        <li>\\n          <a href={linkedin} target=\\"_blank\\" rel=\\"noreferrer\\">\\n            <i className=\\"fa-brands fa-linkedin\\"></i> LinkedIn\\n          </a>\\n        </li>\\n        <li>\\n          <a href={github} target=\\"_blank\\" rel=\\"noreferrer\\">\\n            <i className=\\"fa-brands fa-github\\"></i> GitHub\\n          </a>\\n        </li>\\n      </ul>\\n      <a className=\\"website-button\\" href={website} target=\\"_blank\\" rel=\\"noreferrer\\">\\n        <i className=\\"fa-solid fa-globe\\"></i> My Website\\n      </a>\\n    </div>\\n  );\\n}\\n\\nexport default ContactInfo;\\n```\\n\\n### Skill Tree\\n\\nThe skills section is a small highlight that combines both visual appeal and informative value.\\n\\nEach skill has a name, proficiency label, and a corresponding progress bar, making it easy to see at a glance the areas of expertise and their levels.\\n\\nHere, we directly render from the `resumeData.skills` array, with each entry containing the skill name and proficiency percentage:\\n\\n```javascript title=\\"src/components/Resume/Skills.js\\"\\nimport React from \\"react\\";\\nimport resumeData from \\"../../data/resumeData\\";\\n\\nfunction Skills() {\\n  return (\\n    <div className=\\"section\\">\\n      <h3>Skills</h3>\\n      <ul className=\\"skills-list\\">\\n        {resumeData.skills.map((item, idx) => (\\n          <li key={idx}>\\n            <div className=\\"skill-item\\">\\n              <div className=\\"skill-label\\">\\n                <span>{item.skillName}</span>\\n                <span>{item.levelLabel}</span>\\n              </div>\\n              <div className=\\"skill-bar\\">\\n                <div className=\\"skill-level\\" style={{ width: item.levelWidth }}></div>\\n              </div>\\n            </div>\\n          </li>\\n        ))}\\n      </ul>\\n    </div>\\n  );\\n}\\n\\nexport default Skills;\\n```\\n\\n### About Me\\n\\nThis section is typically used for a short but impactful self-introduction.\\n\\nAvoid falling into vague, template-like phrases. Instead, emphasize your professional skills, technical focus, and the core experiences you\u2019ve accumulated over time:\\n\\n```javascript title=\\"src/components/Resume/AboutMe.js\\"\\nimport React from \\"react\\";\\nimport resumeData from \\"../../data/resumeData\\";\\n\\nfunction AboutMe() {\\n  return (\\n    <div className=\\"section-right\\">\\n      <h3>About Me</h3>\\n      <p className=\\"about-me\\">{resumeData.aboutMe}</p>\\n    </div>\\n  );\\n}\\n\\nexport default AboutMe;\\n```\\n\\n### Work Experience\\n\\nThis section is the centerpiece of the resume. For the format, it\'s recommended to use a card-based structure to organize the information: job title, company name, employment duration, and a few specific descriptions of key responsibilities or achievements. This allows HR or hiring managers to quickly grasp what you\'ve done and what results you\'ve achieved.\\n\\n```javascript title=\\"src/components/Resume/WorkExperience.js\\"\\nimport React from \\"react\\";\\nimport resumeData from \\"../../data/resumeData\\";\\n\\nfunction WorkExperience() {\\n  return (\\n    <div className=\\"section-right\\">\\n      <h3>Work Experience</h3>\\n      <div className=\\"card-container\\">\\n        {resumeData.workExperience.map((exp, idx) => (\\n          <div className=\\"card\\" key={idx}>\\n            <h4>\\n              {exp.role}\\n              <span className=\\"company\\">{exp.company}</span>\\n            </h4>\\n            <div className=\\"date\\">{exp.date}</div>\\n            <ul>\\n              {exp.highlights.map((item, i) => (\\n                <li key={i}>{item}</li>\\n              ))}\\n            </ul>\\n          </div>\\n        ))}\\n      </div>\\n    </div>\\n  );\\n}\\n\\nexport default WorkExperience;\\n```\\n\\n### Personal Achievements\\n\\n:::tip\\nSome companies may discourage engineers from writing blogs, so use this section with discretion.\\n:::\\n\\nIn addition to your full-time job, if you have a blog, open-source projects, or have participated in competitions, it\u2019s definitely worth including.\\n\\nThese details can sometimes highlight an engineer\'s passion and ongoing commitment more than formal job roles.\\n\\n```javascript title=\\"src/components/Resume/PersonalAchievements.js\\"\\nimport React from \\"react\\";\\nimport resumeData from \\"../../data/resumeData\\";\\n\\nfunction PersonalAchievements() {\\n  return (\\n    <div className=\\"section-right\\">\\n      <h3>Personal Achievements</h3>\\n      <div className=\\"card-container\\">\\n        {resumeData.personalAchievements.map((item, idx) => (\\n          <div className=\\"card\\" key={idx}>\\n            <strong>{item.title}</strong>\\n            <p>{item.description}</p>\\n          </div>\\n        ))}\\n      </div>\\n    </div>\\n  );\\n}\\n\\nexport default PersonalAchievements;\\n```\\n\\n### Education\\n\\nThe format here is relatively simple, but it\u2019s still important to ensure the layout is clear and organized.\\n\\nIf you\'re a recent graduate or career changer, your education may take up a larger portion of your resume, so it\'s important to present it well.\\n\\n```javascript title=\\"src/components/Resume/Education.js\\"\\nimport React from \\"react\\";\\nimport resumeData from \\"../../data/resumeData\\";\\n\\nfunction Education() {\\n  return (\\n    <div className=\\"section-right\\" style={{ padding: \\"15px 20px\\" }}>\\n      <h3>Education</h3>\\n      <div className=\\"education-block\\">\\n        {resumeData.education.map((edu, idx) => (\\n          <div className=\\"education-item\\" key={idx}>\\n            <h4>{edu.degree}</h4>\\n            <div className=\\"date\\">{edu.date}</div>\\n            <p>{edu.desc}</p>\\n          </div>\\n        ))}\\n      </div>\\n    </div>\\n  );\\n}\\n\\nexport default Education;\\n```\\n\\n### Left Column\\n\\nWe place the contact information and skills in the left column, along with the name and job title at the top. This creates a clear structure, making the information easy to read and understand at a glance.\\n\\n```javascript title=\\"src/components/Resume/LeftColumn.js\\"\\nimport React from \\"react\\";\\nimport resumeData from \\"../../data/resumeData\\";\\nimport ContactInfo from \\"./ContactInfo\\";\\nimport Skills from \\"./Skills\\";\\n\\nfunction LeftColumn() {\\n  return (\\n    <div className=\\"left-column\\">\\n      <div className=\\"name-title\\">\\n        <h1>{resumeData.name}</h1>\\n        <h2>\\n          <i className=\\"fa-solid fa-robot\\"></i> {resumeData.title}\\n        </h2>\\n      </div>\\n      <ContactInfo />\\n      <Skills />\\n    </div>\\n  );\\n}\\n\\nexport default LeftColumn;\\n```\\n\\n### Right Column\\n\\nThe right column is used to display the dynamic resume content, such as the self-introduction, experience, and educational background. This layout also aligns more closely with the typical reading habits of hiring managers.\\n\\n```javascript title=\\"src/components/Resume/RightColumn.js\\"\\nimport React from \\"react\\";\\nimport AboutMe from \\"./AboutMe\\";\\nimport Education from \\"./Education\\";\\nimport PersonalAchievements from \\"./PersonalAchievements\\";\\nimport WorkExperience from \\"./WorkExperience\\";\\n\\nfunction RightColumn() {\\n  return (\\n    <div className=\\"right-column\\">\\n      <AboutMe />\\n      <WorkExperience />\\n      <PersonalAchievements />\\n      <Education />\\n    </div>\\n  );\\n}\\n\\nexport default RightColumn;\\n```\\n\\n## Final Display\\n\\nFinally, we wrap the left and right column components into a unified container component, and the entire resume is complete.\\n\\n```javascript title=\\"src/components/Resume/ResumeContainer.js\\"\\nimport React from \\"react\\";\\nimport LeftColumn from \\"./LeftColumn\\";\\nimport RightColumn from \\"./RightColumn\\";\\nimport \\"./resume.css\\";\\n\\nfunction ResumeContainer() {\\n  return (\\n    <div className=\\"resume-container\\">\\n      <LeftColumn />\\n      <RightColumn />\\n    </div>\\n  );\\n}\\n\\nexport default ResumeContainer;\\n```\\n\\nI have separated the CSS into its own style file for easier maintenance and customization. Here\u2019s the link to the style file, where all the styles are contained, and you can change the design however you like:\\n\\n- [**src/components/Resume/resume.css**](https://github.com/DocsaidLab/website/blob/main/src/components/Resume/resume.css)\\n\\n## Final Product\\n\\nThe result after execution is shown in the image below.\\n\\nOverall, it looks clean, concise, and organized. It\u2019s actually not any worse than the paid templates out there. Moreover, this resume is 100% customizable and 100% controllable, making it super easy to update the content, add modules, or even change languages.\\n\\nIf you\'re looking to create your own technical resume, using JS to build it yourself might be an interesting approach to try.\\n\\nimport ResumeContainer from \'@site/src/components/Resume/ResumeContainer\';\\nimport { Helmet } from \\"react-helmet\\";\\n\\n<Helmet>\\n  <style>\\n    {`\\n      header, footer { display: none; }\\n      body { background-color: #f9f9f9; }\\n    `}\\n  </style>\\n  <link\\n    rel=\\"stylesheet\\"\\n    href=\\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css\\"\\n  />\\n</Helmet>\\n\\n<ResumeContainer />\\n\\n## Final Step\\n\\nIf you want to convert your HTML file into a PDF, you can use the `puppeteer` package.\\n\\nThis package, developed by the Chrome team, is a headless browser automation tool that allows you to control browser actions programmatically, such as opening webpages, clicking buttons, taking screenshots, and even converting entire pages into PDFs.\\n\\nHere\u2019s a simple example of how to use Puppeteer to output the resume\'s HTML as a PDF:\\n\\n```js title=\\"html2pdf.js\\"\\nconst puppeteer = require(\'puppeteer\');\\n\\n(async () => {\\n  const browser = await puppeteer.launch();\\n  const page = await browser.newPage();\\n\\n  // Load local HTML or a remote URL\\n  await page.goto(\'http://localhost:3000/resume\', {\\n    waitUntil: \'networkidle0\',\\n  });\\n\\n  await page.pdf({\\n    path: \'resume.pdf\',\\n    format: \'A4\',\\n    printBackground: true,\\n  });\\n\\n  await browser.close();\\n})();\\n```\\n\\nNote that the URL in `goto()` can be either a local development server (like localhost) or the deployed resume page URL.\\n\\nSetting `printBackground: true` will retain the CSS background and colors you\u2019ve defined. If your page has dynamic data rendering (e.g., React\'s SPA), make sure the content is fully loaded before exporting. Using `waitUntil: \'networkidle0\'` is a safer approach.\\n\\nWith this, you can easily export your beautifully crafted JS resume into a high-quality PDF, ready to send or print!"},{"id":"pydantic-intro","metadata":{"permalink":"/en/blog/pydantic-intro","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/03-17-pydantic-intro/index.md","title":"Pydantic Introduction: Python Data Validation and Management","description":"A simple introduction to the basic concepts of Pydantic.","date":"2025-03-17T00:00:00.000Z","tags":[{"inline":true,"label":"Python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"Pydantic","permalink":"/en/blog/tags/pydantic"},{"inline":true,"label":"Data Validation","permalink":"/en/blog/tags/data-validation"}],"readingTime":5.505,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"pydantic-intro","title":"Pydantic Introduction: Python Data Validation and Management","authors":"Z. Yuan","image":"/en/img/2025/0317.webp","tags":["Python","Pydantic","Data Validation"],"description":"A simple introduction to the basic concepts of Pydantic."},"unlisted":false,"prevItem":{"title":"Write a Resume with JS!","permalink":"/en/blog/build-a-resume"},"nextItem":{"title":"I, an AI Engineer, Actually Built a Backend System?","permalink":"/en/blog/builds-dashboard-system"}},"content":"I recently used this tool while working on the backend, so I thought I should document it.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is Pydantic?\\n\\n[Pydantic](https://docs.pydantic.dev) is a validation tool written in Python, designed to handle various messy data issues and also help manage configuration files that often gather dust in the corner. It\'s particularly suitable for engineers because it solves many problems:\\n\\n- **Data Validation**: Helps you filter out various incorrect data.\\n- **Type Conversion**: Easily converts the weird data from the front-end (e.g., `\\"123\\"`) into the desired `int`.\\n- **Defining Data Models**: You can write structured code without randomly inserting Dicts.\\n- **Enhancing API Security and Readability**: It just looks good.\\n- **Seamless Integration with FastAPI**: It\'s tightly integrated with FastAPI, saving you from writing hundreds of lines of code.\\n\\nInstallation is straightforward:\\n\\n```bash\\npip install pydantic\\n```\\n\\n## Basic Usage and Common Features\\n\\n### 1. Creating Data Models\\n\\nDefine a structured data model using `BaseModel`:\\n\\n```python\\nfrom pydantic import BaseModel\\n\\nclass User(BaseModel):\\n    id: int\\n    name: str\\n    email: str\\n```\\n\\n### 2. Automatic Validation\\n\\nWhen creating a `User` object, Pydantic automatically validates the data and converts types:\\n\\n```python\\nuser = User(id=1, name=\'Alice\', email=\'alice@example.com\')\\nprint(user)\\n```\\n\\nIf the data type does not match, it raises a `ValidationError`:\\n\\n```python\\ntry:\\n    user = User(id=\'abc\', name=\'Alice\', email=\'alice@example.com\')\\nexcept Exception as e:\\n    print(e)\\n```\\n\\n### 3. Automatic Type Conversion and Default Values\\n\\nExample of automatic type conversion:\\n\\n```python\\nuser = User(id=\'123\', name=\'Bob\', email=\'bob@example.com\')\\nprint(user.id)  # Outputs 123 (int)\\n```\\n\\nExample of default values and optional fields:\\n\\n```python\\nfrom typing import Optional\\n\\nclass User(BaseModel):\\n    id: int\\n    name: str = \'Unknown\'\\n    is_active: bool = True\\n    nickname: Optional[str] = None\\n\\nuser = User(id=10)\\nprint(user.name)      # Unknown\\nprint(user.is_active) # True\\n```\\n\\n### 4. Nested Models\\n\\nWhen defining nested models, Pydantic can automatically parse a `dict` into the corresponding object:\\n\\n```python\\nclass Address(BaseModel):\\n    city: str\\n    zipcode: str\\n\\nclass User(BaseModel):\\n    id: int\\n    name: str\\n    address: Address\\n\\nuser = User(id=1, name=\\"Alice\\", address={\\"city\\": \\"Taipei\\", \\"zipcode\\": \\"100\\"})\\nprint(user.address.city)  # Outputs \\"Taipei\\"\\n```\\n\\n## Advanced Features\\n\\n### 1. Custom Validators\\n\\nUse `validator` to define custom validation logic for fields:\\n\\n```python\\nfrom pydantic import validator\\n\\nclass User(BaseModel):\\n    id: int\\n    email: str\\n\\n    @validator(\'email\')\\n    def email_must_contain_at(cls, v):\\n        if \'@\' not in v:\\n            raise ValueError(\'Invalid email\')\\n        return v\\n```\\n\\n### 2. Cross-field Validation\\n\\nUse `root_validator` to perform cross-field validation:\\n\\n```python\\nfrom pydantic import root_validator\\n\\nclass User(BaseModel):\\n    password1: str\\n    password2: str\\n\\n    @root_validator\\n    def passwords_match(cls, values):\\n        if values.get(\'password1\') != values.get(\'password2\'):\\n            raise ValueError(\'Passwords do not match\')\\n        return values\\n```\\n\\n## Using Pydantic with FastAPI\\n\\nCombine FastAPI with Pydantic to define data models:\\n\\n```python\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\n\\napp = FastAPI()\\n\\nclass Item(BaseModel):\\n    id: int\\n    name: str\\n    price: float\\n\\n@app.post(\\"/items/\\")\\nasync def create_item(item: Item):\\n    return {\\"item\\": item.dict()}\\n```\\n\\n## And More\\n\\nHere are some less common but powerful uses:\\n\\n### 1. Generic Models\\n\\nWhen you want an API response that includes both generic messages (like status or error messages), you can use it this way:\\n\\n```python\\nfrom typing import TypeVar, Generic\\nfrom pydantic import BaseModel\\n\\nT = TypeVar(\\"T\\")\\n\\nclass ResponseModel(BaseModel, Generic[T]):\\n    data: T\\n    message: str\\n```\\n\\nBy using `TypeVar`, you define a type variable `T` that can be replaced with any type. This allows `ResponseModel` to inherit from both `BaseModel` and `Generic[T]`, making it accept different types of `data` as needed. This kind of generic model greatly enhances reusability and flexibility.\\n\\n### 2. Asynchronous Processing\\n\\nUse `parse_obj_as` for asynchronous data parsing, which allows you to convert raw data structures (like lists or dictionaries) into specified Pydantic models, suitable for handling large amounts of data asynchronously.\\n\\n```python\\nfrom pydantic import parse_obj_as\\nfrom typing import List\\n\\nusers = parse_obj_as(List[User], [{\\"id\\": 1, \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\"}])\\n```\\n\\nAlthough `parse_obj_as` itself is not an async function, it can be used in asynchronous environments to quickly parse and validate data fetched from databases or APIs. This ensures that the incoming data conforms to the defined structure and type, improving program robustness and data processing efficiency.\\n\\n### 3. Schema Definition\\n\\nPydantic can automatically generate a JSON schema by calling the model\'s `schema_json()` method:\\n\\n```python\\nuser_schema = User.schema_json()\\nprint(user_schema)\\n```\\n\\nThis feature is very useful for generating API documentation, data validation, and integration with APIs. The generated schema can be included as part of API documentation, making it easier for front-end developers or third-party developers to reference.\\n\\n### 4. ORM Integration\\n\\nIntegrating Pydantic with SQLAlchemy allows you to bridge the gap between database models and API data models, enabling data validation and automatic conversion, simplifying the data handling process.\\n\\n```python\\nfrom pydantic.orm import from_orm\\nfrom sqlalchemy.ext.declarative import declarative_base\\nfrom sqlalchemy import Column, Integer, String\\n\\nBase = declarative_base()\\n\\nclass UserORM(Base):\\n    __tablename__ = \\"users\\"\\n    id = Column(Integer, primary_key=True)\\n    name = Column(String)\\n\\nclass UserSchema(BaseModel):\\n    id: int\\n    name: str\\n\\n    class Config:\\n        orm_mode = True\\n```\\n\\nFirst, define the ORM model (like `UserORM`) in SQLAlchemy, describing the table structure. In the corresponding Pydantic model, set `orm_mode = True` in the `Config` class to allow the model to extract data from ORM objects and perform automatic conversion and validation. The `from_orm` method makes it easy to convert ORM instances into Pydantic models, avoiding manual field mappings and improving data handling security and efficiency.\\n\\nThis integration method is particularly useful in web API development, where you need to return validated data from a database to the front-end, significantly simplifying the code and reducing errors.\\n\\n## Comparison with `dataclass`\\n\\nBoth Pydantic and Python\'s built-in `dataclass` allow you to define structured data models, but there are clear differences in data validation and type conversion.\\n\\nHere\u2019s a comparison table:\\n\\n<div style={{ display: \\"flex\\", justifyContent: \\"center\\" }}>\\n\\n| Feature                          | `dataclass` (built-in)                 | `pydantic.BaseModel` (external library)                |\\n| -------------------------------- | -------------------------------------- | ------------------------------------------------------ |\\n| **Data Validation**              | \u274C Not supported (manual check needed) | \u2705 Automatic validation (types and formats)            |\\n| **Type Conversion**              | \u274C Not supported (manual handling)     | \u2705 Automatic conversion (e.g., `\\"123\\"` \u2192 `int`)        |\\n| **Performance**                  | \u2B50\u2B50\u2B50\u2B50 (CPython native)              | \u2B50\u2B50\u2B50 (v2 introduces Rust core optimizations)         |\\n| **JSON Conversion**              | \u274C Requires manual `json.dumps`        | \u2705 Built-in `.json()` and `.dict()`                    |\\n| **Nested Models**                | \u274C Requires manual nesting             | \u2705 Built-in support, automatic parsing                 |\\n| **Optional Fields**              | \u274C Requires manual `Optional`          | \u2705 Built-in support                                    |\\n| **Environment Variable Reading** | \u274C Not supported                       | \u2705 Supports `BaseSettings` for `.env` reading          |\\n| **Use Case**                     | Lightweight data storage               | API validation, data parsing, and complex applications |\\n\\n</div>\\n\\n## Conclusion\\n\\nAfter going through the basics of Pydantic, doesn\'t the automatic validation and type conversion seem really useful?\\n\\nA package that helps us write less code is definitely worth learning. Let\u2019s give it a try together!"},{"id":"builds-dashboard-system","metadata":{"permalink":"/en/blog/builds-dashboard-system","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/03-16-builds-dashboard-system/index.md","title":"I, an AI Engineer, Actually Built a Backend System?","description":"A sharing of the experience building a backend system from scratch.","date":"2025-03-16T00:00:00.000Z","tags":[{"inline":true,"label":"React","permalink":"/en/blog/tags/react"},{"inline":true,"label":"FastAPI","permalink":"/en/blog/tags/fast-api"},{"inline":true,"label":"User Authentication","permalink":"/en/blog/tags/user-authentication"}],"readingTime":18.29,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"builds-dashboard-system","title":"I, an AI Engineer, Actually Built a Backend System?","authors":"Z. Yuan","image":"/en/img/2025/0316.webp","tags":["React","FastAPI","User Authentication"],"description":"A sharing of the experience building a backend system from scratch."},"unlisted":false,"prevItem":{"title":"Pydantic Introduction: Python Data Validation and Management","permalink":"/en/blog/pydantic-intro"},"nextItem":{"title":"Setting Up Amazon SES DNS on Namecheap","permalink":"/en/blog/amazon-ses-setting-dns-on-namecheap"}},"content":"Good afternoon, or good evening.\\n\\nI\'ve been a bit slack on updating articles lately, so I want to apologize to everyone.\\n\\nBecause I\u2019ve been busy doing something unrelated to my main work: I\u2019ve been working on both the frontend and backend.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Are You Just Bored?\\n\\nWell, yes.\\n\\n---\\n\\nRecently, I had a long holiday, and finally had time to deal with some unresolved issues:\\n\\n- **This website doesn\u2019t have a backend!**\\n\\nYou might know a bit about it, but this website is built on the Docusaurus framework, which is a static site. When I first set up this website, my idea was just to occasionally write some blogs and share paper notes; there wasn\u2019t any special planning or grand goal behind it.\\n\\nThe pros and cons of static vs. dynamic websites are probably not new to you. I even tried using WordPress to set up a site, but after a short while, I was greeted by someone asking for money, which was really frustrating:\\n\\n> **Building a site myself already brings minimal returns, so why do some people always target my wallet?**\\n\\nI ended up using WordPress for a while but eventually gave it up.\\n\\nIt wasn\u2019t just because of expensive paid services, but because its code was hard to customize... okay, I admit it: I just can\u2019t write PHP.\\n\\nIn contrast, I prefer the frontend-backend separation architecture, which is much more comfortable.\\n\\nYou know what happened after that\u2014I chose Docusaurus and stuck with it. After all that, I ended up back with a static site architecture.\\n\\n\uFF0A\\n\\nAs the content of the website accumulated, I started receiving emails from readers, mostly asking about model usage: such as how to configure environments, install dependencies, or even how to resolve strange errors that appeared during execution.\\n\\nThese were easy to handle; I just told them to ask ChatGPT, and that was it. (~Too casual, huh?~)\\n\\nNot casual at all! With the power of AI tools these days, most problems can be resolved directly by asking, saving a lot of time compared to emailing back and forth.\\n\\nOf course, there are still problems AI can\'t solve, like when readers asked if they could directly call my backend API!\\n\\nHey! At first glance, it sounds like a simple request, but when you try to implement it, the issues pile up. Let\u2019s talk about it in a moment.\\n\\n## The First Version of the Backend\\n\\nLet\u2019s rewind a bit.\\n\\nAfter working on a few models, I wanted to make them available for everyone to try. It\'s like building a sandcastle by the sea and eagerly waiting for a big wave to test how sturdy your wall is. Even if the wave washes it away, at least you tried.\\n\\nIt\u2019s the same with models. I don\u2019t think any engineer would build a model and then hide it away... what would be the point of that?\\n\\nSo, I made a few model demos myself and built the first generation of the backend: that\u2019s right! The \\"Playground\\" section in the navigation bar of this site. If you haven\u2019t tried it yet, feel free to explore.\\n\\nYou might ask, why would you need a backend just for a demo?\\n\\nThat\u2019s a fair point. Generally, there are two solutions if you want to use a model on a website:\\n\\n1. **Load the model directly in the browser and run inference in the browser.**\\n\\n   I rejected this solution immediately.\\n\\n   Running models in the browser is technically possible but very cumbersome. At the very least, it needs to support three or more browsers, each with different configuration environments. This reminded me of the frontend colleagues who struggled with WebAssembly. If they knew I was considering this, they would definitely stop me.\\n\\n2. **Provide a backend service to return inference results.**\\n\\n   This is the solution I chose.\\n\\n   The frontend simply handles data input and output, leaving all the troublesome work to the backend. But that means I have to build a backend system to provide the necessary services.\\n\\n---\\n\\nSince I was already using Docusaurus for the frontend, I had to adopt a frontend-backend separation architecture.\\n\\nBefore starting the coding, I drew a diagram. This is what the architecture should look like:\\n\\n```mermaid\\ngraph LR\\n    subgraph Frontend\\n        Input[Send data]\\n        Render[Receive data and render]\\n    end\\n\\n    subgraph Backend\\n        Nginx[Nginx]\\n        FastAPI[Backend API]\\n        SQL[Database]\\n        Model[Model Inference]\\n    end\\n\\n    Input --\x3e|HTTP Request| Nginx\\n    Nginx --\x3e|Forward Request| FastAPI\\n    FastAPI --\x3e |Save Results| SQL\\n    FastAPI --\x3e Model\\n    Model --\x3e|Save Results| SQL\\n    FastAPI --\x3e|HTTP Response| Nginx\\n    Nginx --\x3e|Send Response| Render\\n\\n    style Input fill:#a7e4ae,stroke:#5cb867,stroke-width:2px\\n    style Render fill:#f8e9a1,stroke:#e0c35f,stroke-width:2px\\n    style Nginx fill:#7eaefc,stroke:#4c84d9,stroke-width:2px\\n    style FastAPI fill:#f7b2bd,stroke:#e3798b,stroke-width:2px\\n    style SQL fill:#ffbc75,stroke:#e09a3f,stroke-width:2px\\n    style Model fill:#cbaacb,stroke:#9a6d9a,stroke-width:2px\\n```\\n\\nApart from the architecture diagram, I also created lots of specifications and system operation processes, as well as detailed input and output designs. But it\u2019s too tedious to include everything here, so I\u2019ll just share the architecture diagram.\\n\\nOnce the specifications were ready, I started working based on the diagram, and that\u2019s how the backend logic for the current \\"Playground\\" on the site was built.\\n\\nAt this point, although it seems relatively simple, let\u2019s talk about the technologies I chose:\\n\\n1. **Nginx**\\n\\n   Another common choice here is Apache, which was once the king of web servers, with rich features, many modules, and a large community. But Apache\u2019s configuration can be complex, and it\u2019s not as good at handling high-concurrency connections as Nginx.\\n\\n   Honestly, I don\u2019t think Apache is bad, but I prefer Nginx\u2019s clean and intuitive configuration style. Whether it\u2019s for static resource proxying, reverse proxying, or load balancing, Nginx\u2019s setup is easier to understand. But of course, this is subjective\u2014some people may not like this style.\\n\\n   Ultimately, Nginx was designed to handle a large number of concurrent connections, and its stability and performance have been tested over time. It seems like a solid choice.\\n\\n   ***\\n\\n2. **FastAPI**\\n\\n   The backend frameworks I hear most about are C# and NodeJS, but unfortunately, I\u2019m not familiar with them.\\n\\n   As an AI engineer, I\u2019m most comfortable in the Python environment. If I suddenly switched to learning NodeJS or other backend frameworks, I\u2019d incur a lot of additional learning costs, which could delay development for several months.\\n\\n   So, I chose FastAPI.\\n\\n   FastAPI natively supports asynchronous operations, making it perfect for backend model inference, where requests need to be processed efficiently even if they are time-consuming. Plus, it has Pydantic built-in for validation and can automatically generate API documentation and testing interfaces, which greatly reduces the pain of developing and maintaining the API.\\n\\n   More importantly, FastAPI\u2019s design is clean and easy to understand. It\u2019s not as bulky and complex as Django, nor as free-form as Flask, which could lead to architectural chaos. The development experience is just right.\\n\\n   ***\\n\\n3. **PostgreSQL**\\n\\n   When it comes to databases, I\u2019m actually more familiar with MySQL.\\n\\n   MySQL has a great reputation in the developer community and is the go-to choice for many beginners. But since Oracle acquired MySQL, its open-source license has gradually become more commercial, which raises some concerns about the ecosystem\u2019s uncertainty.\\n\\n   In contrast, PostgreSQL has maintained a stable and steadily growing presence in the open-source community. In addition to continuous community support, PostgreSQL also supports powerful advanced features, such as native JSON data types and GIS for geographical information processing, making it more useful in complex data scenarios.\\n\\n   Furthermore, PostgreSQL integrates smoothly with FastAPI and various ORM tools (like SQLAlchemy), making for a pleasant user experience. It also handles high-concurrency and high-load requests very well, so if traffic increases or backend requirements get more complex in the future, I won\u2019t have to worry about the database becoming a bottleneck.\\n\\n   ***\\n\\nI\u2019ve mentioned a lot, but this might not necessarily be the best combination\u2014it\u2019s just enough to handle the current needs.\\n\\nFor this simple architecture, it works well enough.\\n\\n## Second Generation Backend\\n\\nReturning to the issue mentioned at the beginning, a reader emailed me asking if they could directly call my backend API via code, instead of interacting through the web interface. This is an interesting request, indicating that the user may want a more automated and efficient way to interact with the system.\\n\\nSo, what issues arise when we open the API directly for users to call?\\n\\n1. **It Could Overload the Traffic**\\n\\n   When an API is exposed externally, users can send high-frequency requests via code, which could lead to a sudden surge in server traffic and potentially exceed its load capacity.\\n\\n   For example, some users may fail to set appropriate request frequencies, causing the API to receive a large number of requests in a short period of time, which can affect other users\u2019 ability to access the system normally. Or someone might use a crawler or malicious script to test the API, occupying the server\u2019s bandwidth and computational resources, which could affect service stability.\\n\\n   ***\\n\\n2. **Security Risks**\\n\\n   Once the API is open, if there is no strict authentication and access control, anyone could obtain data they shouldn\'t have access to or even execute dangerous operations. If the API lacks proper authentication mechanisms (such as OAuth or JWT), malicious individuals might simulate legitimate users through simple requests, leading to potential security risks.\\n\\n   ***\\n\\nThere are many other extended problems that could be discussed in detail, but I\u2019ll skip over those for now.\\n\\nIn conclusion, I can\u2019t open the API directly, because both I and my server could face significant risks.\\n\\nSo, to meet the demand for open APIs, I had to build a complete user authentication and API authorization system myself.\\n\\nAlright, here we go again: engineers aren\u2019t afraid of being inexperienced, they\u2019re just afraid of not taking action. Time to write some code!\\n\\n### User Login System\\n\\nA complete user system includes features like user registration, login, permission management, and email verification.\\n\\nIn addition to FastAPI and the PostgreSQL database that I was already using, I also integrated Redis for session caching and token management. To enhance the user experience, I designed email verification and password recovery features, using SMTP services to send verification emails.\\n\\nNow, I can create a new architecture diagram for this part\u2014let me sketch it out simply:\\n\\n```mermaid\\ngraph LR\\n    %% Frontend Section\\n    subgraph Frontend\\n        UserRegister[User Registration]\\n        UserLogin[User Login]\\n    end\\n\\n    %% Backend Section\\n    subgraph Backend\\n        Nginx[Nginx]\\n        FastAPI[Backend API]\\n        UserDB[User Database]\\n        Redis[Redis Cache]\\n        EmailVerification[Email Verification Module]\\n        ForgotPassword[Forgot Password Module]\\n    end\\n\\n    %% Registration Process (Blue)\\n    UserRegister -- \\"Registration Request\\" --\x3e Nginx\\n    Nginx -- \\"Forward Registration Request\\" --\x3e FastAPI\\n    FastAPI -- \\"Validate and Save Data\\" --\x3e UserDB\\n    FastAPI -- \\"Write to Cache/Create Session\\" --\x3e Redis\\n    FastAPI -- \\"Respond with Registration Result\\" --\x3e Nginx\\n    Nginx -- \\"Send Result\\" --\x3e UserRegister\\n\\n    %% Login Process (Orange)\\n    UserLogin -- \\"Login Request\\" --\x3e Nginx\\n    Nginx -- \\"Forward Login Request\\" --\x3e FastAPI\\n    FastAPI -- \\"Query and Validate Data\\" --\x3e UserDB\\n    FastAPI -- \\"Read/Update Session Cache\\" --\x3e Redis\\n    FastAPI -- \\"Respond with Login Result (JWT/Session)\\" --\x3e Nginx\\n    Nginx -- \\"Send Result\\" --\x3e UserLogin\\n\\n    %% Account Email Verification Process (Purple) - Demonstrating Backend Interaction\\n    FastAPI -- \\"Send Verification Email\\" --\x3e EmailVerification\\n    EmailVerification -- \\"Update User Verification Status\\" --\x3e UserDB\\n    EmailVerification -- \\"Respond with Result\\" --\x3e FastAPI\\n\\n    %% Forgot Password Process (Pink) - Demonstrating Backend Interaction\\n    FastAPI -- \\"Forgot Password Request\\" --\x3e ForgotPassword\\n    ForgotPassword -- \\"Update User Password\\" --\x3e UserDB\\n    ForgotPassword -- \\"Respond with Result\\" --\x3e FastAPI\\n\\n    %% Original Arrow Styles (Registration=Blue / Login=Orange)\\n    linkStyle 0 stroke:#007BFF,stroke-width:2px\\n    linkStyle 1 stroke:#007BFF,stroke-width:2px\\n    linkStyle 2 stroke:#007BFF,stroke-width:2px\\n    linkStyle 3 stroke:#007BFF,stroke-width:2px\\n    linkStyle 4 stroke:#007BFF,stroke-width:2px\\n    linkStyle 5 stroke:#007BFF,stroke-width:2px\\n\\n    linkStyle 6 stroke:#fd7e14,stroke-width:2px\\n    linkStyle 7 stroke:#fd7e14,stroke-width:2px\\n    linkStyle 8 stroke:#fd7e14,stroke-width:2px\\n    linkStyle 9 stroke:#fd7e14,stroke-width:2px\\n    linkStyle 10 stroke:#fd7e14,stroke-width:2px\\n    linkStyle 11 stroke:#fd7e14,stroke-width:2px\\n\\n    %% Email Verification Process Arrows (Purple: From Step 12)\\n    linkStyle 12 stroke:#6f42c1,stroke-width:2px\\n    linkStyle 13 stroke:#6f42c1,stroke-width:2px\\n    linkStyle 14 stroke:#6f42c1,stroke-width:2px\\n\\n    %% Forgot Password Process Arrows (Pink: From Step 15)\\n    linkStyle 15 stroke:#d63384,stroke-width:2px\\n    linkStyle 16 stroke:#d63384,stroke-width:2px\\n    linkStyle 17 stroke:#d63384,stroke-width:2px\\n```\\n\\nThis user system needs to include a user database, password encryption, registration email verification, forgotten password reset processes, and other details. I haven\u2019t considered integrating third-party login services yet, such as logging in via Google or Facebook accounts. If I were to add third-party login verification, that would involve a lot more work, which I\u2019ll leave for my future self.\\n\\nSpeaking of registration email verification, this turned out to be more troublesome than expected.\\n\\nI first applied for Amazon SES, but after waiting for a day, they rejected me, saying I looked suspicious. (What?)\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"60%\\"}}>\\n![amazon](./img/img1.jpg)\\n</figure>\\n</div>\\n\\nSince that didn\u2019t work, I thought maybe I could set up my own mail server.\\n\\nAfter a day of setup and troubleshooting, I finally got it working!\\n\\nBut then, the emails I sent were still rejected by Gmail because they also thought I looked suspicious. (\uD83D\uDE2D \uD83D\uDE2D \uD83D\uDE2D)\\n\\nIn the end, after some twists and turns, I found another provider who helped me get it sorted out, but I\u2019ll spare you the details.\\n\\nAfter more than a week of work, I finally completed the specifications I set out to do. Although the result is just a simple page, I still wrote over a thousand lines of code. It feels like a very low-effort payoff.\\n\\nBy the time I reached this point in the coding process, I finally understood why frontend and backend developers often don\u2019t see eye to eye and end up arguing and shifting blame all the time. I can only say that setting clear specifications is really important\u2014though it was just me doing everything, without clear specifications, I\u2019d still end up in a messy situation.\\n\\n### API Token Issuance System\\n\\nAfter finishing the user system, I finally got around to implementing the API Token functionality.\\n\\nThe most common token format these days is JWT (JSON Web Token), and one of its key features is that JWT is self-contained. As long as we verify that the token is valid, we can know the user\'s identity and permissions, reducing the need for frequent database queries and improving performance. JWT can store information like the user ID, permissions, and expiration date, thus avoiding additional API queries.\\n\\nSo I decided to use JWT directly. Users verify their identity through login, after which the system generates a JWT and stores it in Redis. When the user sends an API request, the JWT is sent to the backend as a Bearer Token for authentication. Once the token is verified successfully, the user can proceed to access the backend model inference services.\\n\\nThe architecture for applying for a token is quite simple, and here\u2019s the diagram for it:\\n\\n```mermaid\\ngraph LR\\n    subgraph Frontend\\n        apiClient[API Client]\\n    end\\n\\n    subgraph Backend\\n        Nginx[Nginx]\\n        subgraph Authentication Service\\n            APIToken[API Token Issuance Service]\\n        end\\n        UserDB[User Database]\\n        Redis[Redis Cache]\\n    end\\n\\n    apiClient -- \\"Token Request\\" --\x3e Nginx\\n    Nginx -- \\"Forward Token Request\\" --\x3e APIToken\\n    APIToken -- \\"Query Verification (Redis)\\" --\x3e Redis\\n    APIToken -- \\"Query Verification (UserDB)\\" --\x3e UserDB\\n    APIToken -- \\"Generate and Store Token\\" --\x3e Redis\\n    APIToken -- \\"Respond with Token\\" --\x3e Nginx\\n    Nginx -- \\"Send Token\\" --\x3e apiClient\\n\\n    linkStyle 0 stroke:#00bfff,stroke-width:2px\\n    linkStyle 1 stroke:#00bfff,stroke-width:2px\\n    linkStyle 2 stroke:#00bfff,stroke-width:2px\\n    linkStyle 3 stroke:#00bfff,stroke-width:2px\\n    linkStyle 4 stroke:#00bfff,stroke-width:2px\\n    linkStyle 5 stroke:#00bfff,stroke-width:2px\\n    linkStyle 6 stroke:#00bfff,stroke-width:2px\\n\\n    %% Node Styles\\n    style apiClient fill:#ffe0b3,stroke:#ffb366,stroke-width:2px\\n    style Nginx fill:#7eaefc,stroke:#4c84d9,stroke-width:2px\\n    style APIToken fill:#d9c8f5,stroke:#a58bdd,stroke-width:2px\\n    style UserDB fill:#ffbc75,stroke:#e09a3f,stroke-width:2px\\n    style Redis fill:#cbaacb,stroke:#9a6d9a,stroke-width:2px\\n```\\n\\nOnce the user receives the token, they can use this token to call the API. For this part, Redis is used to limit traffic and calculate the number of calls. The overall calling process is as follows:\\n\\n```mermaid\\ngraph LR\\n    subgraph Frontend\\n        apiClient[API Client]\\n    end\\n\\n    subgraph Backend\\n        Nginx[Nginx]\\n        subgraph Authentication Service\\n            APIToken[API Token Issuance Service]\\n        end\\n        ModelService[Deep Learning Model Inference Service]\\n        UserDB[User Database]\\n        Redis[Redis Cache]\\n    end\\n\\n    apiClient -- \\"Model Inference Request (with Token)\\" --\x3e Nginx\\n    Nginx -- \\"Forward Inference Request\\" --\x3e APIToken\\n    APIToken -- \\"Verify Token (Redis)\\" --\x3e Redis\\n    APIToken -- \\"Verify Token (UserDB)\\" --\x3e UserDB\\n    APIToken -- \\"Token Verified\\" --\x3e ModelService\\n    ModelService -- \\"Respond with Inference Result\\" --\x3e APIToken\\n    APIToken -- \\"Respond with Result\\" --\x3e Nginx\\n    Nginx -- \\"Send Inference Result\\" --\x3e apiClient\\n\\n    %% Change lines to pink\\n    linkStyle 0 stroke:#ff1493,stroke-width:2px\\n    linkStyle 1 stroke:#ff1493,stroke-width:2px\\n    linkStyle 2 stroke:#ff1493,stroke-width:2px\\n    linkStyle 3 stroke:#ff1493,stroke-width:2px\\n    linkStyle 4 stroke:#ff1493,stroke-width:2px\\n    linkStyle 5 stroke:#ff1493,stroke-width:2px\\n    linkStyle 6 stroke:#ff1493,stroke-width:2px\\n    linkStyle 7 stroke:#ff1493,stroke-width:2px\\n\\n    %% Node Styles (can be modified as needed)\\n    style apiClient fill:#ffe0b3,stroke:#ffb366,stroke-width:2px\\n    style Nginx fill:#7eaefc,stroke:#4c84d9,stroke-width:2px\\n    style APIToken fill:#d9c8f5,stroke:#a58bdd,stroke-width:2px\\n    style ModelService fill:#e6e6fa,stroke:#b565a7,stroke-width:2px\\n    style UserDB fill:#ffbc75,stroke:#e09a3f,stroke-width:2px\\n    style Redis fill:#cbaacb,stroke:#9a6d9a,stroke-width:2px\\n```\\n\\n### API Token Management Mechanism\\n\\nAfter designing the Token, I didn\u2019t implement a renewal mechanism. Instead, I allowed users to set the token\'s expiration time and provided manual revocation and deletion mechanisms to ensure flexibility and security in API access permissions.\\n\\nWhen users request a token, they can specify its expiration time (e.g., 1 hour, 1 day, 7 days, or even 1 year). Once the token expires, the system will automatically invalidate it, and users will need to request a new token.\\n\\nThis design allows developers to choose an appropriate token validity period based on their needs, avoiding frequent reauthorization.\\n\\nUsers can revoke or delete their tokens at any time. If a token is compromised or is no longer needed, it can be deleted immediately, rendering it inactive.\\n\\nToken status is stored in Redis. When a user manually revokes a token, the system will immediately mark it as invalid, and subsequent API requests will be rejected. This avoids unnecessary long-term authorization risks and ensures control over the system.\\n\\nHere is the architecture diagram for this process:\\n\\n```mermaid\\nsequenceDiagram\\n    participant Client as API Client\\n    participant Server as Authentication Service\\n    participant Redis as Redis (Token Cache)\\n    participant UserDB as User Database\\n\\n    Client->>Server: Request Token (Specify Expiration)\\n    Server->>UserDB: Save Token and Expiration\\n    Server->>Redis: Cache Token for Faster Validation\\n    Server--\x3e>Client: Return Token\\n\\n    Note over Client,Server: Use Token for API Access\\n\\n    Client->>Server: API Request (With Token)\\n    Server->>Redis: Check if Token is Valid\\n    alt Token Exists in Redis\\n        Redis--\x3e>Server: Token is Valid\\n    else Token Does Not Exist\\n        Server->>UserDB: Query Token\\n        UserDB--\x3e>Server: Token is Valid\\n        Server->>Redis: Cache Token for Efficiency\\n    end\\n    Server--\x3e>Client: API Response\\n\\n    Note over Client,Server: User Decides to Revoke Token\\n\\n    Client->>Server: Request to Delete Token\\n    Server->>UserDB: Delete Token from Database\\n    Server->>Redis: Delete Token from Redis Cache\\n    Server--\x3e>Client: Token Deleted Successfully\\n```\\n\\n:::info\\n**Why Not Use a Renewal Mechanism?**\\n\\n- Allowing users to set expiration times increases API flexibility, avoiding the system from overly interfering with user token management.\\n- Without a renewal mechanism, we can ensure that the token remains valid within the set period, preventing the additional verification burden caused by frequent renewals.\\n- Manual token revocation ensures that if a token is compromised or no longer needed, it can be invalidated immediately, enhancing security.\\n\\nThis design provides a simple yet controlled token management approach, ideal for users who need long-term access but still want to maintain security.\\n:::\\n\\n### API Rate Limiting\\n\\nWith tokens in place, the next challenge is: \\"How to prevent malicious users from abusing the API?\\"\\n\\nSince the API could involve a large number of inference requests (e.g., AI model inference, batch queries), if not limited, it could easily be attacked or affect regular users\' experience. Therefore, I introduced a \\"Rate Limiting\\" mechanism, which uses Redis to track the number of requests and temporarily blocks requests when the limit is reached within a short time.\\n\\nHere\u2019s the architecture diagram for this process:\\n\\n```mermaid\\nsequenceDiagram\\n    participant Client as API Client\\n    participant Nginx as Nginx Proxy\\n    participant AuthService as Authentication Service\\n    participant Redis as Redis (Request Count)\\n    participant ModelService as Deep Learning Model Service\\n\\n    Client->>Nginx: Send API Request (with Token)\\n    Nginx->>Redis: Check Request Count\\n    Redis--\x3e>Nginx: Count Below Limit\\n    Nginx->>AuthService: Validate Token\\n    AuthService->>Redis: Verify Token Validity\\n    Redis--\x3e>AuthService: Token Valid\\n    AuthService->>ModelService: Execute Inference\\n    ModelService--\x3e>AuthService: Return Inference Result\\n    AuthService--\x3e>Nginx: Return API Response\\n    Nginx--\x3e>Client: Send API Response\\n\\n    Note over Client,Redis: If Request Count is Too High, Redis Will Block the Request\\n```\\n\\n## Technology Stack Summary\\n\\nAfter upgrading the backend of this website, the overall technology stack is now updated as follows:\\n\\n- **Frontend Framework**: React (Docusaurus)\\n- **Backend Framework**: FastAPI (Python)\\n- **Database**: PostgreSQL\\n- **Cache Service**: Redis\\n- **Reverse Proxy and Load Balancer**: Nginx\\n\\nApart from the newly added Redis, most of the technologies above were already present in the first-generation backend. It\u2019s just that, at the time, there wasn\u2019t a dedicated frontend page to clearly present this related information. With this upgrade, I\u2019ve added a frontend backend page where users can interact with it and access more services and features.\\n\\nFor me, Python with FastAPI is an everyday essential tool. While this technology stack isn\u2019t necessarily groundbreaking, it has significantly increased development efficiency and the smoothness of usage. As for deployment, I\u2019m still using Docker Compose combined with Nginx and hosting it on my own machine. While I would have liked to move it to the cloud, the cost is too high (low-cost options have poor performance), so I have to make do with what I have.\\n\\nIn summary, the system is now running smoothly. I\u2019ll continue observing and improving it as I go.\\n\\n## Final Thoughts\\n\\nWhen reading papers, I often find the logic abstract, but after reading several times, I can usually reproduce it successfully (ignoring issues with insufficient computing power).\\n\\nFrontend documentation seems straightforward, but every step when implementing it can hide traps\u2014it\u2019s incredibly tricky. (~Just admit it, you\u2019re not familiar with it!~) In the past, late-night debugging often involved dealing with issues like \u201CLoss not converging\u201D or \u201CGPU memory insufficient,\u201D which are daily struggles for an AI engineer. Now, I\u2019m facing frequent React errors, form button malfunctions, and backend API mismatches\u2014such tedious problems.\\n\\nMoreover, all of this was done with ChatGPT available for instant help. If this were before ChatGPT\u2019s release, I\u2019m not so sure if I would have been able to make this backend system work.\\n\\nIf I have time in the future, I will add more features. If you have any suggestions or feedback on this system, feel free to share them with me in the comment section.\\n\\nIf you encounter any bugs or issues while using it, please contact me privately via email to protect the system\u2019s security. I would really appreciate it. If you can provide detailed error messages or screenshots, it would help me quickly locate and fix the issue.\\n\\nI hope you enjoy using this system!"},{"id":"amazon-ses-setting-dns-on-namecheap","metadata":{"permalink":"/en/blog/amazon-ses-setting-dns-on-namecheap","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/03-05-amazon-ses-setting-dns-on-namecheap/index.md","title":"Setting Up Amazon SES DNS on Namecheap","description":"Documenting the process of configuring Amazon SES DNS settings.","date":"2025-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"Amazon SES","permalink":"/en/blog/tags/amazon-ses"},{"inline":true,"label":"Namecheap","permalink":"/en/blog/tags/namecheap"},{"inline":true,"label":"DNS","permalink":"/en/blog/tags/dns"}],"readingTime":2.97,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"amazon-ses-setting-dns-on-namecheap","title":"Setting Up Amazon SES DNS on Namecheap","authors":"Z. Yuan","image":"/en/img/2025/0305.webp","tags":["Amazon SES","Namecheap","DNS"],"description":"Documenting the process of configuring Amazon SES DNS settings."},"unlisted":false,"prevItem":{"title":"I, an AI Engineer, Actually Built a Backend System?","permalink":"/en/blog/builds-dashboard-system"},"nextItem":{"title":"Download Files from Google Drive Using Python","permalink":"/en/blog/download-from-google-drive-using-python"}},"content":"We attempted to enable Amazon SES, and during the verification process, Amazon required us to set up DNS records.\\n\\nIt took us quite some time to find the correct configuration (~about half an hour~).\\n\\nHere, we document the process for future reference.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Amazon SES Setup\\n\\nThere are already comprehensive tutorials available on setting up Amazon SES. Please refer to this guide:\\n\\n- [**Amazon SES Email Service: The Most Complete Application Guide for 2024**](https://www.quantsnote.com/amazon-ses-how-to-apply/)\\n\\nWe followed the steps smoothly until we reached the **\\"Verify a Sending Domain\\"** stage\u2014then we got stuck.\\n\\nSince our domain was purchased from Namecheap, we needed to configure DNS records there.\\n\\nSo, how should we set it up?\\n\\n## DNS Records\\n\\nIn the Amazon SES **\\"Verify a Sending Domain\\"** card, click **\\"Get DNS Records\\"**, and a pop-up window will appear containing a detailed list of DNS records, which looks something like this:\\n\\n![Amazon SES DNS Records](./img/img1.jpg)\\n\\nUsing our domain as an example, `docsaid.org` in the image represents the domain we are verifying.\\n\\nOnce we obtain these DNS records, we can proceed with the setup in Namecheap.\\n\\n:::tip\\nThe original values in each field have been slightly modified to protect our website security.\\n:::\\n\\n## DKIM Records\\n\\nGo to the Namecheap website, log in, and navigate to **\\"Domain List\\"**. Locate the domain you need to configure and click **\\"Manage\\"**.\\n\\nIn the domain management page, find **\\"Advanced DNS\\"** and enter the section.\\n\\n![Setting DKIM Records](./img/img2.jpg)\\n\\nAs shown in the image, there is an **\\"Add New Record\\"** button. Click it and select **\\"CNAME Record\\"**.\\n\\nNow, enter the DKIM records obtained earlier. Pay attention to the following details:\\n\\n1. The **DKIM name** should be entered in the `Host` field. Only enter the part before the domain name; do not include the domain itself.\\n2. The **value** should be entered in the `Value` field\u2014simply copy and paste it.\\n\\n## MAIL FROM Records\\n\\nThis part took us the longest time because only the MX record needs to be added\u2014the other one can be ignored.\\n\\n![Setting MAIL FROM Records](./img/img3.jpg)\\n\\nIn the **\\"Advanced DNS\\"** section of Namecheap, locate **\\"MAIL SETTINGS\\"**. In the dropdown menu, select **\\"Custom MX\\"**.\\n\\nThen, enter the MX record provided by Amazon SES, keeping in mind the following:\\n\\n1. Depending on the subdomain you configured in Amazon SES, enter the corresponding MX record in the `Host` field. In our example, we used `mail`, but if you chose a different subdomain, enter the corresponding value.\\n2. Enter the **value** in the `Value` field. There are two parts:\\n   - Enter `10` in the **\\"Priority\\"** field.\\n   - Enter the remaining part of the value in the **\\"Value\\"** field.\\n\\n:::info\\nNamecheap\'s Mail Settings allow only one of the following options:\\n\\n- If **\\"Email Forwarding\\"** is enabled, Namecheap locks the MX records, restricting them to its own forwarding servers.\\n- If **\\"Custom MX\\"** is enabled, you can manually configure the MX records provided by Amazon SES, allowing it to handle bounced emails.\\n  :::\\n\\n## DMARC Records\\n\\nThis is the easiest part to configure. In Namecheap\'s **\\"Advanced DNS\\"** section, click **\\"Add New Record\\"** and select **\\"TXT Record\\"**.\\n\\nEnter the DMARC record provided by Amazon SES. The only thing to keep in mind is **not to include quotation marks (`\\"`)** in the value.\\n\\n![Setting DMARC Records](./img/img4.jpg)\\n\\n## Summary\\n\\nAt this point, we have completed the Amazon SES DNS setup on Namecheap.\\n\\nAfter completing the setup, wait for about **30 minutes**. Amazon SES will automatically verify your DNS records. If everything is set up correctly, you will receive an email notification from Amazon SES confirming a successful verification.\\n\\nGood luck!"},{"id":"download-from-google-drive-using-python","metadata":{"permalink":"/en/blog/download-from-google-drive-using-python","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/02-19-download-from-google-drive-by-python/index.md","title":"Download Files from Google Drive Using Python","description":"Small files, large files, all can be downloaded.","date":"2025-02-19T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"google-drive","permalink":"/en/blog/tags/google-drive"},{"inline":true,"label":"download","permalink":"/en/blog/tags/download"}],"readingTime":5.41,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"download-from-google-drive-using-python","title":"Download Files from Google Drive Using Python","authors":"Z. Yuan","image":"/en/img/2025/0219.webp","tags":["python","google-drive","download"],"description":"Small files, large files, all can be downloaded."},"unlisted":false,"prevItem":{"title":"Setting Up Amazon SES DNS on Namecheap","permalink":"/en/blog/amazon-ses-setting-dns-on-namecheap"},"nextItem":{"title":"Mounting a USB Drive on Ubuntu","permalink":"/en/blog/mount-disk-on-ubuntu"}},"content":"We wrote a Python script to download files from Google Drive, but sometimes it works fine, while other times it only gives us a strange HTML file...?\\n\\nThe script must be the problem, so we need to fix it.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why Do We Only Get HTML?\\n\\nWhen we send a GET request to Google Drive to download a file, if the file is small (usually less than 100MB), Google may respond directly with the file content, allowing us to download it successfully.\\n\\nHowever, if the file is larger, Google will show a \\"virus scan warning\\" page before the download, notifying the user that the file has not been fully scanned for viruses and offering a button to confirm the download.\\n\\nIf you\'re using a browser, you can manually click the button to download the file. However, when using a Python script, unless there\'s an additional mechanism to simulate the button click or parse the download link from the HTML page, you\'ll only get the HTML of the warning page itself instead of the actual file.\\n\\n:::info\\nThe unexpected HTML file might look something like this:\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    <title>Google Drive - Virus scan warning</title>\\n    <meta http-equiv=\\"content-type\\" content=\\"text/html; charset=utf-8\\" />\\n   ... \x3c!-- other HTML content --\x3e\\n</html>\\n```\\n\\n:::\\n\\n## Solution: Two-Step Request\\n\\nNow that we understand the issue, we can fix it with the following steps:\\n\\n1. **First request**: Send a GET request to `https://docs.google.com/uc?export=download` with the file ID.\\n2. **Check the response**: If the HTTP header contains `content-disposition`, then we have the file content, and we can download it directly. If not, it means we are still on the virus scan warning page, so we need to send another request.\\n3. **Extract the confirmation token**:\\n   - **From cookies**: Google may place a `download_warning_xxxxx` key in the cookies, which contains the token. For example: `token = session.cookies.get(\'download_warning_xxxxx\')`\\n   - **From HTML**: Sometimes, Google may not place the token in the cookies but in the HTML form. For example:\\n     ```html\\n     <form\\n       id=\\"download-form\\"\\n       action=\\"https://drive.usercontent.google.com/download\\"\\n       method=\\"get\\"\\n     >\\n       <input type=\\"hidden\\" name=\\"confirm\\" value=\\"t\\" />\\n       ...\\n     </form>\\n     ```\\n     In this case, we can use [**BeautifulSoup**](https://www.crummy.com/software/BeautifulSoup/) to scrape all the hidden fields, including parameters like `confirm` or `uuid`.\\n4. **Combine the two requests**: After obtaining the `token`, include it in the request, or directly use the form\u2019s `action` URL and corresponding hidden parameters, then send the second request to begin the download.\\n\\n## Code Implementation\\n\\nFirst, install the necessary packages:\\n\\n```bash\\npip install requests tqdm beautifulsoup4\\n```\\n\\nThen use the following Python function, providing the file ID, the name you want to save the file as, and the target directory to automatically determine whether a second request is needed and download the file successfully:\\n\\n```python title=\\"download_from_google.py\\"\\nimport os\\nimport re\\nimport requests\\nfrom tqdm import tqdm\\nfrom bs4 import BeautifulSoup\\n\\ndef download_from_google(file_id: str, file_name: str, target: str = \\".\\"):\\n    \\"\\"\\"\\n    Downloads a file from Google Drive, handling potential confirmation tokens for large files.\\n\\n    Args:\\n        file_id (str):\\n            The ID of the file to download from Google Drive.\\n        file_name (str):\\n            The name to save the downloaded file as.\\n        target (str, optional):\\n            The directory to save the file in. Defaults to the current directory (\\".\\").\\n\\n    Raises:\\n        Exception: If the download fails or the file cannot be created.\\n\\n    Notes:\\n        This function handles both small and large files. For large files, it automatically processes\\n        Google\'s confirmation token to bypass warnings about virus scans or file size limits.\\n\\n    Example:\\n        Download a file to the current directory:\\n            download_from_google(\\n                file_id=\\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\",\\n                file_name=\\"example_file.txt\\"\\n            )\\n\\n        Download a file to a specific directory:\\n            download_from_google(\\n                file_id=\\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\",\\n                file_name=\\"example_file.txt\\",\\n                target=\\"./downloads\\"\\n            )\\n    \\"\\"\\"\\n    # First try: docs.google.com/uc?export=download&id=FileID\\n    base_url = \\"https://docs.google.com/uc\\"\\n    session = requests.Session()\\n    params = {\\n        \\"export\\": \\"download\\",\\n        \\"id\\": file_id\\n    }\\n    response = session.get(base_url, params=params, stream=True)\\n\\n    # If Content-Disposition is present, the file is directly available\\n    if \\"content-disposition\\" not in response.headers:\\n        # Try to get the token from cookies\\n        token = None\\n        for k, v in response.cookies.items():\\n            if k.startswith(\\"download_warning\\"):\\n                token = v\\n                break\\n\\n        # If no token in cookies, extract it from the HTML\\n        if not token:\\n            soup = BeautifulSoup(response.text, \\"html.parser\\")\\n            # Common case: HTML contains a form with id=\\"download-form\\"\\n            download_form = soup.find(\\"form\\", {\\"id\\": \\"download-form\\"})\\n            if download_form and download_form.get(\\"action\\"):\\n                # Extract action URL, which might be drive.usercontent.google.com/download\\n                download_url = download_form[\\"action\\"]\\n                # Collect all hidden inputs\\n                hidden_inputs = download_form.find_all(\\"input\\", {\\"type\\": \\"hidden\\"})\\n                form_params = {}\\n                for inp in hidden_inputs:\\n                    if inp.get(\\"name\\") and inp.get(\\"value\\") is not None:\\n                        form_params[inp[\\"name\\"]] = inp[\\"value\\"]\\n\\n                # Re-send the GET request with these parameters\\n                response = session.get(download_url, params=form_params, stream=True)\\n            else:\\n                # Otherwise, search for confirm=xxx in HTML\\n                match = re.search(r\'confirm=([0-9A-Za-z-_]+)\', response.text)\\n                if match:\\n                    token = match.group(1)\\n                    # Include the confirm token in the request\\n                    params[\\"confirm\\"] = token\\n                    response = session.get(base_url, params=params, stream=True)\\n                else:\\n                    raise Exception(\\"Unable to find the download link or confirmation token in the response. Download failed.\\")\\n\\n        else:\\n            # Use the token obtained from cookies and resend the request\\n            params[\\"confirm\\"] = token\\n            response = session.get(base_url, params=params, stream=True)\\n\\n    # Ensure the download directory exists\\n    os.makedirs(target, exist_ok=True)\\n    file_path = os.path.join(target, file_name)\\n\\n    # Start downloading the file in chunks, with a progress bar\\n    try:\\n        total_size = int(response.headers.get(\'content-length\', 0))\\n        with open(file_path, \\"wb\\") as f, tqdm(\\n            desc=file_name,\\n            total=total_size,\\n            unit=\\"B\\",\\n            unit_scale=True,\\n            unit_divisor=1024,\\n        ) as bar:\\n            for chunk in response.iter_content(chunk_size=32768):\\n                if chunk:\\n                    f.write(chunk)\\n                    bar.update(len(chunk))\\n\\n        print(f\\"File successfully downloaded to: {file_path}\\")\\n\\n    except Exception as e:\\n        raise Exception(f\\"File download failed: {e}\\")\\n```\\n\\n## How to Use\\n\\nAssuming you have a file ID: `YOUR_FILE_ID`, you want to download it as `big_model.onnx` and store it in the `./models` directory, just call it like this:\\n\\n```python\\ndownload_from_google(\\n    file_id=\\"YOUR_FILE_ID\\",\\n    file_name=\\"big_model.onnx\\",\\n    target=\\"./models\\"\\n)\\n```\\n\\nOnce done, you\'ll find the successfully downloaded `big_model.onnx` in the `./models` folder, and you\u2019ll see the download progress in the terminal.\\n\\n## Command-Line Tool\\n\\nIf you\'d prefer to use the command line directly, we can add a wrapper for you:\\n\\n```python title=\\"download_from_google_cli.py\\"\\nfrom download_from_google import download_from_google\\nimport argparse\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\"Download files from Google Drive.\\")\\n    parser.add_argument(\\"--file-id\\", required=True, help=\\"Google Drive file ID.\\")\\n    parser.add_argument(\\"--file-name\\", required=True, help=\\"Output file name.\\")\\n    parser.add_argument(\\"--target\\", default=\\".\\", help=\\"Output directory. Defaults to current folder.\\")\\n    args = parser.parse_args()\\n\\n    download_from_google(file_id=args.file_id, file_name=args.file_name, target=args.target)\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```\\n\\nSave the code above as `download_from_google_cli.py`, then run it from the command line:\\n\\n```bash\\npython download_from_google_cli.py \\\\\\n --file-id YOUR_FILE_ID \\\\\\n --file-name big_model.onnx \\\\\\n --target ./models\\n```\\n\\nIf all goes well, the file will start downloading and show a progress bar.\\n\\nWe\'ve tested with files of 70MB and 900MB, both working fine. As for a 900GB file... (\uD83E\uDD14 \uD83E\uDD14 \uD83E\uDD14)\\n\\nWe haven\'t tested that, and we don\'t have a file that large at hand, so we\u2019ll update when we try it next time!"},{"id":"mount-disk-on-ubuntu","metadata":{"permalink":"/en/blog/mount-disk-on-ubuntu","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/01-25-mount-disk-on-ubuntu/index.md","title":"Mounting a USB Drive on Ubuntu","description":"A guide to mounting a disk on Ubuntu","date":"2025-01-25T00:00:00.000Z","tags":[{"inline":true,"label":"ubuntu","permalink":"/en/blog/tags/ubuntu"},{"inline":true,"label":"mount","permalink":"/en/blog/tags/mount"}],"readingTime":4.07,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"mount-disk-on-ubuntu","title":"Mounting a USB Drive on Ubuntu","authors":"Z. Yuan","image":"/en/img/2025/0125.webp","tags":["ubuntu","mount"],"description":"A guide to mounting a disk on Ubuntu"},"unlisted":false,"prevItem":{"title":"Download Files from Google Drive Using Python","permalink":"/en/blog/download-from-google-drive-using-python"},"nextItem":{"title":"Useful GitHub Markdown Syntax","permalink":"/en/blog/github-markdown-advanced-syntax"}},"content":"After inserting the USB drive into an Ubuntu system, one may unintentionally start staring blankly at the screen.\\n\\nThen it hits you that Ubuntu is an operating system that requires you to \\"mount\\" disks manually.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Check Where the Disk Is\\n\\nBefore mounting the disk, you first need to confirm whether the system has detected it.\\n\\n1. Open the terminal and run the following command:\\n\\n   ```bash\\n   sudo fdisk -l\\n   ```\\n\\n   This command will list all the available disks and partitions in the system.\\n\\n   Locate the disk you want to mount, usually named something like `/dev/sdb` or `/dev/sdc`, with partition names such as `/dev/sda1` or `/dev/sdc1`.\\n\\n   :::tip\\n   You can also use the `lsblk` command, depending on your personal preference.\\n   :::\\n\\n2. Check the file system type of the disk:\\n\\n   Assuming the disk found is `/dev/sda1`, you can check its file system type with the following command:\\n\\n   ```bash\\n   sudo blkid /dev/sda1 | grep TYPE\\n   ```\\n\\n   This command will display the file system type of the partition, such as `ext4`, `ntfs`, or `exfat`, helping you decide the appropriate mounting method.\\n\\n## Create a Mount Directory\\n\\nBefore mounting the disk, you need a directory to serve as the mount point.\\n\\nHere, we\'ll assume the path is `/mnt/mydisk` and run the following command:\\n\\n```bash\\nsudo mkdir -p /mnt/mydisk\\n```\\n\\n:::tip\\nYou can choose any directory name you like, as long as the directory exists and is empty.\\n:::\\n\\n## Mount the Disk\\n\\nChoose the appropriate mount method based on the disk\'s file system type.\\n\\n### ext4\\n\\nRun the following command to mount the disk to the target directory:\\n\\n```bash\\nsudo mount /dev/sda1 /mnt/mydisk\\n```\\n\\nCheck if the mount was successful:\\n\\n```bash\\ndf -h\\n```\\n\\nIf successful, you should see `/mnt/mydisk` in the output.\\n\\n### NTFS or exFAT\\n\\nIf the disk uses NTFS or exFAT file systems, you may need to install the necessary tools.\\n\\n1. Install the required tools:\\n\\n   ```bash\\n   sudo apt update\\n   sudo apt install ntfs-3g exfat-fuse exfat-utils\\n   ```\\n\\n2. Mount the NTFS or exFAT partition (using exFAT as an example):\\n\\n   ```bash\\n   sudo mount -t exfat /dev/sda1 /mnt/mydisk\\n   ```\\n\\n## Common Issues\\n\\n1. **Insufficient Permissions After Mounting**:\\n\\n   Some file systems (like NTFS or exFAT) do not support native Linux permission modification commands (such as `chmod` or `chown`). If you encounter this issue, you can specify the appropriate permissions when mounting.\\n\\n   Unmount first:\\n\\n   ```bash\\n   sudo umount /mnt/mydisk\\n   ```\\n\\n   Remount and specify permissions:\\n\\n   ```bash\\n   sudo mount -t exfat -o uid=1000,gid=1000,fmask=0022,dmask=0022 /dev/sda1 /mnt/mydisk\\n   ```\\n\\n   The meaning of each parameter is as follows:\\n\\n   - `-t exfat`: Specifies the file system type.\\n   - `uid=1000`: Specifies the UID of the file owner.\\n   - `gid=1000`: Specifies the GID of the file group.\\n   - `fmask=0022` and `dmask=0022`: Set the default permissions for files and directories.\\n\\n   Verify if the permissions are correct after mounting:\\n\\n   ```bash\\n   ls -l /mnt/mydisk\\n   ```\\n\\n   :::tip\\n   The `0022` is an octal number, corresponding to `755` permissions.\\n   :::\\n\\n---\\n\\n2. **Mount Directory Not Created**:\\n\\n   If the mount directory does not exist, the mount command will fail. Make sure the directory is created:\\n\\n   ```bash\\n   sudo mkdir -p /mnt/mydisk\\n   ```\\n\\n---\\n\\n3. **Don\'t Know the UID and GID**:\\n\\n   You can use the following command to find the UID and GID of the current user:\\n\\n   ```bash\\n   id\\n   ```\\n\\n   Example output:\\n\\n   ```\\n   uid=1000(username) gid=1000(username)\\n   ```\\n\\n   Here, `uid` is the user ID, and `gid` is the group ID.\\n\\n---\\n\\n4. **Automatically Mount the Disk**:\\n\\n   To automatically mount the disk on every boot, you can configure it in the `/etc/fstab` file.\\n\\n   Open the `/etc/fstab` file with a text editor:\\n\\n   ```bash\\n   sudo vim /etc/fstab\\n   ```\\n\\n   Add the following line at the end of the file (modify it according to the actual disk information):\\n\\n   ```bash\\n   /dev/sda1 /mnt/mydisk ntfs-3g defaults,uid=1000,gid=1000 0 0\\n   ```\\n\\n   After saving and exiting, verify the configuration with:\\n\\n   ```bash\\n   sudo mount -a\\n   ```\\n\\n   If no error message appears, the configuration is successful.\\n\\n---\\n\\n5. **Unmount the Disk**:\\n\\n   To unmount the disk, use the following command:\\n\\n   ```bash\\n   sudo umount /mnt/mydisk\\n   ```\\n\\n   If the disk is in use, you may encounter an error. In this case, use the `-l` option to force the unmount:\\n\\n   ```bash\\n   sudo umount -l /mnt/mydisk\\n   ```\\n\\n   After unmounting, you can confirm with the following command:\\n\\n   ```bash\\n   df -h\\n   ```\\n\\n---\\n\\n6. **Disk Not Formatted**:\\n\\n   A new disk may need to be formatted before use. Use the `mkfs` command to format the disk:\\n\\n   ```bash\\n   sudo mkfs -t ext4 /dev/sda1\\n   ```\\n\\n   This command will format `/dev/sda1` as the ext4 file system. To use a different file system, change the `-t` option.\\n\\n   :::warning\\n   Formatting the disk will erase all data, so make sure to back up important files in advance.\\n   :::\\n\\n## Conclusion\\n\\nIn summary, mounting a disk on Ubuntu is not complicated. Once you master the basic commands and keep the necessary precautions in mind, you can easily complete the process.\\n\\nThis is a simple record of the disk mounting process, and we hope it helps you."},{"id":"github-markdown-advanced-syntax","metadata":{"permalink":"/en/blog/github-markdown-advanced-syntax","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025/01-13-github-markdown-advanced-syntax/index.md","title":"Useful GitHub Markdown Syntax","description":"Introducing uncommon yet practical syntax.","date":"2025-01-13T00:00:00.000Z","tags":[{"inline":true,"label":"GitHub","permalink":"/en/blog/tags/git-hub"},{"inline":true,"label":"Markdown","permalink":"/en/blog/tags/markdown"}],"readingTime":4.13,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"github-markdown-advanced-syntax","title":"Useful GitHub Markdown Syntax","authors":"Z. Yuan","image":"/en/img/2025/0113.webp","tags":["GitHub","Markdown"],"description":"Introducing uncommon yet practical syntax."},"unlisted":false,"prevItem":{"title":"Mounting a USB Drive on Ubuntu","permalink":"/en/blog/mount-disk-on-ubuntu"},"nextItem":{"title":"Extract Font File Information","permalink":"/en/blog/extract-font-info-by-python"}},"content":"Writing README files on GitHub is likely a familiar task, but most of the time, we only use basic syntax.\\n\\nThere are many appealing and practical features you shouldn\'t miss out on!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Basic Syntax\\n\\nLet\'s take 10 seconds to quickly review some commonly used basic syntax:\\n\\n| Function            | Syntax                                         | Example                                        |\\n| ------------------- | ---------------------------------------------- | ---------------------------------------------- |\\n| **Heading**         | `# Heading 1`, `## Heading 2`, `### Heading 3` | `# Heading 1`, `## Heading 2`, `### Heading 3` |\\n| **Bold**            | `**text**` or `__text__`                       | `**bold text**`                                |\\n| **Italic**          | `*text*` or `_text_`                           | `*italic text*`                                |\\n| **Strikethrough**   | `~~text~~`                                     | `~~strikethrough text~~`                       |\\n| **Blockquote**      | `> blockquote`                                 | `> This is a quote`                            |\\n| **Inline Code**     | `` `code` ``                                   | `` `print(\\"Hello, World!\\")` ``                 |\\n| **Code Block**      | ```                                            | `print(\\"Code block\\")`                          |\\n| **Unordered List**  | `- item` or `* item`                           | `- unordered 1` or `* unordered 2`             |\\n| **Ordered List**    | `1. item`                                      | `1. ordered 1` <br /> `2. ordered 2`           |\\n| **Hyperlink**       | `[link text](URL)`                             | `[GitHub](https://github.com)`                 |\\n| **Horizontal Rule** | `---` or `***`                                 | `---` or `***`                                 |\\n| **Emoji**           | `:emoji_code:`                                 | `:smile:`, `:thumbsup:`                        |\\n\\nEasy! Everyone knows these basics.\\n\\nBesides these, Markdown offers numerous advanced features that can make your documents more organized.\\n\\nHere are a few syntax examples:\\n\\n## 1. Task Lists\\n\\nTask lists display plans, development progress, or to-dos in a checkbox format.\\n\\nIn collaborative projects, they clearly show completed and pending tasks, simplifying tracking and delegation.\\n\\n### Writing Example\\n\\n```\\n- [x] Complete initial project setup\\n- [ ] Write user requirements document\\n- [ ] Integrate front-end and back-end code\\n```\\n\\nIn the example above, `[x]` represents completed tasks, while `[ ]` represents pending tasks.\\n\\nIn GitHub\'s Issues, Pull Requests, or Discussions, tasks are updated in real-time once someone checks off an item.\\n\\n### Visual Output\\n\\n- [x] Complete initial project setup\\n- [ ] Write user requirements document\\n- [ ] Integrate front-end and back-end code\\n\\n---\\n\\n## 2. Alerts\\n\\nAlerts provide highlighted frames and icons in documents to display critical information.\\n\\nThis feature isn\'t native to Markdown but is an extension provided by GitHub.\\n\\n:::tip\\nSince this is an extension, it might not render correctly on other Markdown editors or platforms.\\n\\nAdditionally, it differs significantly from Docusaurus syntax, so be cautious when converting content.\\n:::\\n\\n### Writing Example\\n\\nGitHub offers five types of alerts: `NOTE`, `TIP`, `IMPORTANT`, `WARNING`, and `CAUTION`.\\n\\n```markdown\\n> [!NOTE]\\n> Useful information for skimming users.\\n\\n> [!TIP]\\n> Tips for better or easier implementation.\\n\\n> [!IMPORTANT]\\n> Essential details for achieving a goal.\\n\\n> [!WARNING]\\n> Critical info requiring immediate attention.\\n\\n> [!CAUTION]\\n> Warnings about risks or negative outcomes.\\n```\\n\\nFor multi-line content, add `>` at the start of each line:\\n\\n```markdown\\n> [!TIP]\\n> This is a tip.\\n>\\n> Here is another tip.\\n```\\n\\n### Visual Output\\n\\nDue to limitations here, refer to a GitHub screenshot for results:\\n\\n![GitHub Alert Example](./img/img1.jpg)\\n\\n## 3. Footnotes\\n\\nFor long-form documents or academic-style texts, footnotes allow for referencing sources or adding supplementary information.\\n\\nGitHub Markdown supports footnotes, linking supplementary details to the text while keeping it clean.\\n\\n:::info\\nThis section demonstrates custom anchors.\\n\\nIf you\'re not reading this for that purpose, feel free to ignore it.\\n\\n[**Click here for the custom anchor section**](#my-custom-anchor)\\n:::\\n\\n### Writing Example\\n\\n```markdown\\nIn this step, we use a special feature[^1].\\n\\n[^1]: Detailed notes or external links can be added here.\\n```\\n\\nRendered, it places a superscript number in the text that links to detailed notes at the bottom.\\n\\n### Visual Output\\n\\nIn this step, we use a special feature[^1]. (\uD83D\uDC48 Click on the superscript to check it out.)\\n\\n[^1]: This is a footnote for demonstration.\\n\\n## 4. Escaping Characters\\n\\nSometimes, you want certain symbols like asterisks (\\\\*) or backticks (`) displayed as-is, rather than interpreted as Markdown syntax. In such cases, use escaping.\\n\\n### Writing Example\\n\\n```markdown\\nTo display a `*` symbol, add a backslash: \\\\*\\n\\nLet\'s rename \\\\*our-new-project\\\\* to \\\\*our-old-project\\\\*.\\n```\\n\\n### Visual Output\\n\\nTo display a `*` symbol, add a backslash: \\\\*\\n\\nLet\'s rename \\\\*our-new-project\\\\* to \\\\*our-old-project\\\\*.\\n\\n## 5. Custom Anchors\\n\\nWhile headings automatically generate anchors for navigation, sometimes you need to link non-heading content. You can manually insert HTML anchor tags.\\n\\n### Writing Example\\n\\n1. Link to a heading:\\n\\n   ```markdown\\n   [Link to the \\"Footnotes\\" section](#3-footnotes)\\n   ```\\n\\n2. Link to a custom paragraph:\\n\\n   ```markdown\\n   <a name=\\"my-custom-anchor\\"></a>\\n   This is a special paragraph for internal linking.\\n\\n   [Link to this paragraph](#my-custom-anchor)\\n   ```\\n\\n### Visual Output\\n\\n[**Click here to link to the \\"Footnotes\\" section**](#3-footnotes)\\n\\n---\\n\\n<a name=\\"my-custom-anchor\\"></a>\\n**This is a special paragraph for internal linking.**\\n\\n## Conclusion\\n\\nThese features provide more flexibility in document writing and offer richer presentation options.\\n\\nWe hope this article helps you better understand advanced GitHub Markdown syntax and enhances your document writing experience.\\n\\n## References\\n\\n- [**Basic writing and formatting syntax**](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)"},{"id":"extract-font-info-by-python","metadata":{"permalink":"/en/blog/extract-font-info-by-python","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/12-26-extract-font-info-by-python/index.md","title":"Extract Font File Information","description":"Retrieve font file information using Python.","date":"2024-12-26T00:00:00.000Z","tags":[{"inline":true,"label":"font-tools","permalink":"/en/blog/tags/font-tools"},{"inline":true,"label":"Python","permalink":"/en/blog/tags/python"}],"readingTime":14.645,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"extract-font-info-by-python","title":"Extract Font File Information","authors":"Z. Yuan","image":"/en/img/2024/1226.webp","tags":["font-tools","Python"],"description":"Retrieve font file information using Python."},"unlisted":false,"prevItem":{"title":"Useful GitHub Markdown Syntax","permalink":"/en/blog/github-markdown-advanced-syntax"},"nextItem":{"title":"Batch Video Conversion","permalink":"/en/blog/flexible-video-conversion-by-python"}},"content":"Although we frequently use various fonts, we often get stuck when it comes to retrieving font parameters.\\n\\nWe forget after using them, and then look them up again next time?\\n\\nThis makes us seem unprofessional, so we need to write a program to solve the problem.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Install `fontTools`\\n\\nWe use `fontTools` to retrieve font-related information. It is a widely praised Python package for handling font files, allowing us to manipulate and parse various TTF and OTF files.\\n\\nFirst, install `fontTools` using the following command:\\n\\n```bash\\npip install fonttools\\n```\\n\\n:::info\\nIf you\'re interested in `fontTools`, you can refer to their project on GitHub: [**fontTools-github**](https://github.com/fonttools/fonttools)\\n:::\\n\\nOnce installed, you can start running the program.\\n\\n## Key Implementation Points\\n\\nBefore we start using the code, let\u2019s review the key implementation points:\\n\\n1. **Remove Control Characters (`remove_control_characters`)**\\n\\n   This function is used to clean up control or invisible characters from a string, as these characters may interfere with subsequent processing. We need to remove them first. Additionally, some characters are made up of multiple Unicode combinations. We use the `unicodedata` module to normalize the string, converting these composite characters into a single character to ensure consistency.\\n\\n   :::tip\\n   If you don\u2019t need this functionality, you can set `normalize` to `False`.\\n   :::\\n\\n2. **Extract Font Information (`extract_font_info`)**\\n\\n   This function extracts various pieces of information from a font and organizes them into a structured, easy-to-read dictionary. It includes a variety of keys, described as follows:\\n\\n   - `fileName`: The path of the font file on the system.\\n   - `tables`: Lists all available tables in the font file.\\n   - `nameTable`: The raw name table indexed by `nameID`.\\n   - `nameTableReadable`: Maps common `nameID` values (e.g., font family, version) to more readable keys.\\n   - `cmapTable`: A mapping of various encodings (platformID, platEncID) to glyph names.\\n   - `headTable`: Basic font parameter information such as `unitsPerEm`, `xMin`, `yMin`, etc.\\n   - `hheaTable`: Horizontal layout information, including ascent, descent, and line gap.\\n   - `OS2Table`: Information about weight (usWeightClass), width (usWidthClass), and embedding restrictions (fsType).\\n   - `postTable`: PostScript-related information, such as whether the font is monospaced (isFixedPitch) and the italic angle (italicAngle).\\n   - `layoutMetrics`: Typography metrics derived from multiple tables, including bounding box, unitsPerEm, line spacing, etc.\\n   - `summary`: A quick overview of the font, including font family, subfamily, version, weight class, and whether it is italic.\\n\\n---\\n\\nOne of the most important parts is the `cmapTable`, which maps various encodings to corresponding characters. In our implementation, we further process this table to convert encodings into readable platform names and encoding names:\\n\\n1. **Platform Name (`platform_name`)**\\n\\n   `platformID` represents the platform supported by the font. Common platform codes include:\\n\\n   - `0`: Unicode (general font standard)\\n   - `1`: Macintosh (Mac system-specific fonts)\\n   - `3`: Windows (Windows system-specific fonts)\\n\\n   In the code, these codes are converted to their corresponding descriptions via a dictionary:\\n\\n   ```python\\n   platform_name = {\\n       0: \'Unicode\',\\n       1: \'Macintosh\',\\n       3: \'Windows\'\\n   }.get(cmap.platformID, f\\"Platform {cmap.platformID}\\")\\n   ```\\n\\n   This code first checks if the `platformID` exists in the dictionary. If a corresponding value is found, it returns the name (e.g., `\'Unicode\'`). If not, it returns `Platform {cmap.platformID}` to handle custom platform codes not in the dictionary.\\n\\n2. **Encoding Name (`encoding_name`)**\\n\\n   The encoding method in the font is determined by both `platformID` and `platEncID`. Common combinations and their meanings are as follows:\\n\\n   - `(0, 0)`: Unicode 1.0\\n   - `(0, 3)`: Unicode 2.0+\\n   - `(0, 4)`: Unicode 2.0+ with BMP (Basic Multilingual Plane)\\n   - `(1, 0)`: Mac Roman (Macintosh Roman alphabet encoding)\\n   - `(3, 1)`: Windows Unicode BMP (Windows Basic Multilingual Plane encoding)\\n   - `(3, 10)`: Windows Unicode Full (Windows Full Unicode encoding)\\n\\n   In the code, these combinations are stored in a nested dictionary and looked up using the tuple `(platformID, platEncID)`:\\n\\n   ```python\\n   encoding_name = {\\n       (0, 0): \'Unicode 1.0\',\\n       (0, 3): \'Unicode 2.0+\',\\n       (0, 4): \'Unicode 2.0+ with BMP\',\\n       (1, 0): \'Mac Roman\',\\n       (3, 1): \'Windows Unicode BMP\',\\n       (3, 10): \'Windows Unicode Full\'\\n   }.get((cmap.platformID, cmap.platEncID), f\\"Encoding {cmap.platEncID}\\")\\n   ```\\n\\n   If a matching combination is found, the program returns the corresponding description. If no match is found, it defaults to `Encoding {cmap.platEncID}`, used to handle unknown encodings.\\n\\n## Code\\n\\nHere is the complete code. You can export the output information as JSON for further analysis or tracking.\\n\\n```python\\nimport re\\nimport unicodedata\\nfrom pathlib import Path\\nfrom typing import List, Union\\n\\nfrom fontTools.ttLib import TTFont\\n\\n\\ndef load_ttfont(font_path: Union[str, Path], **kwargs) -> TTFont:\\n    \\"\\"\\"Load a TrueType font file.\\"\\"\\"\\n    if isinstance(font_path, Path):\\n        font_path = str(font_path)\\n    return TTFont(font_path, **kwargs)\\n\\n\\ndef remove_control_characters(text: str, normalize: bool = True) -> str:\\n    \\"\\"\\"\\n    Remove control characters and invisible formatting characters from a string.\\n\\n    Args:\\n        text (str): The input string.\\n        normalize (bool): Whether to normalize the text to remove inconsistencies.\\n\\n    Returns:\\n        str: The sanitized string with control and invisible characters removed.\\n    \\"\\"\\"\\n    # Remove basic control characters (C0 and C1 control codes)\\n    sanitized = re.sub(r\'[\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]\', \'\', text)\\n\\n    # Remove specific Unicode control and invisible formatting characters\\n    sanitized = re.sub(\\n        r\'[\\\\u200B-\\\\u200F\\\\u2028-\\\\u202F\\\\u2060-\\\\u206F]\', \'\', sanitized)\\n\\n    # Remove directional formatting characters (optional, adjust if needed)\\n    sanitized = re.sub(r\'[\\\\u202A-\\\\u202E]\', \'\', sanitized)\\n\\n    # Optionally, normalize the text to remove any leftover inconsistencies\\n    if normalize:\\n        sanitized = unicodedata.normalize(\'NFKC\', sanitized)\\n\\n    return sanitized\\n\\n\\ndef extract_font_info(\\n    font_path: Union[str, Path],\\n    normalize: bool = True\\n) -> dict:\\n    \\"\\"\\"Extract detailed metadata and structural information from a font file.\\n\\n    Args:\\n        font_path (Union[str, Path]): Path to the font file.\\n\\n    Returns:\\n        dict: A dictionary containing font metadata and tables, including:\\n\\n            - fileName (str): Path to the font file.\\n            - tables (list): List of available tables in the font.\\n            - nameTable (dict): Raw name table values, keyed by nameID.\\n            - nameTableReadable (dict): Readable name table with keys:\\n                * copyright (str): Copyright information.\\n                * fontFamily (str): Font family name.\\n                * fontSubfamily (str): Font subfamily name.\\n                * uniqueID (str): Unique identifier for the font.\\n                * fullName (str): Full font name.\\n                * version (str): Font version string.\\n                * postScriptName (str): PostScript name.\\n            - cmapTable (dict): Character-to-glyph mappings, keyed by encoding.\\n            - cmapTableIndex (list): List of encoding descriptions.\\n            - headTable (dict): Font header table with keys:\\n                * unitsPerEm (int): Units per em.\\n                * xMin (int): Minimum x-coordinate of the glyph bounding box.\\n                * yMin (int): Minimum y-coordinate of the glyph bounding box.\\n                * xMax (int): Maximum x-coordinate of the glyph bounding box.\\n                * yMax (int): Maximum y-coordinate of the glyph bounding box.\\n            - hheaTable (dict): Horizontal header table with keys:\\n                * ascent (int): Typographic ascent.\\n                * descent (int): Typographic descent.\\n                * lineGap (int): Line gap.\\n            - OS2Table (dict): OS/2 table with keys:\\n                * usWeightClass (int): Weight class.\\n                * usWidthClass (int): Width class.\\n                * fsType (int): Embedding restrictions.\\n            - postTable (dict): PostScript table with keys:\\n                * isFixedPitch (bool): Whether the font is monospaced.\\n                * italicAngle (float): Italic angle of the font.\\n            - layoutMetrics (dict): Font layout metrics with keys:\\n                * unitsPerEm (int): Units per em.\\n                * boundingBox (dict): Bounding box coordinates:\\n                    - xMin (int): Minimum x-coordinate.\\n                    - yMin (int): Minimum y-coordinate.\\n                    - xMax (int): Maximum x-coordinate.\\n                    - yMax (int): Maximum y-coordinate.\\n                * ascent (int): Typographic ascent.\\n                * descent (int): Typographic descent.\\n                * lineGap (int): Line gap.\\n            - summary (dict): High-level font summary with keys:\\n                * fontFamily (str): Font family name.\\n                * fontSubfamily (str): Font subfamily name.\\n                * version (str): Font version.\\n                * weightClass (int): Weight class.\\n                * isItalic (bool): Whether the font is italic.\\n    \\"\\"\\"\\n\\n    if isinstance(font_path, Path):\\n        font_path = str(font_path)\\n\\n    font = TTFont(font_path)\\n    font_info = {}\\n\\n    # File name and available tables\\n    font_info[\'fileName\'] = font_path\\n    font_info[\'tables\'] = list(font.keys())\\n\\n    # Parse name table\\n    name_table = {}\\n    for record in font[\'name\'].names:\\n        try:\\n            raw_string = record.string.decode(\'utf-16-be\').strip()\\n            clean_string = remove_control_characters(raw_string, normalize)\\n            name_table[record.nameID] = clean_string\\n        except UnicodeDecodeError:\\n            name_table[record.nameID] = remove_control_characters(\\n                record.string.decode(errors=\'ignore\'), normalize)\\n    font_info[\'nameTable\'] = name_table\\n\\n    # Readable name table for common nameIDs\\n    name_table_readable = {\\n        \'copyright\': name_table.get(0, \'\'),\\n        \'fontFamily\': name_table.get(1, \'\'),\\n        \'fontSubfamily\': name_table.get(2, \'\'),\\n        \'uniqueID\': name_table.get(3, \'\'),\\n        \'fullName\': name_table.get(4, \'\'),\\n        \'version\': name_table.get(5, \'\'),\\n        \'postScriptName\': name_table.get(6, \'\'),\\n    }\\n    font_info[\'nameTableReadable\'] = {\\n        k: remove_control_characters(v, normalize)\\n        for k, v in name_table_readable.items()\\n    }\\n\\n    # Parse cmap table\\n    cmap_table = {}\\n    cmap_table_index = []\\n\\n    for cmap in font[\'cmap\'].tables:\\n        platform_name = {\\n            0: \'Unicode\',\\n            1: \'Macintosh\',\\n            3: \'Windows\'\\n        }.get(cmap.platformID, f\\"Platform {cmap.platformID}\\")\\n\\n        encoding_name = {\\n            (0, 0): \'Unicode 1.0\',\\n            (0, 3): \'Unicode 2.0+\',\\n            (0, 4): \'Unicode 2.0+ with BMP\',\\n            (1, 0): \'Mac Roman\',\\n            (3, 1): \'Windows Unicode BMP\',\\n            (3, 10): \'Windows Unicode Full\'\\n        }.get((cmap.platformID, cmap.platEncID), f\\"Encoding {cmap.platEncID}\\")\\n\\n        cmap_entries = {}\\n        for codepoint, glyph_name in cmap.cmap.items():\\n            char = chr(codepoint)\\n            cmap_entries[remove_control_characters(char, normalize)] = \\\\\\n                remove_control_characters(glyph_name, normalize)\\n\\n        key = f\\"{platform_name}, {encoding_name}\\"\\n        cmap_table[key] = cmap_entries\\n        cmap_table_index.append(key)\\n\\n    font_info[\'cmapTable\'] = cmap_table\\n    font_info[\'cmapTableIndex\'] = cmap_table_index\\n\\n    # Parse head table\\n    head = font[\'head\']\\n    head_table = {\\n        \'unitsPerEm\': head.unitsPerEm,\\n        \'xMin\': head.xMin,\\n        \'yMin\': head.yMin,\\n        \'xMax\': head.xMax,\\n        \'yMax\': head.yMax,\\n    }\\n    font_info[\'headTable\'] = head_table\\n\\n    # Parse hhea table\\n    hhea = font[\'hhea\']\\n    hhea_table = {\\n        \'ascent\': hhea.ascent,\\n        \'descent\': hhea.descent,\\n        \'lineGap\': hhea.lineGap,\\n    }\\n    font_info[\'hheaTable\'] = hhea_table\\n\\n    # Parse OS/2 table\\n    os2 = font[\'OS/2\']\\n    os2_table = {\\n        \'usWeightClass\': os2.usWeightClass,\\n        \'usWidthClass\': os2.usWidthClass,\\n        \'fsType\': os2.fsType,\\n    }\\n    font_info[\'OS2Table\'] = os2_table\\n\\n    # Parse post table\\n    post = font[\'post\']\\n    post_table = {\\n        \'isFixedPitch\': post.isFixedPitch,\\n        \'italicAngle\': post.italicAngle,\\n    }\\n    font_info[\'postTable\'] = post_table\\n\\n    # Combine layout-related metrics\\n    font_info[\'layoutMetrics\'] = {\\n        \'unitsPerEm\': head_table[\'unitsPerEm\'],\\n        \'boundingBox\': {\\n            \'xMin\': head_table[\'xMin\'],\\n            \'yMin\': head_table[\'yMin\'],\\n            \'xMax\': head_table[\'xMax\'],\\n            \'yMax\': head_table[\'yMax\']\\n        },\\n        \'ascent\': hhea_table[\'ascent\'],\\n        \'descent\': hhea_table[\'descent\'],\\n        \'lineGap\': hhea_table[\'lineGap\']\\n    }\\n\\n    # Font summary\\n    font_info[\'summary\'] = {\\n        \'fontFamily\': name_table_readable[\'fontFamily\'],\\n        \'fontSubfamily\': name_table_readable[\'fontSubfamily\'],\\n        \'version\': name_table_readable[\'version\'],\\n        \'weightClass\': os2.usWeightClass,\\n        \'isItalic\': post_table[\'italicAngle\'] != 0\\n    }\\n\\n    return font_info\\n```\\n\\n## Example Output\\n\\nLet\u2019s take the font file `OcrB-Regular.ttf` as an example. We will call the function and then export the results to a JSON file:\\n\\n```python\\nimport json\\n\\nfont_infos = extract_font_info(\'OcrB-Regular.ttf\')\\njson.dump(font_infos, open(\'OcrB-Regular-Info.json\', \'w\'),\\n          indent=2, ensure_ascii=False)\\n```\\n\\nThe output will be as follows:\\n\\n```json\\n{\\n  \\"fileName\\": \\"/path/to/your/folder/OcrB-Regular.ttf\\",\\n  \\"tables\\": [\\n    \\"GlyphOrder\\",\\n    \\"head\\",\\n    \\"hhea\\",\\n    \\"maxp\\",\\n    \\"OS/2\\",\\n    \\"hmtx\\",\\n    \\"hdmx\\",\\n    \\"cmap\\",\\n    \\"fpgm\\",\\n    \\"prep\\",\\n    \\"cvt \\",\\n    \\"loca\\",\\n    \\"glyf\\",\\n    \\"name\\",\\n    \\"post\\"\\n  ],\\n  \\"nameTable\\": {\\n    \\"0\\": \\"This is a copyrighted typeface program\\",\\n    \\"1\\": \\"OcrB\\",\\n    \\"2\\": \\"Regular\\",\\n    \\"3\\": \\"Altsys Fontographer 3.5  OcrB Regular\\",\\n    \\"4\\": \\"OcrB Regular\\",\\n    \\"5\\": \\"Altsys Fontographer 3.5  4/15/93\\",\\n    \\"6\\": \\"OcrB Regular\\"\\n  },\\n  \\"nameTableReadable\\": {\\n    \\"copyright\\": \\"This is a copyrighted typeface program\\",\\n    \\"fontFamily\\": \\"OcrB\\",\\n    \\"fontSubfamily\\": \\"Regular\\",\\n    \\"uniqueID\\": \\"Altsys Fontographer 3.5  OcrB Regular\\",\\n    \\"fullName\\": \\"OcrB Regular\\",\\n    \\"version\\": \\"Altsys Fontographer 3.5  4/15/93\\",\\n    \\"postScriptName\\": \\"OcrB Regular\\"\\n  },\\n  \\"cmapTable\\": {\\n    \\"Unicode, Unicode 1.0\\": {\\n      \\" \\": \\"nonbreakingspace\\",\\n      \\"!\\": \\"exclam\\",\\n      \\"\\\\\\"\\": \\"quotedbl\\",\\n      \\"#\\": \\"numbersign\\",\\n      \\"$\\": \\"dollar\\",\\n      \\"%\\": \\"percent\\",\\n      \\"&\\": \\"ampersand\\",\\n      \\"\'\\": \\"quotesingle\\",\\n      \\"(\\": \\"parenleft\\",\\n      \\")\\": \\"parenright\\",\\n      \\"*\\": \\"asterisk\\",\\n      \\"+\\": \\"plus\\",\\n      \\",\\": \\"comma\\",\\n      \\"-\\": \\"hyphen\\",\\n      \\".\\": \\"period\\",\\n      \\"/\\": \\"slash\\",\\n      \\"0\\": \\"zero\\",\\n      \\"1\\": \\"one\\",\\n      \\"2\\": \\"two\\",\\n      \\"3\\": \\"three\\",\\n      \\"4\\": \\"four\\",\\n      \\"5\\": \\"five\\",\\n      \\"6\\": \\"six\\",\\n      \\"7\\": \\"seven\\",\\n      \\"8\\": \\"eight\\",\\n      \\"9\\": \\"nine\\",\\n      \\":\\": \\"colon\\",\\n      \\";\\": \\"semicolon\\",\\n      \\"<\\": \\"less\\",\\n      \\"=\\": \\"equal\\",\\n      \\">\\": \\"greater\\",\\n      \\"?\\": \\"question\\",\\n      \\"@\\": \\"at\\",\\n      \\"A\\": \\"A\\",\\n      \\"B\\": \\"B\\",\\n      \\"C\\": \\"C\\",\\n      \\"D\\": \\"D\\",\\n      \\"E\\": \\"E\\",\\n      \\"F\\": \\"F\\",\\n      \\"G\\": \\"G\\",\\n      \\"H\\": \\"H\\",\\n      \\"I\\": \\"I\\",\\n      \\"J\\": \\"J\\",\\n      \\"K\\": \\"K\\",\\n      \\"L\\": \\"L\\",\\n      \\"M\\": \\"M\\",\\n      \\"N\\": \\"N\\",\\n      \\"O\\": \\"O\\",\\n      \\"P\\": \\"P\\",\\n      \\"Q\\": \\"Q\\",\\n      \\"R\\": \\"R\\",\\n      \\"S\\": \\"S\\",\\n      \\"T\\": \\"T\\",\\n      \\"U\\": \\"U\\",\\n      \\"V\\": \\"V\\",\\n      \\"W\\": \\"W\\",\\n      \\"X\\": \\"X\\",\\n      \\"Y\\": \\"Y\\",\\n      \\"Z\\": \\"Z\\",\\n      \\"[\\": \\"bracketleft\\",\\n      \\"\\\\\\\\\\": \\"backslash\\",\\n      \\"]\\": \\"bracketright\\",\\n      \\"^\\": \\"asciicircum\\",\\n      \\"_\\": \\"underscore\\",\\n      \\"`\\": \\"grave\\",\\n      \\"a\\": \\"a\\",\\n      \\"b\\": \\"b\\",\\n      \\"c\\": \\"c\\",\\n      \\"d\\": \\"d\\",\\n      \\"e\\": \\"e\\",\\n      \\"f\\": \\"f\\",\\n      \\"g\\": \\"g\\",\\n      \\"h\\": \\"h\\",\\n      \\"i\\": \\"i\\",\\n      \\"j\\": \\"j\\",\\n      \\"k\\": \\"k\\",\\n      \\"l\\": \\"l\\",\\n      \\"m\\": \\"m\\",\\n      \\"n\\": \\"n\\",\\n      \\"o\\": \\"o\\",\\n      \\"p\\": \\"p\\",\\n      \\"q\\": \\"q\\",\\n      \\"r\\": \\"r\\",\\n      \\"s\\": \\"zcaron\\",\\n      \\"t\\": \\"t\\",\\n      \\"u\\": \\"u\\",\\n      \\"v\\": \\"v\\",\\n      \\"w\\": \\"w\\",\\n      \\"x\\": \\"x\\",\\n      \\"y\\": \\"y\\",\\n      \\"z\\": \\"z\\",\\n      \\"{\\": \\"braceleft\\",\\n      \\"|\\": \\"bar\\",\\n      \\"}\\": \\"braceright\\",\\n      \\"\xa1\\": \\"exclamdown\\",\\n      \\"\xa2\\": \\"cent\\",\\n      \\"\xa3\\": \\"sterling\\",\\n      \\"\xa4\\": \\"currency\\",\\n      \\"\xa5\\": \\"yen\\",\\n      \\"\xa7\\": \\"section\\",\\n      \\" \u0308\\": \\"dieresis\\",\\n      \\"\xab\\": \\"guillemotleft\\",\\n      \\"\xad\\": \\"hyphen\\",\\n      \\" \u0304\\": \\"macron\\",\\n      \\" \u0301\\": \\"acute\\",\\n      \\"\xb7\\": \\"periodcentered\\",\\n      \\" \u0327\\": \\"cedilla\\",\\n      \\"\xbb\\": \\"guillemotright\\",\\n      \\"\xbf\\": \\"questiondown\\",\\n      \\"\xc0\\": \\"Agrave\\",\\n      \\"\xc1\\": \\"Aacute\\",\\n      \\"\xc2\\": \\"Acircumflex\\",\\n      \\"\xc3\\": \\"Atilde\\",\\n      \\"\xc4\\": \\"Adieresis\\",\\n      \\"\xc5\\": \\"Aring\\",\\n      \\"\xc6\\": \\"AE\\",\\n      \\"\xc7\\": \\"Ccedilla\\",\\n      \\"\xc8\\": \\"Egrave\\",\\n      \\"\xc9\\": \\"Eacute\\",\\n      \\"\xca\\": \\"Ecircumflex\\",\\n      \\"\xcb\\": \\"Edieresis\\",\\n      \\"\xcc\\": \\"Igrave\\",\\n      \\"\xcd\\": \\"Iacute\\",\\n      \\"\xce\\": \\"Icircumflex\\",\\n      \\"\xcf\\": \\"Idieresis\\",\\n      \\"\xd0\\": \\"Eth\\",\\n      \\"\xd1\\": \\"Ntilde\\",\\n      \\"\xd2\\": \\"Ograve\\",\\n      \\"\xd3\\": \\"Oacute\\",\\n      \\"\xd4\\": \\"Ocircumflex\\",\\n      \\"\xd5\\": \\"Otilde\\",\\n      \\"\xd6\\": \\"Odieresis\\",\\n      \\"\xd7\\": \\".null\\",\\n      \\"\xd8\\": \\"Oslash\\",\\n      \\"\xd9\\": \\"Ugrave\\",\\n      \\"\xda\\": \\"Uacute\\",\\n      \\"\xdb\\": \\"Ucircumflex\\",\\n      \\"\xdc\\": \\"Udieresis\\",\\n      \\"\xdd\\": \\"Yacute#1\\",\\n      \\"\xde\\": \\"Thorn\\",\\n      \\"\xdf\\": \\"germandbls\\",\\n      \\"\xe0\\": \\"agrave\\",\\n      \\"\xe1\\": \\"aacute\\",\\n      \\"\xe2\\": \\"acircumflex\\",\\n      \\"\xe3\\": \\"atilde\\",\\n      \\"\xe4\\": \\"adieresis\\",\\n      \\"\xe5\\": \\"aring\\",\\n      \\"\xe6\\": \\"ae\\",\\n      \\"\xe7\\": \\"ccedilla\\",\\n      \\"\xe8\\": \\"egrave\\",\\n      \\"\xe9\\": \\"eacute\\",\\n      \\"\xea\\": \\"ecircumflex\\",\\n      \\"\xeb\\": \\"edieresis\\",\\n      \\"\xec\\": \\"igrave\\",\\n      \\"\xed\\": \\"iacute\\",\\n      \\"\xee\\": \\"icircumflex\\",\\n      \\"\xef\\": \\"idieresis\\",\\n      \\"\xf0\\": \\"Yacute\\",\\n      \\"\xf1\\": \\"ntilde\\",\\n      \\"\xf2\\": \\"ograve\\",\\n      \\"\xf3\\": \\"oacute\\",\\n      \\"\xf4\\": \\"ocircumflex\\",\\n      \\"\xf5\\": \\"otilde\\",\\n      \\"\xf6\\": \\"odieresis\\",\\n      \\"\xf8\\": \\"oslash\\",\\n      \\"\xf9\\": \\"ugrave\\",\\n      \\"\xfa\\": \\"uacute\\",\\n      \\"\xfb\\": \\"ucircumflex\\",\\n      \\"\xfc\\": \\"udieresis\\",\\n      \\"\xfd\\": \\"yacute\\",\\n      \\"\xfe\\": \\"thorn\\",\\n      \\"\xff\\": \\"ydieresis\\",\\n      \\"\u0131\\": \\"dotlessi\\",\\n      \\"\u0141\\": \\"Lslash\\",\\n      \\"\u0142\\": \\"lslash\\",\\n      \\"\u0152\\": \\"OE\\",\\n      \\"\u0153\\": \\"oe\\",\\n      \\"\u0160\\": \\"Scaron\\",\\n      \\"\u0161\\": \\"scaron\\",\\n      \\"\u0178\\": \\"Ydieresis\\",\\n      \\"\u017D\\": \\"Zcaron\\",\\n      \\"\u02BA\\": \\"hungarumlaut\\",\\n      \\"\u02C6\\": \\"circumflex\\",\\n      \\"\u02C7\\": \\"caron\\",\\n      \\"\u02C9\\": \\"macron\\",\\n      \\" \u0306\\": \\"breve\\",\\n      \\" \u0307\\": \\"dotaccent\\",\\n      \\" \u030A\\": \\"ring\\",\\n      \\" \u0328\\": \\"ogonek\\",\\n      \\" \u0303\\": \\"tilde\\",\\n      \\"\u2013\\": \\"endash\\",\\n      \\"\u2014\\": \\"emdash\\",\\n      \\"\u2018\\": \\"quoteleft\\",\\n      \\"\u201A\\": \\"quotesinglbase\\",\\n      \\"\u201C\\": \\"quotedblleft\\",\\n      \\"\u201D\\": \\"quotedblright\\",\\n      \\"\u201E\\": \\"quotedblbase\\",\\n      \\"\u2020\\": \\"dagger\\",\\n      \\"\u2021\\": \\"daggerdbl\\",\\n      \\"...\\": \\"ellipsis\\",\\n      \\"\u2039\\": \\"guilsinglleft\\",\\n      \\"\u203A\\": \\"guilsinglright\\",\\n      \\"\u2212\\": \\"minus\\",\\n      \\"\u2219\\": \\"periodcentered\\"\\n    },\\n    \\"Macintosh, Mac Roman\\": {\\n      \\"\\": \\"udieresis\\",\\n      \\" \\": \\"dagger\\",\\n      \\"!\\": \\"exclam\\",\\n      \\"\\\\\\"\\": \\"quotedbl\\",\\n      \\"#\\": \\"numbersign\\",\\n      \\"$\\": \\"dollar\\",\\n      \\"%\\": \\"percent\\",\\n      \\"&\\": \\"ampersand\\",\\n      \\"\'\\": \\"quotesingle\\",\\n      \\"(\\": \\"parenleft\\",\\n      \\")\\": \\"parenright\\",\\n      \\"*\\": \\"asterisk\\",\\n      \\"+\\": \\"plus\\",\\n      \\",\\": \\"comma\\",\\n      \\"-\\": \\"hyphen\\",\\n      \\".\\": \\"period\\",\\n      \\"/\\": \\"slash\\",\\n      \\"0\\": \\"zero\\",\\n      \\"1\\": \\"one\\",\\n      \\"2\\": \\"two\\",\\n      \\"3\\": \\"three\\",\\n      \\"4\\": \\"four\\",\\n      \\"5\\": \\"five\\",\\n      \\"6\\": \\"six\\",\\n      \\"7\\": \\"seven\\",\\n      \\"8\\": \\"eight\\",\\n      \\"9\\": \\"nine\\",\\n      \\":\\": \\"colon\\",\\n      \\";\\": \\"semicolon\\",\\n      \\"<\\": \\"less\\",\\n      \\"=\\": \\"equal\\",\\n      \\">\\": \\"greater\\",\\n      \\"?\\": \\"question\\",\\n      \\"@\\": \\"at\\",\\n      \\"A\\": \\"A\\",\\n      \\"B\\": \\"B\\",\\n      \\"C\\": \\"C\\",\\n      \\"D\\": \\"D\\",\\n      \\"E\\": \\"E\\",\\n      \\"F\\": \\"F\\",\\n      \\"G\\": \\"G\\",\\n      \\"H\\": \\"H\\",\\n      \\"I\\": \\"I\\",\\n      \\"J\\": \\"J\\",\\n      \\"K\\": \\"K\\",\\n      \\"L\\": \\"L\\",\\n      \\"M\\": \\"M\\",\\n      \\"N\\": \\"N\\",\\n      \\"O\\": \\"O\\",\\n      \\"P\\": \\"P\\",\\n      \\"Q\\": \\"Q\\",\\n      \\"R\\": \\"R\\",\\n      \\"S\\": \\"S\\",\\n      \\"T\\": \\"T\\",\\n      \\"U\\": \\"U\\",\\n      \\"V\\": \\"V\\",\\n      \\"W\\": \\"W\\",\\n      \\"X\\": \\"X\\",\\n      \\"Y\\": \\"Y\\",\\n      \\"Z\\": \\"Z\\",\\n      \\"[\\": \\"bracketleft\\",\\n      \\"\\\\\\\\\\": \\"backslash\\",\\n      \\"]\\": \\"bracketright\\",\\n      \\"^\\": \\"asciicircum\\",\\n      \\"_\\": \\"underscore\\",\\n      \\"`\\": \\"grave\\",\\n      \\"a\\": \\"a\\",\\n      \\"b\\": \\"b\\",\\n      \\"c\\": \\"c\\",\\n      \\"d\\": \\"d\\",\\n      \\"e\\": \\"e\\",\\n      \\"f\\": \\"f\\",\\n      \\"g\\": \\"g\\",\\n      \\"h\\": \\"h\\",\\n      \\"i\\": \\"i\\",\\n      \\"j\\": \\"j\\",\\n      \\"k\\": \\"k\\",\\n      \\"l\\": \\"l\\",\\n      \\"m\\": \\"m\\",\\n      \\"n\\": \\"n\\",\\n      \\"o\\": \\"o\\",\\n      \\"p\\": \\"p\\",\\n      \\"q\\": \\"q\\",\\n      \\"r\\": \\"r\\",\\n      \\"s\\": \\"s\\",\\n      \\"t\\": \\"t\\",\\n      \\"u\\": \\"u\\",\\n      \\"v\\": \\"v\\",\\n      \\"w\\": \\"w\\",\\n      \\"x\\": \\"x\\",\\n      \\"y\\": \\"y\\",\\n      \\"z\\": \\"z\\",\\n      \\"{\\": \\"braceleft\\",\\n      \\"|\\": \\"bar\\",\\n      \\"}\\": \\"braceright\\",\\n      \\"\xa2\\": \\"cent\\",\\n      \\"\xa3\\": \\"sterling\\",\\n      \\"\xa4\\": \\"section\\",\\n      \\"\xa7\\": \\"germandbls\\",\\n      \\"\xab\\": \\"acute\\",\\n      \\"\xac\\": \\"dieresis\\",\\n      \\"\xae\\": \\"AE\\",\\n      \\" \u0304\\": \\"Oslash\\",\\n      \\" \u0301\\": \\"yen\\",\\n      \\"3\u20444\\": \\"ae\\",\\n      \\"\xbf\\": \\"oslash\\",\\n      \\"\xc0\\": \\"questiondown\\",\\n      \\"\xc1\\": \\"exclamdown\\",\\n      \\"\xc7\\": \\"guillemotleft\\",\\n      \\"\xc8\\": \\"guillemotright\\",\\n      \\"\xc9\\": \\"ellipsis\\",\\n      \\"\xca\\": \\"nonbreakingspace\\",\\n      \\"\xcb\\": \\"Agrave\\",\\n      \\"\xcc\\": \\"Atilde\\",\\n      \\"\xcd\\": \\"Otilde\\",\\n      \\"\xce\\": \\"OE\\",\\n      \\"\xcf\\": \\"oe\\",\\n      \\"\xd0\\": \\"endash\\",\\n      \\"\xd1\\": \\"emdash\\",\\n      \\"\xd2\\": \\"quotedblleft\\",\\n      \\"\xd3\\": \\"quotedblright\\",\\n      \\"\xd4\\": \\"quoteleft\\",\\n      \\"\xd8\\": \\"ydieresis\\",\\n      \\"\xd9\\": \\"Ydieresis\\",\\n      \\"\xdb\\": \\"currency\\",\\n      \\"\xdc\\": \\"guilsinglleft\\",\\n      \\"\xdd\\": \\"guilsinglright\\",\\n      \\"\xe0\\": \\"daggerdbl\\",\\n      \\"\xe1\\": \\"periodcentered\\",\\n      \\"\xe2\\": \\"quotesinglbase\\",\\n      \\"\xe3\\": \\"quotedblbase\\",\\n      \\"\xe5\\": \\"Acircumflex\\",\\n      \\"\xe6\\": \\"Ecircumflex\\",\\n      \\"\xe7\\": \\"Aacute\\",\\n      \\"\xe8\\": \\"Edieresis\\",\\n      \\"\xe9\\": \\"Egrave\\",\\n      \\"\xea\\": \\"Iacute\\",\\n      \\"\xeb\\": \\"Icircumflex\\",\\n      \\"\xec\\": \\"Idieresis\\",\\n      \\"\xed\\": \\"Igrave\\",\\n      \\"\xee\\": \\"Oacute\\",\\n      \\"\xef\\": \\"Ocircumflex\\",\\n      \\"\xf1\\": \\"Ograve\\",\\n      \\"\xf2\\": \\"Uacute\\",\\n      \\"\xf3\\": \\"Ucircumflex\\",\\n      \\"\xf4\\": \\"Ugrave\\",\\n      \\"\xf5\\": \\"dotlessi\\",\\n      \\"\xf6\\": \\"circumflex\\",\\n      \\"\xf7\\": \\"tilde\\",\\n      \\"\xf8\\": \\"macron\\",\\n      \\"\xf9\\": \\"breve\\",\\n      \\"\xfa\\": \\"dotaccent\\",\\n      \\"\xfb\\": \\"ring\\",\\n      \\"\xfc\\": \\"cedilla\\",\\n      \\"\xfd\\": \\"hungarumlaut\\",\\n      \\"\xfe\\": \\"ogonek\\",\\n      \\"\xff\\": \\"caron\\"\\n    },\\n    \\"Windows, Windows Unicode BMP\\": {\\n      \\" \\": \\"nonbreakingspace\\",\\n      \\"!\\": \\"exclam\\",\\n      \\"\\\\\\"\\": \\"quotedbl\\",\\n      \\"#\\": \\"numbersign\\",\\n      \\"$\\": \\"dollar\\",\\n      \\"%\\": \\"percent\\",\\n      \\"&\\": \\"ampersand\\",\\n      \\"\'\\": \\"quotesingle\\",\\n      \\"(\\": \\"parenleft\\",\\n      \\")\\": \\"parenright\\",\\n      \\"*\\": \\"asterisk\\",\\n      \\"+\\": \\"plus\\",\\n      \\",\\": \\"comma\\",\\n      \\"-\\": \\"hyphen\\",\\n      \\".\\": \\"period\\",\\n      \\"/\\": \\"slash\\",\\n      \\"0\\": \\"zero\\",\\n      \\"1\\": \\"one\\",\\n      \\"2\\": \\"two\\",\\n      \\"3\\": \\"three\\",\\n      \\"4\\": \\"four\\",\\n      \\"5\\": \\"five\\",\\n      \\"6\\": \\"six\\",\\n      \\"7\\": \\"seven\\",\\n      \\"8\\": \\"eight\\",\\n      \\"9\\": \\"nine\\",\\n      \\":\\": \\"colon\\",\\n      \\";\\": \\"semicolon\\",\\n      \\"<\\": \\"less\\",\\n      \\"=\\": \\"equal\\",\\n      \\">\\": \\"greater\\",\\n      \\"?\\": \\"question\\",\\n      \\"@\\": \\"at\\",\\n      \\"A\\": \\"A\\",\\n      \\"B\\": \\"B\\",\\n      \\"C\\": \\"C\\",\\n      \\"D\\": \\"D\\",\\n      \\"E\\": \\"E\\",\\n      \\"F\\": \\"F\\",\\n      \\"G\\": \\"G\\",\\n      \\"H\\": \\"H\\",\\n      \\"I\\": \\"I\\",\\n      \\"J\\": \\"J\\",\\n      \\"K\\": \\"K\\",\\n      \\"L\\": \\"L\\",\\n      \\"M\\": \\"M\\",\\n      \\"N\\": \\"N\\",\\n      \\"O\\": \\"O\\",\\n      \\"P\\": \\"P\\",\\n      \\"Q\\": \\"Q\\",\\n      \\"R\\": \\"R\\",\\n      \\"S\\": \\"S\\",\\n      \\"T\\": \\"T\\",\\n      \\"U\\": \\"U\\",\\n      \\"V\\": \\"V\\",\\n      \\"W\\": \\"W\\",\\n      \\"X\\": \\"X\\",\\n      \\"Y\\": \\"Y\\",\\n      \\"Z\\": \\"Z\\",\\n      \\"[\\": \\"bracketleft\\",\\n      \\"\\\\\\\\\\": \\"backslash\\",\\n      \\"]\\": \\"bracketright\\",\\n      \\"^\\": \\"asciicircum\\",\\n      \\"_\\": \\"underscore\\",\\n      \\"`\\": \\"grave\\",\\n      \\"a\\": \\"a\\",\\n      \\"b\\": \\"b\\",\\n      \\"c\\": \\"c\\",\\n      \\"d\\": \\"d\\",\\n      \\"e\\": \\"e\\",\\n      \\"f\\": \\"f\\",\\n      \\"g\\": \\"g\\",\\n      \\"h\\": \\"h\\",\\n      \\"i\\": \\"i\\",\\n      \\"j\\": \\"j\\",\\n      \\"k\\": \\"k\\",\\n      \\"l\\": \\"l\\",\\n      \\"m\\": \\"m\\",\\n      \\"n\\": \\"n\\",\\n      \\"o\\": \\"o\\",\\n      \\"p\\": \\"p\\",\\n      \\"q\\": \\"q\\",\\n      \\"r\\": \\"r\\",\\n      \\"s\\": \\"zcaron\\",\\n      \\"t\\": \\"t\\",\\n      \\"u\\": \\"u\\",\\n      \\"v\\": \\"v\\",\\n      \\"w\\": \\"w\\",\\n      \\"x\\": \\"x\\",\\n      \\"y\\": \\"y\\",\\n      \\"z\\": \\"z\\",\\n      \\"{\\": \\"braceleft\\",\\n      \\"|\\": \\"bar\\",\\n      \\"}\\": \\"braceright\\",\\n      \\"\xa1\\": \\"exclamdown\\",\\n      \\"\xa2\\": \\"cent\\",\\n      \\"\xa3\\": \\"sterling\\",\\n      \\"\xa4\\": \\"currency\\",\\n      \\"\xa5\\": \\"yen\\",\\n      \\"\xa7\\": \\"section\\",\\n      \\" \u0308\\": \\"dieresis\\",\\n      \\"\xab\\": \\"guillemotleft\\",\\n      \\"\xad\\": \\"hyphen\\",\\n      \\" \u0304\\": \\"macron\\",\\n      \\" \u0301\\": \\"acute\\",\\n      \\"\xb7\\": \\"periodcentered\\",\\n      \\" \u0327\\": \\"cedilla\\",\\n      \\"\xbb\\": \\"guillemotright\\",\\n      \\"\xbf\\": \\"questiondown\\",\\n      \\"\xc0\\": \\"Agrave\\",\\n      \\"\xc1\\": \\"Aacute\\",\\n      \\"\xc2\\": \\"Acircumflex\\",\\n      \\"\xc3\\": \\"Atilde\\",\\n      \\"\xc4\\": \\"Adieresis\\",\\n      \\"\xc5\\": \\"Aring\\",\\n      \\"\xc6\\": \\"AE\\",\\n      \\"\xc7\\": \\"Ccedilla\\",\\n      \\"\xc8\\": \\"Egrave\\",\\n      \\"\xc9\\": \\"Eacute\\",\\n      \\"\xca\\": \\"Ecircumflex\\",\\n      \\"\xcb\\": \\"Edieresis\\",\\n      \\"\xcc\\": \\"Igrave\\",\\n      \\"\xcd\\": \\"Iacute\\",\\n      \\"\xce\\": \\"Icircumflex\\",\\n      \\"\xcf\\": \\"Idieresis\\",\\n      \\"\xd0\\": \\"Eth\\",\\n      \\"\xd1\\": \\"Ntilde\\",\\n      \\"\xd2\\": \\"Ograve\\",\\n      \\"\xd3\\": \\"Oacute\\",\\n      \\"\xd4\\": \\"Ocircumflex\\",\\n      \\"\xd5\\": \\"Otilde\\",\\n      \\"\xd6\\": \\"Odieresis\\",\\n      \\"\xd7\\": \\".null\\",\\n      \\"\xd8\\": \\"Oslash\\",\\n      \\"\xd9\\": \\"Ugrave\\",\\n      \\"\xda\\": \\"Uacute\\",\\n      \\"\xdb\\": \\"Ucircumflex\\",\\n      \\"\xdc\\": \\"Udieresis\\",\\n      \\"\xdd\\": \\"Yacute#1\\",\\n      \\"\xde\\": \\"Thorn\\",\\n      \\"\xdf\\": \\"germandbls\\",\\n      \\"\xe0\\": \\"agrave\\",\\n      \\"\xe1\\": \\"aacute\\",\\n      \\"\xe2\\": \\"acircumflex\\",\\n      \\"\xe3\\": \\"atilde\\",\\n      \\"\xe4\\": \\"adieresis\\",\\n      \\"\xe5\\": \\"aring\\",\\n      \\"\xe6\\": \\"ae\\",\\n      \\"\xe7\\": \\"ccedilla\\",\\n      \\"\xe8\\": \\"egrave\\",\\n      \\"\xe9\\": \\"eacute\\",\\n      \\"\xea\\": \\"ecircumflex\\",\\n      \\"\xeb\\": \\"edieresis\\",\\n      \\"\xec\\": \\"igrave\\",\\n      \\"\xed\\": \\"iacute\\",\\n      \\"\xee\\": \\"icircumflex\\",\\n      \\"\xef\\": \\"idieresis\\",\\n      \\"\xf0\\": \\"Yacute\\",\\n      \\"\xf1\\": \\"ntilde\\",\\n      \\"\xf2\\": \\"ograve\\",\\n      \\"\xf3\\": \\"oacute\\",\\n      \\"\xf4\\": \\"ocircumflex\\",\\n      \\"\xf5\\": \\"otilde\\",\\n      \\"\xf6\\": \\"odieresis\\",\\n      \\"\xf8\\": \\"oslash\\",\\n      \\"\xf9\\": \\"ugrave\\",\\n      \\"\xfa\\": \\"uacute\\",\\n      \\"\xfb\\": \\"ucircumflex\\",\\n      \\"\xfc\\": \\"udieresis\\",\\n      \\"\xfd\\": \\"yacute\\",\\n      \\"\xfe\\": \\"thorn\\",\\n      \\"\xff\\": \\"ydieresis\\",\\n      \\"\u0131\\": \\"dotlessi\\",\\n      \\"\u0141\\": \\"Lslash\\",\\n      \\"\u0142\\": \\"lslash\\",\\n      \\"\u0152\\": \\"OE\\",\\n      \\"\u0153\\": \\"oe\\",\\n      \\"\u0160\\": \\"Scaron\\",\\n      \\"\u0161\\": \\"scaron\\",\\n      \\"\u0178\\": \\"Ydieresis\\",\\n      \\"\u017D\\": \\"Zcaron\\",\\n      \\"\u02BA\\": \\"hungarumlaut\\",\\n      \\"\u02C6\\": \\"circumflex\\",\\n      \\"\u02C7\\": \\"caron\\",\\n      \\"\u02C9\\": \\"macron\\",\\n      \\" \u0306\\": \\"breve\\",\\n      \\" \u0307\\": \\"dotaccent\\",\\n      \\" \u030A\\": \\"ring\\",\\n      \\" \u0328\\": \\"ogonek\\",\\n      \\" \u0303\\": \\"tilde\\",\\n      \\"\u2013\\": \\"endash\\",\\n      \\"\u2014\\": \\"emdash\\",\\n      \\"\u2018\\": \\"quoteleft\\",\\n      \\"\u201A\\": \\"quotesinglbase\\",\\n      \\"\u201C\\": \\"quotedblleft\\",\\n      \\"\u201D\\": \\"quotedblright\\",\\n      \\"\u201E\\": \\"quotedblbase\\",\\n      \\"\u2020\\": \\"dagger\\",\\n      \\"\u2021\\": \\"daggerdbl\\",\\n      \\"...\\": \\"ellipsis\\",\\n      \\"\u2039\\": \\"guilsinglleft\\",\\n      \\"\u203A\\": \\"guilsinglright\\",\\n      \\"\u2212\\": \\"minus\\",\\n      \\"\u2219\\": \\"periodcentered\\"\\n    }\\n  },\\n  \\"cmapTableIndex\\": [\\n    \\"Unicode, Unicode 1.0\\",\\n    \\"Macintosh, Mac Roman\\",\\n    \\"Windows, Windows Unicode BMP\\"\\n  ],\\n  \\"headTable\\": {\\n    \\"unitsPerEm\\": 1000,\\n    \\"xMin\\": -89,\\n    \\"yMin\\": -337,\\n    \\"xMax\\": 691,\\n    \\"yMax\\": 744\\n  },\\n  \\"hheaTable\\": {\\n    \\"ascent\\": 744,\\n    \\"descent\\": -337,\\n    \\"lineGap\\": 0\\n  },\\n  \\"OS2Table\\": {\\n    \\"usWeightClass\\": 400,\\n    \\"usWidthClass\\": 5,\\n    \\"fsType\\": 2\\n  },\\n  \\"postTable\\": {\\n    \\"isFixedPitch\\": 0,\\n    \\"italicAngle\\": 0.0\\n  },\\n  \\"layoutMetrics\\": {\\n    \\"unitsPerEm\\": 1000,\\n    \\"boundingBox\\": {\\n      \\"xMin\\": -89,\\n      \\"yMin\\": -337,\\n      \\"xMax\\": 691,\\n      \\"yMax\\": 744\\n    },\\n    \\"ascent\\": 744,\\n    \\"descent\\": -337,\\n    \\"lineGap\\": 0\\n  },\\n  \\"summary\\": {\\n    \\"fontFamily\\": \\"OcrB\\",\\n    \\"fontSubfamily\\": \\"Regular\\",\\n    \\"version\\": \\"Altsys Fontographer 3.5  4/15/93\\",\\n    \\"weightClass\\": 400,\\n    \\"isItalic\\": false\\n  }\\n}\\n```\\n\\n## References\\n\\n- [**Character to Glyph Mapping Table**](https://developer.apple.com/fonts/TrueType-Reference-Manual/RM06/Chap6cmap.html?utm_source=chatgpt.com)\\n- [**cmap \u2014 Character to Glyph Index Mapping Table**](https://learn.microsoft.com/en-us/typography/opentype/spec/cmap?utm_source=chatgpt.com)"},{"id":"flexible-video-conversion-by-python","metadata":{"permalink":"/en/blog/flexible-video-conversion-by-python","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/12-17-flexible-video-conversion-by-python/index.md","title":"Batch Video Conversion","description":"Build a batch conversion process with Python and ffmpeg to convert to a specified format.","date":"2024-12-17T00:00:00.000Z","tags":[{"inline":true,"label":"Media-Processing","permalink":"/en/blog/tags/media-processing"},{"inline":true,"label":"Python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"ffmpeg","permalink":"/en/blog/tags/ffmpeg"}],"readingTime":3.965,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"flexible-video-conversion-by-python","title":"Batch Video Conversion","authors":"Z. Yuan","image":"/en/img/2024/1217.webp","tags":["Media-Processing","Python","ffmpeg"],"description":"Build a batch conversion process with Python and ffmpeg to convert to a specified format."},"unlisted":false,"prevItem":{"title":"Extract Font File Information","permalink":"/en/blog/extract-font-info-by-python"},"nextItem":{"title":"Automating Ubuntu System Status Checks with ChatGPT","permalink":"/en/blog/system-status-checking-by-chatgpt"}},"content":"A batch of MOV video files were received, but the system does not support reading them. They need to be converted to MP4 for compatibility.\\n\\nI had to write some code myself.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Design Draft\\n\\nThe go-to tool for video conversion is undoubtedly ffmpeg. This open-source tool supports almost all audio and video formats and allows you to control the conversion process through command-line parameters.\\n\\nInitially, we wanted to implement this functionality directly in the frontend, so others could convert files however they wanted...\\n\\nHowever, we encountered difficulties when calling it from the browser and spent an hour trying to resolve the issue without success. So, we decided to handle the conversion locally.\\n\\nConverting locally is much easier. At first, we considered writing a Bash script, but then we realized Python might be easier to maintain, so we chose to use Python together with ffmpeg to accomplish the task.\\n\\n## What is FFMPEG?\\n\\n[ffmpeg](https://ffmpeg.org/) is an extremely powerful open-source multimedia processing tool, widely used for tasks like format conversion, streaming, editing, and merging multimedia files.\\n\\nIt supports a wide range of common and uncommon audio and video formats and includes a large number of codecs. With simple command-line operations, ffmpeg allows you to quickly perform tasks like conversion, cutting, embedding subtitles, resampling, compression, and cross-platform streaming.\\n\\nSince ffmpeg is an open-source project and can be easily installed and run on various operating systems (Linux, macOS, Windows), it has become an indispensable tool in media workflows.\\n\\nIn general, we can accomplish common conversion tasks with simple commands, such as converting a MOV file to MP4:\\n\\n```bash\\nffmpeg -i input.mov -c copy output.mp4\\n```\\n\\nHere, `-i` specifies the input file path, and `-c copy` means copying the video and audio streams directly (without re-encoding), which significantly reduces processing time and maintains the original quality. If you want to adjust quality, encoding parameters, output resolution, bitrate, or channels, ffmpeg provides highly flexible command-line parameters for customization.\\n\\nIn short, it\'s an excellent tool that you should learn to use!\\n\\n## Environment Setup\\n\\nWe are developing on an Ubuntu system, and similar Linux systems can be used as well.\\n\\n1. **Python Environment**: Ensure that Python 3.x is installed:\\n\\n   ```bash\\n   python3 --version\\n   ```\\n\\n2. **Install ffmpeg**: On Ubuntu, you can install ffmpeg with the following commands:\\n\\n   ```bash\\n   sudo apt update\\n   sudo apt install ffmpeg\\n   ```\\n\\n   After installation, check the version:\\n\\n   ```bash\\n   ffmpeg -version\\n   ```\\n\\n3. **Code Structure**: Create a `convert.py` file in your project folder (you can name it differently), and paste the following code into it.\\n\\n## Example Code\\n\\n```python\\nimport subprocess\\nimport sys\\nfrom pathlib import Path\\n\\ndef convert_videos(input_dir: Path, src_format: str, dest_format: str):\\n    # Check if the target directory exists\\n    if not input_dir.is_dir():\\n        print(f\\"Error: The target directory \'{input_dir}\' does not exist.\\")\\n        sys.exit(1)\\n\\n    # Automatically create an output directory\\n    output_dir = input_dir / \\"output\\"\\n    output_dir.mkdir(parents=True, exist_ok=True)\\n\\n    # Ensure format strings start with a period\\n    if not src_format.startswith(\\".\\"):\\n        src_format = f\\".{src_format}\\"\\n    if not dest_format.startswith(\\".\\"):\\n        dest_format = f\\".{dest_format}\\"\\n\\n    # Iterate over all files with the source format (case insensitive)\\n    video_files = [f for f in input_dir.rglob(\\"*\\") if f.suffix.casefold() == src_format.casefold()]\\n\\n    if not video_files:\\n        print(f\\"No {src_format} files found.\\")\\n        sys.exit(0)\\n\\n    for file in video_files:\\n        output_file = output_dir / f\\"{file.stem}{dest_format}\\"\\n        print(f\\"Converting: \'{file}\' -> \'{output_file}\'\\")\\n\\n        # Use ffmpeg to convert the file\\n        try:\\n            subprocess.run(\\n                [\\n                    \\"ffmpeg\\", \\"-i\\", str(file),\\n                    \\"-c\\", \\"copy\\",\\n                    str(output_file)\\n                ],\\n                check=True,\\n                stdout=subprocess.PIPE,\\n                stderr=subprocess.PIPE\\n            )\\n            print(f\\"Conversion successful: \'{output_file}\'\\")\\n        except subprocess.CalledProcessError as e:\\n            print(f\\"Conversion failed: \'{file}\'\\")\\n            print(e.stderr.decode())\\n\\n    print(f\\"All files processed. Output directory: \'{output_dir}\'\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    # Check if the user has provided parameters\\n    if len(sys.argv) != 4:\\n        print(f\\"Usage: python {sys.argv[0]} <input directory> <source format> <destination format>\\")\\n        print(f\\"Example: python {sys.argv[0]} \'videos\' \'MOV\' \'mp4\'\\")\\n        sys.exit(1)\\n\\n    input_dir = Path(sys.argv[1]).resolve()\\n    src_format = sys.argv[2]\\n    dest_format = sys.argv[3]\\n\\n    convert_videos(input_dir, src_format, dest_format)\\n```\\n\\n## How to Use\\n\\n1. **Prepare Source Files**: Place the files to be converted (e.g., MOV, AVI, MKV) into the specified directory (e.g., `videos`).\\n\\n2. **Run the Conversion**: Navigate to the directory containing the script and run the following command:\\n\\n   ```bash\\n   python3 convert.py videos MOV mp4\\n   ```\\n\\n   If you want to convert AVI files to MKV, use:\\n\\n   ```bash\\n   python3 convert.py videos avi mkv\\n   ```\\n\\n   After running the command, the program will generate the converted files in the `videos/output` folder.\\n\\n3. **Check the Results**: Ensure that the `output` folder contains the correctly converted files in the desired format.\\n\\n## Advanced Use\\n\\nIf you want to compress and adjust the quality of the files, you can add specific parameters to the ffmpeg command, such as:\\n\\n```bash\\nffmpeg -i input.avi -c:v libx264 -crf 20 output.mp4\\n```\\n\\nYou can modify the script to adjust how ffmpeg is called for this purpose.\\n\\n## Conclusion\\n\\nThat\'s it! We wrote a simple script during development, and I hope it\'s helpful to you.\\n\\nYou can now start converting your files!"},{"id":"system-status-checking-by-chatgpt","metadata":{"permalink":"/en/blog/system-status-checking-by-chatgpt","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/12-12-system-status-checking-by-chatgpt/index.md","title":"Automating Ubuntu System Status Checks with ChatGPT","description":"Automate system status checks with ChatGPT.","date":"2024-12-12T00:00:00.000Z","tags":[{"inline":true,"label":"System-Monitoring","permalink":"/en/blog/tags/system-monitoring"},{"inline":true,"label":"Automation","permalink":"/en/blog/tags/automation"},{"inline":true,"label":"OpenAI-Integration","permalink":"/en/blog/tags/open-ai-integration"}],"readingTime":12.82,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"system-status-checking-by-chatgpt","title":"Automating Ubuntu System Status Checks with ChatGPT","authors":"Z. Yuan","image":"/en/img/2024/1212.webp","tags":["System-Monitoring","Automation","OpenAI-Integration"],"description":"Automate system status checks with ChatGPT."},"unlisted":false,"prevItem":{"title":"Batch Video Conversion","permalink":"/en/blog/flexible-video-conversion-by-python"},"nextItem":{"title":"Add Author Info to Docusaurus Docs","permalink":"/en/blog/customized-docusaurus-author-to-plugin-content-docs"}},"content":"To analyze issues, we need to check the system\'s status, but we may not be proficient in reading system logs.\\n\\nIn such cases, we can use ChatGPT to automate basic system status checks.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Key Features\\n\\nFirst, we collect system status and performance metrics using built-in Linux commands such as `uptime`, `top`, `free`, `df`, `systemctl`, `sensors`, `smartctl`, `dmesg`, `journalctl`, `ss`, `ip`, `docker`, etc.\\n\\nNext, we divide the raw report into several sections, and use the GPT model (specified as `gpt-4o`) backed by the OpenAI API to automatically generate summaries and compile the information.\\n\\nFinally, we combine multiple summaries into one, generating a system status report in Markdown format, which includes:\\n\\n- System status description\\n- Problem analysis\\n- Improvement and recommendation proposals\\n\\n:::warning\\nThe code in this article may require adjustments as the OpenAI API is updated. Please ensure that the API usage aligns with the code.\\n:::\\n\\n## Prerequisites\\n\\n1. **Required Packages and Commands**:\\n\\n   - `curl`: Used for HTTP communication with the API\\n   - `jq`: Used to parse JSON formatted API responses\\n   - `smartctl`: Used to check disk SMART information\\n   - `systemctl`: Used to check service status and failures\\n   - `docker`: Used to check Docker container status (ignore errors if Docker is not in use)\\n   - `sensors`: Used to retrieve system temperature information (if unavailable, the program will display a Warning)\\n\\n   Ensure that these commands are correctly installed and can run properly on the system.\\n\\n2. **OpenAI API Key**:\\n\\n   This program will use the OpenAI API to generate system report summaries and final analysis.\\n\\n   Ensure you have a valid OpenAI API key and store it in the specified path:\\n\\n   - Default path: `/home/your_user_name/.openai_api_key`\\n\\n   ```bash\\n   echo \\"YOUR_OPENAI_API_KEY\\" > /home/your_user_name/.openai_api_key\\n   chmod 600 /home/your_user_name/.openai_api_key\\n   ```\\n\\n   Make sure to replace `YOUR_OPENAI_API_KEY` with your actual API Key.\\n\\n3. **Working Directory and Disk Space**:\\n\\n   The default working directory is `/var/log/system_checking_by_chatgpt`. The program will create the following files and folders here:\\n\\n   - `raw_report_YYYY-MM-DD.txt`: Raw data collection report\\n   - `chunks_YYYY-MM-DD/`: Files after being chunked\\n   - `summary_YYYY-MM-DD/`: Files containing GPT summaries of each chunk\\n   - `combined_summary_YYYY-MM-DD.txt`: Final summary file combining all chunk summaries\\n   - `final_report_YYYY-MM-DD.md`: Final analysis report in Markdown format\\n   - `debug_YYYY-MM-DD.log`: Debug log file\\n\\n   Ensure there is enough space in this directory (at least 100MB). The program will exit automatically if space is insufficient.\\n\\n4. **Execution Permissions**:\\n\\n   Ensure the program has executable permissions:\\n\\n   ```bash\\n   chmod +x system_checking_by_chatgpt.sh\\n   ```\\n\\n## Execution Method\\n\\nRun the program in the terminal to start generating the report.\\n\\nThe expected execution flow is as follows:\\n\\n1. **Initialization and Checks**: The program will check for the presence of all necessary external commands and confirm that the `OPENAI_API_KEY` is not empty. If there are any issues, it will immediately interrupt and display an error.\\n2. **System Information Collection**: The program will collect system information using various commands and consolidate the output into `raw_report_YYYY-MM-DD.txt`.\\n3. **Chunking and Summarization**: The program will divide `raw_report_YYYY-MM-DD.txt` into several subfiles of approximately 200 lines (`chunk_*`), and each subfile will be processed by the OpenAI API to generate summary files (`summary_chunk_*`), highlighting the key points and issues from that chunk.\\n4. **Merging All Summaries and Final Analysis**: The program will merge all summaries into a single file `combined_summary_YYYY-MM-DD.txt`, and then call the OpenAI API to generate the \\"final report.\\"\\n5. **Generating Final Report**: The final report will be saved as `final_report_YYYY-MM-DD.md`, which will include:\\n   - System status description\\n   - Problem and anomaly analysis\\n   - Suggested solutions and diagnostic recommendations (which may include tables and command examples)\\n\\n## Notes\\n\\n1. **API Key Security**: Please keep the `OPENAI_API_KEY` secure. It should not be exposed, and it is recommended to restrict file read permissions (e.g., set to 600).\\n2. **Smart Adjustment of the Collection Scope**: If you wish to expand or reduce the system information collected, you can add or remove commands in the `collect_system_info()` function.\\n3. **OpenAI API Model and Token Limits**: By default, the model `MODEL=\\"gpt-4o\\"` is used. Ensure this model is available within your API permissions. If the summaries or final report are too long and cause the API to fail or time out, consider adjusting the `CHUNK_SIZE` or reducing the output content.\\n4. **Timezone and Date Format**: The program uses `date +\\"%Y-%m-%d\\"` to format the date, which can be modified as needed.\\n\\n## Example Flow\\n\\nBelow is a complete example of the running process:\\n\\n1. **Verify Environment and API Key**:\\n\\n   ```bash\\n   echo \\"sk-abc123xxx...\\" > /home/your_user_name/.openai_api_key\\n   chmod 600 /home/your_user_name/.openai_api_key\\n   ```\\n\\n2. **Run the Program**:\\n\\n   ```bash\\n   sudo bash system_checking_by_chatgpt.sh\\n   ```\\n\\n   The program will display:\\n\\n   ```\\n   [INFO] Starting program...\\n   [INFO] API key successfully loaded.\\n   [INFO] Starting system information collection...\\n   [INFO] System information collection complete, saved at /var/log/system_checking_by_chatgpt/raw_report_2024-12-12.txt\\n   [INFO] Requesting summary for /var/log/system_checking_by_chatgpt/chunks_2024-12-12/chunk_aa...\\n   ...\\n   [INFO] Final report generated: /var/log/system_checking_by_chatgpt/final_report_2024-12-12.md\\n   [INFO] Program execution complete.\\n   ```\\n\\n3. **View the Report**:\\n\\n   ```bash\\n   less /var/log/system_checking_by_chatgpt/final_report_2024-12-12.md\\n   ```\\n\\n4. **Review the Debug Log (if needed)**:\\n   ```bash\\n   less /var/log/system_checking_by_chatgpt/debug_2024-12-12.log\\n   ```\\n\\n## Code\\n\\n```shell title=\\"system_checking_by_chatgpt.sh\\"\\n#!/usr/bin/env bash\\n\\nset -euo pipefail\\n# set -x  # Enable for debugging if needed\\n\\n########################################\\n# Basic Setup and Checks\\n########################################\\n\\nDATE=$(date +\\"%Y-%m-%d\\")\\nWORK_DIR=\\"/var/log/system_checking_by_chatgpt\\"\\nRAW_REPORT=\\"$WORK_DIR/raw_report_$DATE.txt\\"\\nCHUNKS_DIR=\\"$WORK_DIR/chunks_$DATE\\"\\nSUMMARY_DIR=\\"$WORK_DIR/summary_$DATE\\"\\nFINAL_REPORT=\\"$WORK_DIR/final_report_$DATE.md\\"  # Using Markdown format\\nDEBUG_LOG=\\"$WORK_DIR/debug_$DATE.log\\"\\n\\nmkdir -p \\"$WORK_DIR\\"\\nmkdir -p \\"$CHUNKS_DIR\\"\\nmkdir -p \\"$SUMMARY_DIR\\"\\n\\n# Check required commands\\nREQUIRED_COMMANDS=(\\"curl\\" \\"jq\\" \\"smartctl\\" \\"systemctl\\" \\"docker\\" \\"sensors\\")\\nfor cmd in \\"${REQUIRED_COMMANDS[@]}\\"; do\\n    if ! command -v \\"$cmd\\" &>/dev/null; then\\n        echo \\"[ERROR] Required command $cmd is not installed.\\" | tee -a \\"$DEBUG_LOG\\"\\n        exit 1\\n    fi\\ndone\\n\\n# Check available disk space\\nAVAILABLE_SPACE=$(df \\"$WORK_DIR\\" | tail -1 | awk \'{print $4}\')\\nif [ \\"$AVAILABLE_SPACE\\" -lt 102400 ]; then # less than 100MB\\n    echo \\"[ERROR] Insufficient disk space in $WORK_DIR.\\" | tee -a \\"$DEBUG_LOG\\"\\n    exit 1\\nfi\\n\\n# Logging function: output to stderr to avoid interfering with function returns\\nlog() {\\n    local LEVEL=$1\\n    shift\\n    local MESSAGE=\\"$@\\"\\n    {\\n        echo \\"[$LEVEL] $MESSAGE\\" | tee -a \\"$DEBUG_LOG\\" >&2\\n        logger -t \\"system_checking_by_chatgpt\\" \\"[$LEVEL] $MESSAGE\\"\\n    }\\n}\\n\\nlog \\"INFO\\" \\"Script execution started...\\"\\n\\n########################################\\n# Load API Key\\n########################################\\n\\nOPENAI_KEY_FILE=\\"/home/your_user_name/.openai_api_key\\"\\nif [ ! -f \\"$OPENAI_KEY_FILE\\" ]; then\\n    log \\"ERROR\\" \\"$OPENAI_KEY_FILE does not exist.\\"\\n    exit 1\\nfi\\n\\nOPENAI_API_KEY=$(cat \\"$OPENAI_KEY_FILE\\")\\nif [ -z \\"$OPENAI_API_KEY\\" ]; then\\n    log \\"ERROR\\" \\"OPENAI_API_KEY is empty.\\"\\n    exit 1\\nfi\\nexport OPENAI_API_KEY\\n\\nlog \\"INFO\\" \\"API key successfully loaded.\\"\\n\\nAPI_URL=\\"https://api.openai.com/v1/chat/completions\\"\\nMODEL=\\"gpt-4o\\"\\n\\n########################################\\n# Function Definitions\\n########################################\\n\\ncollect_system_info() {\\n    {\\n        echo \\"=== System Uptime ===\\"\\n        uptime\\n        echo \\"\\"\\n\\n        echo \\"=== Date & Time ===\\"\\n        date\\n        echo \\"\\"\\n\\n        echo \\"=== CPU & Memory Usage ===\\"\\n        top -b -n1 | head -n 20\\n        echo \\"\\"\\n\\n        echo \\"=== Memory Usage (free) ===\\"\\n        free -h\\n        echo \\"\\"\\n\\n        echo \\"=== Disk Usage (df) ===\\"\\n        df -h\\n        echo \\"\\"\\n\\n        echo \\"=== Failed Services ===\\"\\n        systemctl list-units --state=failed\\n        echo \\"\\"\\n\\n        echo \\"=== Temperature Sensors ===\\"\\n        sensors 2>/dev/null || echo \\"[WARNING] Could not retrieve temperature information.\\"\\n        echo \\"\\"\\n\\n        echo \\"=== NVMe / SMART Status ===\\"\\n        sudo smartctl -a /dev/nvme0n1 2>/dev/null || echo \\"[WARNING] Could not check /dev/nvme0n1\\"\\n        echo \\"\\"\\n        sudo smartctl -a /dev/nvme1n1 2>/dev/null || echo \\"[WARNING] Could not check /dev/nvme1n1\\"\\n        echo \\"\\"\\n\\n        echo \\"=== Recent dmesg Entries (Last 100 lines) ===\\"\\n        dmesg | tail -n 100\\n        echo \\"\\"\\n\\n        echo \\"=== System Journal (Last 300 lines, errors only) ===\\"\\n        journalctl -p err -n 300\\n        echo \\"\\"\\n\\n        echo \\"=== Network Status (ss -tulpn) ===\\"\\n        ss -tulpn 2>/dev/null || echo \\"[WARNING] Could not retrieve network status.\\"\\n        echo \\"\\"\\n\\n        echo \\"=== Network Interface Statistics (ip -s link) ===\\"\\n        ip -s link\\n        echo \\"\\"\\n\\n        echo \\"=== Docker Containers (if any) ===\\"\\n        docker ps -a 2>/dev/null || echo \\"[INFO] Docker is not running or not installed.\\"\\n        echo \\"\\"\\n\\n    } > \\"$RAW_REPORT\\"\\n}\\n\\ngenerate_chunk_summaries() {\\n    # Split RAW_REPORT into chunks\\n    CHUNK_SIZE=200\\n    split -l $CHUNK_SIZE \\"$RAW_REPORT\\" \\"$CHUNKS_DIR/chunk_\\"\\n\\n    CHUNKS=(\\"$CHUNKS_DIR\\"/chunk_*)\\n\\n    if [ ${#CHUNKS[@]} -eq 0 ]; then\\n        log \\"ERROR\\" \\"No chunks generated!\\"\\n        exit 1\\n    fi\\n\\n    # Prompt for chunk summaries\\n    CHUNK_SYSTEM_PROMPT=\\"You are a professional system administration consultant. The following is a portion of the system information. Please extract and summarize: (1) Important system status info (e.g., CPU/memory usage, disk usage, network status, service states, error messages), and (2) any abnormalities, errors, or issues requiring attention. Be as detailed as possible, while remaining clear and organized.\\"\\n\\n    summaries=()\\n\\n    for chunk_file in \\"${CHUNKS[@]}\\"; do\\n        CHUNK_CONTENT=$(cat \\"$chunk_file\\")\\n\\n        CHUNK_API_PAYLOAD=$(jq -n \\\\\\n            --arg system_prompt \\"$CHUNK_SYSTEM_PROMPT\\" \\\\\\n            --arg user_prompt \\"$CHUNK_CONTENT\\" \\\\\\n            --arg model \\"$MODEL\\" \\\\\\n            \'{\\n                \\"model\\": $model,\\n                \\"messages\\": [\\n                    {\\"role\\": \\"system\\", \\"content\\": $system_prompt},\\n                    {\\"role\\": \\"user\\", \\"content\\": $user_prompt}\\n                ]\\n            }\')\\n\\n        log \\"INFO\\" \\"Requesting summary for $chunk_file ...\\"\\n        START_TIME=$(date +%s)\\n        RESPONSE=$(curl -sS -X POST \\"$API_URL\\" \\\\\\n          -H \\"Authorization: Bearer $OPENAI_API_KEY\\" \\\\\\n          -H \\"Content-Type: application/json\\" \\\\\\n          -d \\"$CHUNK_API_PAYLOAD\\")\\n\\n        if [ $? -ne 0 ]; then\\n            log \\"ERROR\\" \\"Failed to communicate with ChatGPT API during summary phase: $chunk_file\\"\\n            exit 1\\n        fi\\n        END_TIME=$(date +%s)\\n        log \\"INFO\\" \\"API request took $((END_TIME - START_TIME)) seconds.\\"\\n\\n        SUMMARY=$(echo \\"$RESPONSE\\" | jq -r \'.choices[0].message.content\' 2>>\\"$DEBUG_LOG\\" || echo \\"\\")\\n\\n        if [ -z \\"$SUMMARY\\" ] || [ \\"$SUMMARY\\" = \\"null\\" ]; then\\n            log \\"ERROR\\" \\"No summary returned for chunk: $chunk_file\\"\\n            echo \\"Raw Response: $RESPONSE\\" >> \\"$DEBUG_LOG\\"\\n            exit 1\\n        fi\\n\\n        SUMMARY_FILE=\\"$SUMMARY_DIR/summary_$(basename \\"$chunk_file\\").txt\\"\\n        echo \\"$SUMMARY\\" > \\"$SUMMARY_FILE\\"\\n        summaries+=(\\"$SUMMARY_FILE\\")\\n    done\\n\\n    # Print summary file paths to stdout only\\n    for s in \\"${summaries[@]}\\"; do\\n        echo \\"$s\\"\\n    done\\n}\\n\\ncombine_summaries() {\\n    local summaries=(\\"$@\\")\\n    COMBINED_SUMMARY=\\"$WORK_DIR/combined_summary_$DATE.txt\\"\\n    rm -f \\"$COMBINED_SUMMARY\\"\\n    touch \\"$COMBINED_SUMMARY\\"\\n\\n    log \\"INFO\\" \\"Combining all chunk summaries...\\"\\n    {\\n        echo \\"Below are the combined summaries from multiple chunks:\\"\\n        echo \\"------------------------------------\\"\\n        for sfile in \\"${summaries[@]}\\"; do\\n            echo \\"=== Chunk Summary ===\\"\\n            cat \\"$sfile\\"\\n            echo \\"\\"\\n        done\\n    } > \\"$COMBINED_SUMMARY\\"\\n\\n    echo \\"$COMBINED_SUMMARY\\"\\n}\\n\\ngenerate_final_report() {\\n    local COMBINED_SUMMARY=\\"$1\\"\\n\\n    # Use a HEREDOC for the final prompt to avoid quote issues\\n    FINAL_SYSTEM_PROMPT=$(cat <<EOF\\nYou are a professional system administration consultant. Below is a combined set of summaries from multiple chunks of system information. Please produce a **very detailed final report** in **Markdown format** that includes the following three main sections with clear markdown headings:\\n\\n**1. Current System Status**:\\nDescribe the current resource usage (CPU, memory, disk), service states, network status, critical logs, error messages, and any abnormal events.\\n\\n**2. System Status Analysis**:\\nAnalyze the above information, highlight potential issues, explain possible causes for anomalies, identify performance bottlenecks, unusual conditions, or risks.\\n\\n**3. Recommended Solutions**:\\nBased on your analysis, provide feasible and specific improvement suggestions, diagnostic steps, performance optimizations, error remediation steps, and advice to enhance system stability.\\n\\nMaintain clarity, logical structure, and abundant detail.\\nEOF\\n)\\n\\n    FINAL_INPUT=$(cat \\"$COMBINED_SUMMARY\\")\\n\\n    FINAL_API_PAYLOAD=$(jq -n \\\\\\n        --arg system_prompt \\"$FINAL_SYSTEM_PROMPT\\" \\\\\\n        --arg user_prompt \\"$FINAL_INPUT\\" \\\\\\n        --arg model \\"$MODEL\\" \\\\\\n        \'{\\n            \\"model\\": $model,\\n            \\"messages\\": [\\n                {\\"role\\": \\"system\\", \\"content\\": $system_prompt},\\n                {\\"role\\": \\"user\\", \\"content\\": $user_prompt}\\n            ]\\n        }\')\\n\\n    log \\"INFO\\" \\"Starting final analysis...\\"\\n    log \\"DEBUG\\" \\"Final analysis payload: $FINAL_API_PAYLOAD\\"\\n\\n    START_TIME=$(date +%s)\\n    RESPONSE=$(curl -sS -X POST \\"$API_URL\\" \\\\\\n      -H \\"Authorization: Bearer $OPENAI_API_KEY\\" \\\\\\n      -H \\"Content-Type: application/json\\" \\\\\\n      -d \\"$FINAL_API_PAYLOAD\\")\\n\\n    if [ $? -ne 0 ]; then\\n        log \\"ERROR\\" \\"Failed to communicate with ChatGPT API during final analysis.\\"\\n        exit 1\\n    fi\\n    END_TIME=$(date +%s)\\n    log \\"INFO\\" \\"API request took $((END_TIME - START_TIME)) seconds.\\"\\n\\n    log \\"DEBUG\\" \\"Final analysis response: $RESPONSE\\"\\n\\n    FINAL_ANALYSIS=$(echo \\"$RESPONSE\\" | jq -r \'.choices[0].message.content\' 2>>\\"$DEBUG_LOG\\" || echo \\"\\")\\n\\n    if [ -z \\"$FINAL_ANALYSIS\\" ] || [ \\"$FINAL_ANALYSIS\\" = \\"null\\" ]; then\\n        log \\"ERROR\\" \\"Failed to retrieve the final analysis.\\"\\n        echo \\"Raw Response: $RESPONSE\\" >> \\"$DEBUG_LOG\\"\\n        exit 1\\n    fi\\n\\n    {\\n        echo \\"# Daily System Check Report - $DATE\\"\\n        echo \\"\\"\\n        echo \\"$FINAL_ANALYSIS\\"\\n    } > \\"$FINAL_REPORT\\"\\n\\n    log \\"INFO\\" \\"Final report generated: $FINAL_REPORT\\"\\n}\\n\\n########################################\\n# Main Flow\\n########################################\\n\\nlog \\"INFO\\" \\"Collecting system information...\\"\\ncollect_system_info\\nlog \\"INFO\\" \\"System information collected and saved in $RAW_REPORT\\"\\n\\nmapfile -t summaries < <(generate_chunk_summaries)\\nCOMBINED_SUMMARY_FILE=$(combine_summaries \\"${summaries[@]}\\")\\ngenerate_final_report \\"$COMBINED_SUMMARY_FILE\\"\\n\\nlog \\"INFO\\" \\"Script execution completed.\\"\\n```\\n\\n## Report Example\\n\\n```markdown\\n# Daily System Check Report - 2024-12-12\\n\\n# Detailed System Status Report\\n\\n## 1. Current System Status\\n\\n### Resource Usage\\n\\n- **Uptime and Load Average**: The system has been operating for 3 hours and 44 minutes, with a high load average of 7.06, 7.05, 7.09, which may indicate CPU strain if there are fewer cores than these values suggest.\\n- **CPU Usage**: The overall CPU utilization is 15.5% for user processes, 7.0% for system processes, and 77.5% idle. However, there are four processes utilizing over 100% CPU each, indicating potential multi-threaded applications engaging multiple cores.\\n- **Memory Usage**: The system has a total memory of 125 GiB, with 10 GiB free and 48 GiB actively used. The buffer/cache accounts for 76 GiB, indicating adequate available memory. Swap usage is negligible at 0 out of 2 GiB, suggesting effective memory management.\\n- **Disk Usage**: The root partition is 70% full with 1.2 TB used of 1.8 TB. The /data partition is at 51% usage with 881 GB used.\\n\\n### Thermal and Smart Monitoring\\n\\n- **CPU Temperatures**: Multiple CPU cores (e.g., Core 20, Package id 0, Core 28, Core 12) are running critically hot at 96\xb0C, 85\xb0C, and 84\xb0C, indicating potential overheating issues.\\n- **NVMe Health**: Despite passing the health assessment, 85 \\"Invalid Field in Command\\" errors exist in the error log. The NVMe operates at 53\xb0C, which is safe.\\n\\n### Service States\\n\\n- No failed services are reported besides consistent issues with `snap.firmware-updater.firmware-notifier.service` and a `fwupd-refresh.service` failure. Multiple failures are also noted for `NetworkManager-dispatcher.service`.\\n\\n### Network Status\\n\\n- **Network Interface**: `enp5s0` has multiple `NETDEV WATCHDOG` timeouts, indicating network communication issues. The system has firewall blocks affecting multicast traffic.\\n- **Listening Services**: Active services include Nginx (ports 80 and 443), Docker proxies (ports 8000 and 18080), and SSHD on a high port (20712).\\n- **Traffic**: Interfaces `veth321800f` and `vethc3fefc25` have normal TX/RX operations, while `veth814eefd` shows no RX data, suggesting possible configuration issues.\\n\\n### Critical Logs and Errors\\n\\n- **Out of Memory Events**: Several OOM events resulted in killing critical processes which may impact system stability.\\n- **Other Errors**: An SSH error involving a \\"Connection reset by peer\\" and 85 occurrences of \\"Invalid Field in Command\\" in SMART logs.\\n\\n## 2. System Status Analysis\\n\\n### Potential Issues\\n\\n1. **High CPU Load**: The system load average is high, suggesting significant processing activities, possibly nearing core limits.\\n2. **Overheating**: CPU temperatures exceeding high thresholds pose a risk of thermal throttling or hardware damage.\\n3. **Disk Errors**: SMART errors could indicate misconfigurations or impending hardware issues needing investigation.\\n4. **Network Interruptions**: The recurring `NETDEV WATCHDOG` timeouts and blocked multicast traffic can degrade network performance.\\n5. **Memory Constraints**: Consistent OOM events denote memory overutilization or leaks, necessitating review of processes like `pipewire`, `xdg-permission-`, and more.\\n\\n### Unusual Conditions or Risks\\n\\n- **Failed Service Starts**: Persistent service failures around the firmware updater and NetworkManager services reveal potential system configuration or dependency issues.\\n- **Network Driver**: `r8169` driver might have compatibility problems with hardware leading to frequent timeouts and errors.\\n- **Security Alerts**: AppArmor and firewall logs indicate possible misconfigurations or unneeded traffic blocks that might impact network performance.\\n\\n## 3. Recommended Solutions\\n\\n### Immediate Actions:\\n\\n- **Address CPU Overheating**: Evaluate cooling solutions, including checking fans, conducting thermal paste reapplications, or employing better case ventilation.\\n- **Investigate NVMe Errors**: Review firmware compatibility and update NVMe driver/firmware. Verify command syntaxes used by SMART tools.\\n- **Resolve OOM Events**: Analyze high-memory processes for inefficiencies or leaks, scale up physical memory, or fine-tune swapping parameters.\\n\\n### Network Solutions:\\n\\n- **Optimize NIC Configuration**: Update or reinstall the `r8169` driver, inspect physical network connectivity, and monitor for more accurate diagnostics on `enp5s0`.\\n- **Adjust Firewall Rules**: Review and possibly amend UFW rules to correctly handle multicast traffic without unnecessary blocks.\\n\\n### Enhancing System Stability:\\n\\n- **Review Service and Kernel Configurations**: Correct `snap.firmware-updater.firmware-notifier.service` and `NetworkManager-dispatcher.service` by checking service dependencies and configuration errors.\\n- **Implement Effective Monitoring**: Employ an extensive monitoring setup for regular checks on temperature, network performance, and memory.\\n- **System and Network Upgrades**: Should persistent high load and overheating be non-resolvable through tuning, consider hardware scaling, particularly for CPU and memory enhancements.\\n```"},{"id":"customized-docusaurus-author-to-plugin-content-docs","metadata":{"permalink":"/en/blog/customized-docusaurus-author-to-plugin-content-docs","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/12-10-add-author-to-plugin-content-docs/index.md","title":"Add Author Info to Docusaurus Docs","description":"The official version doesn\u2019t support it, so we had to do it ourselves.","date":"2024-12-10T00:00:00.000Z","tags":[{"inline":true,"label":"Docusaurus","permalink":"/en/blog/tags/docusaurus"},{"inline":true,"label":"Author","permalink":"/en/blog/tags/author"}],"readingTime":6.99,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"customized-docusaurus-author-to-plugin-content-docs","title":"Add Author Info to Docusaurus Docs","authors":"Z. Yuan","image":"/en/img/2024/1210.webp","tags":["Docusaurus","Author"],"description":"The official version doesn\u2019t support it, so we had to do it ourselves."},"unlisted":false,"prevItem":{"title":"Automating Ubuntu System Status Checks with ChatGPT","permalink":"/en/blog/system-status-checking-by-chatgpt"},"nextItem":{"title":"A Brief Introduction to Graph Convolutional Networks","permalink":"/en/blog/graph-convolutional-networks"}},"content":"If you\'re also using Docusaurus to write a website, then you must be aware that Docusaurus has two main content types:\\n\\n- The plugin used for blogs is: `@docusaurus/plugin-content-blog`\\n- The part for technical documentation is: `@docusaurus/plugin-content-docs`\\n\\nAmong them, only `blog` has author info functionality, while `docs` does not.\\n\\nOh no, what a shock!\\n\\n\x3c!-- truncate --\x3e\\n\\n## First, Ask the Official Team\\n\\nWe first went to the Docusaurus GitHub to ask about this feature and see if the official team supports it.\\n\\n- [**How to add author info to docs? #10701**](https://github.com/facebook/docusaurus/discussions/10701)\\n\\n    <div align=\\"center\\">\\n    <figure style={{\\"width\\": \\"90%\\"}}>\\n    ![ask-docusaurus](./img/img1.jpg)\\n    </figure>\\n    </div>\\n\\nMaybe the Docusaurus authors would show some mercy and help us add this feature.\\n\\nBut after waiting for a while, we got a reply from the official team:\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"90%\\"}}>\\n![docusaurus-reply](./img/img2.jpg)\\n</figure>\\n</div>\\n\\nIn short, they tell you to figure it out yourself; the official team doesn\u2019t support it.\\n\\nIt looks like it\'s better to rely on ourselves, so we had no choice but to forge ahead.\\n\\n## Add Author Info\\n\\nIn the original design, the author info is placed in the `blog/authors.yml` file, and the content looks something like this:\\n\\n```yml\\nZ. Yuan:\\n  name: Z. Yuan\\n  title: Dosaid maintainer, Full-Stack AI Engineer\\n  url: https://github.com/zephyr-sh\\n  image_url: https://github.com/zephyr-sh.png\\n  socials:\\n    github: \\"zephyr-sh\\"\\n```\\n\\nWe first create a new file `blog/authors.json` in the same path and rewrite the same content into JSON format:\\n\\n```json\\n{\\n  \\"Z. Yuan\\": {\\n    \\"name\\": \\"Z. Yuan\\",\\n    \\"title\\": \\"Dosaid maintainer, Full-Stack AI Engineer\\",\\n    \\"url\\": \\"https://github.com/zephyr-sh\\",\\n    \\"image_url\\": \\"https://github.com/zephyr-sh.png\\",\\n    \\"socials\\": {\\n      \\"github\\": \\"zephyr-sh\\"\\n    }\\n  }\\n}\\n```\\n\\n:::info\\nDuring development, we found that parsing YML files was cumbersome, and after some testing, we decided that using JSON format is the easiest.\\n:::\\n\\n:::tip\\nAlthough this file is meant for `docs`, we still place it in the `blog` folder so that we remember to update it when making changes.\\n:::\\n\\n## Extract DocItem/Content\\n\\n:::warning\\nFrom this step onward, we need to modify the Docusaurus source code.\\n\\nIf there are future breaking updates to Docusaurus, this modification may cause the website to malfunction. Make sure you have the ability to maintain the website before proceeding.\\n:::\\n\\nFirst, we extract the `DocItemContent` code. Please run the following command:\\n\\n```shell\\nnpx docusaurus swizzle @docusaurus/theme-classic DocItem/Content\\n```\\n\\nAfter running the command, you will encounter a few questions:\\n\\n1. **Which language do you want to use?**\\n\\n   We select `JavaScript`.\\n\\n2. **Which swizzle action do you want to do?**\\n\\n   We select `Eject`.\\n\\n3. **Do you really want to swizzle this unsafe internal component?**\\n\\n   We select `YES: I know what I am doing!`.\\n\\n---\\n\\nNow, you can find a path: `src/theme/DocItem/Content`, where there is an `index.js` file. This is where we need to make changes.\\n\\n- The original code for this program is here: [**docusaurus-theme-classic/src/theme/DocItem/Content**](https://github.com/facebook/docusaurus/blob/main/packages/docusaurus-theme-classic/src/theme/DocItem/Content)\\n\\nThe modified code is as follows:\\n\\n```jsx\\nimport { useDoc } from \\"@docusaurus/plugin-content-docs/client\\";\\nimport { ThemeClassNames } from \\"@docusaurus/theme-common\\";\\nimport DocItemAuthors from \\"@theme/DocItem/Authors\\";\\nimport Heading from \\"@theme/Heading\\";\\nimport MDXContent from \\"@theme/MDXContent\\";\\nimport clsx from \\"clsx\\";\\nimport React from \\"react\\";\\n\\nfunction useSyntheticTitle() {\\n  const { metadata, frontMatter, contentTitle } = useDoc();\\n  const shouldRender =\\n    !frontMatter.hide_title && typeof contentTitle === \\"undefined\\";\\n  if (!shouldRender) {\\n    return null;\\n  }\\n  return metadata.title;\\n}\\n\\nexport default function DocItemContent({ children }) {\\n  const syntheticTitle = useSyntheticTitle();\\n\\n  return (\\n    <div className={clsx(ThemeClassNames.docs.docMarkdown, \\"markdown\\")}>\\n      {syntheticTitle ? (\\n        <header>\\n          <Heading as=\\"h1\\">{syntheticTitle}</Heading>\\n          <DocItemAuthors />\\n          <MDXContent>{children}</MDXContent>\\n        </header>\\n      ) : (\\n        <>\\n          <DocItemAuthors />\\n          <MDXContent>{children}</MDXContent>\\n        </>\\n      )}\\n    </div>\\n  );\\n}\\n```\\n\\nThe key addition is a new module:\\n\\n- `import DocItemAuthors from \\"@theme/DocItem/Authors\\";`\\n\\nThis part will be implemented later.\\n\\n## Implement DocItem/Authors\\n\\nNow, let\'s implement the `Authors` component. Please run the following commands:\\n\\n```shell\\nmkdir -p src/theme/DocItem/Authors\\ntouch src/theme/DocItem/Authors/index.js\\ntouch src/theme/DocItem/Authors/styles.module.css\\n```\\n\\nFor this part, we refer to the `Authors` component in `Blog` and replicate it.\\n\\n- [**docusaurus-theme-classic/src/theme/Blog/Components/Author**](https://github.com/facebook/docusaurus/tree/main/packages/docusaurus-theme-classic/src/theme/Blog/Components/Author)\\n\\nThe code for `DocItem/Authors/index.js` is as follows:\\n\\n```jsx\\nimport { useDoc } from \\"@docusaurus/plugin-content-docs/client\\";\\nimport authorsData from \\"@site/blog/authors.json\\";\\nimport React from \\"react\\";\\nimport {\\n  FaEnvelope,\\n  FaGithub,\\n  FaLinkedin,\\n  FaRss,\\n  FaStackOverflow,\\n  FaTwitter,\\n} from \\"react-icons/fa\\";\\nimport styles from \\"./index.module.css\\";\\n\\nfunction normalizeSocialLink(platform, handleOrUrl) {\\n  const isAbsoluteUrl =\\n    handleOrUrl.startsWith(\\"http://\\") || handleOrUrl.startsWith(\\"https://\\");\\n  if (isAbsoluteUrl) {\\n    return handleOrUrl;\\n  }\\n  switch (platform) {\\n    case \\"x\\":\\n      return `https://x.com/${handleOrUrl}`;\\n    case \\"github\\":\\n      return `https://github.com/${handleOrUrl}`;\\n    case \\"linkedin\\":\\n      return `https://www.linkedin.com/in/${handleOrUrl}/`;\\n    case \\"stackoverflow\\":\\n      return `https://stackoverflow.com/users/${handleOrUrl}`;\\n    case \\"newsletter\\":\\n      return handleOrUrl;\\n    case \\"email\\":\\n      return `mailto:${handleOrUrl}`;\\n    default:\\n      return handleOrUrl;\\n  }\\n}\\n\\nconst socialIconMap = {\\n  x: FaTwitter,\\n  github: FaGithub,\\n  linkedin: FaLinkedin,\\n  stackoverflow: FaStackOverflow,\\n  email: FaEnvelope,\\n  newsletter: FaRss,\\n};\\n\\nexport default function DocItemAuthors() {\\n  const { frontMatter } = useDoc();\\n  let { authors } = frontMatter;\\n\\n  if (!authors) {\\n    return null;\\n  }\\n\\n  if (typeof authors === \\"string\\") {\\n    authors = [authors];\\n  }\\n\\n  const resolvedAuthors = authors\\n    .map((authorKeyOrObj) => {\\n      if (typeof authorKeyOrObj === \\"string\\") {\\n        const authorInfo = authorsData[authorKeyOrObj];\\n        if (!authorInfo) {\\n          console.warn(\\n            `No author data found for key \'${authorKeyOrObj}\' in authors.json`\\n          );\\n          return null;\\n        }\\n        return {\\n          name: authorInfo.name,\\n          title: authorInfo.title,\\n          url: authorInfo.url,\\n          imageURL: authorInfo.image_url,\\n          socials: authorInfo.socials,\\n          description: authorInfo.description,\\n        };\\n      } else {\\n        const { name, title, url, image_url, imageURL, socials, description } =\\n          authorKeyOrObj;\\n        return {\\n          name,\\n          title,\\n          url,\\n          imageURL: imageURL || image_url,\\n          socials,\\n          description,\\n        };\\n      }\\n    })\\n    .filter(Boolean);\\n\\n  if (resolvedAuthors.length === 0) {\\n    return null;\\n  }\\n\\n  return (\\n    <div className={`${styles.docAuthors} margin-bottom--md`}>\\n      {resolvedAuthors.map((author, index) => {\\n        const { name, title, url, imageURL, socials, description } = author;\\n        return (\\n          <div key={index} className={styles.docAuthor}>\\n            {imageURL && (\\n              <img src={imageURL} alt={name} className={styles.docAuthorImg} />\\n            )}\\n            <div>\\n              <div className={styles.docAuthorName}>\\n                {url ? (\\n                  <a href={url} target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n                    {name}\\n                  </a>\\n                ) : (\\n                  name\\n                )}\\n              </div>\\n              {title && <div className={styles.docAuthorTitle}>{title}</div>}\\n              {description && (\\n                <div className={styles.docAuthorDesc}>{description}</div>\\n              )}\\n\\n              {socials && (\\n                <div className={styles.docAuthorSocials}>\\n                  {Object.entries(socials).map(([platform, handleOrUrl]) => {\\n                    const SocialIcon = socialIconMap[platform] || FaEnvelope;\\n                    const normalizedUrl = normalizeSocialLink(\\n                      platform,\\n                      handleOrUrl\\n                    );\\n                    return (\\n                      <a\\n                        key={platform}\\n                        href={normalizedUrl}\\n                        target=\\"_blank\\"\\n                        rel=\\"noopener noreferrer\\"\\n                        className={styles.docAuthorSocialLink}\\n                      >\\n                        <SocialIcon size={20} />\\n                      </a>\\n                    );\\n                  })}\\n                </div>\\n              )}\\n            </div>\\n          </div>\\n        );\\n      })}\\n    </div>\\n  );\\n}\\n```\\n\\nHere, we\'re using `react-icons`. If you haven\'t installed it yet, please run the following command:\\n\\n```shell\\nyarn add react-icons\\n```\\n\\nPlease note that some parts are hardcoded here, for example:\\n\\n```jsx\\nfunction normalizeSocialLink(platform, handleOrUrl) {\\n  const isAbsoluteUrl =\\n    handleOrUrl.startsWith(\\"http://\\") || handleOrUrl.startsWith(\\"https://\\");\\n  if (isAbsoluteUrl) {\\n    return handleOrUrl;\\n  }\\n  switch (platform) {\\n    case \\"x\\":\\n      return `https://x.com/${handleOrUrl}`;\\n    case \\"github\\":\\n      return `https://github.com/${handleOrUrl}`;\\n    case \\"linkedin\\":\\n      return `https://www.linkedin.com/in/${handleOrUrl}/`;\\n    case \\"stackoverflow\\":\\n      return `https://stackoverflow.com/users/${handleOrUrl}`;\\n    case \\"newsletter\\":\\n      return handleOrUrl;\\n    case \\"email\\":\\n      return `mailto:${handleOrUrl}`;\\n    default:\\n      return handleOrUrl;\\n  }\\n}\\n\\nconst socialIconMap = {\\n  x: FaTwitter,\\n  github: FaGithub,\\n  linkedin: FaLinkedin,\\n  stackoverflow: FaStackOverflow,\\n  email: FaEnvelope,\\n  newsletter: FaRss,\\n};\\n```\\n\\nIf the URLs change, you may need to modify these areas.\\n\\nFinally, here is the code for `DocItem/Authors/styles.module.css`:\\n\\n```css\\n.docAuthor {\\n  display: flex;\\n  align-items: center;\\n  margin-bottom: 2rem;\\n}\\n\\n.docAuthorImg {\\n  width: 60px;\\n  height: 60px;\\n  border-radius: 50%;\\n  margin-right: 0.75rem;\\n  object-fit: cover;\\n}\\n\\n.docAuthorName {\\n  font-weight: 600;\\n  font-size: 1rem;\\n  margin-bottom: 0.25rem;\\n  color: #111;\\n}\\n\\n.docAuthorName a {\\n  text-decoration: none;\\n  color: inherit;\\n}\\n\\n.docAuthorName a:hover {\\n  text-decoration: underline;\\n}\\n\\n.docAuthorTitle {\\n  font-size: 0.85rem;\\n  color: #555;\\n  margin-bottom: 0.25rem;\\n  line-height: 1.2;\\n}\\n\\n.docAuthorDesc {\\n  font-size: 0.85rem;\\n  color: #333;\\n  margin-bottom: 0.4rem;\\n  line-height: 1.4;\\n}\\n\\n.docAuthorSocials {\\n  display: flex;\\n  gap: 0.5rem;\\n  flex-wrap: wrap;\\n  align-items: center;\\n}\\n\\n.docAuthorSocialLink {\\n  display: inline-flex;\\n  align-items: center;\\n  text-decoration: none;\\n  color: inherit;\\n  line-height: 1;\\n}\\n\\n.docAuthorSocialLink:hover {\\n  color: var(--ifm-color-primary);\\n}\\n```\\n\\nThe implementation approach here depends on personal style. After testing several times, this design looks quite good.\\n\\nYou can modify these styles according to your own needs.\\n\\n## Adjust the FrontMatter of the Document\\n\\nFinally, in order to display the author info in the `docs` files, we need to add the author\'s information to the `FrontMatter` of the document.\\n\\nFor example, in our website article: [**[20.08] HiPPO: Hippo\'s Memory**](https://docsaid.org/en/papers/mamba/hippo)\\n\\nThe original article looked like this:\\n\\n```mdx\\n# [20.08] HiPPO\\n\\n## Hippo\'s Memory\\n\\n[**HiPPO: Recurrent Memory with Optimal Polynomial Projections**](https://arxiv.org/abs/2008.07669)\\n```\\n\\nNow, in order to include author information, we **cannot** use `#` for the title; instead, we need to define the title using `FrontMatter`.\\n\\nSo we modify it to:\\n\\n```mdx\\n---\\ntitle: \\"[20.08] HiPPO\\"\\nauthors: Z. Yuan\\n---\\n\\n## Hippo\'s Memory\\n\\n[**HiPPO: Recurrent Memory with Optimal Polynomial Projections**](https://arxiv.org/abs/2008.07669)\\n```\\n\\nIn the `authors` field, specify the author\'s name so that the author information will be displayed.\\n\\n:::tip\\nMake sure that the name in `authors` matches the name in `authors.json`, otherwise, the author information will not be shown.\\n:::\\n\\n## Implementation Complete\\n\\nFinally, let\'s take a look at the effect on the website:\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"60%\\"}}>\\n![docusaurus-author](./img/img3.jpg)\\n</figure>\\n</div>\\n\\nAs shown in the image above, we have successfully added author information to the `docs` files. Cheers!\\n\\nThis concludes our implementation process. We hope it is helpful to you."},{"id":"graph-convolutional-networks","metadata":{"permalink":"/en/blog/graph-convolutional-networks","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/12-05-graph-convolutional-networks/index.md","title":"A Brief Introduction to Graph Convolutional Networks","description":"A simple introduction to GCN","date":"2024-12-05T00:00:00.000Z","tags":[{"inline":true,"label":"graph-convolutional-networks","permalink":"/en/blog/tags/graph-convolutional-networks"},{"inline":true,"label":"transformer","permalink":"/en/blog/tags/transformer"}],"readingTime":14.18,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"graph-convolutional-networks","title":"A Brief Introduction to Graph Convolutional Networks","authors":"Z. Yuan","image":"/en/img/2024/1205.webp","tags":["graph-convolutional-networks","transformer"],"description":"A simple introduction to GCN"},"unlisted":false,"prevItem":{"title":"Add Author Info to Docusaurus Docs","permalink":"/en/blog/customized-docusaurus-author-to-plugin-content-docs"},"nextItem":{"title":"A Brief Introduction to Fourier Transform","permalink":"/en/blog/fourier-transform"}},"content":"import GraphDemo from \'@site/src/components/GraphDemo\';\\nimport GraphFeatureMatrixDemo from \'@site/src/components/GraphFeatureMatrixDemo\';\\n\\nThis article was also inspired by my experience while writing paper notes and encountering graph convolutional networks.\\n\\nThe content became too lengthy, so I decided to write a separate article to introduce it.\\n\\n\x3c!-- truncate --\x3e\\n\\nThanks to OpenAI, everyone is now very familiar with Transformers. (Or at least, I hope so?)\\n\\nTerms like self-attention mechanism, multi-head attention, and positional encoding are now household names.\\n\\nSo here, I\u2019ll borrow the concept of Transformer to explain graph convolutional networks.\\n\\n## Graph Convolutional Networks\\n\\nGraph Convolutional Networks, abbreviated as GCN, is a model designed for deep learning on graph-structured data. Unlike traditional Convolutional Neural Networks (CNNs) which mainly deal with regular grid data (like images), GCN can process irregular graph-structured data. It is widely applied in fields such as social networks, knowledge graphs, and bioinformatics.\\n\\nA graph consists of two basic elements:\\n\\n1. **Node**: Represents entities in the data, such as people, items, or concepts.\\n2. **Edge**: Represents relationships between nodes, such as friendships or item similarities.\\n\\nEach \\"node\\" has its own feature vector. For example, for a person, we might use features like height, weight, age, gender, and interests to describe them. These feature vectors form the \\"Feature Matrix\\". The \\"edges\\" are represented by the \\"Adjacency Matrix\\".\\n\\nThe adjacency matrix is used to describe the connection relationships between nodes in a graph. For a graph with $n$ nodes, the adjacency matrix $A$ is an $n \\\\times n$ matrix where:\\n\\n- $A_{ij} = 1$ indicates that there is an edge between node $i$ and node $j$.\\n- $A_{ij} = 0$ indicates that there is no edge between node $i$ and node $j$.\\n\\n:::tip\\nPause for a moment, does this concept feel familiar?\\n\\n- **Isn\u2019t this very similar to the attention matrix in Transformer?**\\n\\nSo, let\'s draw a parallel:\\n\\n- **Node**: This corresponds to a token in Transformer.\\n- **Edge**: This corresponds to the relationship between tokens.\\n  :::\\n\\nWe\u2019ll come back to compare Transformer and GCN later, but let\u2019s continue exploring the basic concepts of GCN:\\n\\nThe properties and characteristics of the adjacency matrix are:\\n\\n- **Symmetry**: If the graph is undirected, the adjacency matrix is symmetric, i.e., $A_{ij} = A_{ji}$.\\n- **Self-connections**: Some graphs allow nodes to have self-connections, i.e., $A_{ii} = 1$, but in most cases, this is set to $A_{ii} = 0$.\\n\\n## Example\\n\\nLet\u2019s consider three people: Alice, Bob, and Carol, and their friendship relationships:\\n\\n- Alice and Bob are friends.\\n- Bob and Carol are friends.\\n- Alice and Carol are not directly friends.\\n\\nThis relationship can be represented by the following adjacency matrix:\\n\\n$$\\nA =\\n\\\\begin{bmatrix}\\n0 & 1 & 0 \\\\\\\\\\n1 & 0 & 1 \\\\\\\\\\n0 & 1 & 0\\n\\\\end{bmatrix}\\n$$\\n\\nIn this matrix:\\n\\n- $A_{12} = A_{21} = 1$: Indicates Alice and Bob are friends.\\n- $A_{23} = A_{32} = 1$: Indicates Bob and Carol are friends.\\n- All other elements are 0, meaning no direct friendship exists.\\n\\nThis adjacency matrix is \\"symmetric\\", so this graph is an \\"undirected graph\\".\\n\\nIf we visualize it, it would look like this:\\n\\n<GraphDemo />\\n\\n## Expanded Adjacency Matrix\\n\\nIn practical applications, the adjacency matrix can be further expanded to represent additional information, such as:\\n\\n1. **Weighted Adjacency Matrix**:\\n\\n   If the strength of relationships between friends varies, we can use weight values to represent this. For example, if the number of interactions between Alice and Bob is 3, and the number of interactions between Bob and Carol is 5, the weighted adjacency matrix can be represented as:\\n\\n   $$\\n   A =\\n   \\\\begin{bmatrix}\\n   0 & 3 & 0 \\\\\\\\\\n   3 & 0 & 5 \\\\\\\\\\n   0 & 5 & 0\\n   \\\\end{bmatrix}\\n   $$\\n\\n2. **Directed Adjacency Matrix**:\\n\\n   If the friendship relationships are directed (e.g., Alice contacts Bob first, but Bob does not contact Alice), the adjacency matrix becomes non-symmetric, like this:\\n\\n   $$\\n   A =\\n   \\\\begin{bmatrix}\\n   0 & 1 & 0 \\\\\\\\\\n   0 & 0 & 1 \\\\\\\\\\n   0 & 1 & 0\\n   \\\\end{bmatrix}\\n   $$\\n\\n   Here, $A_{12} = 1$ indicates that Alice contacted Bob first, while $A_{21} = 0$ shows that Bob did not contact Alice.\\n\\n## Feature Matrix\\n\\nIn addition to the adjacency matrix, each node in the graph can also contain a feature vector, which together form the **Feature Matrix** $X$.\\n\\nFor a graph with $n$ nodes where each node has $d$-dimensional features, the feature matrix $X$ is an $n \\\\times d$ matrix, where the $i$-th row represents the feature vector of node $i$.\\n\\nSuppose each person has two features: age and exercise habits (represented by 1 for yes and 0 for no), we can construct the following feature matrix:\\n\\n$$\\nX =\\n\\\\begin{bmatrix}\\n35 & 1 \\\\\\\\\\n50 & 0 \\\\\\\\\\n22 & 1\\n\\\\end{bmatrix}\\n$$\\n\\nThis matrix can be interpreted as:\\n\\n- The first row $[35, 1]$ represents Alice, who is 35 years old and has exercise habits.\\n- The second row $[50, 0]$ represents Bob, who is 50 years old and does not have exercise habits.\\n- The third row $[22, 1]$ represents Carol, who is 22 years old and has exercise habits.\\n\\nWe use the size of the circles to represent the nodes\' ages, and the color to indicate whether they have exercise habits:\\n\\n<GraphFeatureMatrixDemo />\\n\\n## Mathematics of GCN\\n\\nAfter understanding the adjacency matrix and feature matrix, we can dive into the mathematical principles of Graph Convolutional Networks (GCN).\\n\\nThe core idea of GCN is to perform information propagation and aggregation on graph structures through the \\"convolution operation,\\" in order to learn the representation (i.e., embedding vectors) of the nodes.\\n\\nIn traditional Convolutional Neural Networks (CNNs), the convolution operation mainly operates on the spatial structure of images, extracting features from local regions using convolutional filters. Similarly, GCN operates on the neighborhood structure of graphs, using information from neighboring nodes to update the feature representation of each node.\\n\\nEach layer of a GCN can be seen as a **message-passing mechanism**, which primarily consists of the following steps:\\n\\n1. **Message Aggregation**: Collect information from each node\u2019s neighboring nodes.\\n2. **Message Update**: Combine the aggregated information with the node\u2019s own features and update them via a nonlinear function.\\n\\nThe basic operation of GCN can be described by the following formula:\\n\\n$$\\nH^{(l+1)} = \\\\sigma\\\\left(\\\\hat{A} H^{(l)} W^{(l)}\\\\right)\\n$$\\n\\nWhere:\\n\\n- $H^{(l)}$ is the node feature matrix at the $l$-th layer. For the first layer, $H^{(0)} = X$, which is the input feature matrix.\\n- $W^{(l)}$ is the trainable weight matrix at the $l$-th layer.\\n- $\\\\sigma$ is the nonlinear activation function, such as ReLU.\\n- $\\\\hat{A}$ is the **normalized adjacency matrix**, used to stabilize training and account for the degree of nodes.\\n\\nUsing the original adjacency matrix $A$ directly for message passing could lead to excessively large or small eigenvalues, affecting the model\u2019s stability.\\n\\nThus, we normalize the adjacency matrix as follows:\\n\\n$$\\n\\\\hat{A} = \\\\tilde{D}^{-1/2} \\\\tilde{A} \\\\tilde{D}^{-1/2}\\n$$\\n\\nWhere:\\n\\n- $\\\\tilde{A} = A + I_n$, and $I_n$ is the $n \\\\times n$ identity matrix. This step is called **adding self-loops**, meaning each node is connected to itself.\\n- $\\\\tilde{D}$ is the degree matrix of $\\\\tilde{A}$, where the diagonal elements $\\\\tilde{D}_{ii} = \\\\sum_j \\\\tilde{A}_{ij}$.\\n\\nThis normalization ensures that during message passing, the degree of nodes is taken into account, preventing certain nodes with overly high or low degrees from disproportionately affecting the overall learning process.\\n\\nLet\u2019s continue with the example mentioned earlier, where the adjacency matrix $A$ and feature matrix $X$ are as follows:\\n\\n$$\\nA =\\n\\\\begin{bmatrix}\\n0 & 1 & 0 \\\\\\\\\\n1 & 0 & 1 \\\\\\\\\\n0 & 1 & 0\\n\\\\end{bmatrix},\\n\\\\quad\\nX =\\n\\\\begin{bmatrix}\\n35 & 1 \\\\\\\\\\n50 & 0 \\\\\\\\\\n22 & 1\\n\\\\end{bmatrix}\\n$$\\n\\n- **Step 1: Add Self-Loops**\\n\\n  First, we add self-loops to obtain $\\\\tilde{A}$:\\n\\n  $$\\n  \\\\tilde{A} = A + I_3 =\\n  \\\\begin{bmatrix}\\n  1 & 1 & 0 \\\\\\\\\\n  1 & 1 & 1 \\\\\\\\\\n  0 & 1 & 1\\n  \\\\end{bmatrix}\\n  $$\\n\\n- **Step 2: Calculate the Degree Matrix $\\\\tilde{D}$**\\n\\n  $$\\n  \\\\tilde{D} =\\n  \\\\begin{bmatrix}\\n  2 & 0 & 0 \\\\\\\\\\n  0 & 3 & 0 \\\\\\\\\\n  0 & 0 & 2\\n  \\\\end{bmatrix}\\n  $$\\n\\n- **Step 3: Compute the Normalized Adjacency Matrix $\\\\hat{A}$**\\n\\n  $$\\n  \\\\hat{A} = \\\\tilde{D}^{-1/2} \\\\tilde{A} \\\\tilde{D}^{-1/2} =\\n  \\\\begin{bmatrix}\\n  \\\\frac{1}{\\\\sqrt{2}} & 0 & 0 \\\\\\\\\\n  0 & \\\\frac{1}{\\\\sqrt{3}} & 0 \\\\\\\\\\n  0 & 0 & \\\\frac{1}{\\\\sqrt{2}}\\n  \\\\end{bmatrix}\\n  \\\\begin{bmatrix}\\n  1 & 1 & 0 \\\\\\\\\\n  1 & 1 & 1 \\\\\\\\\\n  0 & 1 & 1\\n  \\\\end{bmatrix}\\n  \\\\begin{bmatrix}\\n  \\\\frac{1}{\\\\sqrt{2}} & 0 & 0 \\\\\\\\\\n  0 & \\\\frac{1}{\\\\sqrt{3}} & 0 \\\\\\\\\\n  0 & 0 & \\\\frac{1}{\\\\sqrt{2}}\\n  \\\\end{bmatrix}\\n  $$\\n\\n  The result of this calculation is:\\n\\n  $$\\n  \\\\hat{A} \\\\approx\\n  \\\\begin{bmatrix}\\n  0.5 & \\\\frac{1}{\\\\sqrt{6}} & 0 \\\\\\\\\\n  \\\\frac{1}{\\\\sqrt{6}} & \\\\frac{1}{3} & \\\\frac{1}{\\\\sqrt{6}} \\\\\\\\\\n  0 & \\\\frac{1}{\\\\sqrt{6}} & 0.5\\n  \\\\end{bmatrix}\\n  $$\\n\\n- **Step 4: Apply the GCN Layer**\\n\\n  Suppose we have a GCN layer\u2019s weight matrix $W^{(0)}$:\\n\\n  $$\\n  W^{(0)} =\\n  \\\\begin{bmatrix}\\n  w_{11} & w_{12} \\\\\\\\\\n  w_{21} & w_{22}\\n  \\\\end{bmatrix}\\n  $$\\n\\n  Then, the feature matrix $H^{(1)}$ for the next layer is:\\n\\n  $$\\n  H^{(1)} = \\\\sigma\\\\left(\\\\hat{A} X W^{(0)}\\\\right)\\n  $$\\n\\n  The specific computation steps are as follows:\\n\\n  1.  **Matrix Multiplication**: First, compute $\\\\hat{A} X$:\\n\\n      $$\\n      \\\\hat{A} X \\\\approx\\n      \\\\begin{bmatrix}\\n      0.5 & \\\\frac{1}{\\\\sqrt{6}} & 0 \\\\\\\\\\n      \\\\frac{1}{\\\\sqrt{6}} & \\\\frac{1}{3} & \\\\frac{1}{\\\\sqrt{6}} \\\\\\\\\\n      0 & \\\\frac{1}{\\\\sqrt{6}} & 0.5\\n      \\\\end{bmatrix}\\n      \\\\begin{bmatrix}\\n      35 & 1 \\\\\\\\\\n      50 & 0 \\\\\\\\\\n      22 & 1\\n      \\\\end{bmatrix}\\n      =\\n      \\\\begin{bmatrix}\\n      0.5 \\\\times 35 + \\\\frac{1}{\\\\sqrt{6}} \\\\times 50 + 0 \\\\times 22 & 0.5 \\\\times 1 + \\\\frac{1}{\\\\sqrt{6}} \\\\times 0 + 0 \\\\times 1 \\\\\\\\\\n      \\\\frac{1}{\\\\sqrt{6}} \\\\times 35 + \\\\frac{1}{3} \\\\times 50 + \\\\frac{1}{\\\\sqrt{6}} \\\\times 22 & \\\\frac{1}{\\\\sqrt{6}} \\\\times 1 + \\\\frac{1}{3} \\\\times 0 + \\\\frac{1}{\\\\sqrt{6}} \\\\times 1 \\\\\\\\\\n      0 \\\\times 35 + \\\\frac{1}{\\\\sqrt{6}} \\\\times 50 + 0.5 \\\\times 22 & 0 \\\\times 1 + \\\\frac{1}{\\\\sqrt{6}} \\\\times 0 + 0.5 \\\\times 1\\n      \\\\end{bmatrix}\\n      $$\\n\\n  2.  **Apply Weight Matrix $W^{(0)}$**: Multiply the result by $W^{(0)}$.\\n\\n  3.  **Apply Nonlinear Function $\\\\sigma$**: Typically, the ReLU function is used, i.e., $\\\\sigma(x) = \\\\max(0, x)$.\\n\\nThrough these steps, GCN is able to combine each node\u2019s features with those of its neighbors, generating new feature representations.\\n\\nThis message-passing and aggregation mechanism enables GCN to capture both local and global information within graph structures, making it effective for a wide range of graph-related tasks such as node classification, graph classification, and link prediction.\\n\\nIn practice, multiple layers of GCN are often stacked to capture information from nodes at further distances. The output of each layer serves as the input for the next, allowing the model to progressively extract higher-level feature representations. For instance, a two-layer GCN enables each node\u2019s representation to include information from its two-hop neighbors, while a three-layer GCN can incorporate information from three-hop neighbors, and so on.\\n\\n## GCN vs Transformer\\n\\nSo, returning to the question we raised earlier:\\n\\nAlthough the design intentions and application scenarios of Graph Neural Networks (GCNs) and Transformers are different, they share many core concepts, such as nodes, edges, and message passing and aggregation based on feature matrices. So what is the relationship between them?\\n\\n- **Can we view GCN as a special case of Transformer?**\\n\\nLet\u2019s first review the core formulas of GCN and Transformer:\\n\\n- **Basic Update Formula of GCN**\\n\\n  $$\\n  H^{(l+1)} = \\\\sigma\\\\left(\\\\hat{A} H^{(l)} W^{(l)}\\\\right)\\n  $$\\n\\n  Where:\\n\\n  - $H^{(l)}$: Node feature matrix at layer $l$;\\n  - $\\\\hat{A} = D^{-\\\\frac{1}{2}} A D^{-\\\\frac{1}{2}}$: Weighted normalized adjacency matrix;\\n  - $W^{(l)}$: Learnable weight matrix at layer $l$;\\n  - $\\\\sigma$: Nonlinear activation function.\\n\\n  This formula aggregates local neighborhood information of nodes using the adjacency matrix $\\\\hat{A}$, capturing both the structural information of the graph and the node features.\\n\\n- **Self-Attention Formula of Transformer**\\n\\n  $$\\n  \\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^\\\\top}{\\\\sqrt{d_k}}\\\\right)V\\n  $$\\n\\n  Where:\\n\\n  - $Q, K, V$: Query, Key, and Value matrices;\\n  - $d_k$: The dimension of the key vector;\\n  - $\\\\text{softmax}$: Used for normalization, ensuring that the weights sum to 1.\\n\\n  This formula enables the learning of global dependencies, with message aggregation depending on the dynamic calculation of similarity between features, independent of a fixed graph structure.\\n\\n### Restricted Attention Mechanism\\n\\nIf we restrict the attention matrix of Transformer to the adjacency matrix of the graph (i.e., $\\\\hat{A}$) and further assume that the attention weights are fixed (determined by the static structure of the graph), then the self-attention formula of Transformer simplifies to:\\n\\n$$\\n\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^\\\\top}{\\\\sqrt{d_k}}\\\\right)V \\\\rightarrow \\\\text{softmax}(\\\\hat{A}) V\\n$$\\n\\nAssuming that the query, key, and value matrices $Q, K, V$ all come from the same node feature matrix $H^{(l)}$, and that the weight matrices $W_Q = W_K = W_V = I$ (identity matrix), the formula becomes:\\n\\n$$\\n\\\\text{Attention}(H^{(l)}, H^{(l)}, H^{(l)}) = \\\\text{softmax}\\\\left(\\\\frac{H^{(l)} H^{(l)\\\\top}}{\\\\sqrt{d_k}}\\\\right) H^{(l)}\\n$$\\n\\nIf we further assume that $\\\\text{softmax}\\\\left(\\\\frac{H^{(l)} H^{(l)\\\\top}}{\\\\sqrt{d_k}}\\\\right) = \\\\hat{A}$, i.e., the attention weights are fixed as the normalized adjacency matrix, then the self-attention formula simplifies to:\\n\\n$$\\n\\\\text{Attention}(H^{(l)}, H^{(l)}, H^{(l)}) = \\\\hat{A} H^{(l)}\\n$$\\n\\nAt this point, the update formula for Transformer becomes:\\n\\n$$\\nH^{(l+1)} = \\\\sigma\\\\left(\\\\hat{A} H^{(l)} W^{(l)}\\\\right)\\n$$\\n\\nWhen comparing the two, in Transformer, the self-attention mechanism allows each node to interact with every other node in the graph, forming global dependencies. Mathematically, this is reflected in the attention weight matrix $\\\\text{softmax}\\\\left(\\\\frac{QK^\\\\top}{\\\\sqrt{d_k}}\\\\right)$, which is a fully connected weight matrix, and the weights are dynamically calculated. In contrast, in GCN, message passing is limited to local neighborhoods, and the weight matrix $\\\\hat{A}$ is sparse and fixed.\\n\\n### Differences in Weight Learning\\n\\nIn Transformer, the attention weights are dynamically learned through the similarity between queries and keys, with the specific formula as follows:\\n\\n$$\\n\\\\alpha_{ij} = \\\\frac{\\\\exp\\\\left(\\\\frac{Q_i K_j^\\\\top}{\\\\sqrt{d_k}}\\\\right)}{\\\\sum_{k} \\\\exp\\\\left(\\\\frac{Q_i K_k^\\\\top}{\\\\sqrt{d_k}}\\\\right)}\\n$$\\n\\nWhere $\\\\alpha_{ij}$ represents the attention weight from node $i$ to node $j$. This dynamic calculation allows the model to adaptively adjust the weights based on different input data, capturing complex relationships.\\n\\nIn contrast, in GCN, the weights are determined by the graph\'s adjacency structure, with the formula:\\n\\n$$\\n\\\\hat{A}_{ij} = \\\\frac{A_{ij}}{\\\\sqrt{d_i d_j}}\\n$$\\n\\nWhere $A_{ij}$ is an element of the adjacency matrix, indicating whether node $i$ and node $j$ are connected, and $d_i$ and $d_j$ are the degrees of nodes $i$ and $j$, respectively.\\n\\nThis fixed weight calculation limits the expressive power of the model but also reduces computational complexity.\\n\\n### Information Propagation Range and Efficiency\\n\\nIn Transformer, information can propagate across the entire graph, which is mathematically reflected as a fully connected attention matrix. The computational complexity is $O(N^2)$, where $N$ is the number of nodes. This makes Transformer potentially face computational bottlenecks when dealing with large graphs.\\n\\nIn contrast, the message passing in GCN is limited to local neighborhoods, and the weight matrix $\\\\hat{A}$ is sparse, with computational complexity typically being $O(N \\\\cdot d)$, where $d$ is the average degree of nodes. This makes GCN more efficient for large-scale graph processing.\\n\\n---\\n\\nTherefore, from the perspective of formula simplification:\\n\\n1. **Attention matrix restricted to the graph\u2019s adjacency matrix $\\\\hat{A}$**:\\n   The original self-attention mechanism in Transformer allows message passing between any two nodes, while GCN limits it to local neighborhoods within the graph. This restriction means that the range of message passing is determined by the graph\'s structure, rather than being dynamically learned.\\n\\n2. **Attention weights are fixed**:\\n   In Transformer, self-attention weights are dynamically computed based on the similarity between queries and keys. If these weights are fixed as $\\\\hat{A}$, i.e., no longer relying on the similarity between node features but entirely dependent on the static structure of the graph, the weights become non-learnable and fixed.\\n\\nGiven these two conditions, we believe GCN can indeed be considered a special case of Transformer.\\n\\n:::tip\\nIt\u2019s important to note that this is just **our** understanding based on our learning process, and it represents a personal perspective.\\n\\nIf this interpretation is incorrect, feel free to correct us!\\n:::\\n\\n## Conclusion\\n\\nGraph Convolutional Networks (GCN) and Transformers have similarities and differences, each excelling in different areas.\\n\\nIn practical applications, they each have their advantages. For example, the self-attention mechanism in Transformer allows the model to dynamically adjust the weights of message passing based on data features, giving it an unparalleled advantage in handling complex and varied data patterns. On the other hand, while the fixed weights in GCN work excellently for some structured data, they are more limited in scenarios requiring dynamic relationships.\\n\\nFurthermore, GCN is more suitable for handling data with clear graph structures, such as social network analysis, knowledge graphs, and recommendation systems, efficiently utilizing topological information. Transformer, however, performs excellently in domains such as natural language processing and computer vision, where sequential or high-dimensional data is handled, capturing long-range dependencies and complex patterns.\\n\\nThe topic of GCN is vast, with various variants and applications, and we\u2019ve only introduced the basic concepts here.\\n\\nWe hope this article has helped you understand some of the fundamental principles and applications of GCN.\\n\\nInterested in learning more about the latest applications of GCN and Transformer? Let\u2019s take a look at the paper!\\n\\n- [**[24.07] Graph Transformers: A Survey**](https://arxiv.org/abs/2407.09777)\\n\\n   <div align=\\"center\\">\\n   <figure style={{\\"width\\": \\"80%\\"}}>\\n   ![Graph Transformers](./img/img1.jpg)\\n   </figure>\\n   </div>\\n\\n:::tip\\nThis paper contains over 200 references. If you read one a day, you\u2019ll have fun for the whole year! (~Just kidding~)\\n:::\\n\\n## References\\n\\n- [**[16.09] Semi-Supervised Classification with Graph Convolutional Networks**](https://arxiv.org/abs/1609.02907)\\n- [**[17.06] Attention is All You Need**](https://arxiv.org/abs/1706.03762)"},{"id":"fourier-transform","metadata":{"permalink":"/en/blog/fourier-transform","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/12-02-fourier-transform/index.md","title":"A Brief Introduction to Fourier Transform","description":"A simple introduction to the basic concepts of Fourier Transform.","date":"2024-12-02T00:00:00.000Z","tags":[{"inline":true,"label":"fourier-transform","permalink":"/en/blog/tags/fourier-transform"},{"inline":true,"label":"signal-processing","permalink":"/en/blog/tags/signal-processing"}],"readingTime":13.82,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"fourier-transform","title":"A Brief Introduction to Fourier Transform","authors":"Z. Yuan","image":"/en/img/2024/1202.webp","tags":["fourier-transform","signal-processing"],"description":"A simple introduction to the basic concepts of Fourier Transform."},"unlisted":false,"prevItem":{"title":"A Brief Introduction to Graph Convolutional Networks","permalink":"/en/blog/graph-convolutional-networks"},"nextItem":{"title":"Fixing pyenv Build Errors","permalink":"/en/blog/fixed-pyenv-install-error"}},"content":"import InteractiveSineWave from \'@site/src/components/InteractiveSineWave\';\\nimport WaveSuperposition from \'@site/src/components/WaveSuperposition\';\\nimport FourierTransformDemo from \'@site/src/components/FourierTransformDemo\';\\n\\nWhen writing my paper notes, I encountered the Fourier Transform and wanted to discuss it, but the topic became too long and started to overshadow the main content.\\n\\nSo I decided to separate this section and provide a simple introduction to the related concepts.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Trigonometric Functions\\n\\nWe all learned trigonometric functions when we were younger, and most of us are probably masters of trigonometry.\\n\\nTrigonometric functions have widespread applications in physics and engineering, particularly in describing waves and vibrations.\\n\\nA wave is essentially a way of transferring energy, and this transfer often appears in the form of \\"periodic oscillations.\\"\\n\\nPeriodic?\\n\\nA common example might be a pendulum, which swings back and forth in a periodic motion. When we observe its motion path, we can see that its displacement changes like a wave, moving from positive values to zero, then to negative values, and then repeating.\\n\\nSo we try to use trigonometric functions to describe the wave more precisely, mathematically represented as:\\n\\n$$\\ny(t) = A \\\\sin(2\\\\pi f t)\\n$$\\n\\nEach variable here is related to the pendulum\'s motion:\\n\\n- $y(t)$: The displacement of the pendulum at time $t$.\\n- $A$: The amplitude, representing the maximum distance the pendulum swings.\\n- $f$: The frequency, representing how many times the pendulum swings per second.\\n\\nWith the sine function, we can more accurately describe the characteristics of the wave.\\n\\nIf you look closely, you will notice that some waves do not start at zero but rather start oscillating after a certain time offset, which is represented by the \\"phase $\\\\phi$.\\"\\n\\nThe complete waveform formula can be written as:\\n\\n$$\\ny(t) = A \\\\sin(2\\\\pi f t + \\\\phi)\\n$$\\n\\nOr it can be expressed using the cosine function:\\n\\n$$\\ny(t) = A \\\\cos(2\\\\pi f t + \\\\phi)\\n$$\\n\\nThe difference between sine and cosine waves is in the phase shift, and both can be used to describe wave variations.\\n\\nAdditionally, to describe the rate of change of the wave more effectively, we can use the \\"angular velocity $\\\\omega$,\\" which represents the rate of phase change per second, with the formula:\\n\\n$$\\n\\\\omega = 2\\\\pi f\\n$$\\n\\nFor easier observation, I have created an interactive chart where you can adjust the amplitude, frequency, and phase to observe the changes in the sine wave.\\n\\n<InteractiveSineWave />\\n\\n### Superposition of Waves\\n\\nWith sine and cosine waves, it\'s like having the basic elements of the x-axis and y-axis; we can combine different waveforms to form more complex waves, such as:\\n\\n$$\\ny(t) = A \\\\cos(2\\\\pi f t) + B \\\\sin(2\\\\pi f t)\\n$$\\n\\nIn this equation, $A$ and $B$ are coefficients that determine the size of the cosine and sine components. By superimposing multiple waves, we can form complex waveforms.\\n\\nYou can use the following interactive chart to adjust the parameters of two sine waves and observe the waveform formed by their superposition.\\n\\n<WaveSuperposition />\\n\\n### Complex Form\\n\\nWhile the trigonometric representation of sine and cosine waves is very intuitive, using complex numbers to represent waveforms is more concise and powerful in mathematical and engineering applications.\\n\\nWhy?\\n\\nBecause complex numbers can unify the combination of sine and cosine into a single formula and make it easier to handle operations like superposition, differentiation, and integration.\\n\\nBefore understanding the complex form, let\'s quickly review the basic concept of complex numbers. A complex number consists of a real part and an imaginary part, written as:\\n\\n$$\\nz = a + bi\\n$$\\n\\nWhere:\\n\\n- $a$ is the real part.\\n- $b$ is the imaginary part.\\n- $i$ is the imaginary unit, satisfying $i^2 = -1$.\\n\\nComplex numbers can also be expressed in \\"polar form\\" as:\\n\\n$$\\nz = r(\\\\cos\\\\theta + i\\\\sin\\\\theta)\\n$$\\n\\nHere:\\n\\n- $r = \\\\sqrt{a^2 + b^2}$ is the modulus of the complex number, representing the distance from the origin to the point $(a, b)$.\\n- $\\\\theta = \\\\tan^{-1}(b/a)$ is the argument of the complex number, representing the angle of rotation from the positive real axis.\\n- $\\\\cos\\\\theta$ and $\\\\sin\\\\theta$ define the direction corresponding to the angle $\\\\theta$.\\n\\nIn Cartesian coordinates, the point $(a, b)$ can be converted to polar coordinates $(r, \\\\theta)$.\\n\\nSubstituting $r$ and $\\\\theta$ into the trigonometric expression, we get $z = r(\\\\cos\\\\theta + i\\\\sin\\\\theta)$.\\n\\n## Euler\'s Formula\\n\\nFrom a mathematical perspective, the exponential function $e^x$ can be expanded into a power series:\\n\\n$$\\ne^x = 1 + \\\\frac{x}{1!} + \\\\frac{x^2}{2!} + \\\\frac{x^3}{3!} + \\\\cdots\\n$$\\n\\nThis series is intuitive when $x$ is a real number, but it also applies when $x$ is a complex number.\\n\\nFor example, when $x = i\\\\theta$, we substitute into the formula to get:\\n\\n$$\\ne^{i\\\\theta} = 1 + \\\\frac{i\\\\theta}{1!} + \\\\frac{(i\\\\theta)^2}{2!} + \\\\frac{(i\\\\theta)^3}{3!} + \\\\cdots\\n$$\\n\\nExpanding $(i\\\\theta)^n$, and noting the periodicity of powers of $i$ ($i^2 = -1, i^3 = -i, i^4 = 1$), we can separate the real and imaginary parts of the expansion:\\n\\nThe real part is:\\n\\n$$\\n1 + \\\\frac{(i\\\\theta)^2}{2!} + \\\\frac{(i\\\\theta)^4}{4!} + \\\\cdots = 1 - \\\\frac{\\\\theta^2}{2!} + \\\\frac{\\\\theta^4}{4!} - \\\\cdots\\n$$\\n\\nThis is exactly the expansion of $\\\\cos\\\\theta$, and similarly, the imaginary part is:\\n\\n$$\\n\\\\frac{(i\\\\theta)}{1!} + \\\\frac{(i\\\\theta)^3}{3!} + \\\\frac{(i\\\\theta)^5}{5!} + \\\\cdots = i\\\\left(\\\\theta - \\\\frac{\\\\theta^3}{3!} + \\\\frac{\\\\theta^5}{5!} - \\\\cdots\\\\right)\\n$$\\n\\nThis is exactly the expansion of $\\\\sin\\\\theta$.\\n\\nTherefore, combining the real and imaginary parts, we obtain the famous Euler\'s formula:\\n\\n$$\\ne^{i\\\\theta} = \\\\cos\\\\theta + i\\\\sin\\\\theta\\n$$\\n\\nHere:\\n\\n- $e^{i\\\\theta}$ is the exponential representation of the complex number, containing the real part $\\\\cos\\\\theta$ and the imaginary part $i\\\\sin\\\\theta$.\\n- The real and imaginary parts of the complex number correspond to the horizontal and vertical movements of the waveform.\\n\\n## Complex Waves\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"40%\\"}}>\\n![ComplexWave](./img/Euler\'s_formula.png)\\n<figcaption>Geometric meaning of Euler\'s formula, source: [**Wikipedia**](https://en.wikipedia.org/wiki/Euler%27s_formula)</figcaption>\\n</figure>\\n</div>\\n\\n---\\n\\nWe can use Euler\'s formula to rewrite the waveform equation. For example, consider the following complex waveform:\\n\\n$$\\nz(t) = A e^{i(2\\\\pi f t + \\\\phi)}\\n$$\\n\\nExpanding $e^{i(2\\\\pi f t + \\\\phi)}$ and using Euler\'s formula, we get:\\n\\n$$\\nz(t) = A \\\\left[\\\\cos(2\\\\pi f t + \\\\phi) + i\\\\sin(2\\\\pi f t + \\\\phi)\\\\right]\\n$$\\n\\nThis represents the same waveform having both:\\n\\n- **Real part** $\\\\Re(z) = A \\\\cos(2\\\\pi f t + \\\\phi)$\\n- **Imaginary part** $\\\\Im(z) = A \\\\sin(2\\\\pi f t + \\\\phi)$\\n\\nThus, if we want to represent sine and cosine waves using complex numbers, we can use the real and imaginary parts of the complex number to represent them:\\n\\n1. **Sine wave** $y(t) = A \\\\sin(2\\\\pi f t + \\\\phi)$:\\n\\n   - We can represent it using the imaginary part of the complex number:\\n     $$\\n     y(t) = \\\\Im \\\\left(A e^{i(2\\\\pi f t + \\\\phi)}\\\\right)\\n     $$\\n\\n2. **Cosine wave** $y(t) = A \\\\cos(2\\\\pi f t + \\\\phi)$:\\n   - We can represent it using the real part of the complex number:\\n     $$\\n     y(t) = \\\\Re \\\\left(A e^{i(2\\\\pi f t + \\\\phi)}\\\\right)\\n     $$\\n\\nHere, $A e^{i(2\\\\pi f t + \\\\phi)}$ is the unified complex representation, where the real and imaginary parts correspond to the cosine and sine waves, respectively.\\n\\n### Superposition of Waves\\n\\nIf we directly use trigonometric functions to represent the superposition of two waveforms, we need to use the addition formulas for trigonometric functions, such as:\\n\\n$$\\n\\\\cos(A + B) = \\\\cos A \\\\cos B - \\\\sin A \\\\sin B\\n$$\\n\\nand\\n\\n$$\\n\\\\sin(A + B) = \\\\sin A \\\\cos B + \\\\cos A \\\\sin B\\n$$\\n\\nWhen superimposing two waveforms:\\n\\n$$\\ny_1(t) = A_1 \\\\cos(2\\\\pi f_1 t + \\\\phi_1), \\\\quad y_2(t) = A_2 \\\\sin(2\\\\pi f_2 t + \\\\phi_2)\\n$$\\n\\nWe would need to expand each waveform separately, apply the cosine and sine addition formulas, and then organize the real and imaginary parts.\\n\\nThis process can be tedious and prone to errors, especially when more waveforms are involved, as the complexity of the formulas increases exponentially.\\n\\n:::tip\\nAside from high school students in Taiwan, I doubt anyone would want to manually calculate this, right?\\n:::\\n\\nIn the complex form, each waveform is represented as:\\n\\n$$\\nz_1 = A_1 e^{i(2\\\\pi f_1 t + \\\\phi_1)}, \\\\quad z_2 = A_2 e^{i(2\\\\pi f_2 t + \\\\phi_2)}\\n$$\\n\\nThe superposed waveform becomes:\\n\\n$$\\nz(t) = z_1 + z_2 = A_1 e^{i(2\\\\pi f_1 t + \\\\phi_1)} + A_2 e^{i(2\\\\pi f_2 t + \\\\phi_2)}\\n$$\\n\\nThe key here is that the addition of complex numbers is \\"linear,\\" meaning the magnitudes and phases of the two waveforms can be directly added or kept separate without needing to expand the trigonometric formulas.\\n\\nOn the other hand, complex numbers inherently contain both real and imaginary parts, which already carry the information for the cosine and sine components of the waveform. Therefore, there is no need to manually handle the decomposition of trigonometric functions.\\n\\nAnother advantage of using complex numbers is that they allow us to clearly separate the \\"amplitude\\" and \\"phase\\" of the waveform:\\n\\n- The amplitude is determined by the magnitude $|z|$.\\n- The phase is determined by the argument $\\\\arg(z)$.\\n\\nWhen superimposing waveforms, these pieces of information can be handled separately or together in calculations, without needing to expand into trigonometric sum or difference formulas. For example, if we want to simply analyze the amplitude of the superposed waveform, we can directly calculate $|z| = \\\\sqrt{\\\\Re(z)^2 + \\\\Im(z)^2}$.\\n\\nFinally, phase rotation of the waveform corresponds to the shift of the complex number\'s argument. If we need to adjust the phase of the superposed waveform, we only need to add a rotation angle $\\\\Delta\\\\theta$ to $e^{i\\\\theta}$, without separately adjusting the phases of the cosine and sine components.\\n\\n## Fourier Transform\\n\\nAll that has been discussed so far serves to help us understand the mathematical formula of the Fourier Transform.\\n\\nThe core concept of the Fourier Transform is:\\n\\n- **Any signal can be represented as a sum of sine and cosine waves**.\\n\\nThis means that even if a signal looks very complex, such as a piece of music, an image, or a pulse, we can still decompose it into simple, basic waveforms. These basic waveforms are the familiar sine and cosine waves, and they combine in different frequencies, amplitudes, and phases to form the complete signal.\\n\\nSine and cosine waves have powerful mathematical properties. For any periodic phenomenon, sine and cosine waves can be viewed as a set of \\"basis functions,\\" just like the $x$, $y$, and $z$ coordinate axes are used to describe positions in space. By appropriately combining them, we can represent any complex shape or variation.\\n\\nSuppose a signal $y(t)$ is a time-varying function. The Fourier Transform helps us answer two key questions:\\n\\n1. **What frequencies exist in this signal?**\\n2. **What are the amplitude and phase of each frequency?**\\n\\nThis frequency decomposition allows us to understand the signal from a completely different perspective. Instead of directly observing the waveform changing over time, we can see the signal\'s spectral characteristics more clearly.\\n\\nThe mathematical definition of the Fourier Transform provides the method for converting a signal from the \\"time domain\\" to the \\"frequency domain,\\" with the core formula being:\\n\\n$$\\nY(f) = \\\\int_{-\\\\infty}^\\\\infty y(t) e^{-i 2\\\\pi f t} \\\\, dt\\n$$\\n\\nThe physical meaning of each part of the formula is as follows:\\n\\n1. **Original Signal $y(t)$**:\\n\\n   $y(t)$ is a function defined in the time domain. This can be any form of signal, such as the sound wave of a piece of music, the amplitude variations of an electrical signal, or a pulse sequence that jumps within a time interval.\\n\\n   $y(t)$ contains the amplitude values of the signal at each moment of time $t$.\\n\\n2. **Complex Exponential Function $e^{-i 2\\\\pi f t}$**:\\n\\n   This is the key part of the Fourier Transform, and it is actually a combination of sine and cosine waves:\\n\\n   $$\\n   e^{-i 2\\\\pi f t} = \\\\cos(2\\\\pi f t) - i \\\\sin(2\\\\pi f t)\\n   $$\\n\\n   In the Fourier Transform, $e^{-i 2\\\\pi f t}$ is used to match a complex waveform with frequency $f$ to $y(t)$, extracting the strength of the signal at that frequency.\\n\\n---\\n\\nThe integral operation is essentially an inner product calculation, used to measure the similarity between $y(t)$ and $e^{-i 2\\\\pi f t}$.\\n\\nThis \\"similarity\\" determines the contribution of the signal at frequency $f$.\\n\\nThe Fourier Transform scans over all possible frequencies $f$.\\n\\nFor each $f$, the value $Y(f)$ calculated by the integral gives the strength at that frequency, which is why $Y(f)$ is called the \\"spectrum.\\"\\n\\nInstead of talking more about it, let\'s try calculating it ourselves:\\n\\nConsider a signal with a single frequency:\\n\\n$$\\ny(t) = A \\\\cos(2\\\\pi f_0 t + \\\\phi)\\n$$\\n\\nUsing Euler\'s formula, we express it as:\\n\\n$$\\ny(t) = \\\\Re\\\\left\\\\{ A e^{i (2\\\\pi f_0 t + \\\\phi)} \\\\right\\\\}\\n$$\\n\\nNow, performing the Fourier Transform:\\n\\n$$\\nY(f) = \\\\int_{-\\\\infty}^\\\\infty y(t) e^{-i 2\\\\pi f t} \\\\, dt\\n$$\\n\\nSubstituting the expression for $y(t)$:\\n\\n$$\\nY(f) = \\\\int_{-\\\\infty}^\\\\infty \\\\Re\\\\left\\\\{ A e^{i (2\\\\pi f_0 t + \\\\phi)} \\\\right\\\\} e^{-i 2\\\\pi f t} \\\\, dt\\n$$\\n\\nSince the core of the Fourier Transform is the integral operation, its effect is to match every point in the time domain with a frequency waveform. If $f = f_0$, the $e^{-i 2\\\\pi f t}$ and $e^{i 2\\\\pi f_0 t}$ align perfectly, and the result of the integral will be non-zero.\\n\\nIf $f \\\\neq f_0$, these two waveforms do not match in phase, and the integral result approaches zero.\\n\\nWhen the frequency matches, we get:\\n\\n$$\\nY(f_0) = A e^{i \\\\phi}\\n$$\\n\\nThis shows that the spectrum $Y(f)$ contains not only the strength of the frequency (given by $A$) but also the phase of the frequency (given by $\\\\phi$).\\n\\nThis is the reason why the Fourier Transform can fully describe a signal.\\n\\n## Fourier Series\\n\\nThe Fourier Series is a special case of the Fourier Transform, mainly used for periodic signals.\\n\\nUnlike the Fourier Transform, which extends the signal\'s spectrum to a continuous range of frequencies, the Fourier Series focuses on using a set of \\"discrete frequencies\\" to describe a periodic signal.\\n\\nThe core concept of the Fourier Series is:\\n\\n- **Any periodic signal can be represented as a linear combination of sine and cosine waves at discrete frequencies**.\\n\\nThis means that as long as the signal is periodic, we can approximate it using a finite number of sine and cosine waves, and as the frequency combinations increase, the approximation becomes more accurate.\\n\\nSuppose a periodic signal $x(t)$ with period $T$ can be represented by the Fourier Series as:\\n\\n$$\\nx(t) = a_0 + \\\\sum_{n=1}^\\\\infty \\\\left[ a_n \\\\cos\\\\left(\\\\frac{2\\\\pi n t}{T}\\\\right) + b_n \\\\sin\\\\left(\\\\frac{2\\\\pi n t}{T}\\\\right) \\\\right]\\n$$\\n\\nHere, $a_0, a_n, b_n$ are the coefficients of the Fourier Series, representing the amplitudes of the sine and cosine waves at different frequencies, and these coefficients are determined by the following formulas:\\n\\n1. **DC Component ($a_0$)**:\\n\\n   $$\\n   a_0 = \\\\frac{1}{T} \\\\int_{0}^T x(t) \\\\, dt\\n   $$\\n\\n   It represents the average value of the signal, or the DC component over one period.\\n\\n2. **Cosine Coefficients ($a_n$)**:\\n\\n   $$\\n   a_n = \\\\frac{2}{T} \\\\int_{0}^T x(t) \\\\cos\\\\left(\\\\frac{2\\\\pi n t}{T}\\\\right) \\\\, dt\\n   $$\\n\\n   It represents the amplitude of the cosine wave at frequency $\\\\frac{n}{T}$.\\n\\n3. **Sine Coefficients ($b_n$)**:\\n\\n   $$\\n   b_n = \\\\frac{2}{T} \\\\int_{0}^T x(t) \\\\sin\\\\left(\\\\frac{2\\\\pi n t}{T}\\\\right) \\\\, dt\\n   $$\\n\\n   It represents the amplitude of the sine wave at frequency $\\\\frac{n}{T}$.\\n\\nSimilarly, to simplify the representation and computation, the Fourier Series is often written in complex form:\\n\\n$$\\nx(t) = \\\\sum_{n=-\\\\infty}^\\\\infty c_n e^{i \\\\frac{2\\\\pi n t}{T}}\\n$$\\n\\nWhere $c_n$ are the complex coefficients, defined as:\\n\\n$$\\nc_n = \\\\frac{1}{T} \\\\int_{0}^T x(t) e^{-i \\\\frac{2\\\\pi n t}{T}} \\\\, dt\\n$$\\n\\nIn this form:\\n\\n- The real part $\\\\Re\\\\{c_n\\\\}$ corresponds to $a_n$.\\n- The imaginary part $\\\\Im\\\\{c_n\\\\}$ corresponds to $b_n$.\\n\\nThe Fourier Series has wide applications in engineering and science, such as studying periodic vibrations in mechanical structures and analyzing periodic electrical signals, like square waves, triangle waves, and sawtooth waves.\\n\\n## Conclusion\\n\\nHere, we can only briefly introduce the relevant concepts of the Fourier Transform.\\n\\nThere is more to explore, such as the Discrete Fourier Transform (DFT), Fast Fourier Transform (FFT), the relationship between Fourier Transform and convolution, frequency-domain filter design and implementation, and the extensions of Fourier analysis in quantum physics and image processing, which could take days to fully discuss.\\n\\nInterested readers can consult further resources to learn more about the principles and applications of Fourier Transform.\\n\\nWith the remaining space, I\'ll add a fun interactive activity at the end to experience the charm of Fourier!\\n\\n## Interactive Activity\\n\\nBased on the settings, the program will plot the corresponding waveform in the time domain and show its analysis results in the frequency domain.\\n\\nYou will see that the frequency domain distribution matches the values on the \\"settings panel,\\" with other frequencies generated to approximate the waveform.\\n\\n<FourierTransformDemo />"},{"id":"fixed-pyenv-install-error","metadata":{"permalink":"/en/blog/fixed-pyenv-install-error","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/11-25-fixed-pyenv-install-error/index.md","title":"Fixing pyenv Build Errors","description":"Troubleshooting build errors","date":"2024-11-25T00:00:00.000Z","tags":[{"inline":true,"label":"pyenv","permalink":"/en/blog/tags/pyenv"},{"inline":true,"label":"python","permalink":"/en/blog/tags/python"}],"readingTime":2.025,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"fixed-pyenv-install-error","title":"Fixing pyenv Build Errors","authors":"Z. Yuan","image":"/en/img/2024/1125.webp","tags":["pyenv","python"],"description":"Troubleshooting build errors"},"unlisted":false,"prevItem":{"title":"A Brief Introduction to Fourier Transform","permalink":"/en/blog/fourier-transform"},"nextItem":{"title":"Update Docusaurus to 3.6.0","permalink":"/en/blog/update-docusaurus-to-3-6-0"}},"content":"Installing pyenv itself is fine.\\n\\nHowever, an error occurs when building the Python version.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Problem Description\\n\\nUsing the command:\\n\\n```shell\\npyenv install 3.10.15\\n```\\n\\nShortly after, the system throws a series of error messages:\\n\\n```shell\\nDownloading Python-3.10.15.tar.xz...\\n-> https://www.python.org/ftp/python/3.10.15/Python-3.10.15.tar.xz\\nInstalling Python-3.10.15...\\n\\nBUILD FAILED (Ubuntu 22.04 using python-build 20180424)\\n\\nInspect or clean up the working tree at /tmp/python-build.20241125102533.16978\\nResults logged to /tmp/python-build.20241125102533.16978.log\\n\\nLast 10 log lines:\\n        ./signal/../sysdeps/unix/sysv/linux/x86_64/libc_sigaction.c:0\\n0x7be2d6829d8f __libc_start_call_main\\n        ../sysdeps/nptl/libc_start_call_main.h:58\\n0x7be2d6829e3f __libc_start_main_impl\\n        ../csu/libc-start.c:392\\nPlease submit a full bug report,\\nwith preprocessed source if appropriate.\\nPlease include the complete backtrace with any bug report.\\nSee <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.\\nmake: *** [Makefile:1856: Python/getargs.o] Error 1\\n```\\n\\nHuh? Just another day in the life of a developer!\\n\\n## Fixing the Problem\\n\\nHere are a few potential fixes:\\n\\n### 1. Install Dependencies\\n\\nFirst, update the system packages:\\n\\n```shell\\nsudo apt update && sudo apt upgrade -y\\n```\\n\\nThen, make sure the necessary packages are installed:\\n\\n```shell\\nsudo apt install -y build-essential libssl-dev zlib1g-dev libbz2-dev \\\\\\n    libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \\\\\\n    libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\\n```\\n\\nThe system tells us that all the above packages are already up-to-date.\\n\\nSo, it\u2019s not the problem here.\\n\\n### 2. Check GCC Version\\n\\nUpdate the GCC version:\\n\\n```shell\\nsudo apt install --reinstall gcc\\n```\\n\\nThen try again, but the same error persists.\\n\\nLooks like this isn\u2019t the issue either.\\n\\n### 3. Check the Log File\\n\\nFrom the error messages, find the location of the log file and take a look:\\n\\n```shell\\nless /tmp/python-build.20241125102533.16978.log\\n```\\n\\nInside, there\u2019s a ton of information. Let\u2019s go straight to the last section:\\n\\n```shell\\ngcc -c -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall    -std=c99 -Wextra -Wno-unused-result -Wno-unused-parameter -Wno-missing-field-initializers -Werror=implicit-function-declaration -fvisibility=hidden  -I./Include/internal  -I. -I./Include -I/home/user/.pyenv/versions/3.10.15/include -I/home/user/.pyenv/versions/3.10.15/include -fPIC -DPy_BUILD_CORE -o Programs/_testembed.o ./Programs/_testembed.c\\nsed -e \\"s,@EXENAME@,/home/user/.pyenv/versions/3.10.15/bin/python3.10,\\" < ./Misc/python-config.in >python-config.py\\nLC_ALL=C sed -e \'s,\\\\$(\\\\([A-Za-z0-9_]*\\\\)),\\\\$\\\\{\\\\1\\\\},g\' < Misc/python-config.sh >python-config\\n/tmp/ccTVJtRi.s: Assembler messages:\\n/tmp/ccTVJtRi.s: Internal error in emit_inc_line_addr at ../../gas/dwarf2dbg.c:1643.\\nPlease report this bug.\\nmake: *** [Makefile:1856: Objects/typeobject.o] Error 1\\nmake: *** Waiting for unfinished jobs....\\n```\\n\\nThe error here says:\\n\\n- **\\"Internal error in emit_inc_line_addr at ../../gas/dwarf2dbg.c\\"**\\n\\nThis points to an issue with the assembler component of binutils, typically indicating a bug or incompatibility in the package.\\n\\n### 4. Reinstall Binutils\\n\\nGreat, now we know the problem. Let\u2019s update binutils:\\n\\n```shell\\nsudo apt update\\nsudo apt install --reinstall binutils\\n```\\n\\nAfter updating, try running:\\n\\n```shell\\npyenv install 3.10.15\\n```\\n\\nThis time, it works, and the problem is resolved."},{"id":"update-docusaurus-to-3-6-0","metadata":{"permalink":"/en/blog/update-docusaurus-to-3-6-0","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/11-09-update-docusaurus-to-360/index.md","title":"Update Docusaurus to 3.6.0","description":"Troubleshooting issues during the update process","date":"2024-11-09T00:00:00.000Z","tags":[{"inline":true,"label":"Docusaurus","permalink":"/en/blog/tags/docusaurus"},{"inline":true,"label":"Update","permalink":"/en/blog/tags/update"}],"readingTime":3.025,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"update-docusaurus-to-3-6-0","title":"Update Docusaurus to 3.6.0","authors":"Z. Yuan","image":"/en/img/2024/1109.webp","tags":["Docusaurus","Update"],"description":"Troubleshooting issues during the update process"},"unlisted":false,"prevItem":{"title":"Fixing pyenv Build Errors","permalink":"/en/blog/fixed-pyenv-install-error"},"nextItem":{"title":"Python Implementation of a Web File Downloader","permalink":"/en/blog/file-crawler-python-implementation"}},"content":"Docusaurus has released version 3.6.0, which includes updates to the bundling tool, significantly speeding up build times.\\n\\nHowever, we ran into some issues during the update!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Update Details\\n\\nIf you\'re not familiar with the recent release, you can check out the latest blog post from the Docusaurus team:\\n\\n- [**Docusaurus 3.6**](https://docusaurus.io/blog/releases/3.6)\\n\\n  <iframe\\n    src=\\"https://docusaurus.io/blog/releases/3.6\\"\\n    width=\\"80%\\"\\n    height=\\"300px\\"\\n    center=\\"true\\"\\n    ></iframe>\\n\\n## Issue Description\\n\\nThe update itself proceeded without issues, but Docusaurus introduced a new feature in this version that allows adding a config setting:\\n\\n```js title=\\"docusaurus.config.js\\"\\nconst config = {\\n  future: {\\n    experimental_faster: true,\\n  },\\n};\\n```\\n\\nWhen we added this setting to our `docusaurus.config.js` file, we encountered the following error:\\n\\n```shell\\nyarn run v1.22.22\\n$ docusaurus start\\n[INFO] Starting the development server...\\n[SUCCESS] Docusaurus website is running at: http://localhost:3000/\\n\u25CF Client \u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0\u25A0 (83%) sealing chunk ids\\nSegmentation fault (core dumped)\\nerror Command failed with exit code 139.\\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\\n```\\n\\n---\\n\\nWhen we saw this error, it was frustrating.\\n\\nA simple `Segmentation fault`? That\u2019s all? Really?\\n\\n## Troubleshooting\\n\\nSince we couldn\'t find any solutions in the official issue tracker, we had to troubleshoot this manually.\\n\\nAfter some investigation, we discovered that the issue stems from using certain Chinese characters in `_category_.json` files.\\n\\nOr, more accurately, specific Chinese characters cause the issue, though we\u2019re unsure exactly which ones.\\n\\nFor example, one of our files originally looked like this:\\n\\n```json title=\\"_category_.json\\"\\n{\\n  \\"label\\": \\"\u5143\u5BCC\u8B49\u5238\\",\\n  \\"position\\": 1,\\n  \\"link\\": {\\n    \\"type\\": \\"generated-index\\"\\n  }\\n}\\n```\\n\\nChanging `\u5143\u5BCC\u8B49\u5238` to `\u4E2D\u6587` allowed the project to run successfully!\\n\\n:::tip\\nBoth labels are in Chinese. Why does one work while the other doesn\u2019t?\\n:::\\n\\nReplacing `\u4E2D\u6587` with an English label also worked:\\n\\n```json title=\\"_category_.json\\"\\n{\\n  \\"label\\": \\"English Label\\",\\n  \\"position\\": 1,\\n  \\"link\\": {\\n    \\"type\\": \\"generated-index\\"\\n  }\\n}\\n```\\n\\n## Additional Issues\\n\\nWe also discovered another issue: the new setting doesn\u2019t support special characters in file names.\\n\\nFor instance, one of our files was named `B\xe9zier`, which caused an error due to the accented character.\\n\\nAfter removing the accent, everything ran smoothly.\\n\\n## Conclusion\\n\\nIn the end, we decided not to enable this new feature.\\n\\nSince our website is relatively small, build speed isn\u2019t a major bottleneck, but this feature would require us to change multiple files.\\n\\nMaybe we\u2019ll revisit it later!\\n\\n## 2024-11-20 Update\\n\\nThe official update notification has been received, and this time the version has been updated to **v3.6.2**, which resolves the issues mentioned in the previous update.\\n\\nIn this version, we can now successfully use the `experimental_faster` setting:\\n\\n```js title=\\"docusaurus.config.js\\"\\nconst config = {\\n  future: {\\n    experimental_faster: true,\\n  },\\n};\\n```\\n\\nTesting shows that the **Segmentation fault** issue no longer occurs.\\n\\nHowever...\\n\\nAfter starting the development environment, modifying files triggers the following error:\\n\\n```shell\\nPanic occurred at runtime. Please file an issue on GitHub with the backtrace below: https://github.com/web-infra-dev/rspack/issues\\nMessage:  Chunk(ChunkUkey(Ukey(606), PhantomData<rspack_core::chunk::Chunk>)) not found in ChunkByUkey\\nLocation: crates/rspack_core/src/lib.rs:328\\n\\nRun with COLORBT_SHOW_HIDDEN=1 environment variable to disable frame filtering.\\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 BACKTRACE \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\\n 1: start_thread\\n    at ./nptl/pthread_create.c:447\\n 2: clone3\\n    at ./misc/../sysdeps/unix/sysv/linux/x86_64/clone3.S:78\\nAborted (core dumped)\\nerror Command failed with exit code 134.\\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\\n```\\n\\nIt seems to be an issue with Rspack. We quickly found a related issue on GitHub:\\n\\n- [**web-infra-dev/rspack: [Bug]: using docusaurus edit mdx or md file, process crash. #8480**](https://github.com/web-infra-dev/rspack/issues/8480)\\n\\nIt looks like we\'re not alone in this! We\'ll have to wait for a further fix.\\n\\n## 2024-11-24 Update\\n\\nContinuing from the previous issue, this time we updated to v3.6.3.\\n\\nThe Rspack issue has been fixed, and we can now happily use it as normal!"},{"id":"file-crawler-python-implementation","metadata":{"permalink":"/en/blog/file-crawler-python-implementation","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/09-23-file-crawler/index.md","title":"Python Implementation of a Web File Downloader","description":"Implement a simple web file downloader.","date":"2024-09-23T00:00:00.000Z","tags":[{"inline":true,"label":"Python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"File Crawler","permalink":"/en/blog/tags/file-crawler"}],"readingTime":1.705,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"file-crawler-python-implementation","title":"Python Implementation of a Web File Downloader","authors":"Z. Yuan","image":"/en/img/2024/0923.webp","tags":["Python","File Crawler"],"description":"Implement a simple web file downloader."},"unlisted":false,"prevItem":{"title":"Update Docusaurus to 3.6.0","permalink":"/en/blog/update-docusaurus-to-3-6-0"},"nextItem":{"title":"Automatically Count Articles in Docusaurus Sidebar","permalink":"/en/blog/customized-docusaurus-sidebars-auto-count"}},"content":"We came across a webpage containing hundreds of PDF file links.\\n\\nAs engineers, if we were to download them manually, it would be highly inefficient, right?\\n\\nSo, what we need here is a small script that will help us download all the files.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Install Required Packages\\n\\nFirst, you need to install the necessary packages. If you haven\'t installed them yet, you can do so using the following command:\\n\\n```bash\\npip install requests beautifulsoup4 urllib3\\n```\\n\\n## The Code\\n\\nWithout further ado, since the script is already written, let\'s dive straight into the code!\\n\\nThe parts highlighted are the ones you\u2019ll need to modify yourself. Adjust the script according to your needs.\\n\\n```python {13,16} title=\\"file_crawler.py\\"\\nimport os\\nfrom urllib.parse import urljoin\\n\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Simulating a browser\'s headers\\nheaders = {\\n    \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\\"\\n}\\n\\n# Web page URL\\nurl = \\"put_your_url_here\\"\\n\\n# Target file format\\ntarget_format = \\".pdf\\"\\n\\n# Send an HTTP GET request with headers\\nresponse = requests.get(url, headers=headers)\\n\\n# Check if the request was successful\\nif response.status_code == 200:\\n    # Use BeautifulSoup to parse the HTML\\n    soup = BeautifulSoup(response.text, \\"html.parser\\")\\n\\n    # Find all <a> tags and filter those with href attributes matching the target format\\n    target_links = []\\n    for link in soup.find_all(\\"a\\"):\\n        href = link.get(\\"href\\")\\n        if href and href.endswith(target_format):  # Specify the file format you want to download\\n            target_links.append(urljoin(url, href))\\n\\n    # Create a folder to save the files\\n    os.makedirs(\\"downloads\\", exist_ok=True)\\n\\n    # Download each file\\n    for url in target_links:\\n        file_name = url.split(\\"/\\")[-1]  # Extract the filename from the URL\\n        file_path = os.path.join(\\"downloads\\", file_name)\\n\\n        # Send a request to download the file\\n        response = requests.get(url, headers=headers)  # Add headers again\\n        if response.status_code == 200:\\n            with open(file_path, \\"wb\\") as f:\\n                f.write(response.content)\\n            print(f\\"Downloaded: {file_name}\\")\\n        else:\\n            print(f\\"Failed to download: {url}\\")\\nelse:\\n    print(f\\"Unable to access the webpage, status code: {response.status_code}\\")\\n```\\n\\n## Running the Script\\n\\nOnce you\u2019re done, you can simply run the script to download all the files matching the target format.\\n\\n```bash\\npython file_crawler.py\\n```"},{"id":"customized-docusaurus-sidebars-auto-count","metadata":{"permalink":"/en/blog/customized-docusaurus-sidebars-auto-count","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/09-14-customized-docusaurus-sidebars-auto-count/index.md","title":"Automatically Count Articles in Docusaurus Sidebar","description":"Adding some new features to the sidebar.","date":"2024-09-14T00:00:00.000Z","tags":[{"inline":true,"label":"Docusaurus","permalink":"/en/blog/tags/docusaurus"},{"inline":true,"label":"Sidebar","permalink":"/en/blog/tags/sidebar"}],"readingTime":4.315,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"customized-docusaurus-sidebars-auto-count","title":"Automatically Count Articles in Docusaurus Sidebar","authors":"Z. Yuan","image":"/en/img/2024/0914.webp","tags":["Docusaurus","Sidebar"],"description":"Adding some new features to the sidebar."},"unlisted":false,"prevItem":{"title":"Python Implementation of a Web File Downloader","permalink":"/en/blog/file-crawler-python-implementation"},"nextItem":{"title":"Customizing the Docusaurus 404 Page","permalink":"/en/blog/customized-docusaurus-404-page"}},"content":"Docusaurus provides a very feature-rich Sidebar by default, but sometimes it doesn\'t fit all our needs.\\n\\nLet\'s tweak it a bit this time.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe goal is simple. The original Sidebar displays the categories we\'ve specified. When the site starts, it looks for the `_category_.json` in each directory level, which contains something like this:\\n\\n```json\\n{\\n  \\"label\\": \\"Classic CNNs\\",\\n  \\"link\\": {\\n    \\"type\\": \\"generated-index\\"\\n  }\\n}\\n```\\n\\nThis would display:\\n\\n- Classic CNNs\\n- ... (other categories)\\n\\nWhat I\'d like to do is count the number of items under each folder and display it directly on the page, like this:\\n\\n- Classic CNNs (8)\\n- ... (other categories)\\n\\n---\\n\\nCan we just add the count directly in the `_category_.json` file?\\n\\n```json\\n{\\n  \\"label\\": \\"Classic CNNs (8)\\",\\n  \\"link\\": {\\n    \\"type\\": \\"generated-index\\"\\n  }\\n}\\n```\\n\\nAnd manually update the count every time we add an article?\\n\\n**Absolutely not! We can\'t write code like that.**\\n\\n## Reference Material\\n\\nTo solve this issue, as usual, let\'s first check Docusaurus\' official documentation:\\n\\n- [**Docusaurus Sidebar**](https://docusaurus.io/docs/sidebar)\\n\\n---\\n\\nThe default Sidebar is autogenerated through the `autogenerated` option:\\n\\n```jsx\\n/**\\n * Creating a sidebar enables you to:\\n - create an ordered group of docs\\n - render a sidebar for each doc of that group\\n - provide next/previous navigation\\n\\n The sidebars can be generated from the filesystem, or explicitly defined here.\\n\\n Create as many sidebars as you want.\\n */\\n\\n// @ts-check\\n\\n/** @type {import(\'@docusaurus/plugin-content-docs\').SidebarsConfig} */\\nconst sidebars = {\\n  // By default, Docusaurus generates a sidebar from the docs folder structure\\n  tutorialSidebar: [{ type: \\"autogenerated\\", dirName: \\".\\" }],\\n\\n  // But you can create a sidebar manually\\n  /*\\n  tutorialSidebar: [\\n    \'intro\',\\n    \'hello\',\\n    {\\n      type: \'category\',\\n      label: \'Tutorial\',\\n      items: [\'tutorial-basics/create-a-document\'],\\n    },\\n  ],\\n   */\\n};\\n\\nexport default sidebars;\\n```\\n\\n---\\n\\nAfter reviewing it, it seems there is no built-in feature that fits our needs. Let\'s build one ourselves.\\n\\n## Implementation\\n\\nI\'ll include comments directly in the code. Here are a few places to note where modifications may be required based on your setup:\\n\\n1. Line 8: `const baseDir = path.join(__dirname, \\"papers\\");`\\n\\n   The `papers` here refers to our folder name. Ensure your directory path is correct.\\n\\n---\\n\\n2. Line 20: `sidebarItems.push(\\"intro\\");`\\n\\n   The `intro` here refers to the name of our homepage. Ensure your homepage name is correct. If there\'s no homepage, you can remove this line.\\n\\n---\\n\\n3. Line 72: `return stat.isDirectory() || (stat.isFile() && item.endsWith(\\".md\\"));`\\n\\n   Here, `.md` is the format of our articles. Adjust it based on your format.\\n\\n---\\n\\nHere\'s the implementation. You can see the result directly on the webpage: [**Papers**](/papers/intro).\\n\\n```jsx showLineNumbers title=\\"/sidebars.js\\"\\nconst fs = require(\\"fs\\"); // Import Node.js file system module to work with files and directories\\nconst path = require(\\"path\\"); // Import Node.js path module to handle file paths\\n\\n/** @type {import(\'@docusaurus/plugin-content-docs\').SidebarsConfig} */\\n// This is a type definition for Docusaurus sidebar configuration to enable IDE autocompletion\\nfunction generateSidebar() {\\n  // Set the base directory path, pointing to the \'papers\' folder. Modify as needed.\\n  const baseDir = path.join(__dirname, \\"papers\\");\\n\\n  // Read all subdirectories under \'papers\', filtering out hidden ones (starting with a dot)\\n  const categories = fs.readdirSync(baseDir).filter((item) => {\\n    const itemPath = path.join(baseDir, item);\\n    // Ensure it\'s a directory and not hidden\\n    return fs.statSync(itemPath).isDirectory() && !item.startsWith(\\".\\");\\n  });\\n\\n  const sidebarItems = []; // Array to store sidebar items\\n\\n  // Add a fixed \'intro\' item at the beginning\\n  sidebarItems.push(\\"intro\\");\\n\\n  // Iterate through all category directories\\n  categories.forEach((category) => {\\n    const categoryPath = path.join(baseDir, category); // Get the full path for each category\\n    const count = countItemsInDirectory(categoryPath); // Count the number of items in the directory\\n\\n    // Try to read the \'_category_.json\' file within the category directory to get label and link\\n    const categoryJsonPath = path.join(categoryPath, \\"_category_.json\\");\\n    let label = category; // Default label is the directory name\\n    let link = undefined; // Default link is undefined\\n    if (fs.existsSync(categoryJsonPath)) {\\n      // If the \'_category_.json\' file exists\\n      const categoryJson = JSON.parse(\\n        fs.readFileSync(categoryJsonPath, \\"utf8\\")\\n      ); // Read and parse the JSON file\\n      label = categoryJson.label || category; // Use the label from the JSON file or default to directory name\\n      link = categoryJson.link; // Use the link from the JSON file\\n    }\\n\\n    // Append the item count to the label\\n    label = `${label} (${count})`;\\n\\n    // Create a sidebar item, with type \'category\', indicating this is a category\\n    const sidebarItem = {\\n      type: \\"category\\",\\n      label: label, // Display label\\n      items: [{ type: \\"autogenerated\\", dirName: category }], // Automatically generate document items under the category\\n    };\\n\\n    if (link) {\\n      // If a link is provided, add it to the category\\n      sidebarItem.link = link;\\n    }\\n\\n    sidebarItems.push(sidebarItem); // Add the category item to the sidebar array\\n  });\\n\\n  // Return an object containing the sidebar configuration\\n  return {\\n    papersSidebar: sidebarItems,\\n  };\\n}\\n\\n// Count the valid items (including subdirectories and Markdown files) in a specified directory\\nfunction countItemsInDirectory(dirPath) {\\n  const items = fs.readdirSync(dirPath).filter((item) => {\\n    const itemPath = path.join(dirPath, item);\\n    // Exclude \'_category_.json\' and hidden files (starting with a dot)\\n    if (item === \\"_category_.json\\" || item.startsWith(\\".\\")) return false;\\n    const stat = fs.statSync(itemPath);\\n    // Only count directories and .md files\\n    return stat.isDirectory() || (stat.isFile() && item.endsWith(\\".md\\"));\\n  });\\n  return items.length; // Return the count of items\\n}\\n\\n// Export the generated sidebar configuration for Docusaurus to use\\nexport default generateSidebar();\\n```"},{"id":"customized-docusaurus-404-page","metadata":{"permalink":"/en/blog/customized-docusaurus-404-page","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/09-10-customized-docusaurus-404-page/index.md","title":"Customizing the Docusaurus 404 Page","description":"The default 404 page needs a revamp!","date":"2024-09-10T00:00:00.000Z","tags":[{"inline":true,"label":"Docusaurus","permalink":"/en/blog/tags/docusaurus"},{"inline":true,"label":"404NotFound","permalink":"/en/blog/tags/404-not-found"}],"readingTime":6.235,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"customized-docusaurus-404-page","title":"Customizing the Docusaurus 404 Page","authors":"Z. Yuan","image":"/en/img/2024/0910.webp","tags":["Docusaurus","404NotFound"],"description":"The default 404 page needs a revamp!"},"unlisted":false,"prevItem":{"title":"Automatically Count Articles in Docusaurus Sidebar","permalink":"/en/blog/customized-docusaurus-sidebars-auto-count"},"nextItem":{"title":"Discrepancy in LayerNorm Calculations?","permalink":"/en/blog/torch-layernorm-mismatch"}},"content":"Docusaurus is a static site generator developed by Meta, designed for building open-source documentation websites.\\n\\nIt provides a simple way to create and maintain websites, supporting custom themes and plugins.\\n\\n\x3c!-- truncate --\x3e\\n\\nIf you\'re unfamiliar with Docusaurus, you can check it out here: [**Docusaurus Official Website**](https://docusaurus.io/)\\n\\nOur site is also built using Docusaurus, but after launching, we noticed that the default 404 page was quite basic.\\n\\nTo enhance user experience, we decided to create a custom 404 page.\\n\\n## References\\n\\nTo solve this issue, we first found a discussion page within the Docusaurus project:\\n\\n- [**How can I customize the 404 page?**](https://github.com/facebook/docusaurus/discussions/6030)\\n\\nBased on this discussion, we implemented the solution.\\n\\nHere\'s our step-by-step process.\\n\\n## Exporting the 404 Page Configuration\\n\\n:::warning\\nStarting from this step, we\u2019ll be modifying Docusaurus\' source code.\\n\\nIf there are destructive version updates in the future, these modifications may cause the website to malfunction. Please make sure you have the ability to maintain the website before proceeding.\\n:::\\n\\nIn Docusaurus, when a 404 error occurs, it redirects to the `NotFound` page of the `@docusaurus/theme-classic` theme.\\n\\nWe need to export this page\u2019s configuration by running the following command:\\n\\n```bash\\nnpm run swizzle @docusaurus/theme-classic NotFound\\n```\\n\\nDuring the process, select `JavaScript`, and then choose `--eject`. This will generate a `NotFound` directory under the `src/theme` folder.\\n\\nIf you\'re curious about the original code, you can find it here:\\n\\n- [**docusaurus-theme-classic/src/theme/NotFound**](https://github.com/facebook/docusaurus/tree/e8c6787ec20adc975dd6cd292a731d01206afe92/packages/docusaurus-theme-classic/src/theme/NotFound)\\n\\nInside the folder, you\'ll find an `index.js` file. Initially, we don\u2019t need to worry about this file. Instead, look inside the `Content` subfolder, where you\'ll find another `index.js` file. This is the one we\u2019ll be modifying.\\n\\nHere\u2019s the original code:\\n\\n```jsx\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\nimport React from \\"react\\";\\nimport clsx from \\"clsx\\";\\nimport Translate from \\"@docusaurus/Translate\\";\\nimport type { Props } from \\"@theme/NotFound/Content\\";\\nimport Heading from \\"@theme/Heading\\";\\n\\nexport default function NotFoundContent({ className }: Props): JSX.Element {\\n  return (\\n    <main className={clsx(\\"container margin-vert--xl\\", className)}>\\n      <div className=\\"row\\">\\n        <div className=\\"col col--6 col--offset-3\\">\\n          <Heading as=\\"h1\\" className=\\"hero__title\\">\\n            <Translate\\n              id=\\"theme.NotFound.title\\"\\n              description=\\"The title of the 404 page\\"\\n            >\\n              Page Not Found\\n            </Translate>\\n          </Heading>\\n          <p>\\n            <Translate\\n              id=\\"theme.NotFound.p1\\"\\n              description=\\"The first paragraph of the 404 page\\"\\n            >\\n              We could not find what you were looking for.\\n            </Translate>\\n          </p>\\n          <p>\\n            <Translate\\n              id=\\"theme.NotFound.p2\\"\\n              description=\\"The 2nd paragraph of the 404 page\\"\\n            >\\n              Please contact the owner of the site that linked you to the\\n              original URL and let them know their link is broken.\\n            </Translate>\\n          </p>\\n        </div>\\n      </div>\\n    </main>\\n  );\\n}\\n```\\n\\n## Start Modifying\\n\\nWe want to add a few features:\\n\\n1. A cute icon.\\n2. A countdown timer that automatically redirects to the homepage.\\n3. Custom text to provide more information to the reader.\\n\\n### Countdown Timer\\n\\nFirst, we\u2019ll add a countdown timer using `useEffect`.\\n\\n```jsx\\nimport React, { useEffect, useState } from \\"react\\";\\n\\nconst [countdown, setCountdown] = useState(15);\\n\\nuseEffect(() => {\\n  const timer = setInterval(() => {\\n    setCountdown((prevCountdown) =>\\n      prevCountdown > 0 ? prevCountdown - 1 : 0\\n    );\\n  }, 1000);\\n\\n  if (countdown === 0) {\\n    window.location.href = \\"/\\";\\n  }\\n\\n  return () => clearInterval(timer);\\n}, [countdown]);\\n```\\n\\nThis will automatically redirect to the homepage once the countdown reaches zero.\\n\\n### Adding an Icon\\n\\nWe found a cute icon on a free icon website:\\n\\n- [**Freepik**](https://www.freepik.com/icons/error)\\n\\nAfter downloading it, we placed it in the `static/img` directory and referenced it in `index.js`.\\n\\n```jsx\\n<img\\n  src=\\"/img/error-icon.png\\"\\n  alt=\\"Error icon\\"\\n  style={{\\n    width: \\"150px\\",\\n    height: \\"150px\\",\\n    marginBottom: \\"20px\\",\\n    animation: \\"bounce 1s infinite\\",\\n  }}\\n/>\\n\\n<style>{`\\n    @keyframes bounce {\\n    0%, 100% {\\n        transform: translateY(0);\\n    }\\n    50% {\\n        transform: translateY(-10px);\\n    }\\n    }\\n`}</style>\\n```\\n\\nThis `<img>` tag displays an error icon, with some additional styling to make it bounce:\\n\\n- `src=\\"/img/error-icon.png\\"`: The image source, pointing to a local file.\\n- `alt=\\"Error icon\\"`: Alternative text, displayed if the image can\u2019t load.\\n- The `style` attribute defines the icon\'s size and adds a bounce animation.\\n\\n### Custom Text\\n\\nThe default 404 page looks something like this:\\n\\n<div style={{ textAlign: \'center\' }}>\\n<iframe\\n  src=\\"https://docusaurus.io/non-exist\\"\\n  width=\\"80%\\"\\n  height=\\"500px\\"\\n  center=\\"true\\"\\n></iframe>\\n</div>\\n\\n---\\n\\nWe updated it with the following text:\\n\\n```jsx\\n<p style={{ fontSize: \'1.2rem\', marginBottom: \'20px\' }}>\\nSorry, we couldn\'t find the page you were looking for.\\n</p>\\n<p style={{ fontSize: \'1.2rem\', marginBottom: \'20px\' }}>\\nIt\u2019s possible that the site structure has changed, and you might have clicked an outdated link.\\n</p>\\n<p style={{ fontSize: \'1.2rem\', marginBottom: \'20px\' }}>\\nPlease use the navigation bar above to find the information you\'re looking for.\\n</p>\\n```\\n\\nSince we frequently update our site, some paths may be outdated in Google\u2019s index, causing users to land on the wrong page.\\n\\nWe use this opportunity to inform users that:\\n\\n- The page they\u2019re looking for likely still exists, but has been moved!\\n\\nWe hope that with this message, users can navigate the site and find what they need.\\n\\n---\\n\\nFeel free to customize this section to suit your needs.\\n\\n### Final Result\\n\\nHere\u2019s what our final 404 page looks like:\\n\\n<br /><br />\\n\\n<div className=\\"row\\" style={{\\n    display: \'flex\',\\n    justifyContent: \'center\',\\n    alignItems: \'center\',\\n    flexDirection: \'column\',\\n    textAlign: \'center\',\\n    animation: \'fadeIn 0.5s ease-in-out\',\\n  }}>\\n\\n<img\\nsrc=\\"/img/error-icon.png\\"\\nalt=\\"Error icon\\"\\nstyle={{\\n      width: \'150px\',\\n      height: \'150px\',\\n      marginBottom: \'20px\',\\n      animation: \'bounce 1s infinite\',\\n    }}\\n/>\\n\\n  <div>\\n    <p style={{ fontSize: \'1.2rem\', marginBottom: \'20px\' }}>\\n      \u5F88\u62B1\u6B49\uFF0C\u6211\u5011\u7121\u6CD5\u627E\u5230\u60A8\u8981\u7684\u9801\u9762\u3002\\n    </p>\\n    <p style={{ fontSize: \'1.2rem\', marginBottom: \'20px\' }}>\\n      \u7DB2\u9801\u7D50\u69CB\u5DF2\u7D93\u4FEE\u6539\u4E86\uFF0C\u800C\u60A8\u53EF\u80FD\u9078\u5230\u904E\u6642\u7684\u9023\u7D50\u3002\\n    </p>\\n    <p style={{ fontSize: \'1.2rem\', marginBottom: \'20px\' }}>\\n      \u8ACB\u9EDE\u64CA\u4E0A\u65B9\u5C0E\u822A\u6B04\uFF0C\u6216\u8A31\u53EF\u4EE5\u627E\u5230\u60A8\u8981\u7684\u8CC7\u8A0A\u3002\\n    </p>\\n  </div>\\n\\n  <style>{`\\n    @keyframes fadeIn {\\n      from { opacity: 0; }\\n      to { opacity: 1; }\\n    }\\n    @keyframes bounce {\\n      0%, 100% {\\n        transform: translateY(0);\\n      }\\n      50% {\\n        transform: translateY(-10px);\\n      }\\n    }\\n  `}</style>\\n\\n</div>\\n\\n### Full Code\\n\\nFinally, here is the complete code:\\n\\n```jsx title=\'src/theme/NotFound/Content/index.js\'\\nimport Translate from \\"@docusaurus/Translate\\";\\nimport Heading from \\"@theme/Heading\\";\\nimport clsx from \\"clsx\\";\\nimport React, { useEffect, useState } from \\"react\\";\\n\\nexport default function NotFoundContent({ className }) {\\n  const [countdown, setCountdown] = useState(15);\\n\\n  useEffect(() => {\\n    const timer = setInterval(() => {\\n      setCountdown((prevCountdown) =>\\n        prevCountdown > 0 ? prevCountdown - 1 : 0\\n      );\\n    }, 1000);\\n\\n    if (countdown === 0) {\\n      window.location.href = \\"/\\";\\n    }\\n\\n    return () => clearInterval(timer);\\n  }, [countdown]);\\n\\n  return (\\n    <main className={clsx(\\"container margin-vert--xl\\", className)}>\\n      <div\\n        className=\\"row\\"\\n        style={{\\n          display: \\"flex\\",\\n          justifyContent: \\"center\\",\\n          alignItems: \\"center\\",\\n          flexDirection: \\"column\\",\\n          textAlign: \\"center\\",\\n          animation: \\"fadeIn 0.5s ease-in-out\\",\\n        }}\\n      >\\n        <img\\n          src=\\"/img/error-icon.png\\"\\n          alt=\\"Error icon\\"\\n          style={{\\n            width: \\"150px\\",\\n            height: \\"150px\\",\\n            marginBottom: \\"20px\\",\\n            animation: \\"bounce 1s infinite\\",\\n          }}\\n        />\\n\\n        <div>\\n          <Heading as=\\"h1\\" className=\\"hero__title\\">\\n            <Translate\\n              id=\\"theme.NotFound.title\\"\\n              description=\\"The title of the 404 page\\"\\n            >\\n              Page Not Found\\n            </Translate>\\n          </Heading>\\n          <p style={{ fontSize: \\"1.2rem\\", marginBottom: \\"20px\\" }}>\\n            Sorry, we couldn\'t find the page you were looking for.\\n          </p>\\n          <p style={{ fontSize: \\"1.2rem\\", marginBottom: \\"20px\\" }}>\\n            It\u2019s possible that the site structure has changed, and you might\\n            have clicked an outdated link.\\n          </p>\\n          <p style={{ fontSize: \\"1.2rem\\", marginBottom: \\"20px\\" }}>\\n            Please use the navigation bar above to find the information you\'re\\n            looking for.\\n          </p>\\n          <p aria-live=\\"polite\\" style={{ fontSize: \\"1rem\\", color: \\"#555\\" }}>\\n            {countdown > 0\\n              ? `Redirecting to the homepage in ${countdown} seconds...`\\n              : \\"Redirecting...\\"}\\n          </p>\\n        </div>\\n\\n        <style>{`\\n          @keyframes fadeIn {\\n            from { opacity: 0; }\\n            to { opacity: 1; }\\n          }\\n          @keyframes bounce {\\n            0%, 100% {\\n              transform: translateY(0);\\n            }\\n            50% {\\n              transform: translateY(-10px);\\n            }\\n          }\\n        `}</style>\\n      </div>\\n    </main>\\n  );\\n}\\n```"},{"id":"torch-layernorm-mismatch","metadata":{"permalink":"/en/blog/torch-layernorm-mismatch","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/08-20-torch-layernorm-mismatch/index.md","title":"Discrepancy in LayerNorm Calculations?","description":"Curious about the numbers? Let\'s calculate and compare.","date":"2024-08-20T00:00:00.000Z","tags":[{"inline":true,"label":"PyTorch","permalink":"/en/blog/tags/py-torch"},{"inline":true,"label":"LayerNorm","permalink":"/en/blog/tags/layer-norm"}],"readingTime":3.255,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"torch-layernorm-mismatch","title":"Discrepancy in LayerNorm Calculations?","authors":"Z. Yuan","image":"/en/img/2024/0820.webp","tags":["PyTorch","LayerNorm"],"description":"Curious about the numbers? Let\'s calculate and compare."},"unlisted":false,"prevItem":{"title":"Customizing the Docusaurus 404 Page","permalink":"/en/blog/customized-docusaurus-404-page"},"nextItem":{"title":"Get All Stock Code Information from TWSE","permalink":"/en/blog/get-taiwan-all-stocks-info"}},"content":"Today, I decided to manually calculate the values of LayerNorm.\\n\\n\x3c!-- truncate --\x3e\\n\\nWe all know that the formula for LayerNorm is as follows:\\n\\n$$\\n\\\\text{LayerNorm}(x) = \\\\frac{x - \\\\mu}{\\\\sqrt{\\\\text{Var[}x\\\\text{]} + \\\\epsilon}} \\\\times \\\\gamma + \\\\beta\\n$$\\n\\nWhere $\\\\mu$ is the mean of $x$, and $\\\\text{Var}$ is the variance of $x$.\\n\\nWith this information, let\'s calculate it ourselves, ignoring $\\\\gamma$ and $\\\\beta$ for simplicity:\\n\\n```python\\nimport torch\\n\\nx = torch.rand(16, 768)\\nmu = x.mean(dim=-1, keepdim=True)\\nvar = x.var(dim=-1, keepdim=True)\\neps = 1e-5\\ny = (x - mu) / (var + eps).sqrt()\\n```\\n\\nThis yields the following values:\\n\\n```python\\n# tensor([[ 0.1219, -0.0222, -1.4742,  ...,  0.1738, -0.6124, -0.3001],\\n#         [-1.6009, -1.5814,  1.5357,  ...,  0.1917,  1.3787, -0.2772],\\n#         [ 0.3738,  1.0520,  0.4403,  ...,  1.1353, -0.7488, -0.9137],\\n#         ...,\\n#         [ 0.8823, -1.5427,  0.4725,  ..., -1.2544, -1.5354, -0.4305],\\n#         [ 1.4548,  0.3059, -0.6732,  ..., -0.7109,  0.4908, -1.2447],\\n#         [-0.4067,  0.5974, -0.9113,  ..., -0.2511, -0.2279, -0.9675]])\\n```\\n\\nNext, let\'s compare these results with PyTorch\'s `torch.nn.LayerNorm`:\\n\\n```python\\nlayer_norm = torch.nn.LayerNorm(768, elementwise_affine=False, bias=False)\\n\\ny_ln = layer_norm(x)\\n```\\n\\nThis yields:\\n\\n```python\\n# tensor([[ 0.1220, -0.0222, -1.4752,  ...,  0.1739, -0.6128, -0.3003],\\n#         [-1.6020, -1.5824,  1.5367,  ...,  0.1918,  1.3796, -0.2774],\\n#         [ 0.3741,  1.0527,  0.4406,  ...,  1.1360, -0.7493, -0.9143],\\n#         ...,\\n#         [ 0.8829, -1.5437,  0.4728,  ..., -1.2552, -1.5364, -0.4308],\\n#         [ 1.4557,  0.3061, -0.6736,  ..., -0.7113,  0.4911, -1.2455],\\n#         [-0.4069,  0.5978, -0.9119,  ..., -0.2513, -0.2281, -0.9681]])\\n```\\n\\nWhen we compare the two, why are they different?\\n\\n## Unbiased Estimation\\n\\nAfter a quick search, I found that `torch.var` has a parameter called `correction`, which defaults to `1`, meaning it uses an unbiased estimate.\\n\\nThis means that it divides by `N-1` instead of `N`, whereas `torch.nn.LayerNorm` uses `N`.\\n\\nSo let\'s modify the `torch.var` function by setting `correction=0`:\\n\\n```python\\nvar = x.var(dim=-1, correction=0, keepdim=True)\\n```\\n\\n:::tip\\n`correction` is an alias for `unbiased`, introduced in PyTorch 2.0.0.\\n\\nIn earlier versions, you would set it using `unbiased=False`:\\n\\n```python\\nvar = x.var(dim=-1, unbiased=False, keepdim=True)\\n```\\n\\n:::\\n\\nNow, let\'s compare the results again:\\n\\n```python\\n# tensor([[ 0.1220, -0.0222, -1.4752,  ...,  0.1739, -0.6128, -0.3003],\\n#         [-1.6020, -1.5824,  1.5367,  ...,  0.1918,  1.3796, -0.2774],\\n#         [ 0.3741,  1.0527,  0.4406,  ...,  1.1360, -0.7493, -0.9143],\\n#         ...,\\n#         [ 0.8829, -1.5437,  0.4728,  ..., -1.2552, -1.5364, -0.4308],\\n#         [ 1.4557,  0.3061, -0.6736,  ..., -0.7113,  0.4911, -1.2455],\\n#         [-0.4069,  0.5978, -0.9119,  ..., -0.2513, -0.2281, -0.9681]])\\n```\\n\\nNow the numbers match perfectly!\\n\\n## Why Doesn\'t LayerNorm Use Unbiased Estimation?\\n\\nTo summarize briefly, it\'s about stability and simplification of calculations.\\n\\nIf you\'re interested in a deeper dive, here are some key points:\\n\\n- **Stability in Small Batch Calculations**\\n\\n  LayerNorm is typically applied to the features of individual samples (e.g., each neuron or feature) rather than across the entire batch. The number of features per sample is usually much larger than the number of samples. Thus, using population variance provides a more stable and accurate estimate, especially with small sample sizes.\\n\\n- **Reduced Importance of Unbiased Estimation**\\n\\n  The unbiased nature of sample variance (i.e., dividing by n-1 instead of n) is crucial when estimating population parameters from a sample. However, in deep learning regularization and normalization, like in LayerNorm, the impact of this bias is relatively minor. This is because these computations are used to normalize activations rather than to estimate overall statistics. Using population variance simplifies calculations with minimal effect on training outcomes.\\n\\n- **Stability in Gradient Calculations**\\n\\n  Stable gradients are vital during backpropagation. Using population variance leads to smoother and more stable gradient calculations, avoiding additional noise that might arise from small sample sizes. This contributes to better convergence and training performance.\\n\\n- **Simplified Calculations**\\n\\n  From a computational perspective, calculating population variance is slightly simpler than sample variance because it omits a subtraction operation (i.e., dividing by n rather than n-1). While this isn\'t a decisive factor, it\'s a consideration in the design process.\\n\\n## Conclusion\\n\\nThis article was inspired by a sudden curiosity.\\n\\nI hope this explanation helps clarify the issue for you."},{"id":"get-taiwan-all-stocks-info","metadata":{"permalink":"/en/blog/get-taiwan-all-stocks-info","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/06-10-get-taiwan-all-stocks-info/index.md","title":"Get All Stock Code Information from TWSE","description":"Get information on all stocks listed on TWSE using Python.","date":"2024-06-10T00:00:00.000Z","tags":[{"inline":true,"label":"beautifulsoup4","permalink":"/en/blog/tags/beautifulsoup-4"},{"inline":true,"label":"TWSE","permalink":"/en/blog/tags/twse"}],"readingTime":2.905,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"get-taiwan-all-stocks-info","title":"Get All Stock Code Information from TWSE","authors":"Z. Yuan","image":"/en/img/2024/0610.webp","tags":["beautifulsoup4","TWSE"],"description":"Get information on all stocks listed on TWSE using Python."},"unlisted":false,"prevItem":{"title":"Discrepancy in LayerNorm Calculations?","permalink":"/en/blog/torch-layernorm-mismatch"},"nextItem":{"title":"Simple Configuration of Python Environment on Win11","permalink":"/en/blog/windows-python-settings"}},"content":"The stock codes in the Taiwanese market change periodically, making manual tracking impractical.\\n\\nIt\'s time to automate!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Setup Environment\\n\\nLet\'s tackle the challenge by writing a program. First, install the necessary packages:\\n\\n```bash\\npip install requests beautifulsoup4 json\\n```\\n\\n:::tip\\nAssuming you have a functional Python environment.\\n:::\\n\\n## Target Webpages\\n\\nStock-related data resides on the website of the Taiwan Stock Exchange (TWSE). Let\'s identify the target pages:\\n\\n- [**Taiwan Stock Exchange/Securities Code Announcement**](https://www.twse.com.tw/en/products/code/announcement.html)\\n\\n    <div align=\\"center\\">\\n    <figure style={{\\"width\\": \\"80%\\"}}>\\n    ![TWSE](./img/img1.jpg)\\n    </figure>\\n    </div>\\n\\nNote down these three URLs:\\n\\n```python\\nurls = [\\n    \\"https://isin.twse.com.tw/isin/C_public.jsp?strMode=2\\", # Listed securities\\n    \\"https://isin.twse.com.tw/isin/C_public.jsp?strMode=4\\", # OTC securities\\n    \\"https://isin.twse.com.tw/isin/C_public.jsp?strMode=5\\"  # Emerging stocks\\n]\\n```\\n\\n## Parsing the Webpage\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"80%\\"}}>\\n![stock_table](./img/img2.jpg)\\n</figure>\\n</div>\\n\\nUpon inspecting the webpage, we identify that the main table corresponds to the HTML tag `class=h4`.\\n\\nNow that we\'ve located our target, let\'s start coding:\\n\\n```python title=\\"update_stocks_code.py\\"\\nimport json\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Retrieve content from Taiwan Stock Exchange announcements\\nurls = [\\n    \\"https://isin.twse.com.tw/isin/C_public.jsp?strMode=2\\", # Listed securities\\n    \\"https://isin.twse.com.tw/isin/C_public.jsp?strMode=4\\", # OTC securities\\n    \\"https://isin.twse.com.tw/isin/C_public.jsp?strMode=5\\"  # Emerging stocks\\n]\\n\\n# All data infos\\ndata = {}\\n\\ntotal_urls = len(urls)\\nfor index, url in enumerate(urls, start=1):\\n    print(f\\"Processing URL {index}/{total_urls}: {url}\\")\\n\\n    response = requests.get(url)\\n    response.encoding = \'big5\'  # Set the correct encoding\\n\\n    # Parse HTML using BeautifulSoup\\n    soup = BeautifulSoup(response.text, \'html.parser\')\\n    table = soup.find(\'table\', {\'class\': \'h4\'})\\n\\n    if not table:\\n        print(f\\"Table not found for URL: {url}\\")\\n        continue\\n\\n    for row in table.find_all(\'tr\')[1:]:  # Skip header row\\n        cells = row.find_all(\'td\')\\n        if len(cells) != 7:\\n            continue\\n\\n        code, name = cells[0].text.split(\\"\\\\u3000\\")\\n        internationality = cells[1].text\\n        list_date = cells[2].text\\n        market_type = cells[3].text\\n        industry_type = cells[4].text\\n\\n        data[code] = {\\n            \\"Name\\": name,\\n            \\"Code\\": code,\\n            \\"Market Type\\": market_type,\\n            \\"Industry Type\\": industry_type,\\n            \\"Listing Date\\": list_date,\\n            \\"International Code\\": internationality\\n        }\\n\\nwith open(\\"stock_infos.json\\", \\"w\\", encoding=\\"utf-8\\") as f:\\n    json.dump(data, f, ensure_ascii=False, indent=2)\\n\\nprint(\\"All data has been processed and saved to stock_infos.json\\")\\n```\\n\\n## Output Results\\n\\n```json title=\\"stock_infos.json\\"\\n{\\n  \\"1101\\": {\\n    \\"Name\\": \\"Taiwan Cement\\",\\n    \\"Code\\": \\"1101\\",\\n    \\"Market Type\\": \\"Listed\\",\\n    \\"Industry Type\\": \\"Cement Industry\\",\\n    \\"Listing Date\\": \\"1962/02/09\\",\\n    \\"International Code\\": \\"TW0001101004\\"\\n  },\\n  \\"1102\\": {\\n    \\"Name\\": \\"Asia Cement\\",\\n    \\"Code\\": \\"1102\\",\\n    \\"Market Type\\": \\"Listed\\",\\n    \\"Industry Type\\": \\"Cement Industry\\",\\n    \\"Listing Date\\": \\"1962/06/08\\",\\n    \\"International Code\\": \\"TW0001102002\\"\\n  },\\n  ...omitting the rest...\\n}\\n```\\n\\nWe output the result as a JSON file for convenient integration with other programs.\\n\\n## FAQs\\n\\n### I only want ordinary stocks.\\n\\nI assume you mean stocks with \\"four-digit\\" codes, excluding ETFs, warrants, etc. To achieve this, simply add a filtering condition in the program:\\n\\n```python\\nif len(code) != 4:\\n    continue\\n```\\n\\n### I only want specific industries.\\n\\nThis requirement can be extended to specific market types, industry types, listing dates, or even the previous \\"ordinary stocks\\" question. We just need to load the output JSON file into Pandas and filter with conditions:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.read_json(\\"stock_infos.json\\", orient=\\"index\\")\\ntarget = df[df[\\"Industry Type\\"] == \\"Cement Industry\\"]\\n```\\n\\n### The program is broken.\\n\\nThat could be due to changes in the TWSE website\'s layout, causing the HTML structure to alter. We\'ll need to adjust the code accordingly.\\n\\n## Conclusion\\n\\nBy periodically running this program, we can obtain the latest stock information.\\n\\n:::tip\\nSince you\'re here, why not take a moment to enjoy a cup of tea before you go?\\n\\nWe also wrote an API to integrate FinMind\'s stock data. By clicking the download button below, you can retrieve the latest stock information from FinMind. The data format may differ from the one used by the stock exchange, but you probably won\'t mind.\\n\\nimport StockDownloader from \'@site/src/components/StockDownloader.js\';\\n\\n<StockDownloader lang=\\"en\\"/>\\n:::"},{"id":"windows-python-settings","metadata":{"permalink":"/en/blog/windows-python-settings","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/06-05-windows-python-settings/index.md","title":"Simple Configuration of Python Environment on Win11","description":"Setting up Python on Windows.","date":"2024-06-05T00:00:00.000Z","tags":[{"inline":true,"label":"win11","permalink":"/en/blog/tags/win-11"},{"inline":true,"label":"python","permalink":"/en/blog/tags/python"}],"readingTime":5.305,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"windows-python-settings","title":"Simple Configuration of Python Environment on Win11","authors":"Z. Yuan","image":"/en/img/2024/0605.webp","tags":["win11","python"],"description":"Setting up Python on Windows."},"unlisted":false,"prevItem":{"title":"Get All Stock Code Information from TWSE","permalink":"/en/blog/get-taiwan-all-stocks-info"},"nextItem":{"title":"LaTeX Syntax Quick Reference","permalink":"/en/blog/latex-usage"}},"content":"We recently received a task that required development on a Windows-based system.\\n\\nIt\'s been a long time! It\'s been several years since we last used Windows.\\n\\n\x3c!-- truncate --\x3e\\n\\nWe tried a few methods and finally settled on using PowerShell to set up the Python environment.\\n\\nWe initially thought about using WSL for configuration, but that would turn the environment into a Linux environment... (?)\\n\\nIf we already have a Linux environment at hand, why bother configuring one on Windows?\\n\\n## Operating Environment\\n\\nWe\'re using the virtual environment tool provided by Mac: [**Parallels Desktop**](https://www.parallels.com/products/desktop/).\\n\\nWith Parallels Desktop, we can smoothly run a Windows system on Mac, and the currently installed one is Windows 11.\\n\\n:::tip\\nThe Apple M1 chip is ARM-based, and it faced many compatibility issues when it was first launched, but it has improved a lot over the years.\\n:::\\n\\n## Installing Chocolatey\\n\\nFirst, we need to launch PowerShell.\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"70%\\"}}>\\n![PowerShell](./img/img1.jpg)\\n</figure>\\n</div>\\n\\nChocolatey is a package manager for Windows. It automates the process of installing, upgrading, and managing software using NuGet and PowerShell technologies. It\'s similar to `apt-get` or `yum` on Linux, allowing Windows users to manage software in a simple and consistent way.\\n\\nInstalling Chocolatey is relatively simple. Just run the following command in PowerShell with administrative privileges:\\n\\n```powershell\\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\'https://community.chocolatey.org/install.ps1\'))\\n```\\n\\nAfter installation, enter the following command to verify if Chocolatey was installed successfully:\\n\\n```powershell\\nchoco -v\\n```\\n\\nIf you see the version number of Chocolatey, it means the installation was successful.\\n\\n:::tip\\nHere\'s what the above command does:\\n\\n1. **Set-ExecutionPolicy Bypass -Scope Process -Force**:\\n\\n   - This command sets the PowerShell execution policy.\\n   - `Set-ExecutionPolicy` is used to change the execution policy to allow or disallow the execution of PowerShell scripts.\\n   - `Bypass` means bypassing all execution policies without any restrictions.\\n   - `-Scope Process` means applying this change only to the current PowerShell session, not affecting the entire system.\\n   - `-Force` is used to forcefully execute this operation without prompting for confirmation.\\n\\n2. **[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072**:\\n\\n   - This command sets the network security protocol.\\n   - `[System.Net.ServicePointManager]::SecurityProtocol` is used to get or set the protocol type.\\n   - `-bor 3072` adds the TLS 1.2 (3072) protocol to the existing protocols. `-bor` is a bitwise operator, indicating a bitwise OR operation, allowing multiple protocols to be enabled simultaneously.\\n\\n3. **iex ((New-Object System.Net.WebClient).DownloadString(\'https://community.chocolatey.org/install.ps1\'))**:\\n   - This command downloads and executes the Chocolatey installation script.\\n   - `iex` is shorthand for `Invoke-Expression`, used to execute the content passed to it as a string.\\n   - `New-Object System.Net.WebClient` is used to create a new WebClient object, which is used for downloading data.\\n   - `.DownloadString(\'https://community.chocolatey.org/install.ps1\')` is used to download the script content from the specified URL.\\n   - Overall, the command `iex ((New-Object System.Net.WebClient).DownloadString(\'https://community.chocolatey.org/install.ps1\'))` downloads and immediately executes the PowerShell script located at \'https://community.chocolatey.org/install.ps1\', which is responsible for installing Chocolatey.\\n\\nIn simple terms, the purpose of this command is: temporarily set the PowerShell execution policy to allow script execution, configure the network security protocol to support TLS 1.2, and then download and execute the Chocolatey installation script.\\n:::\\n\\n:::info\\n**Common Chocolatey Commands:**\\n\\n- **Install**\\n  ```powershell\\n  choco install <packageName>\\n  ```\\n- **Upgrade**\\n  ```powershell\\n  choco upgrade <packageName>\\n  ```\\n- **List installed packages**\\n  ```powershell\\n  choco list --localonly\\n  ```\\n- **Uninstall**\\n  ```powershell\\n  choco uninstall <packageName>\\n  ```\\n  :::\\n\\n## Installing git\\n\\nAfter installation, continue running PowerShell as an administrator, and execute the following command to install Git:\\n\\n```powershell\\nchoco install git -y\\n```\\n\\nAfter installation, enter the following command to verify if Git was installed successfully:\\n\\n```powershell\\ngit --version\\n# >>> git version 2.45.2.windows.1\\n```\\n\\n## Installing Python\\n\\nWe use pyenv to manage Python versions.\\n\\n:::tip\\nAlthough there are many choices on the market, such as Anaconda, Miniconda, WinPython, etc., we still chose pyenv.\\n\\nBecause we often develop on Linux, seeing pyenv feels familiar.\\n:::\\n\\nInstalling and using `pyenv` on Windows is not usually straightforward because `pyenv` is designed for Unix-like environments. However, you can use the `pyenv-win` project, which is a Windows port of `pyenv`.\\n\\nFollow these steps:\\n\\n### Step 1: Installing `pyenv-win`\\n\\n- [**Reference: pyenv-win/docs/installation.md**](https://github.com/pyenv-win/pyenv-win/blob/master/docs/installation.md)\\n\\nDownload the `pyenv-win` project:\\n\\n```powershell\\nInvoke-WebRequest -UseBasicParsing -Uri \\"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\\" -OutFile \\"./install-pyenv-win.ps1\\"; &\\"./install-pyenv-win.ps1\\"\\n```\\n\\nIf you encounter any `UnauthorizedAccess` errors as shown below, please launch Windows PowerShell with the \\"Run as Administrator\\" option and execute:\\n\\n```powershell\\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope LocalMachine\\n```\\n\\nAfter completion, rerun the installation command above.\\n\\n### Step 2: Verify Installation\\n\\n1. **Restart PowerShell**:\\n\\n   - Close and reopen the PowerShell window to apply the environment variable changes.\\n\\n2. **Check `pyenv` installation**:\\n\\n   - Enter the following command to check the `pyenv` version:\\n\\n     ```powershell\\n     pyenv --version\\n     ```\\n\\n### Step 3: Use `pyenv` to install Python versions\\n\\n1. **View available Python versions**:\\n\\n   - List all available Python versions with the following command:\\n\\n     ```powershell\\n     pyenv install --list\\n     ```\\n\\n2. **Install a specific Python version**:\\n\\n- For example, to install Python 3.10.11:\\n\\n  ```powershell\\n  pyenv install 3.10.11\\n  ```\\n\\n3. **Set a global Python version**:\\n\\n   - This step is not necessary, but if you want to use the same Python version across all shells, you can set the installed Python version as the global default:\\n\\n     ```powershell\\n     pyenv global 3.10.11\\n     ```\\n\\n4. **Verify Python installation**:\\n\\n   - Enter the following command to verify if Python was installed successfully:\\n\\n     ```powershell\\n     python --version\\n     ```\\n\\n## Installing VS Code\\n\\nFinally, we install Visual Studio Code as our development tool.\\n\\nHere, we\'ll go to the [**VS Code official website**](https://code.visualstudio.com/Download) to download the installer and then proceed with the installation.\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"70%\\"}}>\\n![VS Code](./img/img2.jpg)\\n</figure>\\n</div>\\n\\nAfter installation, we want to be able to use the `code` command directly in PowerShell to open VS Code.\\n\\nSo, we need to add the installation path of VS Code to the environment variables. Copy the following path:\\n\\n```powershell\\nC:\\\\Users\\\\your_user_name\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin\\n```\\n\\n:::tip\\nRemember to replace `your_user_name` with your username.\\n:::\\n\\n## Side Note\\n\\nActually, we find the PowerShell display screen really hard to look at.\\n\\nFor this part, we suggest using [**oh-my-posh**](https://ohmyposh.dev/) to beautify the appearance of PowerShell.\\n\\nBut this part is not the focus of this article. Interested readers can check out:\\n\\n- [**Tutorial - Customize Your Prompt with Oh My Posh for PowerShell or WSL**](https://learn.microsoft.com/en-us/windows/terminal/tutorials/custom-prompt-setup)\\n\\n## Conclusion\\n\\nThrough PowerShell, we can quickly set up the Python environment and install development tools.\\n\\nOf course, the above only covers the most basic configurations, leaving this article as a reference."},{"id":"latex-usage","metadata":{"permalink":"/en/blog/latex-usage","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/06-04-latex-usage/index.md","title":"LaTeX Syntax Quick Reference","description":"Quick reference guide for LaTeX syntax","date":"2024-06-04T00:00:00.000Z","tags":[{"inline":true,"label":"LaTeX","permalink":"/en/blog/tags/la-te-x"},{"inline":true,"label":"Math","permalink":"/en/blog/tags/math"}],"readingTime":4.625,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"latex-usage","title":"LaTeX Syntax Quick Reference","authors":"Z. Yuan","image":"/en/img/2024/0604.webp","tags":["LaTeX","Math"],"description":"Quick reference guide for LaTeX syntax"},"unlisted":false,"prevItem":{"title":"Simple Configuration of Python Environment on Win11","permalink":"/en/blog/windows-python-settings"},"nextItem":{"title":"Implementing ANLS","permalink":"/en/blog/impl-normalized-levenshtein-similarity"}},"content":"Every time you need it, it\'s always a hassle to find, so we decided to write a LaTeX syntax quick reference table.\\n\\n\x3c!-- truncate --\x3e\\n\\n| **Category**           | **Description**          |                   **Syntax**                   |                  **Display**                  |\\n| ---------------------- | ------------------------ | :--------------------------------------------: | :-------------------------------------------: |\\n| **Text Styles**        | Bold                     |                 `\\\\textbf{AB}`                  |                 $\\\\textbf{AB}$                 |\\n|                        | Italic                   |                 `\\\\textit{AB}`                  |                 $\\\\textit{AB}$                 |\\n|                        | Underline                |                `\\\\underline{AB}`                |               $\\\\underline{AB}$                |\\n|                        | Overline                 |                `\\\\overline{AB}`                 |                $\\\\overline{AB}$                |\\n|                        | Typewriter Font          |                 `\\\\texttt{AB}`                  |                 $\\\\texttt{AB}$                 |\\n| **Math Structures**    | Fraction                 |                 `\\\\frac{A}{B}`                  |                 $\\\\frac{A}{B}$                 |\\n|                        | Fraction (Display)       |                 `\\\\dfrac{a}{b}`                 |                $\\\\dfrac{a}{b}$                 |\\n|                        | Fraction (Inline)        |                 `\\\\tfrac{a}{b}`                 |                $\\\\tfrac{a}{b}$                 |\\n|                        | Old-Style Fraction       |                 `{A \\\\over B}`                  |                 $\\\\frac{A}{B}$                 |\\n|                        | Binomial Coeff.          |                 `\\\\binom{n}{k}`                 |                $\\\\binom{n}{k}$                 |\\n|                        | Old-Style Binomial       |                `{n \\\\choose k}`                 |                $\\\\binom{n}{k}$                 |\\n|                        | Square Root              |                   `\\\\sqrt{x}`                   |                  $\\\\sqrt{x}$                   |\\n|                        | n-th Root                |                 `\\\\sqrt[n]{x}`                  |                 $\\\\sqrt[n]{x}$                 |\\n|                        | Exponent                 |                     `a^b`                      |                     $a^b$                     |\\n|                        | Subscript                |                     `a_b`                      |                     $a_b$                     |\\n|                        | Integral                 |          `\\\\int_a^b x^2 \\\\mathrm{d} x`           |          $\\\\int_a^b x^2 \\\\mathrm{d} x$          |\\n|                        | Summation                |        `\\\\sum_{n=1}^\\\\infty \\\\frac{1}{n}`         |        $\\\\sum_{n=1}^\\\\infty \\\\frac{1}{n}$        |\\n|                        | Limit                    |       `\\\\lim_{x \\\\to \\\\infty} \\\\frac{1}{x}`        |       $\\\\lim_{x \\\\to \\\\infty} \\\\frac{1}{x}$       |\\n|                        | Product                  |               `\\\\prod_{i=1}^n i`                |               $\\\\prod_{i=1}^n i$               |\\n| **Math Symbols**       | Sine                     |                 `\\\\sin{\\\\theta}`                 |                $\\\\sin{\\\\theta}$                 |\\n|                        | Cosine                   |                 `\\\\cos{\\\\theta}`                 |                $\\\\cos{\\\\theta}$                 |\\n|                        | Plus-Minus               |                     `\\\\pm`                      |                     $\\\\pm$                     |\\n|                        | Multiplication           |                    `\\\\times`                    |                   $\\\\times$                    |\\n|                        | Division                 |                     `\\\\div`                     |                    $\\\\div$                     |\\n|                        | Subset Equal             |                  `\\\\subseteq`                   |                  $\\\\subseteq$                  |\\n|                        | Superset Equal           |                  `\\\\supseteq`                   |                  $\\\\supseteq`                  |\\n|                        | Implies                  |                   `\\\\implies`                   |                  $\\\\implies$                   |\\n|                        | Implied By               |                  `\\\\impliedby`                  |                 $\\\\impliedby$                  |\\n|                        | If and Only If           |                     `\\\\iff`                     |                    $\\\\iff$                     |\\n|                        | Intersection             |                     `\\\\cap`                     |                    $\\\\cap$                     |\\n|                        | Union                    |                     `\\\\cup`                     |                    $\\\\cup$                     |\\n|                        | Logical And              |                    `\\\\land`                     |                    $\\\\land$                    |\\n|                        | Logical Or               |                     `\\\\lor`                     |                    $\\\\lor$                     |\\n|                        | Logical Not              |                     `\\\\neg`                     |                    $\\\\neg$                     |\\n|                        | Not Equal To             |                     `\\\\neq`                     |                    $\\\\neq$                     |\\n|                        | Approximately Equal      |                   `\\\\approx`                    |                   $\\\\approx$                   |\\n| **Greek Letters**      | Lowercase \u03B1              |                    `\\\\alpha`                    |                   $\\\\alpha$                    |\\n|                        | Uppercase \u0391              |                    `\\\\Alpha`                    |                   $\\\\Alpha$                    |\\n|                        | Lowercase \u03B2              |                    `\\\\beta`                     |                    $\\\\beta$                    |\\n|                        | Uppercase \u0392              |                    `\\\\Beta`                     |                    $\\\\Beta$                    |\\n|                        | Lowercase \u03B3              |                    `\\\\gamma`                    |                   $\\\\gamma$                    |\\n|                        | Uppercase \u0393              |                    `\\\\Gamma`                    |                   $\\\\Gamma$                    |\\n|                        | Lowercase \u03B4              |                    `\\\\delta`                    |                   $\\\\delta$                    |\\n|                        | Uppercase \u0394              |                    `\\\\Delta`                    |                   $\\\\Delta$                    |\\n|                        | Lowercase \u03B5              |                   `\\\\epsilon`                   |                  $\\\\epsilon$                   |\\n|                        | Uppercase \u0395              |                   `\\\\Epsilon`                   |                  $\\\\Epsilon$                   |\\n|                        | Variant Lowercase \u03C6      |                   `\\\\varphi`                    |                   $\\\\varphi$                   |\\n|                        | Uppercase \u03A6              |                     `\\\\Phi`                     |                    $\\\\Phi$                     |\\n|                        | Lowercase \u03C7              |                     `\\\\chi`                     |                    $\\\\chi$                     |\\n|                        | Uppercase \u03A7              |                     `\\\\Chi`                     |                    $\\\\Chi$                     |\\n|                        | Lowercase \u03BC              |                     `\\\\mu`                      |                     $\\\\mu$                     |\\n|                        | Uppercase \u039C              |                     `\\\\Mu`                      |                     $\\\\Mu$                     |\\n|                        | Lowercase \u03C9              |                    `\\\\omega`                    |                   $\\\\omega$                    |\\n|                        | Uppercase \u03A9              |                    `\\\\Omega`                    |                   $\\\\Omega$                    |\\n| **Matrices & Vectors** | Matrix (with brackets)   | `\\\\begin{pmatrix} a & b \\\\\\\\ c & d \\\\end{pmatrix}` | $\\\\begin{pmatrix} a & b\\\\\\\\ c & d \\\\end{pmatrix}$ |\\n|                        | Matrix (no brackets)     |  `\\\\begin{matrix} x & y \\\\\\\\ z & w \\\\end{matrix}`  | $\\\\begin{matrix} x & y \\\\\\\\ z & w \\\\end{matrix}$  |\\n|                        | Vector                   |                   `\\\\vec{v}`                    |                   $\\\\vec{v}$                   |\\n|                        | Identity Matrix          |                  `\\\\mathbf{I}`                  |                 $\\\\mathbf{I}$                  |\\n|                        | Zero Matrix              |                  `\\\\mathbf{0}`                  |                 $\\\\mathbf{0}$                  |\\n| **Miscellaneous**      | Angle                    |                    `\\\\angle`                    |                   $\\\\angle$                    |\\n|                        | Triangle                 |                  `\\\\triangle`                   |                  $\\\\triangle$                  |\\n|                        | Square                   |                   `\\\\square`                    |                   $\\\\square$                   |\\n|                        | Space                    |                    `\\\\quad`                     |                    $\\\\quad$                    |\\n|                        | Proportional To          |                   `\\\\propto`                    |                   $\\\\propto$                   |\\n|                        | Because                  |                   `\\\\because`                   |                  $\\\\because$                   |\\n|                        | Therefore                |                  `\\\\therefore`                  |                 $\\\\therefore$                  |\\n|                        | Integer Set              |                  `\\\\mathbb{Z}`                  |                 $\\\\mathbb{Z}$                  |\\n|                        | Probability Set          |                  `\\\\mathbb{P}`                  |                 $\\\\mathbb{P}$                  |\\n|                        | Real Number Set          |                  `\\\\mathbb{R}`                  |                 $\\\\mathbb{R}$                  |\\n|                        | Complex Number Set       |                  `\\\\mathbb{C}`                  |                 $\\\\mathbb{C}$                  |\\n|                        | Imaginary Part           |                     `\\\\Im`                      |                     $\\\\Im$                     |\\n|                        | Real Part                |                     `\\\\Re`                      |                     $\\\\Re`                     |\\n|                        | Empty Set                |                  `\\\\emptyset`                   |                  $\\\\emptyset$                  |\\n|                        | Fancy Empty Set          |                 `\\\\varnothing`                  |                 $\\\\varnothing$                 |\\n|                        | Element Of               |                     `\\\\in`                      |                     $\\\\in$                     |\\n|                        | Not Element Of           |                   `\\\\not\\\\in`                    |                   $\\\\not\\\\in$                   |\\n|                        | Counterclockwise Arrow   |               `\\\\circlearrowleft`               |              $\\\\circlearrowleft$               |\\n|                        | Clockwise Arrow          |              `\\\\circlearrowright`               |              $\\\\circlearrowright$              |\\n|                        | Planck\'s Constant        |                    `\\\\hbar`                     |                    $\\\\hbar$                    |\\n|                        | Natural Log              |                     `\\\\ln`                      |                     $\\\\ln$                     |\\n|                        | Constant $\\\\pi$           |                     `\\\\pi`                      |                     $\\\\pi$                     |\\n| **Formatting**         | Function Color           |         `f(x) = a{\\\\color{red}{x}} + b`         |        $f(x) = a{\\\\color{red}{x}} + b$         |\\n|                        | Color Box                |        `\\\\colorbox{#eeeeee}{Color Box}`         |        $\\\\colorbox{#eeeeee}{Color Box}$        |\\n|                        | Calligraphic Font        |            `{\\\\cal ABCDE12345abced}`            |           ${\\\\cal ABCDE12345abced}$            |\\n|                        | Framed Text              |              `\\\\fbox{boxed text}`               |              $\\\\fbox{boxed text}$              |\\n|                        | Boxed Display Style Text |             `\\\\boxed{boxed\\\\ text}`              |             $\\\\boxed{boxed\\\\ text}$             |\\n|                        | Fraktur Font             |           `{\\\\frak ABCDE12345abcde}`            |           ${\\\\frak ABCDE12345abcde}$           |\\n|                        | Italic Font              |        `{\\\\it abefg12345}\\\\ abcdefg12345`        |       ${\\\\it abefg12345}\\\\ abcdefg12345$        |\\n|                        | Minimum                  |               `\\\\min\\\\limits_{n}`                |               $\\\\min\\\\limits_{n}$               |\\n|                        | Bold Greek Letter        |             `\\\\boldsymbol{\\\\alpha}`              |             $\\\\boldsymbol{\\\\alpha}$             |"},{"id":"impl-normalized-levenshtein-similarity","metadata":{"permalink":"/en/blog/impl-normalized-levenshtein-similarity","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/05-16-impl-normalized-levenshtein-similarity/index.md","title":"Implementing ANLS","description":"Average Normalized Levenshtein Similarity","date":"2024-05-16T00:00:00.000Z","tags":[{"inline":true,"label":"pytorch","permalink":"/en/blog/tags/pytorch"},{"inline":true,"label":"anls","permalink":"/en/blog/tags/anls"}],"readingTime":5.72,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"impl-normalized-levenshtein-similarity","title":"Implementing ANLS","authors":"Z. Yuan","image":"/en/img/2024/0516.webp","tags":["pytorch","anls"],"description":"Average Normalized Levenshtein Similarity"},"unlisted":false,"prevItem":{"title":"LaTeX Syntax Quick Reference","permalink":"/en/blog/latex-usage"},"nextItem":{"title":"Equivalent Basic Commands between Python and JS","permalink":"/en/blog/python-js-basic-command-equivalents"}},"content":"Average Normalized Levenshtein Similarity, abbreviated as ANLS, is a metric used to compute the similarity between two strings.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn natural language processing (NLP), it\'s often necessary to compare the similarity of two strings.\\n\\nLevenshtein Similarity is a common measure that assesses the \\"edit distance\\" between two strings, which is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other. However, Levenshtein Similarity itself isn\'t intuitive as it depends on the lengths of the strings.\\n\\nTo address this issue, we can normalize Levenshtein Similarity to the [0, 1] range, making it easier to understand and compare the similarity between different strings, known as Normalized Levenshtein Similarity (NLS).\\n\\nAs NLS refers to the similarity between sets of strings, we can further extend it to ANLS, which computes the average similarity among multiple sets of strings, thereby quantifying the performance of a model.\\n\\nAnd then...\\n\\nWe always struggle to find implementations, so we decided to write one ourself.\\n\\n## References\\n\\n- [**torchmetrics.text.EditDistance**](https://lightning.ai/docs/torchmetrics/stable/text/edit.html)\\n\\n## Import Necessary Libraries\\n\\nFirst, we need to import some necessary libraries, especially the `EditDistance` implemented by `torchmetrics`:\\n\\n```python\\nfrom typing import Any, Literal, Optional, Sequence, Union\\n\\nimport torch\\nfrom torch import Tensor\\nfrom torchmetrics.metric import Metric\\nfrom torchmetrics.text import EditDistance\\nfrom torchmetrics.utilities.data import dim_zero_cat\\n```\\n\\nSince `EditDistance` can already compute the Levenshtein distance, we can directly use it to calculate the edit distance between two strings. However, `EditDistance` doesn\'t provide normalization functionality, so we need to implement this part ourselves.\\n\\n## Implement Normalization Functionality\\n\\nHere, we inherit the interface of `torchmetrics.metric.Metric`, so we need to implement the `update` and `compute` methods:\\n\\n```python\\nclass NormalizedLevenshteinSimilarity(Metric):\\n\\n    def __init__(\\n        self,\\n        substitution_cost: int = 1,\\n        reduction: Optional[Literal[\\"mean\\", \\"sum\\", \\"none\\"]] = \\"mean\\",\\n        **kwargs: Any\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.edit_distance = EditDistance(\\n            substitution_cost=substitution_cost,\\n            reduction=None  # Set to None to get distances for all string pairs\\n        )\\n\\n        # ...\\n```\\n\\nHere are a few key points:\\n\\n1. Ensure that the input `preds` and `target` are lists of strings, otherwise the function will calculate on a character level.\\n2. Calculate the maximum length of each string, so that we can perform normalization.\\n\\n```python\\ndef update(self, preds: Union[str, Sequence[str]], target: Union[str, Sequence[str]]) -> None:\\n    \\"\\"\\"Update state with predictions and targets.\\"\\"\\"\\n\\n    if isinstance(preds, str):\\n        preds = [preds]\\n    if isinstance(target, str):\\n        target = [target]\\n\\n    distances = self.edit_distance(preds, target)\\n    max_lengths = torch.tensor([\\n        max(len(p), len(t))\\n        for p, t in zip(preds, target)\\n    ], dtype=torch.float)\\n\\n    ratio = torch.where(\\n        max_lengths == 0,\\n        torch.zeros_like(distances).float(),\\n        distances.float() / max_lengths\\n    )\\n\\n    nls_values = 1 - ratio\\n\\n    # ...\\n```\\n\\n## Implement the `reduction` Parameter\\n\\nWe also need to accommodate the `reduction` parameter, where if we specify `mean`, it will be the common ANLS score.\\n\\nIn addition to the usual `mean`, we can also use `sum` or `none` to fulfill different needs.\\n\\n```python\\ndef _compute(\\n    self,\\n    nls_score: Tensor,\\n    num_elements: Union[Tensor, int],\\n) -> Tensor:\\n    \\"\\"\\"Compute the ANLS over state.\\"\\"\\"\\n    if nls_score.numel() == 0:\\n        return torch.tensor(0, dtype=torch.int32)\\n    if self.reduction == \\"mean\\":\\n        return nls_score.sum() / num_elements\\n    if self.reduction == \\"sum\\":\\n        return nls_score.sum()\\n    if self.reduction is None or self.reduction == \\"none\\":\\n        return nls_score\\n\\ndef compute(self) -> torch.Tensor:\\n    \\"\\"\\"Compute the NLS over state.\\"\\"\\"\\n    if self.reduction == \\"none\\" or self.reduction is None:\\n        return self._compute(dim_zero_cat(self.nls_values_list), 1)\\n    return self._compute(self.nls_score, self.num_elements)\\n```\\n\\nHere, it\'s noteworthy that when we specify `reduction` as `none`, we need to return all NLS values instead of computing the average. In this case, I referenced the implementation of `torchmetrics.text.EditDistance`, using `dim_zero_cat` to concatenate values in the list together, ensuring that the return value is a `Tensor`.\\n\\n## Implementation\\n\\nThe complete implementation is as follows:\\n\\n```python\\nfrom typing import Any, Literal, Optional, Sequence, Union\\n\\nimport torch\\nfrom torch import Tensor\\nfrom torchmetrics.metric import Metric\\nfrom torchmetrics.text import EditDistance\\nfrom torchmetrics.utilities.data import dim_zero_cat\\n\\n\\nclass NormalizedLevenshteinSimilarity(Metric):\\n    \\"\\"\\"\\n    Normalized Levenshtein Similarity (NLS) is a metric that computes the\\n    normalized Levenshtein similarity between two sequences.\\n    This metric is calculated as 1 - (levenshtein_distance / max_length),\\n    where `levenshtein_distance` is the Levenshtein distance between the two\\n    sequences and `max_length` is the maximum length of the two sequences.\\n\\n    NLS aims to provide a similarity measure for character sequences\\n    (such as text), making it useful in areas like text similarity analysis,\\n    Optical Character Recognition (OCR), and Natural Language Processing (NLP).\\n\\n    This class inherits from `Metric` and uses the `EditDistance` class to\\n    compute the Levenshtein distance.\\n\\n    Inputs to the ``update`` and ``compute`` methods are as follows:\\n\\n    - ``preds`` (:class:`~Union[str, Sequence[str]]`):\\n        Predicted text sequences or a collection of sequences.\\n    - ``target`` (:class:`~Union[str, Sequence[str]]`):\\n        Target text sequences or a collection of sequences.\\n\\n    Output from the ``compute`` method is as follows:\\n\\n    - ``nls`` (:class:`~torch.Tensor`): A tensor containing the NLS value.\\n        Returns 0.0 when there are no samples; otherwise, it returns the NLS.\\n\\n    Args:\\n        substitution_cost:\\n            The cost of substituting one character for another. Default is 1.\\n        reduction:\\n            Method to aggregate metric scores.\\n            Default is \'mean\', options are \'sum\' or None.\\n\\n            - ``\'mean\'``: takes the mean over samples, which is ANLS.\\n            - ``\'sum\'``: takes the sum over samples\\n            - ``None`` or ``\'none\'``: returns the score per sample\\n\\n        kwargs: Additional keyword arguments.\\n\\n    Example::\\n        Multiple strings example:\\n\\n        >>> metric = NormalizedLevenshteinSimilarity(reduction=None)\\n        >>> preds = [\\"rain\\", \\"lnaguaeg\\"]\\n        >>> target = [\\"shine\\", \\"language\\"]\\n        >>> metric(preds, target)\\n        tensor([0.4000, 0.5000])\\n        >>> metric = NormalizedLevenshteinSimilarity(reduction=\\"mean\\")\\n        >>> metric(preds, target)\\n        tensor(0.4500)\\n    \\"\\"\\"\\n\\n    def __init__(\\n        self,\\n        substitution_cost: int = 1,\\n        reduction: Optional[Literal[\\"mean\\", \\"sum\\", \\"none\\"]] = \\"mean\\",\\n        **kwargs: Any\\n    ) -> None:\\n        super().__init__(**kwargs)\\n        self.edit_distance = EditDistance(\\n            substitution_cost=substitution_cost,\\n            reduction=None  # Set to None to get distances for all string pairs\\n        )\\n\\n        allowed_reduction = (None, \\"mean\\", \\"sum\\", \\"none\\")\\n        if reduction not in allowed_reduction:\\n            raise ValueError(\\n                f\\"Expected argument `reduction` to be one of {allowed_reduction}, but got {reduction}\\")\\n        self.reduction = reduction\\n\\n        if self.reduction == \\"none\\" or self.reduction is None:\\n            self.add_state(\\n                \\"nls_values_list\\",\\n                default=[],\\n                dist_reduce_fx=\\"cat\\"\\n            )\\n        else:\\n            self.add_state(\\n                \\"nls_score\\",\\n                default=torch.tensor(0.0),\\n                dist_reduce_fx=\\"sum\\"\\n            )\\n            self.add_state(\\n                \\"num_elements\\",\\n                default=torch.tensor(0),\\n                dist_reduce_fx=\\"sum\\"\\n            )\\n\\n    def update(self, preds: Union[str, Sequence[str]], target: Union[str, Sequence[str]]) -> None:\\n        \\"\\"\\"Update state with predictions and targets.\\"\\"\\"\\n        if isinstance(preds, str):\\n            preds = [preds]\\n        if isinstance(target, str):\\n            target = [target]\\n\\n        distances = self.edit_distance(preds, target)\\n        max_lengths = torch.tensor([\\n            max(len(p), len(t))\\n            for p, t in zip(preds, target)\\n        ], dtype=torch.float)\\n\\n        ratio = torch.where(\\n            max_lengths == 0,\\n            torch.zeros_like(distances).float(),\\n            distances.float() / max_lengths\\n        )\\n\\n        nls_values = 1 - ratio\\n\\n        if self.reduction == \\"none\\" or self.reduction is None:\\n            self.nls_values_list.append(nls_values)\\n        else:\\n            self.nls_score += nls_values.sum()\\n            self.num_elements += nls_values.shape[0]\\n\\n    def _compute(\\n        self,\\n        nls_score: Tensor,\\n        num_elements: Union[Tensor, int],\\n    ) -> Tensor:\\n        \\"\\"\\"Compute the ANLS over state.\\"\\"\\"\\n        if nls_score.numel() == 0:\\n            return torch.tensor(0, dtype=torch.int32)\\n        if self.reduction == \\"mean\\":\\n            return nls_score.sum() / num_elements\\n        if self.reduction == \\"sum\\":\\n            return nls_score.sum()\\n        if self.reduction is None or self.reduction == \\"none\\":\\n            return nls_score\\n\\n    def compute(self) -> torch.Tensor:\\n        \\"\\"\\"Compute the NLS over state.\\"\\"\\"\\n        if self.reduction == \\"none\\" or self.reduction is None:\\n            return self._compute(dim_zero_cat(self.nls_values_list), 1)\\n        return self._compute(self.nls_score, self.num_elements)\\n\\n\\nif __name__ == \\"__main__\\":\\n    anls = NormalizedLevenshteinSimilarity(reduction=\'mean\')\\n    preds = [\\"rain\\", \\"lnaguaeg\\"]\\n    target = [\\"shine\\", \\"language\\"]\\n    print(anls(preds, target))\\n```\\n\\n## At Last\\n\\nCan we guarantee that this implementation is correct?\\n\\nThe answer is no. If you find any issues, please let us know. Thank you very much!"},{"id":"python-js-basic-command-equivalents","metadata":{"permalink":"/en/blog/python-js-basic-command-equivalents","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/04-07-cmd-js-vs-python/index.md","title":"Equivalent Basic Commands between Python and JS","description":"Mapping basic cmds between Py and JS.","date":"2024-04-07T00:00:00.000Z","tags":[{"inline":true,"label":"npm","permalink":"/en/blog/tags/npm"},{"inline":true,"label":"pip","permalink":"/en/blog/tags/pip"}],"readingTime":2.715,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"python-js-basic-command-equivalents","title":"Equivalent Basic Commands between Python and JS","authors":"Z. Yuan","image":"/en/img/2024/0407.webp","tags":["npm","pip"],"description":"Mapping basic cmds between Py and JS."},"unlisted":false,"prevItem":{"title":"Implementing ANLS","permalink":"/en/blog/impl-normalized-levenshtein-similarity"},"nextItem":{"title":"Common VSCode Configuration Settings","permalink":"/en/blog/vscode-settings"}},"content":"As engineers, we are destined to keep learning new technologies.\\n\\nWhile we are more accustomed to Python, we discovered some similar commands when starting to learn JavaScript, such as `npm`, `npx`, and `nvm`.\\n\\nSo we tried to map these commands, hoping it might ease the transition into new skills.\\n\\n\x3c!-- truncate --\x3e\\n\\nI\'m interested in aligning these commands, hoping it might ease the transition into new skills.\\n\\n## npm vs. pip\\n\\n:::tip\\n**They are both: Package Managers**\\n:::\\n\\nnpm (Node Package Manager) and pip essentially serve the same purpose: they are package managers for Node.js and Python respectively. Package managers are crucial for sharing, reusing, and managing repositories or modules.\\n\\n- **Installing Packages**: In npm, we use the `npm install <package-name>` command to add a library to our project. Similarly, pip achieves the same goal through `pip install <package-name>`.\\n- **Version Control**: npm tracks package versions through the `package.json` file, ensuring that every member of a development team uses the same version of a library. Pip relies on `requirements.txt` or newer tools like pipenv and poetry to achieve similar functionality.\\n- **Package Publishing**: npm enables developers to publish their packages to the npm registry for use by the global Node.js community. Pip provides this capability through PyPI (Python Package Index), allowing the sharing of Python packages.\\n\\n## npx vs. -m Flag\\n\\n:::tip\\n**They are both: Tools for Direct Command Execution**\\n:::\\n\\nnpx (npm package runner) and Python\'s `-m` flag address the need to execute package commands directly in the terminal without global installation.\\n\\n- **Direct Execution**: npx allows you to directly execute any package installed in the project\'s local `node_modules` folder (or fetch it from the npm registry if not installed), while Python achieves similar results through the `-m` flag, allowing direct execution of modules, such as starting a simple HTTP server with `python -m http.server`.\\n\\n:::note\\n**npm run vs. npx run**\\n\\n- npm run: In JavaScript projects, npm run is used to execute scripts defined in the package.json file. This is a common approach to perform project-specific tasks like testing, building, or deploying.\\n- npx run: While npx is typically used to execute single commands or packages, it primarily serves to execute packages not globally installed. npx run is not a standard command; common usage of npx doesn\'t include the keyword \\"run\\" but directly follows with the package name or command.\\n  :::\\n\\n## nvm, pyenv, and conda\\n\\n:::tip\\n**They are all: Version Management Tools**\\n:::\\n\\nSwitching between different versions of Node.js or Python can be cumbersome without proper tools. nvm (Node Version Manager), pyenv, and conda provide solutions to this problem, allowing developers to install and switch between multiple versions of Node.js or Python on the same machine.\\n\\n- **Version Switching**: nvm uses commands like `nvm use <version>` to switch Node.js versions. Pyenv and conda offer similar functionalities for Python; pyenv switches versions through `pyenv global <version>` or `pyenv local <version>`, while conda uses `conda activate <environment-name>` to switch to different environments, each capable of having different Python versions and packages.\\n- **Multi-Version Management**: These tools facilitate managing multiple versions on the same machine, addressing potential conflicts due to version discrepancies.\\n\\n## Conclusion\\n\\nWhile these commands may not align perfectly, they do share some similarities, hopefully making the transition a bit smoother."},{"id":"vscode-settings","metadata":{"permalink":"/en/blog/vscode-settings","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/03-31-vscode-settings/index.md","title":"Common VSCode Configuration Settings","description":"VSCode settings for future reference.","date":"2024-03-31T00:00:00.000Z","tags":[{"inline":true,"label":"vscode","permalink":"/en/blog/tags/vscode"},{"inline":true,"label":"settings","permalink":"/en/blog/tags/settings"}],"readingTime":2.68,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"vscode-settings","title":"Common VSCode Configuration Settings","authors":"Z. Yuan","tags":["vscode","settings"],"image":"/en/img/2024/0331.webp","description":"VSCode settings for future reference."},"unlisted":false,"prevItem":{"title":"Equivalent Basic Commands between Python and JS","permalink":"/en/blog/python-js-basic-command-equivalents"},"nextItem":{"title":"Setting Up Nextcloud: A Guide","permalink":"/en/blog/setting-up-nextcloud"}},"content":"A while ago, due to unknown reasons, the VSCode configuration file disappeared, and we had to reconfigure it, which took us some time.\\n\\nIt also provided an opportunity to review our own configuration files and record some commonly used configuration files.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Configuration Settings\\n\\n```json\\n{\\n  \\"editor.fontFamily\\": \\"Fira Code, MesloLGS NF\\",\\n  \\"editor.fontLigatures\\": true,\\n  \\"files.associations\\": {\\n    \\"Dockerfile_base\\": \\"dockerfile\\"\\n  },\\n  \\"markdown.preview.fontSize\\": 15,\\n  \\"debug.console.fontSize\\": 14,\\n  \\"explorer.confirmDragAndDrop\\": true,\\n  \\"editor.minimap.enabled\\": true,\\n  \\"editor.minimap.maxColumn\\": 80,\\n  \\"editor.smoothScrolling\\": true,\\n  \\"editor.rulers\\": [80, 120],\\n  \\"workbench.colorCustomizations\\": {\\n    \\"editorRuler.foreground\\": \\"#ff4081\\",\\n    \\"minimap.background\\": \\"#00000050\\",\\n    \\"editor.background\\": \\"#1e1e1e\\",\\n    \\"editor.foreground\\": \\"#d4d4d4\\"\\n  },\\n  \\"terminal.integrated.fontFamily\\": \\"Fira Code, MesloLGS NF\\",\\n  \\"files.trimTrailingWhitespace\\": true,\\n  \\"files.trimFinalNewlines\\": true,\\n  \\"diffEditor.ignoreTrimWhitespace\\": true,\\n  \\"python.terminal.activateEnvironment\\": true,\\n  \\"git.ignoreLegacyWarning\\": true,\\n  \\"git.autofetch\\": true,\\n  \\"editor.largeFileOptimizations\\": false,\\n  \\"editor.mouseWheelZoom\\": true,\\n  \\"editor.codeActionsOnSave\\": {\\n    \\"source.organizeImports\\": true,\\n    \\"source.fixAll\\": true\\n  },\\n  \\"editor.formatOnSave\\": true,\\n  \\"workbench.editorAssociations\\": {\\n    \\"*.ipynb\\": \\"jupyter-notebook\\"\\n  },\\n  \\"debug.onTaskErrors\\": \\"abort\\",\\n  \\"explorer.confirmDelete\\": true,\\n  \\"terminal.integrated.copyOnSelection\\": true,\\n  \\"terminal.integrated.cursorBlinking\\": true,\\n  \\"terminal.integrated.cursorStyle\\": \\"line\\",\\n  \\"remote.downloadExtensionsLocally\\": true,\\n  \\"terminal.integrated.scrollback\\": 10000,\\n  \\"editor.cursorStyle\\": \\"line\\",\\n  \\"editor.insertSpaces\\": true,\\n  \\"editor.lineNumbers\\": \\"on\\",\\n  \\"editor.wordWrap\\": \\"on\\",\\n  \\"workbench.editor.wrapTabs\\": false,\\n  \\"files.watcherExclude\\": {\\n    \\"**/.git/objects/**\\": true,\\n    \\"**/.git/subtree-cache/**\\": true,\\n    \\"**/node_modules/*/**\\": true\\n  },\\n  \\"notebook.cellToolbarLocation\\": {\\n    \\"default\\": \\"right\\",\\n    \\"jupyter-notebook\\": \\"left\\"\\n  },\\n  \\"github.copilot.editor.enableAutoCompletions\\": true,\\n  \\"github.copilot.enable\\": {\\n    \\"*\\": true,\\n    \\"plaintext\\": false,\\n    \\"markdown\\": true,\\n    \\"scminput\\": false\\n  },\\n  \\"workbench.colorTheme\\": \\"Monokai Pro\\",\\n  \\"editor.multiCursorModifier\\": \\"ctrlCmd\\",\\n  \\"editor.wordWrapColumn\\": 120,\\n  \\"files.autoSave\\": \\"onFocusChange\\"\\n}\\n```\\n\\n## Parameter Descriptions\\n\\n- `editor.fontFamily`: Sets the font family; here, Fira Code and MesloLGS NF fonts are used.\\n- `editor.fontLigatures`: Sets whether ligatures in the font are enabled.\\n- `files.associations`: Sets file associations; here, Dockerfile_base is associated with dockerfile.\\n- `markdown.preview.fontSize`: Sets the font size for markdown preview.\\n- `debug.console.fontSize`: Sets the font size for the debug console.\\n- `explorer.confirmDragAndDrop`: Sets whether drag and drop confirmation is enabled.\\n- `editor.minimap.enabled`: Sets whether the minimap is enabled.\\n- `editor.minimap.maxColumn`: Sets the maximum number of columns in the minimap.\\n- `editor.smoothScrolling`: Sets whether smooth scrolling is enabled.\\n- `editor.rulers`: Sets the column numbers for indentation guides.\\n- `workbench.colorCustomizations`: Sets color customizations.\\n- `terminal.integrated.fontFamily`: Sets the terminal font family.\\n- `files.trimTrailingWhitespace`: Sets whether trailing whitespace is trimmed.\\n- `files.trimFinalNewlines`: Sets whether final newlines are trimmed.\\n- `diffEditor.ignoreTrimWhitespace`: Sets whether whitespace in diffs is ignored.\\n- `python.terminal.activateEnvironment`: Sets whether the Python environment is activated.\\n- `git.ignoreLegacyWarning`: Sets whether Git warnings are ignored.\\n- `git.autofetch`: Sets whether autofetch is enabled.\\n- `editor.largeFileOptimizations`: Sets whether optimizations for large files are enabled.\\n- `editor.mouseWheelZoom`: Sets whether mouse wheel zoom is enabled.\\n- `editor.codeActionsOnSave`: Sets code actions on save.\\n- `editor.formatOnSave`: Sets whether formatting is applied on save.\\n- `workbench.editorAssociations`: Sets editor associations.\\n- `debug.onTaskErrors`: Sets action on task errors.\\n- `explorer.confirmDelete`: Sets whether delete confirmation is enabled.\\n- `terminal.integrated.copyOnSelection`: Sets whether copying is done on selection.\\n- `terminal.integrated.cursorBlinking`: Sets terminal cursor blinking.\\n- `terminal.integrated.cursorStyle`: Sets terminal cursor style.\\n- `remote.downloadExtensionsLocally`: Sets whether extensions are downloaded locally.\\n- `terminal.integrated.scrollback`: Sets terminal scrollback buffer size.\\n- `editor.cursorStyle`: Sets cursor style.\\n- `editor.insertSpaces`: Sets whether spaces are inserted.\\n- `editor.lineNumbers`: Sets whether line numbers are displayed.\\n- `editor.wordWrap`: Sets whether word wrap is enabled.\\n- `workbench.editor.wrapTabs`: Sets whether tab wrapping is enabled.\\n- `files.watcherExclude`: Sets file watcher exclusion patterns.\\n- `notebook.cellToolbarLocation`: Sets notebook cell toolbar location.\\n- `github.copilot.editor.enableAutoCompletions`: Sets whether auto completions are enabled in GitHub Copilot.\\n- `github.copilot.enable`: Sets whether GitHub Copilot is enabled.\\n- `workbench.colorTheme`: Sets the color theme.\\n- `editor.multiCursorModifier`: Sets the multi-cursor modifier.\\n- `editor.wordWrapColumn`: Sets the column for word wrapping.\\n- `files.autoSave`: Sets auto-save behavior."},{"id":"setting-up-nextcloud","metadata":{"permalink":"/en/blog/setting-up-nextcloud","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/03-04-setup-nextcloud/index.md","title":"Setting Up Nextcloud: A Guide","description":"Documenting the process of setting up Nextcloud on Ubuntu 22.04.","date":"2024-03-04T00:00:00.000Z","tags":[{"inline":true,"label":"Nextcloud","permalink":"/en/blog/tags/nextcloud"},{"inline":true,"label":"Docker","permalink":"/en/blog/tags/docker"}],"readingTime":5.77,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"setting-up-nextcloud","title":"Setting Up Nextcloud: A Guide","authors":"Z. Yuan","tags":["Nextcloud","Docker"],"image":"/en/img/2024/0304.webp","description":"Documenting the process of setting up Nextcloud on Ubuntu 22.04."},"unlisted":false,"prevItem":{"title":"Common VSCode Configuration Settings","permalink":"/en/blog/vscode-settings"},"nextItem":{"title":"The PyTorch List Trap","permalink":"/en/blog/pytorch-training-out-of-memory"}},"content":"I used to store my files on Google Drive and download them using the `wget` command.\\n\\nUntil one day, Google made a small update, and suddenly, the original download command no longer worked...\\n\\nVery frustrating.\\n\\nSince that\'s the case, I decided to try Nextcloud. Here\u2019s how I set it up on Ubuntu 22.04.\\n\\n\x3c!-- truncate --\x3e\\n\\n:::tip\\nBefore you begin, make sure you have a domain name and point it to your server.\\n\\nIf you\'re unsure how to do this, just ask ChatGPT, and it will guide you through the process.\\n:::\\n\\n## Installing Nextcloud\\n\\n**First Question: Why use Nextcloud?**\\n\\n- I want a private cloud and don\u2019t want to store my files on someone else\'s server.\\n\\n**Second Question: What\u2019s the difference between Nextcloud and Owncloud?**\\n\\n- Nextcloud was forked from Owncloud by its original developers. The functionalities are quite similar, but Nextcloud develops faster.\\n\\n**Third Question: How do you install Nextcloud?**\\n\\n- This is a bit more complicated because there are many installation methods, each with its pros and cons.\\n- In this article, the only installation method I recommend is using Docker.\\n\\n## Setting Up Nextcloud All-in-One\\n\\n- Refer to the official documentation: [**Nextcloud All-in-One**](https://github.com/nextcloud/all-in-one)\\n\\n**First, make sure you have Docker and Docker Compose installed.**\\n\\nNext, create a directory for Nextcloud and write a `docker-compose.yml` configuration file:\\n\\n```bash\\nmkdir nextcloud\\nvim nextcloud/docker-compose.yml\\n```\\n\\nCopy the following content into the `docker-compose.yml`:\\n\\n```yaml\\nservices:\\n  nextcloud-aio-mastercontainer:\\n    image: nextcloud/all-in-one:latest  # Specify the Docker container image to use\\n    init: true  # Prevent zombie processes; keep this option\\n    restart: always  # Set the container restart policy to auto-restart with Docker daemon\\n    container_name: nextcloud-aio-mastercontainer  # Set the container name; do not change to avoid update issues\\n    volumes:\\n      - nextcloud_aio_mastercontainer:/mnt/docker-aio-config  # Storage location for mastercontainer; this setting cannot be changed\\n      - /var/run/docker.sock:/var/run/docker.sock:ro  # Mount Docker socket to control other containers and functionalities; adjust for Windows/macOS or rootless mode\\n    ports:\\n      - 80:80    # For obtaining valid certificates via the AIO interface, optional\\n      - 8080:8080  # Default AIO interface (self-signed certificate); if port 8080 on host is occupied, change it (e.g., 8081:8080)\\n      - 8443:8443  # For accessing the AIO interface and obtaining valid certificates; optional\\n\\nvolumes:\\n  nextcloud_aio_mastercontainer:\\n    name: nextcloud_aio_mastercontainer  # Docker volume name; this setting cannot be changed\\n```\\n\\nFor more detailed settings, refer to the official documentation: [**compose.yaml**](https://github.com/nextcloud/all-in-one/blob/main/compose.yaml)\\n\\n## Configuring System Services\\n\\nAfter completing the above settings, configure the system service.\\n\\n```bash\\nsudo vim /etc/systemd/system/nextcloud.service\\n```\\n\\nPaste the following content:\\n\\n```bash {7}\\n[Unit]\\nDescription=NextCloud Docker Compose\\nRequires=docker.service\\nAfter=docker.service\\n\\n[Service]\\nWorkingDirectory=/home/[YourName]/nextcloud\\nExecStart=/usr/bin/docker compose up --remove-orphans\\nExecStop=/usr/bin/docker compose down\\nRestart=always\\n\\n[Install]\\nWantedBy=multi-user.target\\n```\\n\\nMake sure to replace `[YourName]` with your actual username.\\n\\n## Starting Nextcloud\\n\\n```bash\\nsudo systemctl enable nextcloud\\nsudo systemctl start nextcloud\\n```\\n\\n## Setting Up Nextcloud\\n\\n1. **Access the Nextcloud AIO Interface**:\\n\\n   After the initial startup, visit `https://ip.address.of.this.server:8080` to access the Nextcloud AIO interface, replacing `ip.address.of.this.server` with your server\u2019s IP address. Ensure Docker and Nextcloud AIO are correctly installed and running; the first startup may take a few minutes.\\n\\n   It\'s recommended to access the 8080 port using the IP address instead of the domain, as HTTP Strict Transport Security (HSTS) might restrict domain access. HSTS requires the browser to connect to the site only over HTTPS.\\n\\n2. **Using the Self-Signed Certificate**:\\n\\n   When accessed via port 8080, the system may use a self-signed certificate for secure communication.\\n\\n   Since this certificate isn\u2019t issued by a trusted certificate authority (CA), the browser may show an untrusted warning. You\u2019ll need to manually accept it as directed by the browser. It\u2019s advisable to use this only in testing environments. Self-signed certificates are not suitable for production environments.\\n\\n3. **Automating Valid Certificate Retrieval**:\\n\\n   If your firewall or router has opened or correctly forwarded ports 80 and 8443, and you have pointed a domain to the server, you can automatically obtain a valid certificate from a trusted CA (like Let\'s Encrypt) via `https://your-domain-that-points-to-this-server.tld:8443`, which improves security and convenience.\\n\\n   Make sure to replace `your-domain-that-points-to-this-server.tld` with the correct domain and ensure that DNS settings are effective, checking that the firewall allows the connection.\\n\\n4. **Open Ports for Nextcloud Talk**:\\n\\n   To ensure Nextcloud Talk (for video calls and messaging) works properly, you need to open ports 3478/TCP and 3478/UDP in the firewall or router for the Talk container.\\n\\n   If you\'re in a NAT environment, verify that port forwarding is correctly configured and check with your ISP to ensure they haven\u2019t blocked these UDP ports.\\n\\n## Common Issues\\n\\n1. **Dynamic IP at Home, How to Point to Domain?**\\n\\n    Besides using dynamic DNS solutions like No-IP, I found that directly applying for a fixed IP from Chunghwa Telecom was the fastest and most stable solution.\\n\\n    :::tip\\n    If you\'re not a reader from Taiwan, you might want to ask your country\'s telecom provider if they offer this service.\\n    :::\\n\\n2. **I Don\u2019t Want to Use Docker, Are There Alternatives?**\\n\\n    Yes, you can install Nextcloud directly, but you\'ll have to manage all dependencies and environment configurations yourself, which can lead to many challenges.\\n\\n    After being stuck multiple times, I ended up going back to the Docker method, so why not use Docker from the start?\\n\\n3. **Why Can\u2019t I Connect After Setup?**\\n\\n    Check if your firewall settings allow the necessary connections. If the firewall is correctly configured, there may be issues with port forwarding on your router. It\u2019s recommended to check Docker logs for more detailed error information.\\n\\n## Final Thoughts\\n\\nAfter entering the setup URL, you\u2019ll be greeted by a more advanced setup interface.\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"70%\\"}}>\\n![login_1](./img/login_1.jpg)\\n</figure>\\n</div>\\n\\nAt this point, you may be surprised to find:\\n\\n- **I don\u2019t have a password!**\\n\\nThe system generates a password on first login, but you may have overlooked it.\\n\\nIf you forgot it, don\u2019t worry, you can query it with the following command:\\n\\n```bash\\nsudo grep password /var/lib/docker/volumes/nextcloud_aio_mastercontainer/_data/data/configuration.json\\n```\\n\\nAfter logging in, you will see the following setup screen:\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"70%\\"}}>\\n![login_2](./img/login_2.jpg)\\n</figure>\\n</div>\\n\\nThis screen indicates that the setup is complete.\\n\\nFor the first login, input the domain you previously prepared, and the system will download the necessary Docker images and restart automatically. After the startup is complete, you can begin using Nextcloud and it\u2019s recommended to change the default password and check other security settings as soon as possible.\\n\\n## Conclusion\\n\\nAfter completing the steps above, input your domain in the URL bar, and you\u2019ll see a nice interface, which is your private cloud.\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"70%\\"}}>\\n![login_3](./img/login_3.jpg)\\n</figure>\\n</div>\\n\\nThis interface has many features\u2014you can manage your files through it, and also share them.\\n\\nAdditionally, you can download the Nextcloud app on your phone to manage your files directly from your device.\\n\\nWith Nextcloud, you no longer need to worry about Google Drive\u2019s storage limitations.\\n\\n:::tip\\nIf you have other services running on your server, you can use Nginx as a reverse proxy to forward the Nextcloud domain.\\n\\nThis is beyond the scope of this chapter, but we can discuss it later.\\n:::"},{"id":"pytorch-training-out-of-memory","metadata":{"permalink":"/en/blog/pytorch-training-out-of-memory","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/02-20-pytorch-training-oom/index.md","title":"The PyTorch List Trap","description":"Discovering and solving PyTorch OOM issues.","date":"2024-02-20T00:00:00.000Z","tags":[{"inline":true,"label":"PyTorch","permalink":"/en/blog/tags/py-torch"},{"inline":true,"label":"OOM","permalink":"/en/blog/tags/oom"}],"readingTime":1.78,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"pytorch-training-out-of-memory","title":"The PyTorch List Trap","authors":"Z. Yuan","tags":["PyTorch","OOM"],"image":"/en/img/2024/0220.webp","description":"Discovering and solving PyTorch OOM issues."},"unlisted":false,"prevItem":{"title":"Setting Up Nextcloud: A Guide","permalink":"/en/blog/setting-up-nextcloud"},"nextItem":{"title":"Convert PDF to Images with Python","permalink":"/en/blog/convert-pdf-to-images"}},"content":"As a professional PyTorch user, you should already be familiar with how to train models, tune hyperparameters, optimize performance, and more.\\n\\nHow could you possibly write a program that encounters OOM (Out of Memory) issues?\\n\\n\x3c!-- truncate --\x3e\\n\\n:::tip\\nThis is referring to system memory, not GPU memory.\\n:::\\n\\n## Problem Description\\n\\nThere are many potential causes for OOM errors, but this time, I will focus on a specific issue that even professional workers often encounter:\\n\\n- You might be using a List structure!\\n\\nBased on my recent experience training models, I identified the exact scenario when the memory leak occurs.\\n\\nConsider the following code example:\\n\\n```python\\nfrom torch.utils.data import Dataset, DataLoader\\nimport numpy as np\\nimport torch\\n\\n\\nclass DataIter(Dataset):\\n\\n    def __init__(self):\\n        self.data_np = np.array([x for x in range(10000000)])\\n        self.data = [x for x in range(10000000)]\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        data = self.data[idx]\\n        data = np.array([data], dtype=np.int64)\\n        return torch.tensor(data)\\n\\n\\ntrain_data = DataIter()\\ntrain_loader = DataLoader(train_data, batch_size=300, num_workers=18)\\n\\nfor i, item in enumerate(train_loader):\\n    if i % 1000 == 0:\\n        print(i)\\n```\\n\\n---\\n\\nAfter reviewing this example, let\'s cut to the chase:\\n\\n- **Do you see the `self.data` List? This List will cause the OOM problem.**\\n\\nI did some research and found that this doesn\'t seem to be a PyTorch issue, but rather a Python issue.\\n\\nIn any case, don\'t use List; use Numpy or Tensor to store data instead. This way, you won\u2019t encounter OOM issues.\\n\\nAt least in this example, doing so was effective.\\n\\n## What About Me?\\n\\nYou might say: \\"I wrote it the same way, and nothing happened!\\"\\n\\n---\\n\\nThe world is great until you encounter a large dataset.\\n\\nBased on my own test results, when the dataset is small, using a List does not cause memory leaks.\\n\\nMore specifically:\\n\\n- Using around 10,000 data points? No problem!\\n- Using over a million data points? Boom, it crashes!\\n\\nSo, if your dataset is small, you may never encounter this issue.\\n\\nAs for the boundary of the dataset size, I\u2019m not sure... My guess is that it\u2019s some anomaly that occurs at a specific moment in the interaction between Python and PyTorch."},{"id":"convert-pdf-to-images","metadata":{"permalink":"/en/blog/convert-pdf-to-images","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/02-14-pdf2imgs/index.md","title":"Convert PDF to Images with Python","description":"Solve the problem with the open-source pdf2image package.","date":"2024-02-14T00:00:00.000Z","tags":[{"inline":true,"label":"Python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"pdf2image","permalink":"/en/blog/tags/pdf-2-image"}],"readingTime":2.005,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"convert-pdf-to-images","title":"Convert PDF to Images with Python","authors":"Z. Yuan","tags":["Python","pdf2image"],"image":"/en/img/2024/0214.webp","description":"Solve the problem with the open-source pdf2image package."},"unlisted":false,"prevItem":{"title":"The PyTorch List Trap","permalink":"/en/blog/pytorch-training-out-of-memory"},"nextItem":{"title":"Reading HEIC Images and Accelerating Loading with Python","permalink":"/en/blog/opencv-imread"}},"content":"During development, you may often need to convert PDF files into image formats, whether for document display, data processing, or content sharing.\\n\\nThis article introduces a handy Python module: [**pdf2image**](https://github.com/Belval/pdf2image/tree/master), which can convert PDF files into PIL images.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Install Dependencies\\n\\n`pdf2image` depends on two tools: `pdftoppm` and `pdftocairo`, with different installation methods depending on the operating system:\\n\\n- **Mac**: Install Poppler via Homebrew by running the following in the terminal:\\n\\n  ```shell\\n  brew install poppler\\n  ```\\n- **Linux**: Most Linux distributions have `pdftoppm` and `pdftocairo` pre-installed. If not, you can install them with the following command:\\n\\n  ```shell\\n  sudo apt-get install poppler-utils   # For Ubuntu/Debian systems\\n  ```\\n- **Using `conda`**: You can install Poppler via `conda` on any platform:\\n\\n  ```shell\\n  conda install -c conda-forge poppler\\n  ```\\n\\n  Once installed, you can install `pdf2image`.\\n\\n## Install `pdf2image`\\n\\nTo install, run the following command in the terminal:\\n\\n```shell\\npip install pdf2image\\n```\\n\\n## Usage\\n\\nThe basic usage for converting a PDF to an image is quite simple.\\n\\nHere\u2019s an example of how to convert each page of a PDF into a PIL image object and save it as a file:\\n\\n```python\\nfrom pdf2image import convert_from_path\\n\\n# Convert PDF to a list of images\\nimages = convert_from_path(\'/path/to/your/pdf/file.pdf\')\\n\\n# Save each page as a PNG image\\nfor i, image in enumerate(images):\\n    image.save(f\'output_page_{i+1}.png\', \'PNG\')\\n```\\n\\nIf you want to convert from binary data, you can do it as follows:\\n\\n```python\\nwith open(\'/path/to/your/pdf/file.pdf\', \'rb\') as f:\\n    pdf_data = f.read()\\n\\nimages = convert_from_bytes(pdf_data)\\n```\\n\\n## Optional Parameters and Advanced Settings\\n\\n`pdf2image` provides rich optional parameters that allow you to customize the quality and range of the output images:\\n\\n- **DPI Setting**: Adjusting the `dpi` parameter can increase the image resolution, suitable for cases where high-quality images are required:\\n\\n  ```python\\n  images = convert_from_path(\'/path/to/your/pdf/file.pdf\', dpi=300)\\n  ```\\n\\n- **Specify Page Range**: Use the `first_page` and `last_page` parameters to convert only specific pages:\\n\\n  ```python\\n  images = convert_from_path(\'/path/to/your/pdf/file.pdf\', first_page=2, last_page=5)\\n  ```\\n\\n- **Output Image Format**: The `fmt` parameter allows you to specify the output image format, such as JPEG or PNG:\\n\\n  ```python\\n  images = convert_from_path(\'/path/to/your/pdf/file.pdf\', fmt=\'jpeg\')\\n  ```\\n\\n- **Error Handling**: During the conversion process, you might encounter format errors or corrupted files. It\u2019s recommended to use try/except to catch exceptions:\\n\\n  ```python\\n  try:\\n      images = convert_from_path(\'/path/to/your/pdf/file.pdf\')\\n  except Exception as e:\\n      print(\\"Conversion failed:\\", e)\\n  ```\\n\\n## Conclusion\\n\\n`pdf2image` is a useful tool. For more parameters and detailed usage, refer to the [**official pdf2image documentation**](https://github.com/Belval/pdf2image/tree/master)."},{"id":"opencv-imread","metadata":{"permalink":"/en/blog/opencv-imread","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/02-13-imread/index.md","title":"Reading HEIC Images and Accelerating Loading with Python","description":"Let\u2019s optimize OpenCV imread!","date":"2024-02-13T00:00:00.000Z","tags":[{"inline":true,"label":"HEIC","permalink":"/en/blog/tags/heic"},{"inline":true,"label":"TurboJPEG","permalink":"/en/blog/tags/turbo-jpeg"}],"readingTime":2.76,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"opencv-imread","title":"Reading HEIC Images and Accelerating Loading with Python","authors":"Z. Yuan","tags":["HEIC","TurboJPEG"],"image":"/en/img/2024/0213.webp","description":"Let\u2019s optimize OpenCV imread!"},"unlisted":false,"prevItem":{"title":"Convert PDF to Images with Python","permalink":"/en/blog/convert-pdf-to-images"},"nextItem":{"title":"Daily Error Troubleshooting Log","permalink":"/en/blog/error-record"}},"content":"When you want to read an image, you may use OpenCV\'s `imread` function.\\n\\nUnfortunately, this function is not perfect, and sometimes you may encounter certain issues.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Basic Usage\\n\\nThe basic usage of the `imread` function is quite simple; just pass in the path of the image:\\n\\n```python\\nimport cv2\\n\\nimage = cv2.imread(\'path/to/image.jpg\')\\n```\\n\\nSupported image formats include common formats like BMP, JPG, PNG, TIF, etc.\\n\\n## Limitation 1: HEIC Format\\n\\nPhotos taken on iOS devices are usually in HEIC format, which is not supported by OpenCV. If you try to read a HEIC image using the `imread` function, you will get a `None` return value.\\n\\nTo solve this, we need to use the `pyheif` package to read HEIC images and then convert them to a `numpy.ndarray`.\\n\\nFirst, install the required packages:\\n\\n```bash\\nsudo apt install libheif-dev\\npip install pyheif\\n```\\n\\nThen, write a simple function:\\n\\n```python\\nimport cv2\\nimport pyheif\\nimport numpy as np\\n\\ndef read_heic_to_numpy(file_path: str):\\n    heif_file = pyheif.read(file_path)\\n    data = heif_file.data\\n    if heif_file.mode == \\"RGB\\":\\n        numpy_array = np.frombuffer(data, dtype=np.uint8).reshape(\\n            heif_file.size[1], heif_file.size[0], 3)\\n    elif heif_file.mode == \\"RGBA\\":\\n        numpy_array = np.frombuffer(data, dtype=np.uint8).reshape(\\n            heif_file.size[1], heif_file.size[0], 4)\\n    else:\\n        raise ValueError(\\"Unsupported HEIC color mode\\")\\n    return numpy_array\\n\\n\\nimg = read_heic_to_numpy(\'path/to/image.heic\')\\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\n```\\n\\n## Limitation 2: Slow JPG Loading\\n\\nIn some cases, the `imread` function reads JPG images very slowly. This is because OpenCV uses the `libjpeg` library to read JPG images, and `libjpeg` is not particularly efficient.\\n\\nTo address this, we introduce the `TurboJPEG` package, which is an alternative to `libjpeg` and offers better performance.\\n\\nAs before, first install the required packages:\\n\\n```bash\\nsudo apt install libturbojpeg exiftool\\npip install PyTurboJPEG\\n```\\n\\nThen, write some code to speed it up:\\n\\nGenerally, it speeds up the process by about 2-3 times.\\n\\n```python\\nimport cv2\\nimport piexif\\nfrom enum import IntEnum\\nfrom pathlib import Path\\nfrom turbojpeg import TurboJPEG\\n\\n\\njpeg = TurboJPEG()\\n\\n\\nclass ROTATE(IntEnum):\\n    ROTATE_90 = cv2.ROTATE_90_CLOCKWISE\\n    ROTATE_180 = cv2.ROTATE_180\\n    ROTATE_270 = cv2.ROTATE_90_COUNTERCLOCKWISE\\n\\n\\ndef imrotate90(img, rotate_code: ROTATE) -> np.ndarray:\\n    return cv2.rotate(img.copy(), rotate_code)\\n\\n\\ndef get_orientation_code(stream: Union[str, Path, bytes]):\\n    code = None\\n    try:\\n        exif_dict = piexif.load(stream)\\n        if piexif.ImageIFD.Orientation in exif_dict[\\"0th\\"]:\\n            orientation = exif_dict[\\"0th\\"][piexif.ImageIFD.Orientation]\\n            if orientation == 3:\\n                code = ROTATE.ROTATE_180\\n            elif orientation == 6:\\n                code = ROTATE.ROTATE_90\\n            elif orientation == 8:\\n                code = ROTATE.ROTATE_270\\n    finally:\\n        return code\\n\\n\\ndef jpgdecode(byte_: bytes) -> Union[np.ndarray, None]:\\n    try:\\n        bgr_array = jpeg.decode(byte_)\\n        code = get_orientation_code(byte_)\\n        bgr_array = imrotate90(\\n            bgr_array, code) if code is not None else bgr_array\\n    except:\\n        bgr_array = None\\n\\n    return bgr_array\\n\\n\\ndef jpgread(img_file: Union[str, Path]) -> Union[np.ndarray, None]:\\n    with open(str(img_file), \'rb\') as f:\\n        binary_img = f.read()\\n        bgr_array = jpgdecode(binary_img)\\n\\n    return bgr_array\\n\\nimg = jpgread(\'path/to/image.jpg\')\\n```\\n\\nThis will speed up the reading of JPG images.\\n\\n## Conclusion\\n\\nWhat if we want this program to be smarter and choose the best loading method on its own?\\n\\nHere, we can consolidate the code above to select the appropriate loading method based on the image format:\\n\\n```python title=\\"custom_imread.py\\"\\ndef imread(\\n    path: Union[str, Path],\\n    color_base: str = \'BGR\',\\n    verbose: bool = False\\n) -> Union[np.ndarray, None]:\\n\\n    if not Path(path).exists():\\n        raise FileExistsError(f\'{path} can not found.\')\\n\\n    if Path(path).suffix.lower() == \'.heic\':\\n        img = read_heic_to_numpy(str(path))\\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\n    else:\\n        img = jpgread(path)\\n        img = cv2.imread(str(path)) if img is None else img\\n\\n    if img is None:\\n        if verbose:\\n            warnings.warn(\\"Got a None type image.\\")\\n        return\\n\\n    if color_base != \'BGR\':\\n        img = imcvtcolor(img, cvt_mode=f\'BGR2{color_base}\')\\n\\n    return img\\n```"},{"id":"error-record","metadata":{"permalink":"/en/blog/error-record","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/02-04-error-record/index.md","title":"Daily Error Troubleshooting Log","description":"A log of simple issues and their solutions.","date":"2024-02-04T00:00:00.000Z","tags":[{"inline":true,"label":"error","permalink":"/en/blog/tags/error"},{"inline":true,"label":"record","permalink":"/en/blog/tags/record"}],"readingTime":4.5,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"error-record","title":"Daily Error Troubleshooting Log","authors":"Z. Yuan","tags":["error","record"],"image":"/en/img/2024/0204.webp","description":"A log of simple issues and their solutions."},"unlisted":false,"prevItem":{"title":"Reading HEIC Images and Accelerating Loading with Python","permalink":"/en/blog/opencv-imread"},"nextItem":{"title":"User Switching Tool in Containers: gosu","permalink":"/en/blog/gosu-usage"}},"content":"Writing code always comes with a bunch of problems.\\n\\nI am recording some of the trivial issues and their solutions here.\\n\\n:::tip\\nThis article will be continuously updated.\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. Error when running `npx docusaurus start`\\n\\n- **Error Message:**\\n\\n  ```bash\\n  file:///home/user/workspace/blog/node_modules/@docusaurus/core/bin/docusaurus.mjs:30\\n  process.env.BABEL_ENV ??= \'development\';\\n                      ^^^\\n\\n  SyntaxError: Unexpected token \'??=\'\\n  ```\\n\\n- **Solution:**\\n\\n  The `??=` operator requires Node.js version 15.0.0 or higher.\\n\\n  ```bash\\n  nvm install node\\n  nvm use node\\n  ```\\n\\n## 2. \'choco\' command not recognized\\n\\n- **Error Message:**\\n\\n  ```shell\\n  PS C:\\\\Windows\\\\System32> choco install git -y\\n  >>\\n  choco : The term \'choco\' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\\n  At line:1 char:1\\n  + choco install git -y\\n  + ~~~~~\\n      + CategoryInfo          : ObjectNotFound: (choco:String) [], CommandNotFoundException\\n      + FullyQualifiedErrorId : CommandNotFoundException\\n  ```\\n\\n- **Solution:**\\n\\n  This indicates that Chocolatey was not successfully installed, often due to not running PowerShell as an Administrator.\\n\\n  Run PowerShell as an Administrator, and then execute the Chocolatey installation command again.\\n\\n  ```shell\\n  Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\'https://community.chocolatey.org/install.ps1\'))\\n  ```\\n\\n## 3. Chocolatey installation failure\\n\\n- **Error Message:**\\n\\n  ```shell\\n  Warning: An existing Chocolatey installation was detected. Installation will not continue. This script will not overwrite existing installations.\\n  If there is no Chocolatey installation at \'C:\\\\ProgramData\\\\chocolatey\', delete the folder and attempt the installation again.\\n\\n  Please use choco upgrade chocolatey to handle upgrades of Chocolatey itself.\\n  If the existing installation is not functional or a prior installation did not complete, follow these steps:\\n  - Backup the files at the path listed above so you can restore your previous installation if needed.\\n  - Remove the existing installation manually.\\n  - Rerun this installation script.\\n  - Reinstall any packages previously installed, if needed (refer to the lib folder in the backup).\\n\\n  Once installation is completed, the backup folder is no longer needed and can be deleted.\\n  ```\\n\\n- **Solution:**\\n\\n  This indicates that Chocolatey is already installed. Please remove the old installation before reinstalling.\\n\\n  ```shell\\n  Remove-Item \\"C:\\\\ProgramData\\\\chocolatey\\" -Recurse -Force\\n  ```\\n\\n## 4. Remote port forwarding\\n\\n- **Description:**\\n\\n  You\u2019ve started a service on a remote machine, such as TensorBoard, but can\'t access it directly, so you need to forward the port through your local machine.\\n\\n- **Solution:**\\n\\n  Assuming the service is running on port 6006 on the remote machine, and you want to access it on the same port on your local machine.\\n\\n  When using SSH to log in, you can forward the port using the `-L` parameter:\\n\\n  ```bash\\n  ssh -L 6006:localhost:6006 user@remote_ip_address\\n  ```\\n\\n  This way, you can access the TensorBoard service on the remote machine via `http://localhost:6006` on your local machine.\\n\\n## 5. Inconsistent Web Rendering Behavior in Development and Deployment Environments\\n\\n- **Description:**\\n\\n  You\'ve set the layout style of the blog in `custom.css`:\\n\\n  ```css\\n  .container {\\n    max-width: 90%;\\n    padding: 0 15px;\\n    margin: 0 auto;\\n  }\\n  ```\\n\\n  In the deployment phase, this style seems to be overridden by other higher-priority styles, but in the development phase, this style is normal.\\n\\n- **Solution:**\\n\\n  Be more specific in selecting the target:\\n\\n  ```css\\n  body .container {\\n    max-width: 90%;\\n    padding: 0 15px;\\n    margin: 0 auto;\\n  }\\n  ```\\n\\n## 6. Turbojpeg Warning When Reading Images\\n\\n- **Description**\\n\\n  When reading images, the following warning messages appear:\\n\\n  ```shell\\n  turbojpeg.py:940: UserWarning: Corrupt JPEG data: 18 extraneous bytes before marker 0xc4\\n  turbojpeg.py:940: UserWarning: Corrupt JPEG data: bad Huffman code\\n  turbojpeg.py:940: UserWarning: Corrupt JPEG data: premature end of data segment\\n  ```\\n\\n- **Solution**\\n\\n  To avoid these annoying warnings, you should filter out the problematic images:\\n\\n  ```python\\n  import cv2\\n  import warnings\\n\\n  data = [\'test1.jpg\', \'test2.jpg\', \'test3.jpg\']\\n\\n  for d in data:\\n    with warnings.catch_warnings(record=True) as w:\\n      warnings.simplefilter(\\"always\\", UserWarning)\\n\\n      # Read the image and see if there are any warnings\\n      cv2.imread(d)\\n\\n      # Remove the data if warnings are present\\n      if w:\\n        data.remove(d)\\n\\n  # Saving the filtered data\\n  ```\\n\\n## 7. `Docusaurus` Deployment: `showLastUpdateTime: true` Not Working\\n\\n- **Description**\\n\\n  In `docusaurus.config.js`, you set `showLastUpdateTime: true` and `showLastUpdateAuthor: true,` but after deployment, you find that it has no effect. The rendered page displays the same time and author?\\n\\n- **Solution**\\n\\n  The problem is caused by an incorrect setting when checking out the branch during deployment, which prevents `git` from correctly obtaining the last update time and author.\\n\\n  Change it like this:\\n\\n  ```yaml\\n  steps:\\n    - uses: actions/checkout@v4\\n      with:\\n        fetch-depth: 0\\n  ```\\n\\n  Setting `fetch-depth: 0` will solve the problem.\\n\\n## 8. Checking Error Logs Inside a Docker Container\\n\\n- **Description**\\n\\n  A service is running inside a Docker container, but the service has encountered an error and is not functioning properly, so you need to check the error logs.\\n\\n- **Solution**\\n\\n  First, check the target container\'s ID:\\n\\n  ```bash\\n  docker ps\\n  ```\\n\\n  Then, view the logs:\\n\\n  ```bash\\n  docker logs container_id\\n  ```\\n\\n  Entering two separate commands to view the desired error messages is quite cumbersome, so we can simplify it into a single command.\\n\\n  Assuming the target name to be checked is: TARGET_NAME, the command is as follows:\\n\\n  ```bash\\n  docker logs $(docker ps -q --filter \\"ancestor=TARGET_NAME\\")\\n  ```\\n\\n## 9. Checking the i18n Status in Docusaurus\\n\\n- **Description**\\n\\n  How can we check the current language status in `Docusaurus`?\\n\\n- **Solution**\\n\\n  You can use `i18n` to retrieve the current language status:\\n\\n  ```javascript\\n  import useDocusaurusContext from \\"@docusaurus/useDocusaurusContext\\";\\n\\n  const {\\n    i18n: { currentLocale },\\n  } = useDocusaurusContext();\\n  ```\\n\\n  Then, you can use the `currentLocale` variable to obtain the corresponding language data."},{"id":"gosu-usage","metadata":{"permalink":"/en/blog/gosu-usage","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/02-03-usage-gosu/index.md","title":"User Switching Tool in Containers: gosu","description":"A tool that is so useful, you definitely need to learn how to use it.","date":"2024-02-03T00:00:00.000Z","tags":[{"inline":true,"label":"docker","permalink":"/en/blog/tags/docker"},{"inline":true,"label":"gosu","permalink":"/en/blog/tags/gosu"}],"readingTime":5.255,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"gosu-usage","title":"User Switching Tool in Containers: gosu","authors":"Z. Yuan","tags":["docker","gosu"],"image":"/en/img/2024/0203.webp","description":"A tool that is so useful, you definitely need to learn how to use it."},"unlisted":false,"prevItem":{"title":"Daily Error Troubleshooting Log","permalink":"/en/blog/error-record"},"nextItem":{"title":"Building a New Computer","permalink":"/en/blog/buy-a-new-computer"}},"content":"Docker technology has been widely used in deployment and management.\\n\\nWe often package various applications and their dependencies together to ensure consistent operation across different environments.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Common Problems\\n\\nHowever, if you use it frequently, you\'re bound to run into a few common issues.\\n\\n### TTY Conversion\\n\\nA common situation is when you output a file inside the container.\\n\\nAfter exiting the container, you may notice that the file permissions are set to root.\\n\\nAt this point, you need to use `chown` to change the file\'s permissions.\\n\\nOver and over again, isn\'t that annoying?\\n\\n---\\n\\nAnother case is when using `sudo` to start an interactive application inside a Docker container. These applications may not correctly detect the terminal because `sudo` may not handle the terminal\'s ownership and control properly when creating a new session.\\n\\nAs a result, applications that need terminal interaction may not work properly or encounter input/output errors when trying to use them.\\n\\n### Signal Forwarding\\n\\nSuppose you have a container running a web server, like Apache or Nginx.\\n\\nTypically, you might use command-line tools to manage the container, including starting and stopping it. Inside the container, if you start the web server using `sudo`, then `sudo` will create a new process to run the web server.\\n\\nThe problem arises when you want to stop or restart the container. The container management system will send a signal (like SIGTERM) to notify the processes inside the container to stop. However, if the web server is started via `sudo`, this signal may only be sent to the `sudo` process, not the actual web server process. This means the web server may not receive the stop signal and thus cannot perform proper cleanup and shutdown.\\n\\n:::tip\\nThe design of `sudo` is to improve security, allowing regular users to execute commands as other users (typically root). In this process, `sudo` creates a new session to execute the command.\\n\\nThis behavior typically doesn\u2019t cause issues in traditional operating system environments, but in lightweight virtualization environments like containers, it may lead to signal forwarding problems, as the new session created by `sudo` may not be compatible with the way the container management system sends signals.\\n:::\\n\\n## What is gosu?\\n\\n- [**gosu GitHub repository**](https://github.com/tianon/gosu)\\n\\ngosu is a tool specifically designed for containers. Its purpose is to make it easier and safer to execute commands inside containers.\\n\\nWhen you need to run a program as a different user (e.g., switching from the admin user to a regular user), gosu comes in handy. Its core functionality is directly inspired by the way `Docker/libcontainer` launches applications inside containers (in fact, it directly uses the `/etc/passwd` handling code from the `libcontainer` codebase).\\n\\nIf you\'re not interested in the technical details, simply put, gosu is like an assistant: when you tell it \\"please run this command as this user,\\" it does it for you and then exits, leaving no trace behind.\\n\\n### Practical Use Case\\n\\nThe most common use of gosu is in Docker entrypoint scripts, where it downgrades the container from the root user to a regular user to avoid permission issues.\\n\\nHere\u2019s an example:\\n\\nFirst, add a few lines to your Dockerfile to install gosu:\\n\\n```Dockerfile title=\\"Dockerfile\\"\\n# Base the image on an existing base image\\nFROM some_base_image:latest\\n\\nWORKDIR /app\\n\\n# Install gosu\\nRUN apt-get update && apt-get install -y gosu && rm -rf /var/lib/apt/lists/*\\n\\n# Prepare the entrypoint script\\nCOPY entrypoint.sh /entrypoint.sh\\nRUN chmod +x /entrypoint.sh\\n\\nENTRYPOINT [\\"/entrypoint.sh\\"]\\nCMD [\\"default_command\\"]\\n```\\n\\nNext, create an entrypoint script `entrypoint.sh`, which dynamically creates users based on environment variables and then uses gosu to switch users to run the command:\\n\\n```bash title=\\"entrypoint.sh\\"\\n#!/bin/bash\\n# Check if USER_ID and GROUP_ID environment variables are set\\nif [ ! -z \\"$USER_ID\\" ] && [ ! -z \\"$GROUP_ID\\" ]; then\\n    # Create user group and user\\n    groupadd -g \\"$GROUP_ID\\" usergroup\\n    useradd -u \\"$USER_ID\\" -g usergroup -m user\\n    # Use gosu to execute the command\\n    exec gosu user \\"$@\\"\\nelse\\n    exec \\"$@\\"\\nfi\\n```\\n\\nFor a real example, refer to: [**Example training docker**](https://github.com/DocsaidLab/Otter/blob/main/docker/Dockerfile)\\n\\n## Security Considerations\\n\\nWhile gosu is very convenient in container environments, developers have pointed out potential security risks. Any tool that allows user identity switching should be used cautiously.\\n\\nIt\u2019s like the key to your house: while it\'s very useful, if misused, it might leave your security door wide open. Therefore, when using gosu, teams should ensure they fully understand the use case to avoid abuse in unsafe scenarios.\\n\\nFor related discussions, refer to: [**Keeping the TTY across a privilege boundary might be insecure #37**](https://github.com/tianon/gosu/issues/37)\\n\\n:::info\\nI know you\u2019re too lazy to read, so here\u2019s a brief excerpt of the key points:\\n\\n**A developer raised concerns that keeping TTY across a privilege boundary might pose a security risk.**\\n\\nWhen a program switches from high to low privilege, if a new virtual terminal is not created, file descriptors (such as standard input/output) that were not closed in the parent process might be used by the new process. Using the TIOCSTI ioctl call, an attacker can inject input characters into the TTY buffer, simulating keyboard input and executing unauthorized commands. This type of vulnerability is allowed by design.\\n\\nFor example, the following code will inject \\"id\\\\n\\" character by character into standard input:\\n\\n```c\\n#include <unistd.h>\\n#include <sys/ioctl.h>\\n#include <stdio.h>\\n\\nint main()\\n{\\n    for (char *cmd = \\"id\\\\n\\"; *cmd; cmd++) {\\n        if (ioctl(STDIN_FILENO, TIOCSTI, cmd)) {\\n            fprintf(stderr, \\"ioctl failed\\\\n\\");\\n            return 1;\\n        }\\n    }\\n    return 0;\\n}\\n```\\n\\nThis code is considered malicious because it deliberately uses the TIOCSTI ioctl call to simulate keyboard input and inject commands into the terminal without user interaction. This allows for malicious injection or privilege escalation attacks, making it a security risk.\\n\\nSince Docker assigns a new TTY and replaces the parent shell, the injected commands cannot affect the host or the original terminal, so the risk is lower. In unexpected interactive environments, if you run gosu directly in the terminal without replacing the original TTY, an attacker\u2019s program could successfully inject commands, creating a security vulnerability.\\n\\nHowever, this vulnerability primarily affects unintended usage scenarios. If used as designed, such as in Docker containers as an entrypoint with Docker assigning a new TTY, the risk is significantly reduced.\\n\\nTherefore, as long as it\'s used in the intended environment, there\'s no need to overly worry about the risk.\\n:::"},{"id":"buy-a-new-computer","metadata":{"permalink":"/en/blog/buy-a-new-computer","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/10-12-buy-a-new-computer/index.md","title":"Building a New Computer","description":"Documenting the process of assembling a new computer.","date":"2023-10-12T00:00:00.000Z","tags":[{"inline":true,"label":"rtx4090","permalink":"/en/blog/tags/rtx-4090"},{"inline":true,"label":"13900k","permalink":"/en/blog/tags/13900-k"}],"readingTime":11.5,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"buy-a-new-computer","title":"Building a New Computer","authors":"Z. Yuan","tags":["rtx4090","13900k"],"image":"/en/img/2023/1012.webp","description":"Documenting the process of assembling a new computer."},"unlisted":false,"prevItem":{"title":"User Switching Tool in Containers: gosu","permalink":"/en/blog/gosu-usage"},"nextItem":{"title":"Managing Python Versions with pyenv","permalink":"/en/blog/pyenv-installation"}},"content":"I suddenly thought about buying a new computer.\\n\\nNot for any particular reason, just to use the RTX 4090 for training models.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Disclaimer\\n\\nRecently, I\'ve been busy for a while, putting together a machine for training purposes, and I thought I\u2019d leave a record of the process.\\n\\n1. If you decide to assemble a machine based on this record and encounter any problems or incur any losses, it\'s not my responsibility.\\n2. This article is solely for personal record-keeping and has no intention of recommending or advertising anything.\\n3. All the products or brands mentioned in this article are for record-keeping purposes and do not imply any commercial partnership or endorsement.\\n\\n## Purchase Details\\n\\nBelow is the list of items we purchased:\\n\\n| Item           | Description                                                                        | Quantity | Total Price (NTD) |\\n| -------------- | ---------------------------------------------------------------------------------- | -------- | ----------------- |\\n| CPU            | Intel i9-13900K \u301024 cores/32 threads\u30113.0GHz-5.8GHz 36MB LGA1700 125W             | 1        | $ 19,500          |\\n| Motherboard    | Gigabyte Z790 AORUS ELITE (ATX/Realtek 2.5Gb) 16+1+2 Power Phases                  | 1        | $ 7,990           |\\n| Graphics Card  | Gigabyte RTX 4090 AERO OC 24G White Windforce Cooling Metal Backplate              | 1        | $ 60,990          |\\n| Memory         | Kingston 64 GB (Dual Channel 32GBx2) DDR5 6000 CL36 FURY Beast White               | 2        | $ 12,258          |\\n| SSD            | Micron Crucial P5 Plus 2TB (Gen4 M.2 PCIe 4.0) NVMe SSD (Read 6600M / Write 5500M) | 2        | $ 7,800           |\\n| Liquid Cooling | NZXT Kraken 360 RGB White                                                          | 1        | $ 7,990           |\\n| Case           | ASUS TUF Gaming GT502 White ATX                                                    | 1        | $ 5,490           |\\n| Case Fans      | ASUS TUF Gaming TF120 ARGB White Triple Fan Pack (Case Gift)                       | 1        | -                 |\\n| Case Fans      | ASUS TUF Gaming TF120 ARGB White Triple Fan Pack (Additional Purchase)             | 1        | $ 1,690           |\\n| Case Fans      | ASUS TUF Gaming TF120 ARGB White Single Fan (Additional Purchase)                  | 4        | $ 2,196           |\\n| PSU            | FSP HYDRO PTM PRO 1200W ATX3.0 Platinum Fully Modular                              | 1        | $ 5,790           |\\n| Cables         | SilverStone 1 to 4 A.RGB Splitter Cable (SST-CPL03)                                | 2        | $ 250             |\\n\\n- **Total Price (including tax)**: NTD 131,944, approximately 4,100 USD\\n- **Date**: September 2023\\n\\n:::info\\nWe checked the prices of pre-built computers with similar specs, which were around NT$180,000.\\n:::\\n\\n## RTX 4090\\n\\nIt all started with this.\\n\\nWe first checked out some options at a local retailer and found that there were a few brands offering the RTX 4090: ASUS, Gigabyte, and MSI.\\n\\nThen we watched a review video:\\n\\n- [**MSI RTX 4090 Suprim X vs. RTX 4090 Gaming X Trio, Thermals, Power & Overclocking**](https://www.youtube.com/watch?v=uyMnMPLYk2w)\\n\\nIt seemed that MSI\u2019s cards ran particularly hot, while the other two brands had similar temperatures.\\n\\nNext, we found a relevant article:\\n\\n- [**[Info] 6 RTX 4090 Cooling Comparisons**](https://forum.gamer.com.tw/C.php?bsn=60030&snA=615853)\\n\\nOverall, Gigabyte performed the best, so we chose the best-looking white Windforce version from their lineup:\\n\\nSpecifications:\\n\\n- Item: GIGABYTE RTX 4090 AERO OC 24G White Windforce Cooling Metal Backplate\\n- Price: NT$60,900 from the local retailer\\n- Length: 34.2 cm\\n\\n  ![gigabyte](./img/img2.jpg)\\n\\n:::tip\\nThis generation of GPUs also involves the ATX standard, which we\u2019ll discuss further in the PSU section.\\n:::\\n\\n## Motherboard\\n\\nAfter deciding on the graphics card, the next step was the motherboard.\\n\\nHere we encountered the issue of \\"socket compatibility,\\" as different motherboards support different CPUs. So we did some research:\\n\\n- Intel 12th Gen pairs with 600 series motherboards, using the 1700 socket.\\n- Intel 13th Gen pairs with 700 series motherboards, using the 1700 socket.\\n- Intel 14th Gen pairs with 700 series motherboards, using the 1700 socket.\\n\\nIt seems that the 1700 socket is a more long-term choice, so we went with a 700 series motherboard:\\n\\n### 1700 Socket\\n\\nAccording to [**Wikipedia\u2019s description of LGA 1700**](https://zh.wikipedia.org/zh-tw/LGA_1700):\\n\\nLGA 1700 is a socket designed by Intel for its Alder Lake microarchitecture (12th Gen Intel Core) and Raptor Lake microarchitecture (13th and 14th Gen Intel Core) desktop processors, launched in 2021. It replaces the LGA 1200 and supports DDR5 memory. Due to the increased number of contact points, previous heatsinks compatible with LGA 1200 and LGA 1151 (37.5 mm x 37.5 mm) are incompatible with LGA 1700 (45.0 mm x 37.5 mm).\\n\\n### Chipset Specifications\\n\\nDuring our research, we also came across various motherboards like B660, H670, Z690, etc. So we dug deeper:\\n\\n<details>\\n  <summary>Click to expand</summary>\\n\\n| **Feature**                                                   | **H610**                                     | **B660**                                     | **H670**                                     | **Z690**                                     | **W680**                                     | **Z790**                                     |\\n| ------------------------------------------------------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- |\\n| **CPU Overclocking**                                          | No                                           | No (Supports memory overclocking)            | No (Supports memory overclocking)            | Yes                                          | Yes                                          | Yes                                          |\\n| **Bus Interface**                                             | DMI 4.0 x4                                   | DMI 4.0 x8                                   | DMI 4.0 x8                                   | DMI 4.0 x8                                   | DMI 4.0 x8                                   | DMI 4.0 x8                                   |\\n| **Supported CPUs**                                            | Alder Lake, Raptor Lake, Raptor Lake Refresh | Alder Lake, Raptor Lake, Raptor Lake Refresh | Alder Lake, Raptor Lake, Raptor Lake Refresh | Alder Lake, Raptor Lake, Raptor Lake Refresh | Alder Lake, Raptor Lake, Raptor Lake Refresh | Alder Lake, Raptor Lake, Raptor Lake Refresh |\\n| **Supported Memory**                                          | Up to 64GB, DDR4 3200 / DDR5 4800            | Up to 128GB, DDR4 3200 / DDR5 4800           | Up to 128GB, DDR4 3200 / DDR5 4800           | Up to 128GB, DDR4 3200 / DDR5 4800           | Up to 128GB, DDR4 3200 / DDR5 4800           | Up to 128GB, DDR4 3200 / DDR5 4800           |\\n| **Maximum DIMM Slots**                                        | 2                                            | 4                                            | 4                                            | 4                                            | 4                                            | 4                                            |\\n| **Maximum USB 2.0 Ports**                                     | 10                                           | 12                                           | 14                                           | 14                                           | 14                                           | 14                                           |\\n| **USB 3.2 Gen 1 (5 Gbit/s)**                                  | Up to 4                                      | Up to 6                                      | Up to 8                                      | Up to 10                                     | Up to 10                                     | Up to 10                                     |\\n| **USB 3.2 Gen 2 (10 Gbit/s)**                                 | Up to 1                                      | Up to 2                                      | Up to 4                                      | Up to 4                                      | Up to 4                                      | Up to 10                                     |\\n| **USB 3.2 Gen 2x2 (20 Gbit/s)**                               | No                                           | Up to 2                                      | Up to 4                                      | Up to 4                                      | Up to 4                                      | Up to 5                                      |\\n| **Maximum SATA 3.0 Ports**                                    | 4                                            | 4                                            | 8                                            | 8                                            | 8                                            | 8                                            |\\n| **Processor PCI Express 5.0 Configuration**                   | 1x16                                         | 1x16+1x4                                     | 1x16+1x4 or 2x8+1x4                          | 1x16+1x4 or 2x8+1x4                          | 1x16+1x4 or 2x8+1x4                          | 1x16 or 2x8                                  |\\n| **PCH PCI Express 4.0 Configuration**                         | No                                           | 6                                            | 12                                           | 16                                           | 16                                           | 16                                           |\\n| **PCH PCI Express 3.0 Configuration**                         | 8                                            | 12                                           | 12                                           | 12                                           | 12                                           | 12                                           |\\n| **Built-in Display Support (Digital Ports/Pipelines)**        | 3                                            | 4                                            | 4                                            | 4                                            | 4                                            | 4                                            |\\n| **Built-in Wireless (802.11ax / Wi-Fi 6E)**                   | Yes                                          | Yes                                          | Yes                                          | Yes                                          | Yes                                          | Yes                                          |\\n| **SATA RAID 0/1/5/10 Support**                                | No                                           | Yes                                          | Yes                                          | Yes                                          | Yes                                          | Yes                                          |\\n| **Intel Optane Memory Support**                               | No                                           | Yes                                          | Yes                                          | Yes                                          | Yes                                          | Yes                                          |\\n| **Intel Smart Sound Technology**                              | Yes                                          | Yes                                          | Yes                                          | Yes                                          | Yes                                          | Yes                                          |\\n| **Intel Active Management, Trusted Execution & vPro Support** | No                                           | No                                           | No                                           | No                                           | Yes                                          | No                                           |\\n| **Chipset TDP**                                               | 6W                                           | 6W                                           | 6W                                           | 6W                                           | 6W                                           | 6W                                           |\\n| **Release Date**                                              | Q1 2022                                      | Q1 2022                                      | Q1 2022                                      | Q4 2021                                      | Q2 2022                                      | Q4 2022                                      |\\n\\n</details>\\n\\nWhoa! That\u2019s a lot of information. In short, we went with the best option and chose a Z790!\\n\\nSpecifications:\\n\\n- Item: GIGABYTE Z790 AORUS ELITE (ATX/Realtek 2.5Gb) 16+1+2 Power Phases\\n- Price: NT$7,990 from the local retailer\\n\\n  ![gigabyte](./img/img3.jpg)\\n\\n## CPU\\n\\nWe\u2019ve heard that Intel\u2019s 13th and 14th Gen CPUs have some peculiar issues.\\n\\nBut since we\u2019re not overclocking, it shouldn\u2019t be a big deal. So, we decided to go with the 13th Gen Intel CPU!\\n\\nSpecifications:\\n\\n- Item: Intel i9-13900K \u301024 cores/32 threads\u30113.0GHz-5.8GHz 36MB 1700 Socket 125W\\n- Price: NT$19,500 from the local retailer\\n\\n:::tip\\nWe\u2019re running Ubuntu, and we did experience some crashes initially.\\n\\nHowever, after a system update, the issue disappeared, so we\u2019re not sure if it was a CPU defect.\\n:::\\n\\n## Memory\\n\\nWe believe that the three major memory manufacturers are Micron, Kingston, and Adata. Since we\u2019re not hardcore gamers, we decided to stick with one of these three.\\n\\nCapacity is key here. Since this rig is for model training, filling it up with 128GB is a basic requirement, right?\\n\\nSpecifications:\\n\\n- Item: Kingston 64 GB (Dual Channel 32GBx2) DDR5 6000 CL36 FURY Beast White\\n- Price: NT$6,129 per set from the local retailer\\n- Quantity: 2 sets\\n\\n  ![kingston](./img/img4.jpg)\\n\\n## SSD\\n\\nFor this part, we found a guide from a storage expert on PTT:\\n\\n- [**SSD (Solid State Drive) Purchasing Guide**](https://www.ptt.cc/bbs/PC_Shopping/M.1675587557.A.3D3.html)\\n\\nThere\u2019s a lot of extended reading and comparisons of various manufacturers, but honestly, there\u2019s too much technical knowledge here.\\n\\nWhile we were getting overwhelmed, we seemed to catch a conclusion that recommended Micron, so we went with that! (~Seems a bit too casual?~)\\n\\n---\\n\\nAfter some more research, we concluded that we should avoid QLC SSDs due to their slower write speeds and shorter lifespan. On the other hand, PCIe 5 SSDs seem to have some issues (link below), so we chose TLC technology and PCIe 4.0 SSDs.\\n\\n- [**Crucial, Corsair PCIe 5.0 SSDs running hot, throttling, and shutting down without heatsink**](https://www.neowin.net/news/crucial-corsair-pcie-50-ssds-running-hot-throttling-and-shutting-down-without-heatsink/?fbclid=IwAR0aM7igqoPCImgSMKCtPTNLRw5nOeGJPxLN3HYN89CsTSFEEtl2YsDqbCU)\\n\\n:::tip\\nHowever, technology advances quickly, so these issues might not be a concern in the near future! It\u2019s always best to check the latest information.\\n:::\\n\\n---\\n\\nSince we\u2019re training models, we opted for 4TB of storage to accommodate the datasets.\\n\\nSpecifications:\\n\\n- Item: Micron Crucial P5 Plus 2TB PCIe 4.0 NVMe SSD (Read 6600M / Write 5500M) TLC\\n- Price: NT$3,900 per unit from the local retailer\\n- Quantity: 2 units\\n\\n  ![micron](./img/img5.jpg)\\n\\n## Liquid Cooling\\n\\nIn this category, there are brands like ASUS, MSI, Corsair, and NZXT. Although there seemed to be plenty of choices, we couldn\u2019t find any particularly reliable comparison reviews online, so our focus shifted to warranty duration.\\n\\nMost brands offer a 3-year warranty, but NZXT offers 6+1 years, so we chose NZXT.\\n\\nSpecifications:\\n\\n- Item: NZXT Kraken 360 RGB White\\n- Price: NT$7,990 from the local retailer\\n- Thickness: 5.6 cm\\n\\n  ![nzxt](./img/img6.jpg)\\n\\n:::tip\\nIf you\u2019re an expert in this field, feel free to provide us with more professional advice. We\u2019ll prioritize it when building our next rig.\\n:::\\n\\n## Case and Fans\\n\\nFor this part, we just chose a case that could fit everything and then bought some additional fans to install.\\n\\nSpecifications:\\n\\n- Items:\\n  - ASUS TUF Gaming GT502 White ATX\\n  - TUF Gaming TF120 ARGB White Triple Fan Pack (Case Gift) x1\\n  - TUF Gaming TF120 ARGB White Triple Fan Pack (Additional Purchase) x1\\n  - TUF Gaming TF120 ARGB White Single Fan (Additional Purchase) x4\\n- Prices:\\n\\n  - Case with Gift: NT$5,490 from the local retailer\\n  - Additional Triple Fan Pack: NT$1,690 per set from the local retailer\\n  - Additional Single Fan: NT$549 per fan from the local retailer\\n\\n    ![asus case](./img/img7.jpg)\\n\\n## Power Supply Unit\\n\\nFor this part, we just searched online for recommended power supply brands of the year.\\n\\nAny brand that appeared on the lists seemed reliable to us, so we focused on warranty duration and price and went with FSP.\\n\\nSpecifications:\\n\\n- Item: FSP HYDRO PTM PRO 1200W ATX3.0 Platinum Fully Modular\\n- Price: NT$5,790 from the local retailer\\n\\n  ![fsp](./img/img8.jpg)\\n\\n---\\n\\nContinuing the discussion about the GPU, we need to add some information about the ATX 3.0 standard:\\n\\n- [**ATX 3.0 Standard**](https://zh.wikipedia.org/zh-tw/ATX%E8%A6%8F%E6%A0%BC)\\n\\n  The ATX 3.0 standard was released in February 2022. It includes a new 16-Pin 12VHPWR power interface, capable of delivering up to 600W of power to the graphics card. These components contain data lines used to negotiate power functions with the power supply unit, ensuring they do not consume more power than the PSU can provide. The standard also has stricter requirements for handling power spikes.\\n\\nTo meet the power demands of the RTX 4090, it\u2019s important to consider ATX 3.0 compatibility when selecting a power supply. Shortly after the RTX 4090\u2019s release, many cases of power issues were reported, mostly due to insufficient power supply, causing instability in the system.\\n\\n## Completion\\n\\nAfter buying all these components, we weren\u2019t entirely confident in our ability to assemble everything correctly, so we entrusted the final assembly to a technician at the store. Our photography skills aren\u2019t great, but you can check out the video below to see what the final product looks like\u2014it\u2019s quite similar, except our graphics card and all four memory sticks are also white.\\n\\n- [**ASUS TUF GT502 CUSTOM SETUP | Black & White PC BUILD | i9-13900k | ASUS TUF RTX 4070TI | ProArt Z790**](https://www.youtube.com/watch?v=puMYF4wpzTQ)\\n\\n  ![final](./img/img9.jpg)\\n\\n---\\n\\nThat wraps up the assembly process. Now it\u2019s time to enjoy training some models with our new rig!\\n\\n## Afterword 1\\n\\n:::info\\nUpdated in June 2024\\n:::\\n\\nLoaded with model training, based on Taiwan\'s residential electricity meter billing, the monthly electricity bill is about NT$5,000.\\n\\n## Afterword 2\\n\\n:::info\\nUpdated in October 2024\\n:::\\n\\nAfter using it for a year, I feel that buying a 4TB hard drive is too conservative:\\n\\n1. The WebFace42M dataset requires 0.9TB.\\n2. The ImageNet 21k dataset requires 1.3TB.\\n3. ...\\n\\nThere are other large-scale datasets, and 4TB is simply not enough. Next time, I need to buy at least 16TB."},{"id":"pyenv-installation","metadata":{"permalink":"/en/blog/pyenv-installation","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/10-10-pyenv-installation/index.md","title":"Managing Python Versions with pyenv","description":"A record of installing and using pyenv.","date":"2023-10-10T00:00:00.000Z","tags":[{"inline":true,"label":"pyenv","permalink":"/en/blog/tags/pyenv"},{"inline":true,"label":"virtualenv","permalink":"/en/blog/tags/virtualenv"}],"readingTime":4.46,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"pyenv-installation","title":"Managing Python Versions with pyenv","authors":"Z. Yuan","tags":["pyenv","virtualenv"],"image":"/en/img/2023/1010.webp","description":"A record of installing and using pyenv."},"unlisted":false,"prevItem":{"title":"Building a New Computer","permalink":"/en/blog/buy-a-new-computer"},"nextItem":{"title":"Recording and Troubleshooting Model Training Environment Issues","permalink":"/en/blog/python-env-info-collector"}},"content":"In the past, when using Python, I mostly relied on Conda for management. Nowadays, pyenv has become the go-to tool.\\n\\nIn this article, I briefly document how to install and use pyenv and provide necessary supplemental information for different operating systems.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nBefore installing `pyenv`, you need to have `Git` installed on your system.\\n\\n:::info\\nThe pyenv package provides a [**troubleshooting guide**](https://github.com/pyenv/pyenv/wiki/Common-build-problems).\\n\\nIf you encounter issues during the installation, you can refer to this page.\\n:::\\n\\n## Common Issues and Solutions\\n\\nHere are a few important cases and their solutions:\\n\\n- **Missing dependencies**\\n  Please first install all necessary packages and build tools according to the [**official pyenv dependency guide**](https://github.com/pyenv/pyenv/wiki#suggested-build-environment).\\n\\n- **zlib extension compilation failure**\\n\\n  The common error message is:\\n\\n  - `ERROR: The Python zlib extension was not compiled. Missing the zlib?`\\n\\n  Solution:\\n  - On Ubuntu/Debian systems, install `zlib1g` and `zlib1g-dev`:\\n    ```bash\\n    sudo apt install zlib1g zlib1g-dev\\n    ```\\n  - On macOS, if you installed zlib with Homebrew, set the environment variable:\\n    ```bash\\n    CPPFLAGS=\\"-I$(brew --prefix zlib)/include\\" pyenv install -v <python-version>\\n    ```\\n\\n- **OpenSSL extension compilation failure**\\n\\n  If you see:\\n\\n  - `ERROR: The Python ssl extension was not compiled. Missing the OpenSSL lib?`\\n\\n  Solution:\\n  - Make sure the OpenSSL development packages are installed (e.g., on Ubuntu use `sudo apt install libssl-dev`, on Fedora use `sudo dnf install openssl-devel`).\\n  - If OpenSSL is installed in a non-standard location, set the following:\\n    ```bash\\n    CPPFLAGS=\\"-I<openssl-install-path>/include\\" \\\\\\n    LDFLAGS=\\"-L<openssl-install-path>/lib\\" \\\\\\n    pyenv install -v <python-version>\\n    ```\\n\\n- **System resources are insufficient**\\n\\n  If you encounter the \\"resource temporarily unavailable\\" error, try reducing the make parallelism:\\n\\n  ```bash\\n  MAKE_OPTS=\'-j 1\' pyenv install <python-version>\\n  ```\\n\\n- **python-build definition not found**\\n\\n  If you encounter the `python-build: definition not found` error, update the python-build definitions:\\n\\n  ```bash\\n  cd ~/.pyenv/plugins/python-build && git pull\\n  ```\\n\\n- **macOS architecture-related errors**\\n\\n  If you see errors like `ld: symbol(s) not found for architecture x86_64` or `ld: symbol(s) not found for architecture arm64`, ensure that the Homebrew packages match the correct architecture and check if additional environment variables (such as CPPFLAGS, LDFLAGS, and CONFIGURE_OPTS) need to be set.\\n\\nFor more detailed information, refer to [**Common build problems**](https://github.com/pyenv/pyenv/wiki/Common-build-problems).\\n\\n## Cross-platform Considerations\\n\\n- **Linux/macOS:**\\n  - The installation method is generally the same, and you can directly use the commands in the next section.\\n  - Install the necessary compilation dependencies based on your operating system (e.g., on Ubuntu, you may need to install `build-essential`, `libssl-dev`, `zlib1g-dev`, etc.).\\n\\n- **Windows users:**\\n  - pyenv is natively designed for Unix-like environments, so it\u2019s recommended to use the [**pyenv-win**](https://github.com/pyenv-win/pyenv-win) version.\\n  - Alternatively, you can use WSL, Git Bash, or similar tools on Windows to get a Unix-like environment.\\n\\n- **Other Shell Users:**\\n  - If you use a shell other than bash or zsh (such as fish), refer to the corresponding shell configuration files for adjustments.\\n\\n## Installing `pyenv`\\n\\n1. **Run the installation command:**\\n\\n   You can quickly install `pyenv` with the following command:\\n\\n   ```bash\\n   curl https://pyenv.run | bash\\n   ```\\n\\n   This command will fetch and execute the installation script from the `pyenv-installer` repository on GitHub.\\n\\n2. **Configure your shell environment:**\\n\\n   After installation, follow the [**setup guide**](https://github.com/pyenv/pyenv#set-up-your-shell-environment-for-pyenv) to configure your shell environment to ensure `pyenv` works properly.\\n\\n   For bash, add the following lines to your `.bashrc` file:\\n\\n   ```bash\\n   export PATH=\\"$HOME/.pyenv/bin:$PATH\\"\\n   eval \\"$(pyenv init --path)\\"\\n   eval \\"$(pyenv virtualenv-init -)\\"\\n   ```\\n\\n   For zsh, add the same lines to your `.zshrc` file; for other shells, refer to their respective configuration files.\\n\\n3. **Restart your shell:**\\n\\n   After completing the above steps, reload the configuration:\\n\\n   ```bash\\n   exec $SHELL\\n   ```\\n\\n## Using `pyenv`\\n\\nOnce installed and configured, you can use `pyenv` to manage multiple Python versions:\\n\\n- **Install a new Python version:**\\n\\n  ```bash\\n  pyenv install 3.10.14\\n  ```\\n\\n- **Switch the global Python version:**\\n\\n  ```bash\\n  pyenv global 3.10.14\\n  ```\\n\\n- **Use a specific version in a particular directory:**\\n\\n  ```bash\\n  pyenv local 3.8.5\\n  ```\\n\\n## Virtual Environments\\n\\nVirtual environments are crucial in Python development as they help you use independent Python versions and dependencies in different projects, avoiding conflicts.\\n\\n:::tip\\nI personally recommend using virtual environments in every Python project, even if the environment is accidentally damaged, it can be easily deleted and recreated.\\n:::\\n\\n### Installation\\n\\n`pyenv` provides the `pyenv-virtualenv` plugin, making virtual environment management more convenient.\\n\\nThis functionality is now integrated into `pyenv` and can be used directly:\\n\\n```bash\\npyenv virtualenv 3.10.14 your-env-name\\n```\\n\\nHere, `3.10.14` is the Python version you want to use (make sure it\'s installed), and `your-env-name` is the name of the virtual environment.\\n\\n### Usage\\n\\nActivate the virtual environment:\\n\\n```bash\\npyenv activate your-env-name\\n```\\n\\n### Deleting\\n\\nWhen you no longer need the virtual environment, you can delete it with:\\n\\n```bash\\npyenv virtualenv-delete your-env-name\\n```\\n\\n## Updating `pyenv`\\n\\nIf you need to update `pyenv` to the latest version, you can follow these methods:\\n\\n- **Using the update plugin:** If you\u2019ve installed the [**pyenv-update**](https://github.com/pyenv/pyenv-update) plugin, you can directly execute:\\n\\n  ```bash\\n  pyenv update\\n  ```\\n\\n- **Manual update:**\\n  Go to the `~/.pyenv` directory and update using Git:\\n\\n  ```bash\\n  cd ~/.pyenv\\n  git pull\\n  ```\\n\\n## Removing `pyenv`\\n\\nIf you decide to stop using `pyenv`, follow these steps to remove it:\\n\\n1. **Remove the `pyenv` installation directory:**\\n\\n   ```bash\\n   rm -fr ~/.pyenv\\n   ```\\n\\n2. **Clean up shell configurations:**\\n\\n   Remove or comment out the lines related to `pyenv` in `.bashrc`, `.zshrc` (or other shell configuration files), then restart the shell:\\n\\n   ```bash\\n   exec $SHELL\\n   ```\\n\\n## Conclusion\\n\\nThese are the commonly used commands. I hope you have a great Python environment!"},{"id":"python-env-info-collector","metadata":{"permalink":"/en/blog/python-env-info-collector","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-22-python-env-info-collector/index.md","title":"Recording and Troubleshooting Model Training Environment Issues","description":"A custom logging tool.","date":"2023-09-22T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"training-log","permalink":"/en/blog/tags/training-log"}],"readingTime":6.26,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"python-env-info-collector","title":"Recording and Troubleshooting Model Training Environment Issues","authors":"Z. Yuan","tags":["python","training-log"],"image":"/en/img/2023/0922.webp","description":"A custom logging tool."},"unlisted":false,"prevItem":{"title":"Managing Python Versions with pyenv","permalink":"/en/blog/pyenv-installation"},"nextItem":{"title":"Setting Up PyPiServer on Ubuntu with Docker","permalink":"/en/blog/setting-up-pypiserver-on-ubuntu-with-docker"}},"content":"I\u2019m sharing a Python tool I created myself.\\n\\nThe main function is to quickly view model training-related information. While it\'s not a comprehensive tool, it should be enough for basic troubleshooting.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introduction\\n\\nWhen a model training fails, you\'ll want to know what caused it. At this point, you\u2019ll need to check the environment information of the training machine, such as Python version, PyTorch version, CUDA version, GPU information, CPU details, RAM stats, disk info, IP address, and more.\\n\\nIt\u2019s quite annoying!\\n\\nSo, we need to write a few programs to help solve the problem. Typically, we\u2019ll log the environment information into the training logs when the training starts, so that when a model fails, we can immediately check the logs.\\n\\n## Installation\\n\\nFirst, let\'s install the necessary packages:\\n\\n```bash\\npip install psutil requests\\n```\\n\\n:::tip\\nThe complete code is available on GitHub, and I will also include it at the end of this article.\\n\\n- [**system_info.py**](https://github.com/DocsaidLab/Capybara/blob/main/capybara/utils/system_info.py)\\n  :::\\n\\n## Using `get_package_versions`\\n\\nI integrated the program into `capybara-docsaid`, so you might consider installing it directly:\\n\\n```bash\\npip install capybara-docsaid\\n```\\n\\nOnce installed, you can call the related functions within your Python program:\\n\\n```python title=\\"get_package_versions.py\\"\\nfrom capybara import get_package_versions\\n\\nget_package_versions()\\n```\\n\\nThe output will be:\\n\\n```json\\n{\\n  \\"PyTorch Version\\": \\"2.1.1+cu121\\",\\n  \\"PyTorch Lightning Version\\": \\"2.1.2\\",\\n  \\"TensorFlow Error\\": \\"No module named \'tensorflow\'\\",\\n  \\"Keras Error\\": \\"No module named \'keras\'\\",\\n  \\"NumPy Version\\": \\"1.24.4\\",\\n  \\"Pandas Version\\": \\"2.0.3\\",\\n  \\"Scikit-learn Version\\": \\"1.3.2\\",\\n  \\"OpenCV Version\\": \\"4.8.1\\"\\n}\\n```\\n\\n- PyTorch Version: PyTorch version\\n- PyTorch Lightning Version: PyTorch Lightning version\\n- TensorFlow Error: TensorFlow version\\n- Keras Error: Keras version\\n- NumPy Version: NumPy version\\n- Pandas Version: Pandas version\\n- Scikit-learn Version: Scikit-learn version\\n- OpenCV Version: OpenCV version\\n\\n## Using `get_gpu_cuda_versions`\\n\\nNext, let\u2019s record the CUDA version. The program is as follows:\\n\\n```python title=\\"get_gpu_cuda_versions.py\\"\\nfrom capybara import get_gpu_cuda_versions\\n\\nget_gpu_cuda_versions()\\n```\\n\\nThe output will be:\\n\\n```json\\n{\\n  \\"CUDA Version\\": \\"12.1\\",\\n  \\"NVIDIA Driver Version\\": \\"535.129.03\\"\\n}\\n```\\n\\n- CUDA Version: CUDA version\\n- NVIDIA Driver Version: NVIDIA driver version\\n\\n## Using `get_system_info`\\n\\nNext, let\u2019s retrieve basic system information. The program is as follows:\\n\\n```python title=\\"get_system_info.py\\"\\nfrom capybara import get_system_info\\n\\nget_system_info()\\n```\\n\\nThe output will be:\\n\\n```json\\n{\\n  \\"OS Version\\": \\"Linux-6.2.0-37-generic-x86_64-with-glibc2.34\\",\\n  \\"CPU Model\\": \\"13th Gen Intel(R) Core(TM) i9-13900K\\",\\n  \\"Physical CPU Cores\\": 24,\\n  \\"Logical CPU Cores (incl. hyper-threading)\\": 32,\\n  \\"Total RAM (GB)\\": 125.56,\\n  \\"Available RAM (GB)\\": 110.9,\\n  \\"Disk Total (GB)\\": 1832.21,\\n  \\"Disk Used (GB)\\": 188.94,\\n  \\"Disk Free (GB)\\": 1550.12,\\n  \\"GPU Info\\": \\"NVIDIA GeForce RTX 4090\\",\\n  \\"IPV4 Address\\": [\\"192.168.---.---\\"],\\n  \\"IPV4 Address (External)\\": \\"---.---.---.---\\",\\n  \\"MAC Address\\": [\\"--.--.--.--.--.--\\"]\\n}\\n```\\n\\n- OS Version: Operating system version\\n- CPU Model: CPU model\\n- Physical CPU Cores: Number of physical CPU cores\\n- Logical CPU Cores (incl. hyper-threading): Number of logical CPU cores (including hyper-threading)\\n- Total RAM (GB): Total RAM capacity (GB)\\n- Available RAM (GB): Available RAM capacity (GB)\\n- Disk Total (GB): Total disk capacity (GB)\\n- Disk Used (GB): Used disk capacity (GB)\\n- Disk Free (GB): Free disk capacity (GB)\\n- GPU Info: GPU information\\n- IPV4 Address: Internal IPV4 address\\n- IPV4 Address (External): External IPV4 address\\n- MAC Address: MAC address\\n\\n## Notes and Alternatives\\n\\nMy system environment is Ubuntu, so if you\'re running this on a different operating system, there may be some discrepancies.\\n\\nHere are a few points to note:\\n\\n- Due to operating system restrictions, some functions might not work on all platforms. For example, `get_cpu_info` won\u2019t show the full CPU model on Windows. In such cases, you can consider using other tools or manually obtaining this information.\\n- If you\u2019re in a Windows environment, you cannot directly use `nvidia-smi` to get GPU information. Ensure you have installed NVIDIA drivers and related tools, and execute them in the command prompt.\\n- The external IP address is obtained from `https://httpbin.org/ip`, so ensure your network connection is working smoothly.\\n\\n## Code\\n\\n```python showLineNumbers\\nimport platform\\nimport socket\\nimport subprocess\\n\\nimport psutil\\nimport requests\\n\\n\\ndef get_package_versions():\\n    \\"\\"\\"\\n    Get versions of commonly used packages in deep learning and data science.\\n\\n    Returns:\\n        dict: Dictionary containing versions of installed packages.\\n    \\"\\"\\"\\n    versions_info = {}\\n\\n    # PyTorch\\n    try:\\n        import torch\\n        versions_info[\\"PyTorch Version\\"] = torch.__version__\\n    except Exception as e:\\n        versions_info[\\"PyTorch Error\\"] = str(e)\\n\\n    # PyTorch Lightning\\n    try:\\n        import pytorch_lightning as pl\\n        versions_info[\\"PyTorch Lightning Version\\"] = pl.__version__\\n    except Exception as e:\\n        versions_info[\\"PyTorch Lightning Error\\"] = str(e)\\n\\n    # TensorFlow\\n    try:\\n        import tensorflow as tf\\n        versions_info[\\"TensorFlow Version\\"] = tf.__version__\\n    except Exception as e:\\n        versions_info[\\"TensorFlow Error\\"] = str(e)\\n\\n    # Keras\\n    try:\\n        import keras\\n        versions_info[\\"Keras Version\\"] = keras.__version__\\n    except Exception as e:\\n        versions_info[\\"Keras Error\\"] = str(e)\\n\\n    # NumPy\\n    try:\\n        import numpy as np\\n        versions_info[\\"NumPy Version\\"] = np.__version__\\n    except Exception as e:\\n        versions_info[\\"NumPy Error\\"] = str(e)\\n\\n    # Pandas\\n    try:\\n        import pandas as pd\\n        versions_info[\\"Pandas Version\\"] = pd.__version__\\n    except Exception as e:\\n        versions_info[\\"Pandas Error\\"] = str(e)\\n\\n    # Scikit-learn\\n    try:\\n        import sklearn\\n        versions_info[\\"Scikit-learn Version\\"] = sklearn.__version__\\n    except Exception as e:\\n        versions_info[\\"Scikit-learn Error\\"] = str(e)\\n\\n    # OpenCV\\n    try:\\n        import cv2\\n        versions_info[\\"OpenCV Version\\"] = cv2.__version__\\n    except Exception as e:\\n        versions_info[\\"OpenCV Error\\"] = str(e)\\n\\n    # ... and so on for any other packages you\\"re interested in\\n\\n    return versions_info\\n\\n\\ndef get_gpu_cuda_versions():\\n    \\"\\"\\"\\n    Get GPU and CUDA versions using popular Python libraries.\\n\\n    Returns:\\n        dict: Dictionary containing CUDA and GPU driver versions.\\n    \\"\\"\\"\\n\\n    cuda_version = None\\n\\n    # Attempt to retrieve CUDA version using PyTorch\\n    try:\\n        import torch\\n        cuda_version = torch.version.cuda\\n    except ImportError:\\n        pass\\n\\n    # If not retrieved via PyTorch, try using TensorFlow\\n    if not cuda_version:\\n        try:\\n            import tensorflow as tf\\n            cuda_version = tf.version.COMPILER_VERSION\\n        except ImportError:\\n            pass\\n\\n    # If still not retrieved, try using CuPy\\n    if not cuda_version:\\n        try:\\n            import cupy\\n            cuda_version = cupy.cuda.runtime.runtimeGetVersion()\\n        except ImportError:\\n            cuda_version = \\"Error: None of PyTorch, TensorFlow, or CuPy are installed.\\"\\n\\n    # Try to get Nvidia driver version using nvidia-smi command\\n    try:\\n        smi_output = subprocess.check_output([\\n            \\"nvidia-smi\\",\\n            \\"--query-gpu=driver_version\\",\\n            \\"--format=csv,noheader,nounits\\"\\n        ]).decode(\\"utf-8\\").strip()\\n        nvidia_driver_version = smi_output.split(\\"\\\\n\\")[0]\\n    except Exception as e:\\n        nvidia_driver_version = f\\"Error getting NVIDIA driver version: {e}\\"\\n\\n    return {\\n        \\"CUDA Version\\": cuda_version,\\n        \\"NVIDIA Driver Version\\": nvidia_driver_version\\n    }\\n\\n\\ndef get_cpu_info():\\n    \\"\\"\\"\\n    Retrieve the CPU model name based on the platform.\\n\\n    Returns:\\n        str: CPU model name or \\"N/A\\" if not found.\\n    \\"\\"\\"\\n    if platform.system() == \\"Windows\\":\\n        return platform.processor()\\n    elif platform.system() == \\"Darwin\\":\\n        # For macOS\\n        command = \\"sysctl -n machdep.cpu.brand_string\\"\\n        return subprocess.check_output(command, shell=True).strip().decode()\\n    elif platform.system() == \\"Linux\\":\\n        # For Linux\\n        command = \\"cat /proc/cpuinfo | grep \\"model name\\" | uniq\\"\\n        return subprocess.check_output(command, shell=True).strip().decode().split(\\":\\")[1].strip()\\n    else:\\n        return \\"N/A\\"\\n\\n\\ndef get_external_ip():\\n    try:\\n        response = requests.get(\\"https://httpbin.org/ip\\")\\n        return response.json()[\\"origin\\"]\\n    except Exception as e:\\n        return f\\"Error obtaining IP: {e}\\"\\n\\n\\ndef get_system_info():\\n    \\"\\"\\"\\n    Fetch system information like OS version, CPU info, RAM, Disk usage, etc.\\n\\n    Returns:\\n        dict: Dictionary containing system information.\\n    \\"\\"\\"\\n    info = {\\n        \\"OS Version\\": platform.platform(),\\n        \\"CPU Model\\": get_cpu_info(),\\n        \\"Physical CPU Cores\\": psutil.cpu_count(logical=False),\\n        \\"Logical CPU Cores (incl. hyper-threading)\\": psutil.cpu_count(logical=True),\\n        \\"Total RAM (GB)\\": round(psutil.virtual_memory().total / (1024 ** 3), 2),\\n        \\"Available RAM (GB)\\": round(psutil.virtual_memory().available / (1024 ** 3), 2),\\n        \\"Disk Total (GB)\\": round(psutil.disk_usage(\\"/\\").total / (1024 ** 3), 2),\\n        \\"Disk Used (GB)\\": round(psutil.disk_usage(\\"/\\").used / (1024 ** 3), 2),\\n        \\"Disk Free (GB)\\": round(psutil.disk_usage(\\"/\\").free / (1024 ** 3), 2)\\n    }\\n\\n    # Try to fetch GPU information using nvidia-smi command\\n    try:\\n        gpu_info = subprocess.check_output(\\n            [\\"nvidia-smi\\", \\"--query-gpu=name\\", \\"--format=csv,noheader,nounits\\"]\\n        ).decode(\\"utf-8\\").strip()\\n        info[\\"GPU Info\\"] = gpu_info\\n    except Exception:\\n        info[\\"GPU Info\\"] = \\"N/A or Error\\"\\n\\n    # Get network information\\n    addrs = psutil.net_if_addrs()\\n    info[\\"IPV4 Address\\"] = [\\n        addr.address for addr in addrs.get(\\"enp5s0\\", []) if addr.family == socket.AF_INET\\n    ]\\n\\n    info[\\"IPV4 Address (External)\\"] = get_external_ip()\\n\\n    # Determine platform and choose correct address family for MAC\\n    if hasattr(socket, \\"AF_LINK\\"):\\n        AF_LINK = socket.AF_LINK\\n    elif hasattr(psutil, \\"AF_LINK\\"):\\n        AF_LINK = psutil.AF_LINK\\n    else:\\n        raise Exception(\\n            \\"Cannot determine the correct AF_LINK value for this platform.\\")\\n\\n    info[\\"MAC Address\\"] = [\\n        addr.address for addr in addrs.get(\\"enp5s0\\", []) if addr.family == AF_LINK\\n    ]\\n\\n    return info\\n```"},{"id":"setting-up-pypiserver-on-ubuntu-with-docker","metadata":{"permalink":"/en/blog/setting-up-pypiserver-on-ubuntu-with-docker","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-13-ubuntu-docker-pypiserver/index.md","title":"Setting Up PyPiServer on Ubuntu with Docker","description":"Recording the process of setting up PyPiServer on Ubuntu.","date":"2023-09-13T00:00:00.000Z","tags":[{"inline":true,"label":"docker","permalink":"/en/blog/tags/docker"},{"inline":true,"label":"pypiserver","permalink":"/en/blog/tags/pypiserver"}],"readingTime":4.535,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"setting-up-pypiserver-on-ubuntu-with-docker","title":"Setting Up PyPiServer on Ubuntu with Docker","authors":"Z. Yuan","tags":["docker","pypiserver"],"image":"/en/img/2023/0913.webp","description":"Recording the process of setting up PyPiServer on Ubuntu."},"unlisted":false,"prevItem":{"title":"Recording and Troubleshooting Model Training Environment Issues","permalink":"/en/blog/python-env-info-collector"},"nextItem":{"title":"Set Up SSH Server on Ubuntu","permalink":"/en/blog/ubuntu-install-ssh"}},"content":"Recently, I set up a PyPi Server to host my package installation files. I used an Ubuntu system with Docker for the setup, and I\u2019ll document the process here.\\n\\n:::tip\\nI assume the reader already has Docker installed on Ubuntu and is familiar with basic Docker operations. If not, you might want to first review related knowledge.\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n## Pull the Image\\n\\n```bash\\ndocker pull pypiserver/pypiserver:latest\\n```\\n\\n## Create Directory\\n\\nNo need for much explanation, simply create a directory under your home directory to store Python packages.\\n\\n```bash\\nmkdir ~/packages\\n```\\n\\nYou can choose your preferred name, but make sure to update the configuration files accordingly.\\n\\n## Set Up htpasswd\\n\\n:::tip\\nIf you don\u2019t want to set up a password, you can skip this step.\\n:::\\n\\nhtpasswd is a file format used to store usernames and passwords, which PyPiServer uses for user authentication. This is a simple yet effective way to enhance PyPiServer\'s security.\\n\\nFirst, install `apache2-utils`:\\n\\n```bash\\nsudo apt install apache2-utils\\n```\\n\\nThen, use the following command to create a new `.htpasswd` file:\\n\\n```bash\\nhtpasswd -c ~/.htpasswd [username]\\n```\\n\\nIt will prompt you to enter the password for `username`. After entering the password, the `.htpasswd` file will be created in your home directory. Once the file is created, we can run `pypiserver` with the `docker run` command and use the `.htpasswd` file for authentication.\\n\\n## Mount as a Background Service\\n\\nTo run the Docker container as a background service, we will use Docker Compose and Systemd.\\n\\nIf you haven\'t installed Docker Compose yet, follow the installation instructions:\\n\\n- [**Official Docker Compose Installation Guide**](https://docs.docker.com/compose/install/)\\n\\nNote that Docker Compose has undergone significant updates recently, and many commands have changed. The most notable change is that the previously used `docker-compose` command is now simply `docker compose`.\\n\\nTo install the latest version of Docker Compose, follow these steps:\\n\\n```bash\\nsudo apt update\\nsudo apt install docker-compose-plugin\\n```\\n\\nCheck if Docker Compose is installed correctly:\\n\\n```bash\\ndocker compose version\\n```\\n\\n### Create Configuration File\\n\\nCreate a `docker-compose.yml` file in a convenient location and add the following content:\\n\\n```yaml {6-7} title=\\"docker-compose.yml\\"\\nversion: \\"3.3\\"\\nservices:\\n  pypiserver:\\n    image: pypiserver/pypiserver:latest\\n    volumes:\\n      - /home/[username]/auth:/data/auth\\n      - /home/[username]/packages:/data/packages\\n    command: run -P /data/auth/.htpasswd -a update,download,list /data/packages\\n    ports:\\n      - \\"8080:8080\\"\\n```\\n\\n- Make sure to replace `[username]` with your actual username.\\n- You can modify the external port mapping, for example: `\\"18080:8080\\"`.\\n\\n:::tip\\nYou can refer to the example provided by `pypiserver` here: [**docker-compose.yml**](https://github.com/pypiserver/pypiserver/blob/master/docker-compose.yml)\\n:::\\n\\nIf you don\u2019t want to set up a password for authentication, modify the `command` line in the `docker-compose.yml` file as follows:\\n\\n```yaml\\ncommand: run -a . -P . /data/packages --server wsgiref\\n```\\n\\n### Create Systemd Service\\n\\nCreate a configuration file for the service:\\n\\n```bash\\nsudo vim /etc/systemd/system/pypiserver.service\\n```\\n\\nAdd the following content:\\n\\n```bash {7} title=\\"/etc/systemd/system/pypiserver.service\\"\\n[Unit]\\nDescription=PypiServer Docker Compose\\nRequires=docker.service\\nAfter=docker.service\\n\\n[Service]\\nWorkingDirectory=/path/to/your/docker-compose/directory\\nExecStart=/usr/bin/docker compose up --remove-orphans\\nExecStop=/usr/bin/docker compose down\\nRestart=always\\n\\n[Install]\\nWantedBy=multi-user.target\\n```\\n\\n- Make sure to replace `/path/to/your/docker-compose/directory` with the actual path where your `docker-compose.yml` file is located, without the filename.\\n- Verify that the Docker path is correct by using `which docker` if needed.\\n\\n### Start the Service\\n\\nTell systemd to reload the new service configuration:\\n\\n```bash\\nsudo systemctl daemon-reload\\n```\\n\\nStart the service:\\n\\n```bash\\nsudo systemctl enable pypiserver.service\\nsudo systemctl start pypiserver.service\\n```\\n\\n## Check the Status\\n\\nTo check the current status of the service, use:\\n\\n```bash\\nsudo systemctl status pypiserver.service\\n```\\n\\nThis will show the current status of the `pypiserver` service, including whether it is running and the most recent log output.\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"80%\\"}}>\\n![pypiserver status](./img/pypiserver.jpg)\\n</figure>\\n</div>\\n\\n## Get Started\\n\\nNow, you can start using `pip` to install and upload packages.\\n\\n### Upload a Package\\n\\nLet\u2019s assume we have a package named `example_package-0.1-py3-none-any.whl`. Use `twine` to upload the package:\\n\\n```bash\\npip install twine\\ntwine upload --repository-url http://localhost:8080/ example_package-0.1-py3-none-any.whl\\n```\\n\\n- Make sure that `localhost:8080` is the correct address and port of your PyPiServer.\\n\\n### Download and Install a Package\\n\\nTo install a package, specify the address and port of your PyPiServer:\\n\\n```bash\\npip install --index-url http://localhost:8080/ example_package\\n```\\n\\n### Use Basic Authentication\\n\\nIf you have set up basic authentication for your PyPiServer, you will need to provide credentials when uploading or downloading packages:\\n\\n- Upload a package:\\n\\n  ```bash\\n  twine upload \\\\\\n    --repository-url http://localhost:8080/ \\\\\\n    --username [username] \\\\\\n    --password [password] \\\\\\n    example_package-0.1-py3-none-any.whl\\n  ```\\n\\n- Install a package:\\n\\n  ```bash\\n  pip install \\\\\\n    --index-url http://[username]:[password]@localhost:8080/ \\\\\\n    example_package\\n  ```\\n\\n## Set Up `pip.conf`\\n\\nIf you often install packages from this server, you might not want to specify the `--index-url` every time you use `pip install`. Instead, you can add the related configuration to your `pip.conf`.\\n\\n### Configuration File\\n\\nThe `pip.conf` file can exist in several places, and they are checked in the following order of priority:\\n\\n- Priority 1: Site-level configuration file:\\n\\n  - `/home/[username]/.pyenv/versions/3.x.x/envs/main/pip.conf`\\n\\n- Priority 2: User-level configuration file:\\n\\n  - `/home/[username]/.pip/pip.conf`\\n  - `/home/[username]/.config/pip/pip.conf`\\n\\n- Priority 3: Global configuration file:\\n\\n  - `/etc/pip.conf`\\n  - `/etc/xdg/pip/pip.conf`\\n\\nOnce you identify the correct `pip.conf` file for your current Python environment, add the following content:\\n\\n```bash\\n[global]\\nindex-url = http://[server-ip]:8080/\\ntrusted-host = [server-ip]\\n```\\n\\nMake sure to replace `[server-ip]:8080` with the correct address and port of your PyPiServer.\\n\\nAfter this configuration, whenever you run `pip install [package_name]`, the system will automatically use the server address set in `pip.conf` as the package source.\\n\\n## Conclusion\\n\\nNow, you should have successfully set up your own PyPi server and learned how to upload and download packages.\\n\\nI hope this article helps resolve your setup issues."},{"id":"ubuntu-install-ssh","metadata":{"permalink":"/en/blog/ubuntu-install-ssh","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-12-ubuntu-install-ssh/index.md","title":"Set Up SSH Server on Ubuntu","description":"Server setup and passwordless login tutorial.","date":"2023-09-12T00:00:00.000Z","tags":[{"inline":true,"label":"ubuntu","permalink":"/en/blog/tags/ubuntu"},{"inline":true,"label":"ssh","permalink":"/en/blog/tags/ssh"}],"readingTime":1.54,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"ubuntu-install-ssh","title":"Set Up SSH Server on Ubuntu","authors":"Z. Yuan","tags":["ubuntu","ssh"],"image":"/en/img/2023/0912.webp","description":"Server setup and passwordless login tutorial."},"unlisted":false,"prevItem":{"title":"Setting Up PyPiServer on Ubuntu with Docker","permalink":"/en/blog/setting-up-pypiserver-on-ubuntu-with-docker"},"nextItem":{"title":"Auto-Run GitHub Runner","permalink":"/en/blog/ubuntu-github-runner-systemd"}},"content":"SSH is a network protocol that allows users to securely access and manage remote servers.\\n\\nThis time, we\u2019ll document the detailed steps for passwordless login.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Install OpenSSH Server\\n\\nOpen the terminal.\\n\\nEnter the following commands to install the OpenSSH server:\\n\\n```bash\\nsudo apt update\\nsudo apt install openssh-server\\n```\\n\\n## Check SSH Server Status\\n\\nUse the following command to check the SSH server\u2019s status:\\n\\n```bash\\nsudo systemctl status ssh\\n```\\n\\nIf you see \u201CActive: active (running),\u201D then the SSH server has started successfully.\\n\\n## SSH Passwordless Login Setup:\\n\\n### Generate SSH Key Pair on the Client\\n\\nOpen the terminal.\\n\\nEnter the following command to generate the key pair:\\n\\n```bash\\nssh-keygen\\n```\\n\\nFollow the prompts. The default settings are usually sufficient. When asked for a password, simply press Enter to create a key pair without a password.\\n\\n### Copy the Public Key to the Server\\n\\nUse the `ssh-copy-id` command to copy the public key to the server. Replace [username] and [server-ip] with your server details.\\n\\n```bash\\nssh-copy-id [username]@[server-ip]\\n```\\n\\nFor example:\\n\\n```bash\\nssh-copy-id john@192.168.0.100\\n```\\n\\nIf the server uses a different SSH port (e.g., 2222), use the `-p` parameter:\\n\\n```bash\\nssh-copy-id -p 2222 john@192.168.0.100\\n```\\n\\nThis command will prompt you for the server\'s password.\\n\\nOnce verified successfully, your public key will be added to the server\'s `~/.ssh/authorized_keys` file.\\n\\n### Test Passwordless Login\\n\\nTry SSH into the server:\\n\\n```bash\\nssh [username]@[server-ip]\\n```\\n\\nIf everything is configured correctly, you should be able to log into the server without a password.\\n\\n## Disable Password Authentication\\n\\nWith the SSH key, you may want to disable password authentication for increased security.\\n\\nThis can be configured in the server\u2019s `/etc/ssh/sshd_config`:\\n\\n```bash\\nsudo vim /etc/ssh/sshd_config\\n```\\n\\nFind the `PasswordAuthentication` option in the file and set it to `no`.\\n\\nAfter completing these steps, congratulations! You can now happily use SSH without a password."},{"id":"ubuntu-github-runner-systemd","metadata":{"permalink":"/en/blog/ubuntu-github-runner-systemd","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-10-ubuntu-github-action-runner-systemd/index.md","title":"Auto-Run GitHub Runner","description":"Run and automatically start in the background using Ubuntu Systemd.","date":"2023-09-10T00:00:00.000Z","tags":[{"inline":true,"label":"github-action","permalink":"/en/blog/tags/github-action"},{"inline":true,"label":"runner","permalink":"/en/blog/tags/runner"}],"readingTime":2.445,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"ubuntu-github-runner-systemd","title":"Auto-Run GitHub Runner","authors":"Z. Yuan","tags":["github-action","runner"],"image":"/en/img/2023/0910.webp","description":"Run and automatically start in the background using Ubuntu Systemd."},"unlisted":false,"prevItem":{"title":"Set Up SSH Server on Ubuntu","permalink":"/en/blog/ubuntu-install-ssh"},"nextItem":{"title":"Login to RTF8207W Router","permalink":"/en/blog/login-rtf8207w"}},"content":"When collaborating with GitHub, we often use private hosts for CI/CD tasks.\\n\\nGitHub provides documentation to guide users through the initial installation steps, and following the steps will successfully configure the setup.\\n\\n\x3c!-- truncate --\x3e\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"80%\\"}}>\\n![github_set_runner](./img/github_set_runner.jpg)\\n</figure>\\n<figcaption>Documentation</figcaption>\\n</div>\\n\\n## Problem Description\\n\\nHowever, your host will often need to reboot. If no configuration is set up, the Runner service will fall asleep indefinitely. This issue can be forgotten until someone notices there\u2019s no response or receives a complaint, which may have already been days later.\\n\\nThis kind of problem can happen repeatedly and become very annoying!\\n\\nTherefore, we need to make it run automatically!\\n\\n## Configuration Process\\n\\nTo automatically run a task after the host boots, we can use systemd.\\n\\n1. **Create a new systemd service file:**\\n\\n   ```bash\\n   sudo vim /etc/systemd/system/actions-runner.service\\n   ```\\n\\n2. **Paste the following content into the file:**\\n\\n   ```bash {7-9} title=\\"/etc/systemd/system/actions-runner.service\\"\\n   [Unit]\\n   Description=GitHub Action Runner\\n   After=network.target\\n\\n   [Service]\\n   Type=simple\\n   User=your-username\\n   WorkingDirectory=/home/your-username/actions-runner\\n   ExecStart=/home/your-username/actions-runner/run.sh\\n   Restart=always\\n   RestartSec=5\\n\\n   [Install]\\n   WantedBy=multi-user.target\\n   ```\\n\\n   Pay attention to the highlighted areas:\\n\\n   - `User`, `ExecStart`, and `WorkingDirectory` should be changed to your own username.\\n\\n3. **Tell systemd to reload the new service configuration:**\\n\\n   ```bash\\n   sudo systemctl daemon-reload\\n   ```\\n\\n4. **Enable the service so it starts automatically when the host boots:**\\n\\n   ```bash\\n   sudo systemctl enable actions-runner.service\\n   ```\\n\\n5. **Now you can manually start the service or reboot to test it:**\\n\\n   ```bash\\n   sudo systemctl start actions-runner.service\\n   ```\\n\\nWith this method, the actions-runner will automatically run in the background when your host boots.\\n\\nIf you want to stop the service, you can use the following command:\\n\\n```bash\\nsudo systemctl stop actions-runner.service\\n```\\n\\n:::warning\\nMake sure that `run.sh` has executable permissions.\\n:::\\n\\n## Check the Status\\n\\nWhen managing a service with systemd, you can check the logs to understand its current status.\\n\\nUse the following command:\\n\\n```bash\\nsudo journalctl -u actions-runner.service -f\\n```\\n\\nExplanation of the parameters:\\n\\n- `-u actions-runner.service`: Only show logs for the service named actions-runner.\\n- `-f`: This option makes journalctl follow the latest logs, so you can see the new output in real time.\\n\\nAdditionally, if you want to check the current status of the service, you can use:\\n\\n```bash\\nsudo systemctl status actions-runner.service\\n```\\n\\nThis will display the current status of the `actions-runner` service, including whether it\'s running and the most recent log output:\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"80%\\"}}>\\n![action-service](./img/action-service.jpg)\\n</figure>\\n</div>\\n\\n## Reconfigure\\n\\nThis part is off-topic and unrelated to auto-running.\\n\\nIf the original Runner is missing (usually due to switching a repository between Public and Private), or the Runner hasn\u2019t been invoked in a long time and got lost!\\n\\nIn this case, you\u2019ll need to reconfigure:\\n\\n1. Obtain a new Token from your GitHub account.\\n2. Go to your actions-runner folder (or another folder you named) and delete the `.runner` file, then run the configuration command:\\n\\n   ```bash\\n   ./config.sh --url ... (use the new Token configuration)\\n   ```\\n\\nThe other steps are the same, and after configuring, just restart the service."},{"id":"login-rtf8207w","metadata":{"permalink":"/en/blog/login-rtf8207w","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-04-login-rtf8207w/index.md","title":"Login to RTF8207W Router","description":"Stealthily login to the RTF8207W router.","date":"2023-09-04T00:00:00.000Z","tags":[{"inline":true,"label":"rtf8207w","permalink":"/en/blog/tags/rtf-8207-w"}],"readingTime":0.77,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"login-rtf8207w","title":"Login to RTF8207W Router","authors":"Z. Yuan","tags":["rtf8207w"],"image":"/en/img/2023/0904.webp","description":"Stealthily login to the RTF8207W router."},"unlisted":false,"prevItem":{"title":"Auto-Run GitHub Runner","permalink":"/en/blog/ubuntu-github-runner-systemd"},"nextItem":{"title":"Fail2ban: Protecting SSH Services","permalink":"/en/blog/fail2ban-settings"}},"content":"This time, I want to log into the Chunghwa Telecom \\"Turtle,\\" the RTF8207W modem.\\n\\n\x3c!-- truncate --\x3e\\n\\n### 1. Login to the Management Interface\\n\\nYou can usually access it by entering `192.168.1.1` or another IP address into your web browser. The specific address may vary depending on your router model.\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"60%\\"}}>\\n![login-rtf8207w](./img/RTF8207W-login.jpg)\\n</figure>\\n</div>\\n\\n### 2. Engineer Account\\n\\nTypically, the username is `cht`, and the password is the last four digits of the model number plus the last four digits of the MAC address.\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"60%\\"}}>\\n![password](./img/RTF8207W.jpg)\\n</figure>\\n</div>\\n\\n### 3. Login\\n\\nOnce logged in, you can do whatever you need.\\n\\n:::tip\\nThe same logic generally applies to other models.\\n:::\\n\\n:::danger\\nIf you accidentally mess up the settings and cause a network crash that prevents you from reconnecting, please contact the **Chunghwa Telecom engineers** yourself.\\n\\nThis article is for reference only and bears no responsibility.\\n:::"},{"id":"fail2ban-settings","metadata":{"permalink":"/en/blog/fail2ban-settings","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-03-fail2ban-settings/index.md","title":"Fail2ban: Protecting SSH Services","description":"Block all the malicious access outside.","date":"2023-09-03T00:00:00.000Z","tags":[{"inline":true,"label":"ubuntu","permalink":"/en/blog/tags/ubuntu"},{"inline":true,"label":"fail2ban","permalink":"/en/blog/tags/fail-2-ban"}],"readingTime":1.98,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"fail2ban-settings","title":"Fail2ban: Protecting SSH Services","authors":"Z. Yuan","tags":["ubuntu","fail2ban"],"image":"/en/img/2023/0903.webp","description":"Block all the malicious access outside."},"unlisted":false,"prevItem":{"title":"Login to RTF8207W Router","permalink":"/en/blog/login-rtf8207w"},"nextItem":{"title":"Unicode Table","permalink":"/en/blog/unicode-table"}},"content":"After you successfully open an external SSH channel, you\u2019ll immediately notice a bunch of malicious connections trying to log into your machine.\\n\\n\x3c!-- truncate --\x3e\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"40%\\"}}>\\n![attack from ssh](./img/ban_1.jpg)\\n</figure>\\n<figcaption>Illustration of a malicious attack</figcaption>\\n</div>\\n\\n---\\n\\nA common approach is to use Fail2ban to protect our host. This is a software that prevents servers from brute-force attacks.\\n\\nWhen the system detects suspicious activity (e.g., repeated failed login attempts), Fail2ban automatically modifies firewall rules to block the attacker\'s IP address.\\n\\n## 1. Install Fail2ban\\n\\nOn most Linux distributions, you can use a package manager to install Fail2ban.\\n\\nSince my host is Ubuntu, I\u2019ll use apt to install it:\\n\\n```bash\\nsudo apt install fail2ban\\n```\\n\\n## 2. Configuration\\n\\nThe configuration file is located at `/etc/fail2ban/jail.conf`.\\n\\nBut wait!\\n\\nInstead of directly modifying this file, copy it to `jail.local` and modify that:\\n\\n```bash\\nsudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local\\n```\\n\\nEdit `jail.local`:\\n\\n```bash\\nsudo vim /etc/fail2ban/jail.local\\n```\\n\\nThis file contains several important configuration parameters with the following corresponding functions:\\n\\n- **ignoreip:** IP addresses or ranges to ignore, e.g., 127.0.0.1/8\\n- **bantime:** Block time in seconds (default is 600 seconds)\\n- **findtime:** Time period to observe how many failed attempts (default is 600 seconds)\\n- **maxretry:** Maximum number of allowed failed attempts within the `findtime` period.\\n\\n## 3. Start and Monitor\\n\\nStart Fail2ban:\\n\\n```bash\\nsudo service fail2ban start\\n```\\n\\nCheck the status of Fail2ban:\\n\\n```bash\\nsudo fail2ban-client status\\n```\\n\\n## 4. Add Custom Rules\\n\\nIf you want to set specific rules for a particular service, you can add or modify the corresponding section in `jail.local`, such as for SSH:\\n\\n```bash\\n[sshd]\\nenabled = true\\nport = ssh\\nfilter = sshd\\nlogpath = /var/log/auth.log\\nmaxretry = 3\\n```\\n\\n## 5. Test\\n\\nAfter making changes to the configuration, restart Fail2ban to apply the changes:\\n\\n```bash\\nsudo service fail2ban restart\\n```\\n\\nThen, test from another machine or use a different IP to attempt multiple failed logins and see if it gets blocked.\\n\\n## 6. View\\n\\nMake sure to regularly check the log files and update rules for optimal protection.\\n\\n```bash\\nsudo fail2ban-client status sshd\\n```\\n\\n## 7. Unban\\n\\nIf you were blocked during testing, remember to unban your test IP:\\n\\n```bash\\nsudo fail2ban-client set sshd unbanip <IP_ADDRESS>\\n```\\n\\n## Conclusion\\n\\nThe entire process is a bit tedious, but not complicated.\\n\\nI hope this article helps you successfully complete the relevant configuration."},{"id":"unicode-table","metadata":{"permalink":"/en/blog/unicode-table","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-02-unicode-table/index.md","title":"Unicode Table","description":"Unicode Table for easy reference.","date":"2023-09-02T00:00:00.000Z","tags":[{"inline":true,"label":"unicode","permalink":"/en/blog/tags/unicode"}],"readingTime":25.66,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"unicode-table","title":"Unicode Table","authors":"Z. Yuan","tags":["unicode"],"image":"/en/img/2023/0902.webp","description":"Unicode Table for easy reference."},"unlisted":false,"prevItem":{"title":"Fail2ban: Protecting SSH Services","permalink":"/en/blog/fail2ban-settings"},"nextItem":{"title":"Configuring Selective Traffic Routing for VPN","permalink":"/en/blog/mac-selective-vpn-routing"}},"content":"Since every time I need to look up Unicode codes, I end up spending half an hour searching through the Wiki tables, I decided to store some basic information for future use.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is Unicode?\\n\\nUnicode is an international character encoding standard maintained by the non-profit organization Unicode Consortium. This standard unifies most of the world\u2019s writing systems, enabling information exchange across platforms and in multilingual environments.\\n\\nIn the Unicode standard, each character corresponds to a unique numeric code, known as a \\"code point.\\" A code point is an important concept in the Unicode standard, used to determine the location of each unique character. The range of code points spans from U+0000 to U+10FFFF.\\n\\nThese code points are divided into several subsets based on different functions and uses, such as:\\n\\n- **Basic Multilingual Plane (BMP)**: Includes commonly used characters such as Latin letters, Greek letters, Chinese characters, etc.\\n- **Supplementary Planes**: Includes more ideographic characters, supplementary characters, etc.\\n\\n:::tip\\nIf you have more specific needs, we still recommend checking the original tables: [**Unicode Code Points**](https://zh.wikipedia.org/zh-tw/Unicode%E5%8D%80%E6%AE%B5)\\n:::\\n\\n## References\\n\\n- [**Unicode Character Plane Mapping**](https://zh.wikipedia.org/zh-tw/Unicode%E5%AD%97%E7%AC%A6%E5%B9%B3%E9%9D%A2%E6%98%A0%E5%B0%84)\\n- [**Unicode Code Points**](https://zh.wikipedia.org/zh-tw/Unicode%E5%8D%80%E6%AE%B5)\\n\\n## Unicode Code Point Range Table\\n\\nIf you require further details, we recommend consulting the original table: [**Unicode Code Points**](https://en.wikipedia.org/wiki/Unicode#Code_points)\\n\\n|  Plane   | Code Point Range |         Chinese Name         |                   English Name                   |\\n| :------: | :--------------: | :--------------------------: | :----------------------------------------------: |\\n|  0 BMP   |    0000-007F     |         \u57FA\u672C\u62C9\u4E01\u5B57\u6BCD         |                   Basic Latin                    |\\n|  0 BMP   |    0080-00FF     |        \u62C9\u4E01\u5B57\u6BCD\u88DC\u5145-1        |                Latin-1 Supplement                |\\n|  0 BMP   |    0100-017F     |        \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55-A        |                 Latin Extended-A                 |\\n|  0 BMP   |    0180-024F     |        \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55-B        |                 Latin Extended-B                 |\\n|  0 BMP   |    0250-02AF     |         \u570B\u969B\u97F3\u6A19\u64F4\u5C55         |                  IPA Extensions                  |\\n|  0 BMP   |    02B0-02FF     |         \u4F54\u4F4D\u4FEE\u98FE\u7B26\u865F         |             Spacing Modifier Letters             |\\n|  0 BMP   |    0300-036F     |         \u7D44\u5408\u9644\u52A0\u7B26\u865F         |           Combining Diacritical Marks            |\\n|  0 BMP   |    0370-03FF     |     \u5E0C\u81D8\u5B57\u6BCD\u548C\u79D1\u666E\u7279\u5B57\u6BCD     |                 Greek and Coptic                 |\\n|  0 BMP   |    0400-04FF     |          \u897F\u91CC\u723E\u5B57\u6BCD          |                     Cyrillic                     |\\n|  0 BMP   |    0500-052F     |        \u897F\u91CC\u723E\u5B57\u6BCD\u88DC\u5145        |               Cyrillic Supplement                |\\n|  0 BMP   |    0530-058F     |         \u4E9E\u7F8E\u5C3C\u4E9E\u5B57\u6BCD         |                     Armenian                     |\\n|  0 BMP   |    0590-05FF     |         \u5E0C\u4F2F\u4F86\u6587\u5B57\u6BCD         |                      Hebrew                      |\\n|  0 BMP   |    0600-06FF     |         \u963F\u62C9\u4F2F\u6587\u5B57\u6BCD         |                      Arabic                      |\\n|  0 BMP   |    0700-074F     |          \u6558\u5229\u4E9E\u5B57\u6BCD          |                      Syriac                      |\\n|  0 BMP   |    0750-077F     |         \u963F\u62C9\u4F2F\u6587\u88DC\u5145         |                Arabic Supplement                 |\\n|  0 BMP   |    0780-07BF     |           \u5B83\u62FF\u5B57\u6BCD           |                      Thaana                      |\\n|  0 BMP   |    07C0-07FF     |         \u897F\u975E\u66F8\u9762\u6587\u5B57         |                       NKo                        |\\n|  0 BMP   |    0800-083F     |         \u6492\u746A\u5229\u4E9E\u5B57\u6BCD         |                    Samaritan                     |\\n|  0 BMP   |    0840-085F     |          \u66FC\u9054\u5B89\u5B57\u6BCD          |                     Mandaic                      |\\n|  0 BMP   |    0860-086F     |         \u6558\u5229\u4E9E\u6587\u88DC\u5145         |                Syriac Supplement                 |\\n|  0 BMP   |    0870-089F     |       \u963F\u62C9\u4F2F\u5B57\u6BCD\u64F4\u5C55-B       |                Arabic Extended-B                 |\\n|  0 BMP   |    08A0-08FF     |       \u963F\u62C9\u4F2F\u5B57\u6BCD\u64F4\u5C55-A       |                Arabic Extended-A                 |\\n|  0 BMP   |    0900-097F     |            \u5929\u57CE\u6587            |                    Devanagari                    |\\n|  0 BMP   |    0980-09FF     |           \u5B5F\u52A0\u62C9\u6587           |                     Bengali                      |\\n|  0 BMP   |    0A00-0A7F     |           \u53E4\u6728\u57FA\u6587           |                     Gurmukhi                     |\\n|  0 BMP   |    0A80-0AFF     |          \u53E4\u5409\u62C9\u7279\u6587          |                     Gujarati                     |\\n|  0 BMP   |    0B00-0B7F     |           \u5967\u91CC\u4E9E\u6587           |                      Oriya                       |\\n|  0 BMP   |    0B80-0BFF     |           \u6CF0\u7C73\u723E\u6587           |                      Tamil                       |\\n|  0 BMP   |    0C00-0C7F     |           \u6CF0\u76E7\u56FA\u6587           |                      Telugu                      |\\n|  0 BMP   |    0C80-0CFF     |           \u5361\u7D0D\u9054\u6587           |                     Kannada                      |\\n|  0 BMP   |    0D00-0D7F     |         \u99AC\u62C9\u96C5\u62C9\u59C6\u6587         |                    Malayalam                     |\\n|  0 BMP   |    0D80-0DFF     |           \u50E7\u4F3D\u7F85\u6587           |                     Sinhala                      |\\n|  0 BMP   |    0E00-0E7F     |             \u6CF0\u6587             |                       Thai                       |\\n|  0 BMP   |    0E80-0EFF     |             \u5BEE\u6587             |                       Lao                        |\\n|  0 BMP   |    0F00-0FFF     |             \u85CF\u6587             |                     Tibetan                      |\\n|  0 BMP   |    1000-109F     |            \u7DEC\u7538\u6587            |                     Myanmar                      |\\n|  0 BMP   |    10A0-10FF     |          \u55AC\u6CBB\u4E9E\u5B57\u6BCD          |                     Georgian                     |\\n|  0 BMP   |    1100-11FF     |           \u8AFA\u6587\u5B57\u6BCD           |                   Hangul Jamo                    |\\n|  0 BMP   |    1200-137F     |         \u8863\u7D22\u6BD4\u4E9E\u5B57\u6BCD         |                     Ethiopic                     |\\n|  0 BMP   |    1380-139F     |       \u8863\u7D22\u6BD4\u4E9E\u5B57\u6BCD\u88DC\u5145       |               Ethiopic Supplement                |\\n|  0 BMP   |    13A0-13FF     |           \u5207\u7F85\u57FA\u6587           |                     Cherokee                     |\\n|  0 BMP   |    1400-167F     |   \u7D71\u4E00\u52A0\u62FF\u5927\u539F\u4F4F\u6C11\u97F3\u7BC0\u6587\u5B57   |      Unified Canadian Aboriginal Syllabics       |\\n|  0 BMP   |    1680-169F     |           \u6B50\u7518\u5B57\u6BCD           |                      Ogham                       |\\n|  0 BMP   |    16A0-16FF     |           \u76E7\u6069\u5B57\u6BCD           |                      Runic                       |\\n|  0 BMP   |    1700-171F     |          \u4ED6\u52A0\u797F\u5B57\u6BCD          |                     Tagalog                      |\\n|  0 BMP   |    1720-173F     |           \u54C8\u52AA\u8AFE\u6587           |                     Hanunoo                      |\\n|  0 BMP   |    1740-175F     |          \u5E03\u5E0C\u5FB7\u5B57\u6BCD          |                      Buhid                       |\\n|  0 BMP   |    1760-177F     |         \u5854\u683C\u73ED\u74E6\u5B57\u6BCD         |                     Tagbanwa                     |\\n|  0 BMP   |    1780-17FF     |            \u9AD8\u68C9\u6587            |                      Khmer                       |\\n|  0 BMP   |    1800-18AF     |            \u8499\u53E4\u6587            |                    Mongolian                     |\\n|  0 BMP   |    18B0-18FF     | \u7D71\u4E00\u52A0\u62FF\u5927\u539F\u4F4F\u6C11\u97F3\u7BC0\u6587\u5B57\u64F4\u5145 |  Unified Canadian Aboriginal Syllabics Extended  |\\n|  0 BMP   |    1900-194F     |            \u6797\u5E03\u6587            |                      Limbu                       |\\n|  0 BMP   |    1950-197F     |           \u5FB7\u5B8F\u50A3\u6587           |                      Tai Le                      |\\n|  0 BMP   |    1980-19DF     |           \u65B0\u50A3\u4EC2\u6587           |                    New Tai Le                    |\\n|  0 BMP   |    19E0-19FF     |          \u9AD8\u68C9\u6587\u7B26\u865F          |                  Khmer Symbols                   |\\n|  0 BMP   |    1A00-1A1F     |            \u5E03\u5409\u6587            |                     Buginese                     |\\n|  0 BMP   |    1A20-1AAF     |            \u8001\u50A3\u6587            |                     Tai Tham                     |\\n|  0 BMP   |    1AB0-1AFF     |       \u7D44\u5408\u9644\u52A0\u7B26\u865F\u64F4\u5C55       |       Combining Diacritical Marks Extended       |\\n|  0 BMP   |    1B00-1B7F     |           \u5CC7\u91CC\u5B57\u6BCD           |                     Balinese                     |\\n|  0 BMP   |    1B80-1BBF     |           \u5DFD\u4ED6\u5B57\u6BCD           |                    Sundanese                     |\\n|  0 BMP   |    1BC0-1BFF     |          \u5DF4\u5854\u514B\u5B57\u6BCD          |                      Batak                       |\\n|  0 BMP   |    1C00-1C4F     |            \u7D68\u5DF4\u6587            |                      Lepcha                      |\\n|  0 BMP   |    1C50-1C7F     |           \u6851\u5854\u5229\u6587           |                     Ol Chiki                     |\\n|  0 BMP   |    1C80-1C8F     |       \u897F\u91CC\u723E\u5B57\u6BCD\u64F4\u5C55-C       |               Cyrillic Extended-C                |\\n|  0 BMP   |    1C90-1CBF     |        \u55AC\u6CBB\u4E9E\u5B57\u6BCD\u64F4\u5C55        |                Georgian Extended                 |\\n|  0 BMP   |    1CC0-1CCF     |         \u5DFD\u4ED6\u5B57\u6BCD\u88DC\u5145         |               Sundanese Supplement               |\\n|  0 BMP   |    1CD0-1CFF     |           \u5420\u9640\u64F4\u5C55           |                 Vedic Extensions                 |\\n|  0 BMP   |    1D00-1D7F     |           \u97F3\u6A19\u64F4\u5C55           |               Phonetic Extensions                |\\n|  0 BMP   |    1D80-1DBF     |         \u97F3\u6A19\u64F4\u5C55\u88DC\u5145         |          Phonetic Extensions Supplement          |\\n|  0 BMP   |    1DC0-1DFF     |       \u7D44\u5408\u9644\u52A0\u7B26\u865F\u88DC\u5145       |      Combining Diacritical Marks Supplement      |\\n|  0 BMP   |    1E00-1EFF     |       \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55\u9644\u52A0       |            Latin Extended Additional             |\\n|  0 BMP   |    1F00-1FFF     |         \u5E0C\u81D8\u5B57\u6BCD\u64F4\u5C55         |                  Greek Extended                  |\\n|  0 BMP   |    2000-206F     |           \u4E00\u822C\u6A19\u9EDE           |               General Punctuation                |\\n|  0 BMP   |    2070-209F     |          \u4E0A\u6A19\u53CA\u4E0B\u6A19          |           Superscripts and Subscripts            |\\n|  0 BMP   |    20A0-20CF     |           \u8CA8\u5E63\u7B26\u865F           |                 Currency Symbols                 |\\n|  0 BMP   |    20D0-20FF     |      \u7B26\u865F\u7528\u7D44\u5408\u9644\u52A0\u7B26\u865F      |     Combining Diacritical Marks for Symbols      |\\n|  0 BMP   |    2100-214F     |          \u985E\u5B57\u6BCD\u7B26\u865F          |                Letterlike Symbols                |\\n|  0 BMP   |    2150-218F     |           \u6578\u5B57\u5F62\u5F0F           |                   Number Forms                   |\\n|  0 BMP   |    2190-21FF     |             \u7BAD\u982D             |                      Arrows                      |\\n|  0 BMP   |    2200-22FF     |          \u6578\u5B78\u904B\u7B97\u5B50          |              Mathematical Operators              |\\n|  0 BMP   |    2300-23FF     |         \u96DC\u9805\u6280\u8853\u7B26\u865F         |             Miscellaneous Technical              |\\n|  0 BMP   |    2400-243F     |           \u63A7\u5236\u5716\u5F62           |                 Control Pictures                 |\\n|  0 BMP   |    2440-245F     |         \u5149\u5B78\u5B57\u5143\u8FA8\u8B58         |          Optical Character Recognition           |\\n|  0 BMP   |    2460-24FF     |         \u570D\u7E5E\u5B57\u6BCD\u6578\u5B57         |              Enclosed Alphanumerics              |\\n|  0 BMP   |    2500-257F     |            \u5236\u8868\u7B26            |                   Box Drawing                    |\\n|  0 BMP   |    2580-259F     |           \u65B9\u584A\u5143\u7D20           |                  Block Elements                  |\\n|  0 BMP   |    25A0-25FF     |           \u5E7E\u4F55\u5716\u5F62           |                 Geometric Shapes                 |\\n|  0 BMP   |    2600-26FF     |           \u96DC\u9805\u7B26\u865F           |              Miscellaneous Symbols               |\\n|  0 BMP   |    2700-27BF     |           \u88DD\u98FE\u7B26\u865F           |                     Dingbats                     |\\n|  0 BMP   |    27C0-27EF     |        \u96DC\u9805\u6578\u5B78\u7B26\u865F-A        |       Miscellaneous Mathematical Symbols-A       |\\n|  0 BMP   |    27F0-27FF     |          \u8FFD\u52A0\u7BAD\u982D-A          |              Supplemental Arrows-A               |\\n|  0 BMP   |    2800-28FF     |           \u9EDE\u5B57\u5716\u6848           |                 Braille Patterns                 |\\n|  0 BMP   |    2900-297F     |          \u8FFD\u52A0\u7BAD\u982D-B          |              Supplemental Arrows-B               |\\n|  0 BMP   |    2980-29FF     |        \u96DC\u9805\u6578\u5B78\u7B26\u865F-B        |       Miscellaneous Mathematical Symbols-B       |\\n|  0 BMP   |    2A00-2AFF     |        \u88DC\u5145\u6578\u5B78\u904B\u7B97\u5B50        |       Supplemental Mathematical Operators        |\\n|  0 BMP   |    2B00-2BFF     |        \u96DC\u9805\u7B26\u865F\u548C\u7BAD\u982D        |         Miscellaneous Symbols and Arrows         |\\n|  0 BMP   |    2C00-2C5F     |         \u683C\u62C9\u54E5\u91CC\u5B57\u6BCD         |                    Glagolitic                    |\\n|  0 BMP   |    2C60-2C7F     |        \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55-C        |                 Latin Extended-C                 |\\n|  0 BMP   |    2C80-2CFF     |          \u79D1\u666E\u7279\u5B57\u6BCD          |                      Coptic                      |\\n|  0 BMP   |    2D00-2D2F     |        \u55AC\u6CBB\u4E9E\u5B57\u6BCD\u88DC\u5145        |               Georgian Supplement                |\\n|  0 BMP   |    2D30-2D7F     |           \u63D0\u975E\u7D0D\u6587           |                     Tifinagh                     |\\n|  0 BMP   |    2D80-2DDF     |       \u8863\u7D22\u6BD4\u4E9E\u5B57\u6BCD\u64F4\u5145       |                Ethiopic Extended                 |\\n|  0 BMP   |    2DE0-2DFF     |       \u897F\u91CC\u723E\u5B57\u6BCD\u64F4\u5C55-A       |               Cyrillic Extended-A                |\\n|  0 BMP   |    2E00-2E7F     |           \u88DC\u5145\u6A19\u9EDE           |             Supplemental Punctuation             |\\n|  0 BMP   |    2E80-2EFF     |      \u4E2D\u65E5\u97D3\u6F22\u5B57\u90E8\u9996\u88DC\u5145      |             CJK Radicals Supplement              |\\n|  0 BMP   |    2F00-2FDF     |           \u5EB7\u7199\u90E8\u9996           |                 Kangxi Radicals                  |\\n|  0 BMP   |    2FF0-2FFF     |       \u8868\u610F\u6587\u5B57\u63CF\u8FF0\u5B57\u5143       |        Ideographic Description Characters        |\\n|  0 BMP   |    3000-303F     |       \u4E2D\u65E5\u97D3\u7B26\u865F\u548C\u6A19\u9EDE       |           CJK Symbols and Punctuation            |\\n|  0 BMP   |    3040-309F     |            \u5E73\u5047\u540D            |                     Hiragana                     |\\n|  0 BMP   |    30A0-30FF     |            \u7247\u5047\u540D            |                     Katakana                     |\\n|  0 BMP   |    3100-312F     |           \u6CE8\u97F3\u7B26\u865F           |                     Bopomofo                     |\\n|  0 BMP   |    3130-318F     |         \u8AFA\u6587\u76F8\u5BB9\u5B57\u6BCD         |            Hangul Compatibility Jamo             |\\n|  0 BMP   |    3190-319F     |         \u6F22\u6587\u8A13\u8B80\u7B26\u865F         |                      Kanbun                      |\\n|  0 BMP   |    31A0-31BF     |         \u6CE8\u97F3\u7B26\u865F\u64F4\u5C55         |                Bopomofo Extended                 |\\n|  0 BMP   |    31C0-31EF     |          \u4E2D\u65E5\u97D3\u7B46\u756B          |                   CJK Strokes                    |\\n|  0 BMP   |    31F0-31FF     |        \u7247\u5047\u540D\u8A9E\u97F3\u64F4\u5C55        |           Katakana Phonetic Extensions           |\\n|  0 BMP   |    3200-32FF     |     \u4E2D\u65E5\u97D3\u570D\u7E5E\u5B57\u5143\u53CA\u6708\u4EFD     |         Enclosed CJK Letters and Months          |\\n|  0 BMP   |    3300-33FF     |        \u4E2D\u65E5\u97D3\u76F8\u5BB9\u5B57\u5143        |                CJK Compatibility                 |\\n|  0 BMP   |    3400-4DBF     |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 A  |        CJK Unified Ideographs Extension A        |\\n|  0 BMP   |    4DC0-4DFF     |       \u6613\u7D93\u516D\u5341\u56DB\u5366\u7B26\u865F       |             Yijing Hexagram Symbols              |\\n|  0 BMP   |    4E00-9FFF     | \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57 (\u57FA\u672C\u5340)  |              CJK Unified Ideographs              |\\n|  0 BMP   |    A000-A48F     |           \u5F5D\u6587\u97F3\u7BC0           |                   Yi Syllables                   |\\n|  0 BMP   |    A490-A4CF     |           \u5F5D\u6587\u90E8\u9996           |                   Yi Radicals                    |\\n|  0 BMP   |    A4D0-A4FF     |            \u5088\u50F3\u6587            |                       Lisu                       |\\n|  0 BMP   |    A500-A63F     |            \u74E6\u4F0A\u6587            |                       Vai                        |\\n|  0 BMP   |    A640-A69F     |       \u897F\u91CC\u723E\u5B57\u6BCD\u64F4\u5C55-B       |               Cyrillic Extended-B                |\\n|  0 BMP   |    A6A0-A6FF     |          \u5DF4\u59C6\u7A46\u6587\u5B57          |                      Bamum                       |\\n|  0 BMP   |    A700-A71F     |         \u8072\u8ABF\u4FEE\u98FE\u7B26\u865F         |              Modifier Tone Letters               |\\n|  0 BMP   |    A720-A7FF     |        \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55-D        |                 Latin Extended-D                 |\\n|  0 BMP   |    A800-A82F     |          \u932B\u723E\u8D6B\u7279\u6587          |                   Syloti Nagri                   |\\n|  0 BMP   |    A830-A83F     |       \u901A\u7528\u5370\u5EA6\u6578\u5B57\u5F62\u5F0F       |            Common Indic Number Forms             |\\n|  0 BMP   |    A840-A87F     |           \u516B\u601D\u5DF4\u6587           |                     Phags-pa                     |\\n|  0 BMP   |    A880-A8DF     |         \u7D22\u62C9\u4EC0\u7279\u62C9\u6587         |                    Saurashtra                    |\\n|  0 BMP   |    A8E0-A8FF     |          \u5929\u57CE\u6587\u64F4\u5C55          |               Devanagari Extended                |\\n|  0 BMP   |    A900-A92F     |           \u514B\u8036\u5B57\u6BCD           |                     Kayah Li                     |\\n|  0 BMP   |    A930-A95F     |           \u52D2\u59DC\u5B57\u6BCD           |                      Rejang                      |\\n|  0 BMP   |    A960-A97F     |        \u8AFA\u6587\u5B57\u6BCD\u64F4\u5C55-A        |              Hangul Jamo Extended-A              |\\n|  0 BMP   |    A980-A9DF     |           \u722A\u54C7\u5B57\u6BCD           |                     Javanese                     |\\n|  0 BMP   |    A9E0-A9FF     |         \u7DEC\u7538\u6587\u64F4\u5C55-B         |                Myanmar Extended-B                |\\n|  0 BMP   |    AA00-AA5F     |             \u5360\u6587             |                       Cham                       |\\n|  0 BMP   |    AA60-AA7F     |         \u7DEC\u7538\u6587\u64F4\u5C55-A         |                Myanmar Extended-A                |\\n|  0 BMP   |    AA80-AADF     |            \u50A3\u8D8A\u6587            |                     Tai Viet                     |\\n|  0 BMP   |    AAE0-AAFF     |          \u6885\u6CF0\u6587\u64F4\u5145          |             Meetei Mayek Extensions              |\\n|  0 BMP   |    AB00-AB2F     |      \u8863\u7D22\u6BD4\u4E9E\u5B57\u6BCD\u64F4\u5145-A      |               Ethiopic Extended-A                |\\n|  0 BMP   |    AB30-AB6F     |        \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55-E        |                 Latin Extended-E                 |\\n|  0 BMP   |    AB70-ABBF     |         \u5207\u7F85\u57FA\u6587\u88DC\u5145         |               Cherokee Supplement                |\\n|  0 BMP   |    ABC0-ABFF     |            \u6885\u6CF0\u6587            |                   Meetei Mayek                   |\\n|  0 BMP   |    AC00-D7AF     |           \u8AFA\u6587\u97F3\u7BC0           |                 Hangul Syllables                 |\\n|  0 BMP   |    D7B0-D7FF     |        \u8AFA\u6587\u5B57\u6BCD\u64F4\u5C55-B        |              Hangul Jamo Extended-B              |\\n|  0 BMP   |    D800-DB7F     |          \u9AD8\u534A\u4EE3\u7528\u5340          |                 High Surrogates                  |\\n|  0 BMP   |    DB80-DBFF     |        \u9AD8\u534A\u79C1\u4EBA\u4EE3\u7528\u5340        |           High Private Use Surrogates            |\\n|  0 BMP   |    DC00-DFFF     |          \u4F4E\u534A\u4EE3\u7528\u5340          |                  Low Surrogates                  |\\n|  0 BMP   |    E000-F8FF     |            \u79C1\u7528\u5340            |                 Private Use Area                 |\\n|  0 BMP   |    F900-FAFF     |      \u4E2D\u65E5\u97D3\u76F8\u5BB9\u8868\u610F\u6587\u5B57      |           CJK Compatibility Ideographs           |\\n|  0 BMP   |    FB00-FB4F     |         \u5B57\u6BCD\u8868\u9054\u5F62\u5F0F         |          Alphabetic Presentation Forms           |\\n|  0 BMP   |    FB50-FDFF     |     \u963F\u62C9\u4F2F\u5B57\u6BCD\u8868\u9054\u5F62\u5F0F-A     |           Arabic Presentation Forms-A            |\\n|  0 BMP   |    FE00-FE0F     |          \u8B8A\u9AD4\u9078\u64C7\u7B26          |               Variation Selectors                |\\n|  0 BMP   |    FE10-FE1F     |           \u8C4E\u6392\u5F62\u5F0F           |                  Vertical Forms                  |\\n|  0 BMP   |    FE20-FE2F     |         \u7D44\u5408\u7528\u534A\u7B26\u865F         |               Combining Half Marks               |\\n|  0 BMP   |    FE30-FE4F     |        \u4E2D\u65E5\u97D3\u76F8\u5BB9\u5F62\u5F0F        |             CJK Compatibility Forms              |\\n|  0 BMP   |    FE50-FE6F     |         \u5C0F\u5BEB\u8B8A\u9AD4\u5F62\u5F0F         |               Small Form Variants                |\\n|  0 BMP   |    FE70-FEFF     |     \u963F\u62C9\u4F2F\u5B57\u6BCD\u8868\u9054\u5F62\u5F0F-B     |           Arabic Presentation Forms-B            |\\n|  0 BMP   |    FF00-FFEF     |        \u534A\u5F62\u53CA\u5168\u5F62\u5B57\u5143        |          Halfwidth and Fullwidth Forms           |\\n|  0 BMP   |    FFF0-FFFF     |             \u7279\u6B8A             |                     Specials                     |\\n|  1 SMP   |   10000-1007F    |     \u7DDA\u5F62\u6587\u5B57 B \u97F3\u7BC0\u6587\u5B57      |                Linear B Syllabary                |\\n|  1 SMP   |   10080-100FF    |     \u7DDA\u5F62\u6587\u5B57 B \u8868\u610F\u6587\u5B57      |                Linear B Ideograms                |\\n|  1 SMP   |   10100-1013F    |          \u611B\u7434\u6D77\u6578\u5B57          |                  Aegean Numbers                  |\\n|  1 SMP   |   10140-1018F    |          \u53E4\u5E0C\u81D8\u6578\u5B57          |              Ancient Greek Numbers               |\\n|  1 SMP   |   10190-101CF    |           \u53E4\u4EE3\u7B26\u865F           |                 Ancient Symbols                  |\\n|  1 SMP   |   101D0-101FF    |         \u6590\u65AF\u6258\u65AF\u5713\u76E4         |                  Phaistos Disc                   |\\n|  1 SMP   |   10280-1029F    |          \u5442\u57FA\u4E9E\u5B57\u6BCD          |                      Lycian                      |\\n|  1 SMP   |   102A0-102DF    |          \u5361\u91CC\u4E9E\u5B57\u6BCD          |                      Carian                      |\\n|  1 SMP   |   102E0-102FF    |        \u79D1\u666E\u7279\u958F\u9918\u6578\u5B57        |               Coptic Epact Numbers               |\\n|  1 SMP   |   10300-1032F    |         \u53E4\u7FA9\u5927\u5229\u5B57\u6BCD         |                    Old Italic                    |\\n|  1 SMP   |   10330-1034F    |           \u54E5\u7279\u5B57\u6BCD           |                      Gothic                      |\\n|  1 SMP   |   10350-1037F    |          \u53E4\u5F7C\u723E\u59C6\u6587          |                    Old Permic                    |\\n|  1 SMP   |   10380-1039F    |         \u70CF\u52A0\u91CC\u7279\u5B57\u6BCD         |                     Ugaritic                     |\\n|  1 SMP   |   103A0-103DF    |        \u53E4\u6CE2\u65AF\u6954\u5F62\u6587\u5B57        |                   Old Persian                    |\\n|  1 SMP   |   10400-1044F    |         \u5FB7\u745F\u96F7\u7279\u5B57\u6BCD         |                     Deseret                      |\\n|  1 SMP   |   10450-1047F    |          \u856D\u4F2F\u7D0D\u5B57\u6BCD          |                     Shavian                      |\\n|  1 SMP   |   10480-104AF    |         \u5967\u65AF\u66FC\u4E9E\u5B57\u6BCD         |                     Osmanya                      |\\n|  1 SMP   |   104B0-104FF    |          \u6B50\u585E\u5947\u5B57\u6BCD          |                      Osage                       |\\n|  1 SMP   |   10500-1052F    |         \u611B\u723E\u5DF4\u6851\u5B57\u6BCD         |                     Elbasan                      |\\n|  1 SMP   |   10530-1056F    |     \u9AD8\u52A0\u7D22\u963F\u723E\u5DF4\u5C3C\u4E9E\u5B57\u6BCD     |                Caucasian Albanian                |\\n|  1 SMP   |   10570-105BF    |          \u7DAD\u65AF\u5EAB\u5947\u6587          |                     Vithkuqi                     |\\n|  1 SMP   |   10600-1077F    |          \u7DDA\u5F62\u6587\u5B57 A          |                     Linear A                     |\\n|  1 SMP   |   10780-107BF    |        \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55-F        |                 Latin Extended-F                 |\\n|  1 SMP   |   10800-1083F    |       \u8CFD\u666E\u52D2\u65AF\u97F3\u7BC0\u6587\u5B57       |                Cypriot Syllabary                 |\\n|  1 SMP   |   10840-1085F    |         \u5E1D\u570B\u4E9E\u62C9\u59C6\u6587         |                 Imperial Aramaic                 |\\n|  1 SMP   |   10860-1087F    |         \u5E15\u723E\u9081\u62C9\u5B57\u6BCD         |                    Palmyrene                     |\\n|  1 SMP   |   10880-108AF    |          \u7D0D\u5DF4\u6CF0\u5B57\u6BCD          |                    Nabataean                     |\\n|  1 SMP   |   108E0-108FF    |           \u54C8\u7279\u62C9\u6587           |                      Hatran                      |\\n|  1 SMP   |   10900-1091F    |          \u8153\u5C3C\u57FA\u5B57\u6BCD          |                    Phoenician                    |\\n|  1 SMP   |   10920-1093F    |          \u5442\u5E95\u4E9E\u5B57\u6BCD          |                      Lydian                      |\\n|  1 SMP   |   10980-1099F    |        \u9EA5\u7F85\u57C3\u6587\u8056\u66F8\u9AD4        |               Meroitic Hieroglyphs               |\\n|  1 SMP   |   109A0-109FF    |        \u9EA5\u7F85\u57C3\u6587\u8349\u66F8\u9AD4        |                 Meroitic Cursive                 |\\n|  1 SMP   |   10A00-10A5F    |            \u4F49\u76E7\u6587            |                    Kharoshthi                    |\\n|  1 SMP   |   10A60-10A7F    |        \u53E4\u5357\u963F\u62C9\u4F2F\u5B57\u6BCD        |                Old South Arabian                 |\\n|  1 SMP   |   10A80-10A9F    |        \u53E4\u5317\u963F\u62C9\u4F2F\u5B57\u6BCD        |                Old North Arabian                 |\\n|  1 SMP   |   10AC0-10AFF    |           \u6469\u5C3C\u5B57\u6BCD           |                    Manichaean                    |\\n|  1 SMP   |   10B00-10B3F    |         \u963F\u7DAD\u65AF\u9640\u5B57\u6BCD         |                     Avestan                      |\\n|  1 SMP   |   10B40-10B5F    |         \u7891\u523B\u5E15\u63D0\u4E9E\u6587         |              Inscriptional Parthian              |\\n|  1 SMP   |   10B60-10B7F    |         \u7891\u523B\u5DF4\u5217\u7DAD\u6587         |              Inscriptional Pahlavi               |\\n|  1 SMP   |   10B80-10BAF    |         \u8A69\u7BC7\u5DF4\u5217\u7DAD\u6587         |                 Psalter Pahlavi                  |\\n|  1 SMP   |   10C00-10C4F    |           \u53E4\u7A81\u53A5\u6587           |                    Old Turkic                    |\\n|  1 SMP   |   10C80-10CFF    |         \u53E4\u5308\u7259\u5229\u5B57\u6BCD         |                  Old Hungarian                   |\\n|  1 SMP   |   10D00-10D3F    |       \u54C8\u4E43\u6590\u7F85\u8208\u4E9E\u6587\u5B57       |                 Hanifi Rohingya                  |\\n|  1 SMP   |   10E60-10E7F    |          \u76E7\u7C73\u6587\u6578\u5B57          |               Rumi Numeral Symbols               |\\n|  1 SMP   |   10E80-10EBF    |           \u96C5\u8332\u8FEA\u6587           |                      Yezidi                      |\\n|  1 SMP   |   10EC0-10EFF    |       \u963F\u62C9\u4F2F\u5B57\u6BCD\u64F4\u5C55-C       |                Arabic Extended-C                 |\\n|  1 SMP   |   10F00-10F2F    |          \u53E4\u7C9F\u7279\u5B57\u6BCD          |                   Old Sogdian                    |\\n|  1 SMP   |   10F30-10F6F    |           \u7C9F\u7279\u5B57\u6BCD           |                     Sogdian                      |\\n|  1 SMP   |   10F70-10FAF    |           \u56DE\u9DBB\u5B57\u6BCD           |                    Old Uyghur                    |\\n|  1 SMP   |   10FB0-10FDF    |         \u82B1\u524C\u5B50\u6A21\u5B57\u6BCD         |                    Chorasmian                    |\\n|  1 SMP   |   10FE0-10FFF    |           \u57C3\u5229\u9081\u6587           |                     Elymaic                      |\\n|  1 SMP   |   11000-1107F    |           \u5A46\u7F85\u7C73\u6587           |                      Brahmi                      |\\n|  1 SMP   |   11080-110CF    |            \u51F1\u63D0\u6587            |                      Kaithi                      |\\n|  1 SMP   |   110D0-110FF    |         \u7D22\u62C9\u50E7\u5E73\u6587\u5B57         |                   Sora Sompeng                   |\\n|  1 SMP   |   11100-1114F    |           \u67E5\u514B\u99AC\u6587           |                      Chakma                      |\\n|  1 SMP   |   11150-1117F    |          \u99AC\u54C8\u4F73\u5C3C\u6587          |                     Mahajani                     |\\n|  1 SMP   |   11180-111DF    |           \u590F\u62C9\u9054\u6587           |                     Sharada                      |\\n|  1 SMP   |   111E0-111FF    |        \u53E4\u50E7\u4F3D\u7F85\u6587\u6578\u5B57        |             Sinhala Archaic Numbers              |\\n|  1 SMP   |   11200-1124F    |            \u53EF\u5409\u6587            |                      Khojki                      |\\n|  1 SMP   |   11280-112AF    |          \u7A46\u723E\u5854\u5C3C\u6587          |                     Multani                      |\\n|  1 SMP   |   112B0-112FF    |          \u5EAB\u9054\u74E6\u8FEA\u6587          |                    Khudawadi                     |\\n|  1 SMP   |   11300-1137F    |           \u53E4\u862D\u5854\u6587           |                     Grantha                      |\\n|  1 SMP   |   11400-1147F    |           \u7D10\u74E6\u5B57\u6BCD           |                       Newa                       |\\n|  1 SMP   |   11480-114DF    |          \u5E95\u7F85\u50D5\u591A\u6587          |                     Tirhuta                      |\\n|  1 SMP   |   11580-115FF    |           \u6089\u66C7\u6587\u5B57           |                     Siddham                      |\\n|  1 SMP   |   11600-1165F    |            \u83AB\u8FEA\u6587            |                       Modi                       |\\n|  1 SMP   |   11660-1167F    |          \u8499\u53E4\u6587\u88DC\u5145          |               Mongolian Supplement               |\\n|  1 SMP   |   11680-116CF    |           \u5854\u514B\u91CC\u6587           |                      Takri                       |\\n|  1 SMP   |   11700-1174F    |           \u963F\u6D2A\u59C6\u6587           |                       Ahom                       |\\n|  1 SMP   |   11800-1184F    |           \u591A\u683C\u62C9\u6587           |                      Dogra                       |\\n|  1 SMP   |   118A0-118FF    |          \u74E6\u862D\u9F4A\u5730\u6587          |                   Warang Citi                    |\\n|  1 SMP   |   11900-1195F    |           \u5CF6\u5DBC\u5B57\u6BCD           |            Dhives Akuru (Dives Akuru)            |\\n|  1 SMP   |   119A0-119FF    |           \u5357\u8FEA\u57CE\u6587           |                   Nandinagari                    |\\n|  1 SMP   |   11A00-11A4F    |      \u672D\u90A3\u5DF4\u672D\u723E\u65B9\u5F62\u5B57\u6BCD      |                 Zanabazar Square                 |\\n|  1 SMP   |   11A50-11AAF    |          \u7D22\u6C38\u5E03\u6587\u5B57          |                     Soyombo                      |\\n|  1 SMP   |   11AB0-11ABF    |  \u52A0\u62FF\u5927\u539F\u4F4F\u6C11\u97F3\u7BC0\u6587\u5B57\u64F4\u5C55-A  | Unified Canadian Aboriginal Syllabics Extended-A |\\n|  1 SMP   |   11AC0-11AFF    |           \u5305\u6B3D\u8C6A\u6587           |                   Pau Cin Hau                    |\\n|  1 SMP   |   11B00-11B5F    |         \u5929\u57CE\u6587\u64F4\u5C55-A         |              Devanagari Extended-A               |\\n|  1 SMP   |   11C00-11C6F    |          \u62DC\u514B\u8212\u57FA\u6587          |                    Bhaiksuki                     |\\n|  1 SMP   |   11C70-11CBF    |            \u746A\u6B3D\u6587            |                     Marchen                      |\\n|  1 SMP   |   11D00-11D5F    |       \u99AC\u85A9\u62C9\u59C6\u8CA2\u5FB7\u6587\u5B57       |                  Masaram Gondi                   |\\n|  1 SMP   |   11D60-11DAF    |        \u8CA2\u8CC8\u62C9\u8CA2\u5FB7\u6587\u5B57        |                  Gunjala Gondi                   |\\n|  1 SMP   |   11EE0-11EFF    |           \u671B\u52A0\u932B\u6587           |                     Makasar                      |\\n|  1 SMP   |   11F00-11F5F    |            \u5361\u7DAD\u6587            |                       Kawi                       |\\n|  1 SMP   |   11FB0-11FBF    |         \u8001\u5088\u50F3\u6587\u88DC\u5145         |                 Lisu Supplement                  |\\n|  1 SMP   |   11FC0-11FFF    |         \u6CF0\u7C73\u723E\u6587\u88DC\u5145         |                 Tamil Supplement                 |\\n|  1 SMP   |   12000-123FF    |           \u6954\u5F62\u6587\u5B57           |                    Cuneiform                     |\\n|  1 SMP   |   12400-1247F    |    \u6954\u5F62\u6587\u5B57\u6578\u5B57\u548C\u6A19\u9EDE\u7B26\u865F    |        Cuneiform Numbers and Punctuation         |\\n|  1 SMP   |   12480-1254F    |       \u65E9\u671F\u738B\u671D\u6954\u5F62\u6587\u5B57       |             Early Dynastic Cuneiform             |\\n|  1 SMP   |   12F90-12FFF    |     \u8CFD\u666E\u52D2\u65AF-\u7C73\u8AFE\u65AF\u6587\u5B57      |                   Cypro-Minoan                   |\\n|  1 SMP   |   13000-1342F    |          \u57C3\u53CA\u8056\u66F8\u9AD4          |               Egyptian Hieroglyphs               |\\n|  1 SMP   |   13430-1345F    |      \u57C3\u53CA\u8056\u66F8\u9AD4\u683C\u5F0F\u63A7\u5236      |       Egyptian Hieroglyph Format Controls        |\\n|  1 SMP   |   14400-1467F    |      \u5B89\u7D0D\u6258\u5229\u4E9E\u8C61\u5F62\u6587\u5B57      |              Anatolian Hieroglyphs               |\\n|  1 SMP   |   16800-16A3F    |        \u5DF4\u59C6\u7A46\u6587\u5B57\u88DC\u5145        |                 Bamum Supplement                 |\\n|  1 SMP   |   16A40-16A6F    |            \u9ED8\u797F\u6587            |                       Mro                        |\\n|  1 SMP   |   16A70-16ACF    |            \u5510\u85A9\u6587            |                      Tangsa                      |\\n|  1 SMP   |   16AD0-16AFF    |            \u5DF4\u85A9\u6587            |                    Bassa Vah                     |\\n|  1 SMP   |   16B00-16B8F    |           \u6551\u4E16\u82D7\u6587           |                   Pahawh Hmong                   |\\n|  1 SMP   |   16E40-16E9F    |        \u6885\u5FB7\u6CD5\u4F0A\u5FB7\u6797\u6587        |                   Medefaidrin                    |\\n|  1 SMP   |   16F00-16F9F    |          \u67CF\u683C\u7406\u82D7\u6587          |                       Miao                       |\\n|  1 SMP   |   16FE0-16FFF    |      \u8868\u610F\u7B26\u865F\u548C\u6A19\u9EDE\u7B26\u865F      |       Ideographic Symbols and Punctuation        |\\n|  1 SMP   |   17000-187FF    |            \u897F\u590F\u6587            |                      Tangut                      |\\n|  1 SMP   |   18800-18AFF    |          \u897F\u590F\u6587\u90E8\u4EF6          |                Tangut Components                 |\\n|  1 SMP   |   18B00-18CFF    |           \u5951\u4E39\u5C0F\u5B57           |               Khitan Small Script                |\\n|  1 SMP   |   18D00-18D7F    |          \u897F\u590F\u6587\u88DC\u5145          |                Tangut Supplement                 |\\n|  1 SMP   |   1AFF0-1AFFF    |          \u5047\u540D\u64F4\u5C55-B          |                 Kana Extended-B                  |\\n|  1 SMP   |   1B000-1B0FF    |           \u5047\u540D\u88DC\u5145           |                 Kana Supplement                  |\\n|  1 SMP   |   1B100-1B12F    |          \u5047\u540D\u64F4\u5C55-A          |                 Kana Extended-A                  |\\n|  1 SMP   |   1B130-1B16F    |         \u5C0F\u578B\u5047\u540D\u64F4\u5145         |               Small Kana Extension               |\\n|  1 SMP   |   1B170-1B2FF    |             \u5973\u66F8             |                      Nushu                       |\\n|  1 SMP   |   1BC00-1BC9F    |          \u675C\u666E\u96F7\u901F\u8A18          |                     Duployan                     |\\n|  1 SMP   |   1BCA0-1BCAF    |        \u901F\u8A18\u683C\u5F0F\u63A7\u5236\u7B26        |            Shorthand Format Controls             |\\n|  1 SMP   |   1CF00-1CFCF    |      \u8D0A\u73AB\u5C3C\u8056\u6B4C\u97F3\u6A02\u7B26\u865F      |            Znamenny Musical Notation             |\\n|  1 SMP   |   1D000-1D0FF    |        \u62DC\u5360\u5EAD\u97F3\u6A02\u7B26\u865F        |            Byzantine Musical Symbols             |\\n|  1 SMP   |   1D100-1D1FF    |           \u97F3\u6A02\u7B26\u865F           |                 Musical Symbols                  |\\n|  1 SMP   |   1D200-1D24F    |        \u53E4\u5E0C\u81D8\u97F3\u6A02\u8A18\u865F        |          Ancient Greek Musical Notation          |\\n|  1 SMP   |   1D2C0-1D2DF    |        \u5361\u514B\u6258\u7DAD\u514B\u6578\u5B57        |                Kaktovik Numerals                 |\\n|  1 SMP   |   1D2E0-1D2FF    |           \u746A\u96C5\u6578\u5B57           |                  Mayan Numerals                  |\\n|  1 SMP   |   1D300-1D35F    |          \u592A\u7384\u7D93\u7B26\u865F          |              Tai Xuan Jing Symbols               |\\n|  1 SMP   |   1D360-1D37F    |             \u7B97\u7C4C             |              Counting Rod Numerals               |\\n|  1 SMP   |   1D400-1D7FF    |        \u5B57\u6BCD\u548C\u6578\u5B57\u5143\u865F        |        Mathematical Alphanumeric Symbols         |\\n|  1 SMP   |   1D800-1DAAF    |         \u85A9\u9813\u66F8\u5BEB\u7B26\u865F         |                Sutton SignWriting                |\\n|  1 SMP   |   1DF00-1DFFF    |        \u62C9\u4E01\u5B57\u6BCD\u64F4\u5C55-G        |                 Latin Extended-G                 |\\n|  1 SMP   |   1E000-1E02F    |       \u683C\u62C9\u54E5\u91CC\u5B57\u6BCD\u88DC\u5145       |              Glagolitic Supplement               |\\n|  1 SMP   |   1E030-1E08F    |       \u897F\u91CC\u723E\u5B57\u6BCD\u64F4\u5C55-D       |               Cyrillic Extended-D                |\\n|  1 SMP   |   1E100-1E14F    |          \u5275\u4E16\u7D00\u82D7\u6587          |              Nyiakeng Puachue Hmong              |\\n|  1 SMP   |   1E290-1E2BF    |            \u6295\u6295\u6587            |                       Toto                       |\\n|  1 SMP   |   1E2C0-1E2FF    |           \u6587\u55AC\u5B57\u6BCD           |                      Wancho                      |\\n|  1 SMP   |   1E4D0-1E4FF    |          \u8499\u9054\u91CC\u5B57\u6BCD          |                   Nag Mundari                    |\\n|  1 SMP   |   1E7E0-1E7FF    |      \u8863\u7D22\u6BD4\u4E9E\u5B57\u6BCD\u64F4\u5145-B      |               Ethiopic Extended-B                |\\n|  1 SMP   |   1E800-1E8DF    |         \u9580\u5FB7\u57FA\u5361\u5EAB\u6587         |                  Mende Kikakui                   |\\n|  1 SMP   |   1E900-1E95F    |         \u963F\u5FB7\u62C9\u59C6\u5B57\u6BCD         |                      Adlam                       |\\n|  1 SMP   |   1EC70-1ECBF    |        \u5370\u5EA6\u897F\u4E9E\u683C\u6578\u5B57        |               Indic Siyaq Numbers                |\\n|  1 SMP   |   1ED00-1ED4F    |       \u5967\u65AF\u66FC\u897F\u4E9E\u683C\u6578\u5B57       |              Ottoman Siyaq Numbers               |\\n|  1 SMP   |   1EE00-1EEFF    |      \u963F\u62C9\u4F2F\u5B57\u6BCD\u6578\u5B57\u5143\u865F      |      Arabic Mathematical Alphabetic Symbols      |\\n|  1 SMP   |   1F000-1F02F    |            \u9EBB\u5C07\u724C            |                  Mahjong Tiles                   |\\n|  1 SMP   |   1F030-1F09F    |          \u591A\u7C73\u8AFE\u9AA8\u724C          |                   Domino Tiles                   |\\n|  1 SMP   |   1F0A0-1F0FF    |            \u64B2\u514B\u724C            |                  Playing Cards                   |\\n|  1 SMP   |   1F100-1F1FF    |       \u570D\u7E5E\u5B57\u6BCD\u6578\u5B57\u88DC\u5145       |         Enclosed Alphanumeric Supplement         |\\n|  1 SMP   |   1F200-1F2FF    |       \u570D\u7E5E\u8868\u610F\u6587\u5B57\u88DC\u5145       |         Enclosed Ideographic Supplement          |\\n|  1 SMP   |   1F300-1F5FF    |      \u96DC\u9805\u7B26\u865F\u548C\u8C61\u5F62\u6587\u5B57      |      Miscellaneous Symbols and Pictographs       |\\n|  1 SMP   |   1F600-1F64F    |           \u8868\u60C5\u7B26\u865F           |                    Emoticons                     |\\n|  1 SMP   |   1F650-1F67F    |           \u88DD\u98FE\u7B26\u865F           |               Ornamental Dingbats                |\\n|  1 SMP   |   1F680-1F6FF    |        \u4EA4\u901A\u548C\u5730\u5716\u7B26\u865F        |            Transport and Map Symbols             |\\n|  1 SMP   |   1F700-1F77F    |          \u934A\u91D1\u8853\u7B26\u865F          |                Alchemical Symbols                |\\n|  1 SMP   |   1F780-1F7FF    |         \u5E7E\u4F55\u5716\u5F62\u64F4\u5C55         |            Geometric Shapes Extended             |\\n|  1 SMP   |   1F800-1F8FF    |          \u8FFD\u52A0\u7BAD\u982D-C          |              Supplemental Arrows-C               |\\n|  1 SMP   |   1F900-1F9FF    |      \u88DC\u5145\u7B26\u865F\u548C\u8C61\u5F62\u6587\u5B57      |       Supplemental Symbols and Pictographs       |\\n|  1 SMP   |   1FA00-1FA6F    |           \u68CB\u985E\u7B26\u865F           |                  Chess Symbols                   |\\n|  1 SMP   |   1FA70-1FAFF    |     \u7B26\u865F\u548C\u8C61\u5F62\u6587\u5B57\u64F4\u5145-A     |        Symbols and Pictographs Extended-A        |\\n|  1 SMP   |   1FB00-1FBFF    |         \u907A\u7559\u8A08\u7B97\u7B26\u865F         |           Symbols for Legacy Computing           |\\n|  2 SIP   |   20000-2A6DF    |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 B  |        CJK Unified Ideographs Extension B        |\\n|  2 SIP   |   2A700-2B73F    |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 C  |        CJK Unified Ideographs Extension C        |\\n|  2 SIP   |   2B740-2B81F    |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 D  |        CJK Unified Ideographs Extension D        |\\n|  2 SIP   |   2B820-2CEAF    |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 E  |        CJK Unified Ideographs Extension E        |\\n|  2 SIP   |   2CEB0-2EBEF    |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 F  |        CJK Unified Ideographs Extension F        |\\n|  2 SIP   |   2F800-2FA1F    |   \u4E2D\u65E5\u97D3\u76F8\u5BB9\u8868\u610F\u6587\u5B57\u88DC\u5145\u5340   |     CJK Compatibility Ideographs Supplement      |\\n|  3 TIP   |   30000-3134F    |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 G  |        CJK Unified Ideographs Extension G        |\\n|  3 TIP   |   31350-323AF    |  \u4E2D\u65E5\u97D3\u7D71\u4E00\u8868\u610F\u6587\u5B57\u64F4\u5145\u5340 H  |        CJK Unified Ideographs Extension H        |\\n|  14 SSP  |   E0000-E007F    |             \u6A19\u7C64             |                       Tags                       |\\n|  14 SSP  |   E0100-E01EF    |        \u8B8A\u9AD4\u9078\u64C7\u7B26\u88DC\u5145        |          Variation Selectors Supplement          |\\n| 15 PUA-A |   F0000-FFFFF    |       \u88DC\u5145\u79C1\u4EBA\u4F7F\u7528\u5340-A       |         Supplementary Private Use Area-A         |\\n| 16 PUA-B |  100000-10FFFF   |       \u88DC\u5145\u79C1\u4EBA\u4F7F\u7528\u5340-B       |         Supplementary Private Use Area-B         |"},{"id":"mac-selective-vpn-routing","metadata":{"permalink":"/en/blog/mac-selective-vpn-routing","source":"@site/i18n/en/docusaurus-plugin-content-blog/2023/09-01-mac-selective-vpn-routing/index.md","title":"Configuring Selective Traffic Routing for VPN","description":"Setting up VPN routing on macOS.","date":"2023-09-01T00:00:00.000Z","tags":[{"inline":true,"label":"vpn-routing","permalink":"/en/blog/tags/vpn-routing"},{"inline":true,"label":"macos","permalink":"/en/blog/tags/macos"}],"readingTime":2.84,"hasTruncateMarker":true,"authors":[{"name":"Z. Yuan","title":"Dosaid maintainer, Full-Stack AI Engineer","url":"https://github.com/zephyr-sh","socials":{"github":"https://github.com/zephyr-sh","linkedin":"https://www.linkedin.com/in/ze-yuan-sh7/"},"imageURL":"https://github.com/zephyr-sh.png","key":"Z. Yuan","page":null}],"frontMatter":{"slug":"mac-selective-vpn-routing","title":"Configuring Selective Traffic Routing for VPN","authors":"Z. Yuan","tags":["vpn-routing","macos"],"image":"/en/img/2023/0901.webp","description":"Setting up VPN routing on macOS."},"unlisted":false,"prevItem":{"title":"Unicode Table","permalink":"/en/blog/unicode-table"}},"content":"Working remotely using a company-configured VPN, but still wanting to use other machines on your local network.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Configuration Guide\\n\\nThis guide uses macOS as the operating system.\\n\\n## Problem Description\\n\\nFor example:\\n\\n- **Company VPN subnet**: 192.168.25.XXX\\n- **Home subnet**: 192.168.1.XXX\\n\\nWhen you connect to the VPN, all traffic is routed through the company\'s subnet, which means you won\u2019t be able to connect to other machines on the same home network.\\n\\nNot only that, but while you were watching a funny video at home, the company\u2019s network administrator might have been laughing along with you. (???)\\n\\nSomething seems off, right?\\n\\nSo what we need to do is: **Route only the company\'s subnet through the VPN, and let all other traffic use the local network.**\\n\\n:::tip\\nHere, we assume that you\'ve already configured the VPN and it\'s working correctly, and now we\u2019re only addressing the traffic routing issue.\\n\\nIf the VPN isn\u2019t working correctly yet, please check that your VPN settings are properly configured first.\\n:::\\n\\n## Solution\\n\\nStep one: First, identify the company\u2019s internal subnet, such as:\\n\\n- 192.168.25.XXX\\n\\nThis indicates that you\'re working with the 25 subnet. Now, let\u2019s open a file:\\n\\n```bash\\nsudo vim /etc/ppp/ip-up\\n```\\n\\nEnter the following content, but be sure to replace it with your actual subnet:\\n\\n:::warning\\nPlease note, this example assumes the VPN subnet is 192.168.25.XXX. Modify it according to your actual setup.\\n:::\\n\\n```bash\\n#!/bin/sh\\n/sbin/route add -net 192.168.25.0/24 -interface ppp0\\n```\\n\\nLet\u2019s explain what the command above does:\\n\\n1. **/sbin/route**: This is the path to the route command, which is used to configure network routes.\\n2. **-net 192.168.25.0/24**: The parameter specifies that this is a network route, not a host route.\\n    - 192.168.25.0/24 represents the network address and subnet mask, covering the IP range from 192.168.25.0 to 192.168.25.255.\\n3. **-interface ppp0**: Specifies the network interface through which the route should be set.\\n    - In this example, it\'s ppp0 (point-to-point protocol interface 0).\\n\\nThe whole command adds a route through the ppp0 interface to the 192.168.25.0/24 network.\\n\\nWhen your system tries to access any IP address in the 192.168.25.0/24 network, it will route the traffic through the ppp0 interface.\\n\\n---\\n\\nAfter finishing the setup, save and exit the file, then give the file the correct permissions:\\n\\n```bash\\nsudo chmod 755 /etc/ppp/ip-up\\n```\\n\\n## Is it still not working?\\n\\nAt this point, some machines may still be unable to access the internet. So, let\u2019s move on:\\n\\nOpen the System Preferences on macOS and go to Network, as shown in the image below:\\n\\n<div align=\\"center\\">\\n<figure style={{\\"width\\": \\"80%\\"}}>\\n![vpn-setting](./img/vpn-setting.jpg)\\n</figure>\\n</div>\\n\\n- Step 1: Open System Preferences and click on \\"Network.\\"\\n- Step 2: Click on the small options button next to the VPN connection.\\n- Step 3: Select \\"Set Service Order.\\"\\n- Step 4: Drag the VPN service below Wi-Fi in the list.\\n\\n---\\n\\nMany users, after setting up the VPN, move the VPN service to the top of the list, meaning all traffic is routed through the VPN by default.\\n\\nSo here, we need to move the VPN down, which will allow the network settings above it to take effect.\\n\\nOnce that\u2019s done, you\'re all set. If you need other traffic to go through the VPN in the future, simply add the necessary routes to the `ip-up` file.\\n\\n## References\\n\\n1. [shalyf/vpn_route.md](https://gist.github.com/shalyf/d50b0bbf30a4b5020d2b84f4ae8eb4e0)\\n2. [How to selectively route network traffic through VPN on Mac OS X Leopard?](https://superuser.com/questions/4904/how-to-selectively-route-network-traffic-through-vpn-on-mac-os-x-leopard)"}]}}')}}]);