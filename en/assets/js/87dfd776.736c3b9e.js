"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["37867"],{347:function(e,n,i){i.r(n),i.d(n,{default:()=>h,frontMatter:()=>o,metadata:()=>t,assets:()=>l,toc:()=>c,contentTitle:()=>a});var t=JSON.parse('{"id":"model-training-guide/otter_style","title":"Otter Style","description":"Based on Pytorch-Lightning","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/model-training-guide/otter_style.md","sourceDirName":"model-training-guide","slug":"/model-training-guide/otter-style","permalink":"/en/docs/model-training-guide/otter-style","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739242156000,"frontMatter":{"slug":"otter-style","title":"Otter Style","authors":"Z. Yuan"},"sidebar":"tutorialSidebar","previous":{"title":"Model Training Guide","permalink":"/en/docs/model-training-guide/"},"next":{"title":"AutoTraderX","permalink":"/en/docs/autotraderx/"}}'),s=i("85893"),r=i("50065");let o={slug:"otter-style",title:"Otter Style",authors:"Z. Yuan"},a=void 0,l={},c=[{value:"Based on Pytorch-Lightning",id:"based-on-pytorch-lightning",level:2},{value:"Setting Up the Environment",id:"setting-up-the-environment",level:2},{value:"Execute Training",id:"execute-training",level:2},{value:"Parameter Configuration",id:"parameter-configuration",level:3},{value:"Start Training",id:"start-training",level:3},{value:"Convert to ONNX",id:"convert-to-onnx",level:2},{value:"Final Notes",id:"final-notes",level:2}];function d(e){let n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"based-on-pytorch-lightning",children:"Based on Pytorch-Lightning"}),"\n",(0,s.jsx)(n.p,{children:"This chapter introduces the construction method of the project managed by Z. Yuan, primarily based on the Pytorch-Lightning training framework."}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsxs)(n.p,{children:["For detailed implementation details, please refer to: ",(0,s.jsx)(n.a,{href:"https://github.com/DocsaidLab/Otter",children:(0,s.jsx)(n.strong,{children:"DocsaidLab/Otter"})}),"."]}),(0,s.jsx)(n.p,{children:"As for why it is called Otter... there is no special meaning, just a name chosen for differentiation purposes. \uD83D\uDE05"})]}),"\n",(0,s.jsx)(n.h2,{id:"setting-up-the-environment",children:"Setting Up the Environment"}),"\n",(0,s.jsxs)(n.p,{children:["The following sections use the ",(0,s.jsx)(n.code,{children:"DocClassifier"})," project as an example to explain how to set up the model training environment. The content can also be applied to other projects such as ",(0,s.jsx)(n.code,{children:"DocAligner"})," and ",(0,s.jsx)(n.code,{children:"MRZScanner"}),", which are managed by Z. Yuan."]}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsxs)(n.p,{children:["Most deep learning projects only offer the inference module. Currently, only the ",(0,s.jsx)(n.code,{children:"DocClassifier"})," project offers the training module. If you need training modules for other projects, you can refer to the training methods in this chapter and implement them on your own."]}),(0,s.jsxs)(n.p,{children:["For the ",(0,s.jsx)(n.code,{children:"DocClassifier"})," project, refer to: ",(0,s.jsx)(n.a,{href:"https://github.com/DocsaidLab/DocClassifier",children:(0,s.jsx)(n.strong,{children:"DocClassifier github"})})]})]}),"\n",(0,s.jsxs)(n.p,{children:["First, use git to download the ",(0,s.jsx)(n.a,{href:"https://github.com/DocsaidLab/Otter",children:(0,s.jsx)(n.strong,{children:"Otter"})})," module and create the Docker image:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/DocsaidLab/Otter.git\ncd Otter\nbash docker/build.bash\n"})}),"\n",(0,s.jsx)(n.p,{children:"The build file contents are as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",metastring:'title="Otter/docker/build.bash"',children:"docker build \\\n    -f docker/Dockerfile \\\n    -t otter_base_image .\n"})}),"\n",(0,s.jsxs)(n.p,{children:["In the file, you can replace ",(0,s.jsx)(n.code,{children:"otter_base_image"})," with your preferred name, which will be used later during training."]}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.p,{children:"PyTorch Lightning is a lightweight deep learning framework based on PyTorch, designed to simplify the model training process. It separates research code (model definition, forward/backward propagation, optimizer settings, etc.) from engineering code (training loops, logging, checkpoint saving, etc.), allowing researchers to focus on the model itself without dealing with cumbersome engineering details."}),(0,s.jsx)(n.p,{children:"Interested readers can refer to the following resources:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://lightning.ai/",children:(0,s.jsx)(n.strong,{children:"PyTorch Lightning Official Website"})})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/Lightning-AI/pytorch-lightning",children:(0,s.jsx)(n.strong,{children:"PyTorch Lightning GitHub"})})}),"\n"]})]}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsxs)(n.p,{children:["A brief introduction to the ",(0,s.jsx)(n.code,{children:"Otter"})," module:"]}),(0,s.jsx)(n.p,{children:"It includes several basic modules for building models, such as:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"BaseMixin"}),": Basic training model, containing basic training settings."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"BorderValueMixin"})," and ",(0,s.jsx)(n.code,{children:"FillValueMixin"}),": Padding modes for image augmentation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"build_callback"}),": Used to build callback functions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"build_dataset"}),": Used to build datasets."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"build_logger"}),": Used to build logging."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"build_trainer"}),": Used to build the trainer."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"load_model_from_config"}),": Used to load models from configuration files."]}),"\n"]}),(0,s.jsx)(n.p,{children:"It also includes some system information recording features, which require a specific configuration file format to function correctly."}),(0,s.jsx)(n.p,{children:"There's no need to focus on learning this part. Based on experience, each engineer will develop their own model training methods, and this is just one of many possible approaches, provided as a reference."})]}),"\n",(0,s.jsxs)(n.p,{children:["Here is the default ",(0,s.jsx)(n.a,{href:"https://github.com/DocsaidLab/Otter/blob/main/docker/Dockerfile",children:(0,s.jsx)(n.strong,{children:"Dockerfile"})})," we use, specially designed for model training:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",metastring:'title="Otter/docker/Dockerfile"',children:'# syntax=docker/dockerfile:experimental\nFROM nvcr.io/nvidia/pytorch:24.12-py3\n\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONWARNINGS="ignore" \\\n    DEBIAN_FRONTEND=noninteractive \\\n    TZ=Asia/Taipei\n\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends \\\n    tzdata wget git libturbojpeg exiftool ffmpeg poppler-utils libpng-dev \\\n    libtiff5-dev libjpeg8-dev libopenjp2-7-dev zlib1g-dev gcc \\\n    libfreetype6-dev liblcms2-dev libwebp-dev tcl8.6-dev tk8.6-dev python3-tk \\\n    python3-pip libharfbuzz-dev libfribidi-dev libxcb1-dev libfftw3-dev \\\n    libpq-dev python3-dev gosu && \\\n    ln -sf /usr/share/zoneinfo/$TZ /etc/localtime && \\\n    dpkg-reconfigure -f noninteractive tzdata && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\nRUN python -m pip install --no-cache-dir -U pip setuptools wheel\n\nCOPY . /usr/local/otter\nRUN cd /usr/local/otter && \\\n    python setup.py bdist_wheel && \\\n    python -m pip install dist/*.whl && \\\n    cd ~ && rm -rf /usr/local/otter\n\nRUN python -m pip install --no-cache-dir -U \\\n    tqdm colored ipython tabulate tensorboard scikit-learn fire \\\n    albumentations "Pillow>=10.0.0" fitsne opencv-fixer prettytable\n\nRUN python -c "from opencv_fixer import AutoFix; AutoFix()"\nRUN python -c "import capybara; import chameleon"\n\nWORKDIR /code\n\nENV ENTRYPOINT_SCRIPT=/entrypoint.sh\n\nRUN printf \'#!/bin/bash\\n\\\n    if [ ! -z "$USER_ID" ] && [ ! -z "$GROUP_ID" ]; then\\n\\\n    groupadd -g "$GROUP_ID" -o usergroup\\n\\\n    useradd --shell /bin/bash -u "$USER_ID" -g "$GROUP_ID" -o -c "" -m user\\n\\\n    export HOME=/home/user\\n\\\n    chown -R "$USER_ID":"$GROUP_ID" /home/user\\n\\\n    chown -R "$USER_ID":"$GROUP_ID" /code\\n\\\n    exec gosu "$USER_ID":"$GROUP_ID" "$@"\\n\\\n    else\\n\\\n    exec "$@"\\n\\\n    fi\' > "$ENTRYPOINT_SCRIPT" && \\\n    chmod +x "$ENTRYPOINT_SCRIPT"\n\nENTRYPOINT ["/bin/bash", "/entrypoint.sh"]\n\nCMD ["bash"]\n'})}),"\n",(0,s.jsx)(n.p,{children:"Based on the Dockerfile above, we can create a deep learning container that includes multiple tools and libraries, suitable for image processing and machine learning tasks."}),"\n",(0,s.jsx)(n.p,{children:"Here are explanations of several important parts:"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'ENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONWARNINGS="ignore" \\\n    DEBIAN_FRONTEND=noninteractive \\\n    TZ=Asia/Taipei\n'})}),"\n",(0,s.jsx)(n.p,{children:"Setting environment variables:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"PYTHONDONTWRITEBYTECODE=1"})}),": Prevents the generation of ",(0,s.jsx)(n.code,{children:".pyc"})," compilation files, reducing unnecessary file generation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:'PYTHONWARNINGS="ignore"'})}),": Ignores Python warnings."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"DEBIAN_FRONTEND=noninteractive"})}),": Disables interactive prompts for automated deployment."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"TZ=Asia/Taipei"})}),": Sets the timezone to Taipei."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"You can change it to your preferred timezone or add other environment variables."})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:"COPY . /usr/local/otter\nRUN cd /usr/local/otter && \\\n    python setup.py bdist_wheel && \\\n    python -m pip install dist/*.whl && \\\n    cd ~ && rm -rf /usr/local/otter\n"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Copies all content from the current directory to the container's ",(0,s.jsx)(n.code,{children:"/usr/local/otter"})," path."]}),"\n",(0,s.jsxs)(n.li,{children:["Navigates to this directory, generates a wheel package using ",(0,s.jsx)(n.code,{children:"setup.py"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Installs the generated wheel package and then deletes the build directory to clean up the environment."}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'RUN python -m pip install --no-cache-dir -U \\\n    tqdm colored ipython tabulate tensorboard scikit-learn fire \\\n    albumentations "Pillow>=10.0.0" fitsne opencv-fixer prettytable\n'})}),"\n",(0,s.jsx)(n.p,{children:"Installs required third-party Python libraries, including:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"tqdm"})}),": Progress bar tool."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"colored"})}),": Terminal output coloring."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"ipython"})}),": Interactive Python interface."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"tabulate"})}),": Table formatting tool."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"tensorboard"})}),": Deep learning visualization tool."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"scikit-learn"})}),": Machine learning library."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"fire"})}),": Command-line interface generation tool."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"albumentations"})}),": Image augmentation tool."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Pillow"})}),": Image processing library, version 10.0 or higher required."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"fitsne"})}),": Efficient t-SNE implementation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"opencv-fixer"})}),": OpenCV fix tool."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"prettytable"})}),": Table output tool."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"If you need additional tools, you can add the corresponding libraries here."})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'RUN python -c "from opencv_fixer import AutoFix; AutoFix()"\nRUN python -c "import capybara; import chameleon"\n'})}),"\n",(0,s.jsx)(n.p,{children:"These two lines execute simple Python commands to test the installation:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Automatically fixes OpenCV configuration issues."}),"\n",(0,s.jsxs)(n.li,{children:["Tests whether the ",(0,s.jsx)(n.code,{children:"capybara"})," and ",(0,s.jsx)(n.code,{children:"chameleon"})," modules are available."]}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsxs)(n.p,{children:["Since OpenCV often has version-related issues, we use ",(0,s.jsx)(n.code,{children:"opencv-fixer"})," to automatically fix these."]}),(0,s.jsxs)(n.p,{children:["In addition, the ",(0,s.jsx)(n.code,{children:"capbybara"})," module has the function of automatically downloading font files. By calling the module here, the font files can be downloaded to the container in advance to avoid problems during subsequent use."]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'RUN printf \'#!/bin/bash\\n\\\n    if [ ! -z "$USER_ID" ] && [ ! -z "$GROUP_ID" ]; then\\n\\\n    groupadd -g "$GROUP_ID" -o usergroup\\n\\\n    useradd --shell /bin/bash -u "$USER_ID" -g "$GROUP_ID" -o -c "" -m user\\n\\\n    export HOME=/home/user\\n\\\n    chown -R "$USER_ID":"$GROUP_ID" /home/user\\n\\\n    chown -R "$USER_ID":"$GROUP_ID" /code\\n\\\n    exec gosu "$USER_ID":"$GROUP_ID" "$@"\\n\\\n    else\\n\\\n    exec "$@"\\n\\\n    fi\' > "$ENTRYPOINT_SCRIPT" && \\\n    chmod +x "$ENTRYPOINT_SCRIPT"\n'})}),"\n",(0,s.jsx)(n.p,{children:"This script generates a Bash script that implements the following functionalities:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["If the environment variables ",(0,s.jsx)(n.code,{children:"USER_ID"})," and ",(0,s.jsx)(n.code,{children:"GROUP_ID"})," are set, it dynamically creates a user and group with the corresponding IDs and assigns the appropriate permissions."]}),"\n",(0,s.jsxs)(n.li,{children:["It uses ",(0,s.jsx)(n.code,{children:"gosu"})," to switch to the created user and execute commands, ensuring that the operations inside the container are performed with the correct identity."]}),"\n",(0,s.jsx)(n.li,{children:"If these variables are not set, it directly executes the passed command."}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"gosu"})," is a tool used to switch user identities inside a container, avoiding permission issues caused by using ",(0,s.jsx)(n.code,{children:"sudo"}),"."]})}),"\n",(0,s.jsx)(n.h2,{id:"execute-training",children:"Execute Training"}),"\n",(0,s.jsx)(n.p,{children:"We have already built the Docker image. Next, we will use this image to run the model training."}),"\n",(0,s.jsxs)(n.p,{children:["Then, we will enter the ",(0,s.jsx)(n.code,{children:"DocClassifier"})," project. First, please take a look at the content of the ",(0,s.jsx)(n.code,{children:"train.bash"})," file:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/DocsaidLab/DocClassifier/blob/main/docker/train.bash",children:(0,s.jsx)(n.strong,{children:"DocClassifier/docker/train.bash"})})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",metastring:'title="DocClassifier/docker/train.bash"',children:'#!/bin/bash\n\ndocker run \\\n    -e USER_ID=$(id -u) \\\n    -e GROUP_ID=$(id -g) \\\n    --gpus all \\\n    --shm-size=64g \\\n    --ipc=host --net=host \\\n    --cpuset-cpus="0-31" \\\n    -v $PWD/DocClassifier:/code/DocClassifier \\\n    -v /data/Dataset:/data/Dataset \\ # Replace this with your dataset directory\n    -it --rm otter_base_image bash -c "\necho \'\nfrom fire import Fire\nfrom DocClassifier.model import main_classifier_train\n\nif __name__ == \\"__main__\\":\n    Fire(main_classifier_train)\n\' > /code/trainer.py && python /code/trainer.py --cfg_name $1\n"\n'})}),"\n",(0,s.jsx)(n.p,{children:"Here\u2019s an explanation of the above file. If you want to make modifications, you can refer to the related information:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"-e USER_ID=$(id -u)"})," and ",(0,s.jsx)(n.code,{children:"-e GROUP_ID=$(id -g)"})]}),": Passes the current user\u2019s UID and GID into the container to ensure that file operation permissions inside the container match those of the host system."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"--gpus all"})}),": Enables GPU support and allocates all available GPU resources to the container."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"--shm-size=64g"})}),": Sets the shared memory size to 64GB, suitable for deep learning tasks that require large amounts of memory."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"--ipc=host"})," and ",(0,s.jsx)(n.code,{children:"--net=host"})]}),": Shares the host\u2019s inter-process communication and network resources to improve performance and compatibility."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:'--cpuset-cpus="0-31"'})}),": Limits the container to use only CPU cores 0-31 to avoid affecting other processes."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"-v $PWD/DocClassifier:/code/DocClassifier"})}),": Mounts the host\u2019s current directory ",(0,s.jsx)(n.code,{children:"DocClassifier"})," folder to ",(0,s.jsx)(n.code,{children:"/code/DocClassifier"})," inside the container."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"-v /data/Dataset:/data/Dataset"})}),": Mounts the host\u2019s dataset directory to ",(0,s.jsx)(n.code,{children:"/data/Dataset"})," inside the container. Modify it as needed based on the actual path."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"-it"})}),": Runs the container in interactive mode."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"--rm"})}),": Automatically removes the container when it stops, to avoid accumulating temporary containers."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"otter_base_image"})}),": The name of the Docker image previously built. If you have made modifications, replace it with your custom name."]}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"Here are a few common issues:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--gpus"})," error: Check if Docker is installed correctly, refer to ",(0,s.jsx)(n.a,{href:"/en/docs/capybara/advance",children:(0,s.jsx)(n.strong,{children:"Advanced Installation"})}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--cpuset-cpus"}),": Don\u2019t exceed the number of CPU cores on your machine."]}),"\n",(0,s.jsxs)(n.li,{children:["The working directory is set in the Dockerfile: ",(0,s.jsx)(n.code,{children:"WORKDIR /code"}),". If you don\u2019t like it, you can modify it."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"-v"}),": Be sure to check the directories you are mounting, otherwise, files might not be found."]}),"\n",(0,s.jsxs)(n.li,{children:["In the ",(0,s.jsx)(n.code,{children:"DocClassifier"})," project, we need to mount the ImageNet dataset from outside. If you don\u2019t need it, you can delete that part."]}),"\n"]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"After the container starts, we will execute the training command."}),"\n",(0,s.jsx)(n.p,{children:"Here, we will directly write a Python script:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'bash -c "\necho \'\nfrom fire import Fire\nfrom DocClassifier.model import main_classifier_train\n\nif __name__ == \\"__main__\\":\n    Fire(main_classifier_train)\n\' > /code/trainer.py && python /code/trainer.py --cfg_name $1\n"\n'})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"echo"})}),": Writes a Python script into the ",(0,s.jsx)(n.code,{children:"/code/trainer.py"})," file. The functionality of the script is as follows:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"from fire import Fire"})}),": Imports the ",(0,s.jsx)(n.code,{children:"fire"})," library, which is used to generate command-line interfaces."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"from DocClassifier.model import main_classifier_train"})}),": Imports the main training function from the ",(0,s.jsx)(n.code,{children:"DocClassifier.model"})," module."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:'if __name__ == "__main__":'})}),": When the script is executed, it starts ",(0,s.jsx)(n.code,{children:"Fire(main_classifier_train)"}),", binding the command-line arguments to the function."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"python /code/trainer.py --cfg_name $1"})}),": Executes the generated Python script and uses the parameter ",(0,s.jsx)(n.code,{children:"$1"})," passed in as the value for ",(0,s.jsx)(n.code,{children:"--cfg_name"}),". This parameter is typically used to specify a configuration file."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"parameter-configuration",children:"Parameter Configuration"}),"\n",(0,s.jsxs)(n.p,{children:["In the directory where the model is trained, there is usually a dedicated directory for storing configuration files, typically named ",(0,s.jsx)(n.code,{children:"config"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Within this directory, we can define different configuration files for training various models. Here's an example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",metastring:'title="config/lcnet050_cosface_f256_r128_squeeze_lbn_imagenet.yaml"',children:'common:\n  batch_size: 1024\n  image_size: [128, 128]\n  is_restore: False\n  restore_ind: ""\n  restore_ckpt: ""\n  preview_batch: 1000\n  use_imagenet: True\n  use_clip: False\n\nglobal_settings:\n  image_size: [128, 128]\n\ntrainer:\n  max_epochs: 40\n  precision: 32\n  val_check_interval: 1.0\n  gradient_clip_val: 5\n  accumulate_grad_batches: 1\n  accelerator: gpu\n  devices: [0]\n\nmodel:\n  name: ClassifierModel\n  backbone:\n    name: Backbone\n    options:\n      name: timm_lcnet_050\n      pretrained: True\n      features_only: True\n  head:\n    name: FeatureLearningSqueezeLBNHead\n    options:\n      in_dim: 256\n      embed_dim: 256\n      feature_map_size: 4\n  loss:\n    name: CosFace\n    options:\n      s: 64\n      m: 0.4\n    num_classes: -1\n    embed_dim: 256\n\nonnx:\n  name: WarpFeatureLearning\n  input_shape:\n    img:\n      shape: [1, 3, 128, 128]\n      dtype: float32\n  input_names: ["img"]\n  output_names:\n    - feats\n  dynamic_axes:\n    img:\n      "0": batch_size\n    output:\n      "0": batch_size\n  options:\n    opset_version: 16\n    verbose: False\n    do_constant_folding: True\n\ndataset:\n  train_options:\n    name: SynthDataset\n    options:\n      aug_ratio: 1\n      length_of_dataset: 2560000\n      use_imagenet: True\n      use_clip: False\n  valid_options:\n    name: RealDataset\n    options:\n      return_tensor: True\n\ndataloader:\n  train_options:\n    batch_size: -1\n    num_workers: 24\n    shuffle: False\n    drop_last: False\n  valid_options:\n    batch_size: -1\n    num_workers: 16\n    shuffle: False\n    drop_last: False\n\noptimizer:\n  name: AdamW\n  options:\n    lr: 0.001\n    betas: [0.9, 0.999]\n    weight_decay: 0.001\n    amsgrad: False\n\nlr_scheduler:\n  name: PolynomialLRWarmup\n  options:\n    warmup_iters: -1\n    total_iters: -1\n  pl_options:\n    monitor: loss\n    interval: step\n\ncallbacks:\n  - name: ModelCheckpoint\n    options:\n      monitor: valid_fpr@4\n      mode: max\n      verbose: True\n      save_last: True\n      save_top_k: 5\n  - name: LearningRateMonitor\n    options:\n      logging_interval: step\n  - name: RichModelSummary\n    options:\n      max_depth: 3\n  - name: CustomTQDMProgressBar\n    options:\n      unit_scale: -1\n\nlogger:\n  name: TensorBoardLogger\n  options:\n    save_dir: logger\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Each field's key-value pair is predefined in the ",(0,s.jsx)(n.code,{children:"Otter"})," module. As long as you follow this naming convention, everything will work as expected."]}),"\n",(0,s.jsxs)(n.p,{children:["By now, you should understand why we mentioned earlier that there\u2019s no need to specifically learn the ",(0,s.jsx)(n.code,{children:"Otter"})," module!"]}),"\n",(0,s.jsx)(n.p,{children:"Although there is a certain level of abstraction and encapsulation here, it is still a highly customizable framework."}),"\n",(0,s.jsx)(n.p,{children:"Ultimately, you will need to find the approach that works best for you, so there's no need to be overly strict about this particular format."}),"\n",(0,s.jsx)(n.h3,{id:"start-training",children:"Start Training"}),"\n",(0,s.jsxs)(n.p,{children:["Finally, navigate to the parent directory of ",(0,s.jsx)(n.code,{children:"DocClassifier"})," and run the following command to start the training:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Replace with your configuration file name\nbash DocClassifier/docker/train.bash lcnet050_cosface_f256_r128_squeeze_lbn_imagenet\n"})}),"\n",(0,s.jsx)(n.p,{children:"With these steps, you can safely execute the model training task inside the Docker container, while leveraging Docker's isolated environment to ensure consistency and reproducibility. This approach makes project deployment and scaling more convenient and flexible."}),"\n",(0,s.jsx)(n.h2,{id:"convert-to-onnx",children:"Convert to ONNX"}),"\n",(0,s.jsx)(n.p,{children:"This section explains how to convert the model to ONNX format."}),"\n",(0,s.jsxs)(n.p,{children:["First, take a look at the content of the ",(0,s.jsx)(n.code,{children:"to_onnx.bash"})," file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",metastring:'title="DocClassifier/docker/to_onnx.bash"',children:'#!/bin/bash\n\ndocker run \\\n    -e USER_ID=$(id -u) \\\n    -e GROUP_ID=$(id -g) \\\n    --gpus all \\\n    --shm-size=64g \\\n    --ipc=host --net=host \\\n    --cpuset-cpus="0-31" \\\n    -v $PWD/DocClassifier:/code/DocClassifier \\\n    -it --rm otter_base_image bash -c "\necho \'\nfrom fire import Fire\nfrom DocClassifier.model import main_classifier_torch2onnx\n\nif __name__ == \\"__main__\\":\n    Fire(main_classifier_torch2onnx)\n\' > /code/torch2onnx.py && python /code/torch2onnx.py --cfg_name $1\n"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Start by looking at this file, but there's no need to modify it. You need to modify the corresponding file: ",(0,s.jsx)(n.code,{children:"model/to_onnx.py"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"During the training process, you may use multiple branches to supervise the model\u2019s training. However, during the inference phase, you might only need one of these branches. Therefore, we need to convert the model to the ONNX format while retaining only the branches required for inference."}),"\n",(0,s.jsx)(n.p,{children:"For example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class WarpFeatureLearning(nn.Module):\n\n    def __init__(self, model: L.LightningModule):\n        super().__init__()\n        self.backbone = model.backbone\n        self.head = model.head\n\n    def forward(self, img: torch.Tensor):\n        xs = self.backbone(img)\n        features = self.head(xs)\n        return features\n"})}),"\n",(0,s.jsxs)(n.p,{children:["In the example above, we extract only the branch used for inference and encapsulate it into a new model called ",(0,s.jsx)(n.code,{children:"WarpFeatureLearning"}),". Then, we make the corresponding parameter settings in the YAML config:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'onnx:\n  name: WarpFeatureLearning\n  input_shape:\n    img:\n      shape: [1, 3, 128, 128]\n      dtype: float32\n  input_names: ["img"]\n  output_names:\n    - feats\n  dynamic_axes:\n    img:\n      "0": batch_size\n    output:\n      "0": batch_size\n  options:\n    opset_version: 16\n    verbose: False\n    do_constant_folding: True\n'})}),"\n",(0,s.jsx)(n.p,{children:"Describe the model's input dimensions, input names, output names, and the ONNX version."}),"\n",(0,s.jsxs)(n.p,{children:["The conversion part has already been written for you. After completing the modifications, make sure the ",(0,s.jsx)(n.code,{children:"model/to_onnx.py"})," file points to your model. Then, navigate to the parent directory of ",(0,s.jsx)(n.code,{children:"DocClassifier"})," and execute the following command to start the conversion:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Replace with your configuration file name\nbash DocClassifier/docker/to_onnx.bash lcnet050_cosface_f256_r128_squeeze_lbn_imagenet\n"})}),"\n",(0,s.jsx)(n.h2,{id:"final-notes",children:"Final Notes"}),"\n",(0,s.jsx)(n.p,{children:"We still strongly recommend completing all tasks inside Docker to ensure your environment is consistent and to avoid many unnecessary issues."}),"\n",(0,s.jsx)(n.p,{children:"After the above explanation, you should have a general understanding of the model training process. Although in actual applications, you may encounter more challenges, such as dataset preparation, model tuning, monitoring the training process, etc., these issues are too detailed to list individually. This article is meant to provide some basic guidance."}),"\n",(0,s.jsx)(n.p,{children:"In any case, we wish you success in obtaining a great model!"})]})}function h(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},50065:function(e,n,i){i.d(n,{Z:function(){return a},a:function(){return o}});var t=i(67294);let s={},r=t.createContext(s);function o(e){let n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);