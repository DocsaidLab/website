"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["3766"],{43423:function(e,i,n){n.r(i),n.d(i,{frontMatter:()=>a,toc:()=>c,default:()=>h,metadata:()=>s,assets:()=>o,contentTitle:()=>l});var s=JSON.parse('{"id":"multimodality/clip/index","title":"[21.03] CLIP","description":"Breaking the Dimensional Barrier","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/2103-clip/index.md","sourceDirName":"multimodality/2103-clip","slug":"/multimodality/clip/","permalink":"/en/papers/multimodality/clip/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739242156000,"frontMatter":{"title":"[21.03] CLIP","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[21.02] VL-T5","permalink":"/en/papers/multimodality/vlt5/"},"next":{"title":"[21.04] MDETR","permalink":"/en/papers/multimodality/mdetr/"}}'),t=n(74848),r=n(84429);let a={title:"[21.03] CLIP",authors:"Z. Yuan"},l,o={},c=[{value:"Breaking the Dimensional Barrier",id:"breaking-the-dimensional-barrier",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"The Solution",id:"the-solution",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Large-Scale Dataset",id:"large-scale-dataset",level:3},{value:"Training Details",id:"training-details",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Zero-Shot Learning",id:"zero-shot-learning",level:3},{value:"Is CLIP Truly Generalizable?",id:"is-clip-truly-generalizable",level:3},{value:"Human vs. Machine",id:"human-vs-machine",level:3},{value:"Overlap in Data",id:"overlap-in-data",level:3},{value:"Limitations",id:"limitations",level:2},{value:"Ethical Concerns",id:"ethical-concerns",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Key Contributions:",id:"key-contributions",level:3},{value:"Future Directions:",id:"future-directions",level:3}];function d(e){let i={a:"a",admonition:"admonition",blockquote:"blockquote",em:"em",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h2,{id:"breaking-the-dimensional-barrier",children:"Breaking the Dimensional Barrier"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://arxiv.org/abs/2103.00020",children:(0,t.jsx)(i.strong,{children:"Learning Transferable Visual Models From Natural Language Supervision"})})}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.p,{children:"Try Describing This Image:"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"60%"},children:(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"Coffee",src:n(34972).A+"",width:"1024",height:"1024"})})})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"A cup of coffee... um, with a book underneath the cup? Both are on a table?"}),"\n",(0,t.jsx)(i.li,{children:"Is this a corner of a caf\xe9 in the morning?"}),"\n",(0,t.jsx)(i.li,{children:"Brown table, brown chairs, and brown coffee? (Come on, be serious!)"}),"\n",(0,t.jsx)(i.li,{children:"\u2026"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"In fact, this image was generated from the following description:"}),"\n",(0,t.jsxs)(i.blockquote,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.em,{children:"On a peaceful morning, sunlight softly filters through the gaps in the curtains, gently illuminating a simple wooden table. On the table rests a freshly brewed cup of coffee, and the aroma of the coffee mingles with the sunlight, evoking warmth and hope for the day ahead. The shadow of the cup stretches across the table, casting a long reflection, which, along with the green plants by the window, creates a beautiful scene. The surface of the coffee ripples slightly, as if whispering the calmness of the morning and the beauty of life. Beside the cup, an open book lies quietly, waiting to be read. This tranquil morning scene, with coffee, sunlight, greenery, and books, forms a warm and serene moment, telling a story of life\u2019s simplicity and beauty."})}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"When trying to describe this image, you might realize that existing datasets, like ImageNet, with their 20,000 categories and millions of images, feel inadequate."}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"There are countless ways to describe the same image."}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(i.p,{children:"In contrast to the single hierarchical labels in ImageNet, humans can interpret an image from countless angles\u2014through color, shape, emotions, and narrative context."}),"\n",(0,t.jsxs)(i.p,{children:["This highlights the distinction between ",(0,t.jsx)(i.em,{children:"unimodal learning"})," and ",(0,t.jsx)(i.em,{children:"multimodal learning"}),": the former processes information from a single perspective, while the latter integrates different types of information, providing a richer and more comprehensive interpretation."]}),"\n",(0,t.jsx)(i.p,{children:"Traditional datasets like ImageNet often ignore meaningful information, such as object relationships, context, or emotions evoked by an image."}),"\n",(0,t.jsx)(i.p,{children:"The goal of CLIP is to break these limitations by integrating information from different sources, like text and images, to enhance the model\u2019s interpretive capacity\u2014bringing it closer to human-level perception and understanding."}),"\n",(0,t.jsx)(i.p,{children:"So, how did they achieve this?"}),"\n",(0,t.jsx)(i.h2,{id:"the-solution",children:"The Solution"}),"\n",(0,t.jsx)(i.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"CLIP Architecture",src:n(99871).A+"",width:"1726",height:"622"})}),"\n",(0,t.jsx)(i.p,{children:"The above diagram illustrates the pre-training architecture of CLIP."}),"\n",(0,t.jsx)(i.p,{children:"Consider a pair of image and text\u2014such as a picture of a dog with the caption, \u201Ca cute puppy.\u201D"}),"\n",(0,t.jsx)(i.p,{children:"In a training batch, CLIP receives multiple such pairs. The image encoder processes the images (via ResNet or ViT) to extract features, while the text encoder (using a Transformer) extracts textual features."}),"\n",(0,t.jsx)(i.p,{children:"The model compares these features to ensure that the cosine similarity between correctly paired image-text (like the dog image and the caption \u201Ca cute puppy\u201D) is maximized, while the similarity between mismatched pairs (like a dog image and the caption \u201Can apple\u201D) is minimized."}),"\n",(0,t.jsx)(i.p,{children:"What next?"}),"\n",(0,t.jsx)(i.p,{children:"Next, they fed 400 million image-text pairs into the model for training!"}),"\n",(0,t.jsx)(i.h3,{id:"large-scale-dataset",children:"Large-Scale Dataset"}),"\n",(0,t.jsx)(i.p,{children:"Initially, the authors used datasets like MS-COCO, Visual Genome, and YFCC100M."}),"\n",(0,t.jsx)(i.p,{children:"However, these datasets were too small for modern needs. MS-COCO and Visual Genome only offer around 100,000 training images\u2014insufficient compared to other computer vision systems processing billions of Instagram photos. YFCC100M, though larger with 100 million images, suffers from poor metadata quality (e.g., camera exposure settings as captions)."}),"\n",(0,t.jsx)(i.p,{children:"The solution? Build a new large-scale dataset\u2014WIT (WebImageText)\u2014comprising 400 million image-text pairs collected from public web sources."}),"\n",(0,t.jsx)(i.admonition,{type:"tip",children:(0,t.jsxs)(i.p,{children:["Here is the dataset download link: ",(0,t.jsx)(i.a,{href:"https://github.com/google-research-datasets/wit#wit--wikipedia-based-image-text-dataset",children:(0,t.jsx)(i.strong,{children:"WIT: Wikipedia-based Image Text Dataset"})})]})}),"\n",(0,t.jsx)(i.h3,{id:"training-details",children:"Training Details"}),"\n",(0,t.jsx)(i.p,{children:"The authors trained a series of 5 ResNet and 3 Vision Transformer (ViT) models."}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"ResNet Series"}),": They trained ResNet-50 and ResNet-101, followed by scaled versions like RN50x4, RN50x16, and RN50x64\u2014each representing 4x, 16x, and 64x the computational power of ResNet-50."]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"ViT Models"}),": The team trained ViT-B/32, ViT-B/16, and ViT-L/14, running each for 32 epochs using the Adam optimizer with weight decay regularization and cosine learning rate scheduling."]}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"Training the largest ResNet model (RN50x64) required 592 V100 GPUs for 18 days, while ViT-L/14 needed 256 V100 GPUs for 12 days. The ViT-L/14 model received additional fine-tuning at 336-pixel resolution, designated as ViT-L/14@336px."}),"\n",(0,t.jsx)(i.admonition,{type:"tip",children:(0,t.jsx)(i.p,{children:"592 V100 GPUs? Simply incredible!"})}),"\n",(0,t.jsx)(i.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(i.h3,{id:"zero-shot-learning",children:"Zero-Shot Learning"}),"\n",(0,t.jsx)(i.p,{children:"CLIP excels in some areas but has room for improvement:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Performance on Specific Datasets"}),":\nIn datasets with clear feature expressions, CLIP\u2019s zero-shot performance rivals or surpasses fully supervised classifiers."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Few-Shot Matching"}),":\nCLIP can identify new categories (like \u201Czebra\u201D) from textual descriptions, while few-shot classifiers require more training data to achieve comparable results."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Transferability Across Datasets"}),":\nCLIP\u2019s transferability varies across datasets. It performs well with simpler datasets but struggles with more complex ones."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Comparison with Supervised Classifiers"}),":\nWhile supervised classifiers often outperform CLIP, zero-shot learning provides a powerful alternative for certain tasks."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Scaling Trends"}),":\nThe performance improvements suggest further potential for zero-shot learning by expanding data and refining model features."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"is-clip-truly-generalizable",children:"Is CLIP Truly Generalizable?"}),"\n",(0,t.jsx)(i.p,{children:"CLIP outperforms traditional ImageNet-trained models across new domains but still experiences slight performance drops when dealing with unseen distributions."}),"\n",(0,t.jsx)(i.h3,{id:"human-vs-machine",children:"Human vs. Machine"}),"\n",(0,t.jsx)(i.p,{children:"In experiments using datasets like Oxford IIT Pets, humans achieved higher accuracy (up to 94%), especially with a few reference samples. In contrast, CLIP struggled to improve with limited samples, revealing a gap in few-shot learning capabilities compared to humans."}),"\n",(0,t.jsx)(i.h3,{id:"overlap-in-data",children:"Overlap in Data"}),"\n",(0,t.jsx)(i.p,{children:"During pre-training, overlap between the training dataset and evaluation sets raised concerns about inflated performance. While some datasets showed overlap (e.g., Country211), the impact on accuracy was minimal\u2014indicating that overlapping data did not significantly boost performance."}),"\n",(0,t.jsx)(i.h2,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsx)(i.p,{children:"Despite its promise, CLIP faces several challenges:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Performance Gaps"}),":\nCLIP's performance still lags behind state-of-the-art supervised models on many datasets, even with scaling."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Few-Shot Learning Deficiencies"}),":\nCLIP struggles with fine-grained tasks, like distinguishing car models or counting objects."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Generalization Challenges"}),":\nCLIP\u2019s performance drops significantly on datasets like MNIST, highlighting the limitations of relying solely on diverse datasets for generalization."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Data and Computational Costs"}),":\nTraining CLIP demands extensive data and computation, limiting its accessibility."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Evaluation Challenges"}),":\nThe model relies heavily on zero-shot settings, but performance unexpectedly drops when transitioning to few-shot tasks\u2014indicating room for optimization."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Bias and Ethical Issues"}),":\nCLIP\u2019s reliance on web data introduces societal biases, necessitating further research to address these concerns."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"ethical-concerns",children:"Ethical Concerns"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Bias"}),":\nCLIP's training data may reflect social biases, potentially amplifying discrimination or inequality in applications."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Surveillance"}),":\nCLIP\u2019s capabilities raise privacy concerns when used in surveillance tasks, especially for facial recognition or behavior analysis."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(i.p,{children:"CLIP demonstrates how large-scale pre-training from natural language supervision can revolutionize computer vision, achieving impressive performance in zero-shot learning and task transfer."}),"\n",(0,t.jsx)(i.h3,{id:"key-contributions",children:"Key Contributions:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Multimodal Learning"}),":\nCLIP\u2019s ability to integrate images and text offers unprecedented versatility."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Zero-Shot Learning"}),":\nBy leveraging natural language, CLIP performs well even without task-specific training data."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Scalability"}),":\nWith sufficient data, CLIP approaches the performance of supervised models."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"future-directions",children:"Future Directions:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Improved Interpretability"}),":\nWhile CLIP provides high-level explanations, it lacks detailed interpretability and transparency."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Enhanced Few-Shot Learning"}),":\nIntegrating few-shot methods could further improve performance."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"CLIP opens new avenues for multimodal learning, but it also presents challenges\u2014highlighting the importance of balancing performance with ethical considerations for future development."})]})}function h(e={}){let{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},99871:function(e,i,n){n.d(i,{A:()=>s});let s=n.p+"assets/images/arch_clip-ef4a34c3ec1c19ee45e598ca12ff5459.jpg"},34972:function(e,i,n){n.d(i,{A:()=>s});let s=n.p+"assets/images/coffee-740001a9c0f56f99363cdd405c10b4f7.jpg"},84429:function(e,i,n){n.d(i,{R:()=>a,x:()=>l});var s=n(96540);let t={},r=s.createContext(t);function a(e){let i=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:i},e.children)}}}]);