"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["48077"],{66001:function(e,i,s){s.r(i),s.d(i,{default:()=>c,frontMatter:()=>r,metadata:()=>n,assets:()=>o,toc:()=>d,contentTitle:()=>l});var n=JSON.parse('{"id":"multimodality/beit-v3/index","title":"[22.08] BEiT-3","description":"Great Minds Think Alike","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/2208-beit-v3/index.md","sourceDirName":"multimodality/2208-beit-v3","slug":"/multimodality/beit-v3/","permalink":"/en/papers/multimodality/beit-v3/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1735655069000,"frontMatter":{"title":"[22.08] BEiT-3","authors":"Zephyr"},"sidebar":"papersSidebar","previous":{"title":"[22.04] Flamingo","permalink":"/en/papers/multimodality/flamingo/"},"next":{"title":"[22.12] FLIP","permalink":"/en/papers/multimodality/flip/"}}'),t=s("85893"),a=s("50065");let r={title:"[22.08] BEiT-3",authors:"Zephyr"},l=void 0,o={},d=[{value:"Great Minds Think Alike",id:"great-minds-think-alike",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Problem Solving",id:"problem-solving",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Multimodal Adaptation",id:"multimodal-adaptation",level:3},{value:"Pre-training Tasks",id:"pre-training-tasks",level:3},{value:"Model and Training Data",id:"model-and-training-data",level:3},{value:"Training Strategy",id:"training-strategy",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Comparison with Other Models",id:"comparison-with-other-models",level:3},{value:"Module Contribution Analysis",id:"module-contribution-analysis",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let i={a:"a",admonition:"admonition",annotation:"annotation",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h2,{id:"great-minds-think-alike",children:"Great Minds Think Alike"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://arxiv.org/abs/2208.10442",children:(0,t.jsx)(i.strong,{children:"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"})})}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.p,{children:'Everyone knows that BEiT\u2019s signature feature is the "discrete coding."'}),"\n",(0,t.jsx)(i.p,{children:'So, upon encountering this paper, we instinctively expect BEiT-3 to introduce some novel "discrete coding" techniques.'}),"\n",(0,t.jsxs)(i.p,{children:["However, BEiT-3 merely ",(0,t.jsx)(i.strong,{children:"applies"}),' the "discrete coding" method to image pretraining. The main focus of this paper is actually: ',(0,t.jsx)(i.strong,{children:"How can we use a unified approach to handle pretraining tasks for text, images, and multimodal data?"})]}),"\n",(0,t.jsx)(i.p,{children:"That said, reminiscing about past successes is only natural\u2014let\u2019s not dwell too much on it."}),"\n",(0,t.jsx)(i.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(i.p,{children:'The idea of "pretraining images and text together" is no longer groundbreaking.'}),"\n",(0,t.jsx)(i.p,{children:"Previously, common methods, which you might already be familiar with, often involved using multiple pretraining tasks simultaneously, such as: image-text matching, retrieval, and various mask designs."}),"\n",(0,t.jsx)(i.p,{children:"If it doesn\u2019t ring a bell, let\u2019s revisit the BLIP framework we studied earlier:"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"blip arch",src:s(31969).Z+"",width:"1760",height:"706"})})})}),"\n",(0,t.jsx)(i.p,{children:"The model is divided into an image branch and a text branch, with multiple tasks in between to learn cross-modal relationships. While effective, this approach becomes increasingly complex and challenging to maintain or expand as the scale of models and data grows."}),"\n",(0,t.jsx)(i.p,{children:"Since images can be converted into tokens, and text can also be tokenized, why not handle everything using a unified approach? Treating it all as text for processing seems like the most straightforward solution."}),"\n",(0,t.jsx)(i.p,{children:"Previous researchers have undoubtedly considered this. However, the lack of an effective framework suggests that there are inherent challenges."}),"\n",(0,t.jsx)(i.p,{children:'The authors believe that we should address this issue with a "multi-path architecture."'}),"\n",(0,t.jsx)(i.h2,{id:"problem-solving",children:"Problem Solving"}),"\n",(0,t.jsx)(i.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"model arch",src:s(37740).Z+"",width:"1224",height:"570"})})})}),"\n",(0,t.jsx)(i.p,{children:"The multi-path architecture, or its more commonly known name: Mixture-of-Experts (MoE), is a method of combining multiple expert models together. Before BEiT-3 was proposed, a paper applied this architecture in the multimodal domain: VLMo."}),"\n",(0,t.jsxs)(i.admonition,{type:"info",children:[(0,t.jsx)(i.p,{children:"For readers interested in VLMo, you can read:"}),(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://arxiv.org/abs/2111.02358",children:(0,t.jsx)(i.strong,{children:"[21.11] VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"})})}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"vlmo arch",src:s(24153).Z+"",width:"1224",height:"354"})})})}),"\n"]}),"\n"]}),(0,t.jsx)(i.p,{children:'In VLMo, different modalities each have their corresponding pre-training tasks, while BEiT-3 unifies everything under the "mask-predict" method for training the model.'})]}),"\n",(0,t.jsx)(i.p,{children:"Thus, BEiT-3's architecture directly references VLMo's Multiway Transformers."}),"\n",(0,t.jsx)(i.p,{children:"Each Multiway Transformer module consists of a shared self-attention module and a set of feedforward networks (i.e., modality experts) for different modalities. Depending on the modality of the input token, the corresponding expert is used for feature computation."}),"\n",(0,t.jsx)(i.p,{children:"In the authors' implementation, each layer includes a vision expert and a language expert; in addition, the top three layers are configured with vision-language experts for the fusion encoders."}),"\n",(0,t.jsx)(i.p,{children:"Using a set of modality experts allows the model to capture more information specific to each modality. The shared self-attention module learns the alignment between different modalities and performs deep fusion in multimodal tasks (such as vision-language)."}),"\n",(0,t.jsxs)(i.admonition,{type:"tip",children:[(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"Does this series look like the MoE in Switch Transformer?"})}),(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"/en/papers/transformers/switch-transformer/",children:(0,t.jsx)(i.strong,{children:"[21.01] Switch Transformer"})})}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"70%"},children:(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"switch arch",src:s(96157).Z+"",width:"1224",height:"648"})})})}),"\n"]}),"\n"]}),(0,t.jsx)(i.p,{children:"You are right! In the NLP field, MoE architecture was an academic hotspot in 2021, and while it took a few months longer in the CV field, this idea was quickly applied to the multimodal domain."})]}),"\n",(0,t.jsx)(i.h3,{id:"multimodal-adaptation",children:"Multimodal Adaptation"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"multimodal",src:s(45617).Z+"",width:"1374",height:"890"})}),"\n",(0,t.jsx)(i.p,{children:'As shown in the figure above, the authors demonstrate BEiT-3\'s "shared Multiway Transformer" architecture, which can "flexibly switch" or "combine" into the required model form for different downstream tasks.'}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"(a) Vision Encoder"}),": Used for visual tasks that require only image feature representations, such as image classification, object detection, instance segmentation, and semantic segmentation."]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"(b) Language Encoder"}),": Used for language tasks that require only text features, such as text classification, sequence labeling, or dialogue systems combined with other modules."]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"(c) Fusion Encoder"}),': Used for tasks that require deep interaction between images and text, such as Visual Question Answering (VQA), Visual Reasoning (NLVR2), image-text reasoning, where both image and text need to be "seen" simultaneously for integrated understanding.']}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"(d) Dual Encoder"}),': Used for cross-modal retrieval tasks that require "efficient matching," such as image-text retrieval. For example, quickly finding the image most related to a specific text from a large pool of candidate images, or conversely, finding the text most related to a specific image from a large pool of text.']}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"(e) Sequence-to-Sequence Learning"}),": Used for generative tasks that require transforming image inputs into text outputs, such as image captioning or other applications that need image-text conversion."]}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"Whether it's image encoding, text encoding, or complex applications requiring image-text fusion, retrieval, or generation, all can be configured within the same architecture without needing to switch to entirely different models."}),"\n",(0,t.jsx)(i.h3,{id:"pre-training-tasks",children:"Pre-training Tasks"}),"\n",(0,t.jsx)(i.p,{children:"During the pre-training process of BEiT-3, the model randomly masks some of the text tokens or image patches and trains the model to recover these masked tokens. This method helps the model learn the representations of text and images while also capturing the correspondence between them."}),"\n",(0,t.jsx)(i.p,{children:"During training, text data is tokenized using the SentencePiece tokenizer, while image data uses the BEiT v2 tokenizer, which splits images into discrete visual tokens to be reconstructed. The authors randomly mask 15% of the unimodal text tokens and 50% of the text tokens in image-text pairs. For images, the block masking strategy from BEiT is applied, masking 40% of the image patches."}),"\n",(0,t.jsx)(i.p,{children:'Through experiments, the authors found that when using the "mask-predict" task alone, a much smaller pre-training batch size can be used compared to contrastive models.'}),"\n",(0,t.jsx)(i.admonition,{type:"tip",children:(0,t.jsx)(i.p,{children:"This is why BEiT v2 is used here, and thus the paper is called BEiT-3."})}),"\n",(0,t.jsx)(i.h3,{id:"model-and-training-data",children:"Model and Training Data"}),"\n",(0,t.jsx)(i.p,{children:"BEiT-3 has approximately 1.9 billion (1.9B) parameters, with 692 million for the vision experts, 692 million for the language experts, 52 million for the vision-language experts, and 317 million for the shared self-attention module. The architecture draws from ViT-giant."}),"\n",(0,t.jsx)(i.p,{children:'The model consists of 40 layers of Multiway Transformer. Each layer has a hidden dimension of 1408, an intermediate layer dimension of 6144, and 16 attention heads. Each layer contains both a "vision expert" and a "language expert." Additionally, the top three layers of the Multiway Transformer incorporate "vision-language experts" for deeper cross-modal fusion.'}),"\n",(0,t.jsx)(i.p,{children:'BEiT-3 uses both "unimodal" and "multimodal" data for pre-training. Unimodal data refers to data containing only "pure images" or "pure text," while multimodal data refers to "image + text" pairs.'}),"\n",(0,t.jsx)(i.p,{children:"Multimodal data sources: There are about 15 million images and 21 million image-text pairs from five public datasets: Conceptual 12M (CC12M), Conceptual Captions (CC3M), SBU Captions (SBU), COCO, and Visual Genome (VG)."}),"\n",(0,t.jsx)(i.p,{children:"Unimodal data includes 14 million images from ImageNet-21K and 160GB of text data, sourced from English Wikipedia, BookCorpus, OpenWebText3, CC-News, and Stories."}),"\n",(0,t.jsx)(i.h3,{id:"training-strategy",children:"Training Strategy"}),"\n",(0,t.jsx)(i.p,{children:"BEiT-3 pre-training lasts for 1 million steps. Each training batch consists of 6144 samples, including: 2048 images, 2048 text samples, and 2048 image-text pairs."}),"\n",(0,t.jsx)(i.p,{children:"Images are split into 14\xd714 patches, and during pre-training, a resolution of 224\xd7224 is used. The image augmentation strategy is consistent with BEiT, including random scaling and cropping, horizontal flipping, and color jittering."}),"\n",(0,t.jsx)(i.p,{children:"The text tokenizer used is SentencePiece, with a vocabulary size of 64k."}),"\n",(0,t.jsx)(i.p,{children:"Other hyperparameters include:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["AdamW optimizer, with hyperparameters: ",(0,t.jsxs)(i.span,{className:"katex",children:[(0,t.jsx)(i.span,{className:"katex-mathml",children:(0,t.jsx)(i.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(i.semantics,{children:[(0,t.jsxs)(i.mrow,{children:[(0,t.jsxs)(i.msub,{children:[(0,t.jsx)(i.mi,{children:"\u03B2"}),(0,t.jsx)(i.mn,{children:"1"})]}),(0,t.jsx)(i.mo,{children:"="}),(0,t.jsx)(i.mn,{children:"0.9"}),(0,t.jsx)(i.mo,{separator:"true",children:","}),(0,t.jsxs)(i.msub,{children:[(0,t.jsx)(i.mi,{children:"\u03B2"}),(0,t.jsx)(i.mn,{children:"2"})]}),(0,t.jsx)(i.mo,{children:"="}),(0,t.jsx)(i.mn,{children:"0.98"}),(0,t.jsx)(i.mo,{separator:"true",children:","}),(0,t.jsx)(i.mi,{children:"\u03F5"}),(0,t.jsx)(i.mo,{children:"="}),(0,t.jsx)(i.mn,{children:"1"}),(0,t.jsx)(i.mi,{children:"e"}),(0,t.jsx)(i.mo,{children:"\u2212"}),(0,t.jsx)(i.mn,{children:"6"})]}),(0,t.jsx)(i.annotation,{encoding:"application/x-tex",children:"\\beta_1=0.9, \\beta_2=0.98, \\epsilon=1e-6"})]})})}),(0,t.jsxs)(i.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(i.span,{className:"base",children:[(0,t.jsx)(i.span,{className:"strut",style:{height:"0.8889em",verticalAlign:"-0.1944em"}}),(0,t.jsxs)(i.span,{className:"mord",children:[(0,t.jsx)(i.span,{className:"mord mathnormal",style:{marginRight:"0.05278em"},children:"\u03B2"}),(0,t.jsx)(i.span,{className:"msupsub",children:(0,t.jsxs)(i.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(i.span,{className:"vlist-r",children:[(0,t.jsx)(i.span,{className:"vlist",style:{height:"0.3011em"},children:(0,t.jsxs)(i.span,{style:{top:"-2.55em",marginLeft:"-0.0528em",marginRight:"0.05em"},children:[(0,t.jsx)(i.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(i.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(i.span,{className:"mord mtight",children:"1"})})]})}),(0,t.jsx)(i.span,{className:"vlist-s",children:"\u200B"})]}),(0,t.jsx)(i.span,{className:"vlist-r",children:(0,t.jsx)(i.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(i.span,{})})})]})})]}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(i.span,{className:"mrel",children:"="}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(i.span,{className:"base",children:[(0,t.jsx)(i.span,{className:"strut",style:{height:"0.8889em",verticalAlign:"-0.1944em"}}),(0,t.jsx)(i.span,{className:"mord",children:"0.9"}),(0,t.jsx)(i.span,{className:"mpunct",children:","}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsxs)(i.span,{className:"mord",children:[(0,t.jsx)(i.span,{className:"mord mathnormal",style:{marginRight:"0.05278em"},children:"\u03B2"}),(0,t.jsx)(i.span,{className:"msupsub",children:(0,t.jsxs)(i.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(i.span,{className:"vlist-r",children:[(0,t.jsx)(i.span,{className:"vlist",style:{height:"0.3011em"},children:(0,t.jsxs)(i.span,{style:{top:"-2.55em",marginLeft:"-0.0528em",marginRight:"0.05em"},children:[(0,t.jsx)(i.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(i.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(i.span,{className:"mord mtight",children:"2"})})]})}),(0,t.jsx)(i.span,{className:"vlist-s",children:"\u200B"})]}),(0,t.jsx)(i.span,{className:"vlist-r",children:(0,t.jsx)(i.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(i.span,{})})})]})})]}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(i.span,{className:"mrel",children:"="}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(i.span,{className:"base",children:[(0,t.jsx)(i.span,{className:"strut",style:{height:"0.8389em",verticalAlign:"-0.1944em"}}),(0,t.jsx)(i.span,{className:"mord",children:"0.98"}),(0,t.jsx)(i.span,{className:"mpunct",children:","}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(i.span,{className:"mord mathnormal",children:"\u03F5"}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(i.span,{className:"mrel",children:"="}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(i.span,{className:"base",children:[(0,t.jsx)(i.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,t.jsx)(i.span,{className:"mord",children:"1"}),(0,t.jsx)(i.span,{className:"mord mathnormal",children:"e"}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(i.span,{className:"mbin",children:"\u2212"}),(0,t.jsx)(i.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,t.jsxs)(i.span,{className:"base",children:[(0,t.jsx)(i.span,{className:"strut",style:{height:"0.6444em"}}),(0,t.jsx)(i.span,{className:"mord",children:"6"})]})]})]}),"."]}),"\n",(0,t.jsx)(i.li,{children:"Learning rate schedule uses cosine annealing, with a maximum learning rate of 1e-3, reaching this peak learning rate after 10,000 steps."}),"\n",(0,t.jsx)(i.li,{children:"Weight decay is set to 0.05."}),"\n",(0,t.jsx)(i.li,{children:"Random depth technique is applied with a probability of 0.1 to reduce overfitting and enhance model generalization."}),"\n"]}),"\n",(0,t.jsx)(i.admonition,{type:"tip",children:(0,t.jsx)(i.p,{children:"This batch size is actually much smaller than many contrastive learning models, as contrastive models often require very large batch sizes for effective negative sample comparison."})}),"\n",(0,t.jsx)(i.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(i.h3,{id:"comparison-with-other-models",children:"Comparison with Other Models"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"compare",src:s(76003).Z+"",width:"1224",height:"880"})})})}),"\n",(0,t.jsx)(i.p,{children:"The authors performed extensive evaluations of BEiT-3 on various public benchmarks for vision-language and vision tasks."}),"\n",(0,t.jsx)(i.p,{children:"BEiT-3 achieved state-of-the-art (SoTA) performance on multiple tasks, surpassing previous models comprehensively, as summarized in the table below:"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"table",src:s(82713).Z+"",width:"1224",height:"502"})})})}),"\n",(0,t.jsx)(i.h3,{id:"module-contribution-analysis",children:"Module Contribution Analysis"}),"\n",(0,t.jsx)(i.p,{children:'The paper does not provide ablation studies, so we cannot determine exactly which "module" is the most crucial in this architecture from specific data. Therefore, we can only attempt to analyze it ourselves:'}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:'Is the "large amount of data" the core contribution?'})}),"\n",(0,t.jsx)(i.p,{children:"BEiT-3 pre-trains on a large-scale unimodal and multimodal dataset, including 14M ImageNet-21K images, 160GB of text data, and 15M image/21M image-text pairs."}),"\n",(0,t.jsx)(i.p,{children:'Large datasets are indeed a key factor in the performance of current large models, but the paper itself does not "emphasize data quantity" nor does it rely on proprietary data. The authors highlight that it is not about "more is better," but rather achieving superior performance using only publicly available data, compared to previous models that required proprietary data.'}),"\n",(0,t.jsx)(i.p,{children:'The vast data provides a foundation, but it is not the only innovation. The true focus of the paper is on "the unity and effectiveness of the method."'}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"The Expert Model Architecture"})}),"\n",(0,t.jsx)(i.p,{children:"In each Transformer layer of BEiT-3, there is a vision expert, a language expert, and at the top layers, a vision-language expert, all sharing a self-attention module."}),"\n",(0,t.jsx)(i.p,{children:'This differs from the common Mixture-of-Experts (MoE) model. The emphasis here is on "multiple paths with most parameters shared," allowing for maximal interaction or independence between vision and language in the same model. Multiway enables the same model to serve multiple purposes: pure vision path, pure text path, cross-modal path\u2026 This brings engineering convenience and demonstrates the advantage of cross-task transfer.'}),"\n",(0,t.jsx)(i.p,{children:'However, this "multi-path architecture" design originates from VLMo\'s innovation, so it is not considered a key contribution of BEiT-3.'}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"Unified Training Objective Function"})}),"\n",(0,t.jsx)(i.p,{children:'The authors improved the complex approach of "multi-task parallelism" in multimodal pre-training by using a single masked prediction (masked data modeling) to learn cross-modal alignment.'}),"\n",(0,t.jsx)(i.p,{children:'This idea is not entirely new (BERT, BEiT, etc., have used similar Mask-Then-Predict approaches), but in the multimodal domain, many previous models required explicit contrastive losses or image-text matching (ITM) tasks. BEiT-3 only needs one objective function to learn both image-text alignment and cross-modal fusion, making large-scale training "cleaner" without the need to adjust hyperparameters or switch batches for different tasks.'}),"\n",(0,t.jsx)(i.p,{children:"The authors' insistence on \"simplifying the pre-training task\" and its successful validation is a key reason for BEiT-3's outstanding performance on multiple tasks and multimodal applications. This is also one of the core messages of the paper."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"Discrete Image Encoding Based on BEiT"})}),"\n",(0,t.jsx)(i.p,{children:'The BEiT series uses discrete visual tokens (similar to word tokens in NLP) that allow images to undergo "MLM" masking and reconstruction.'}),"\n",(0,t.jsx)(i.p,{children:'This technology itself comes from earlier BEiT versions (v1, v2), not BEiT-3. However, this paper further proves the feasibility of treating "images like another language." By discretizing image encoding, the model can process both text and images using the same BERT-like pre-training mechanism.'}),"\n",(0,t.jsx)(i.p,{children:'Unlike previous methods that require CNNs or patch embeddings for image masking, this "discrete image" strategy is an essential foundation for BEiT-3, though its innovation mainly stems from prior BEiT works.'}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.p,{children:'While large-scale data and the use of discrete visual tokens indeed support the overall method, if we were to highlight the most crucial contribution, we would attribute it to the "improved Multiway Transformers architecture," particularly the use of a "unified masked objective" to train the model.'}),"\n",(0,t.jsx)(i.p,{children:'The authors did not rely solely on "more data" or "larger parameters" to succeed; rather, through "simplifying pre-training tasks" and "flexible sharing of multiple expert paths," they successfully demonstrated the feasibility and advantages of this approach across various downstream applications.'}),"\n",(0,t.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(i.p,{children:"In this paper, the authors propose a general multimodal foundational model: BEiT-3."}),"\n",(0,t.jsx)(i.p,{children:'By treating images as a form of "foreign language," the authors were able to apply the same masked "language modeling" method to train text, images, and image-text pairs, and use the Multiway Transformers architecture, allowing the model to switch flexibly between different modalities.'}),"\n",(0,t.jsx)(i.p,{children:"Compared to most current multimodal pre-training methods, BEiT-3's training process is simpler and more easily scalable, making it a highly attractive design approach."})]})}function c(e={}){let{wrapper:i}={...(0,a.a)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},31969:function(e,i,s){s.d(i,{Z:function(){return n}});let n=s.p+"assets/images/blip_arch-183f39c46ffd54e2ac3fe16c3b773b70.jpg"},76003:function(e,i,s){s.d(i,{Z:function(){return n}});let n=s.p+"assets/images/img1-eb0fbdcac54333c4c6f3eb39fa835836.jpg"},37740:function(e,i,s){s.d(i,{Z:function(){return n}});let n=s.p+"assets/images/img2-c9c3880b298641d43a459e07bef7db25.jpg"},45617:function(e,i,s){s.d(i,{Z:function(){return n}});let n=s.p+"assets/images/img3-9e633cb8861c3d7b47cc67c66ebc8cdf.jpg"},82713:function(e,i,s){s.d(i,{Z:function(){return n}});let n=s.p+"assets/images/img4-af0a0dd7a6800e45ddd81c9de8fcdf45.jpg"},96157:function(e,i,s){s.d(i,{Z:function(){return n}});let n=s.p+"assets/images/switch_arch-d4831854f50165c0a08988fc6b6258a4.jpg"},24153:function(e,i,s){s.d(i,{Z:function(){return n}});let n=s.p+"assets/images/vlmo_arch-b1bd3b2f628dcbd0e33e9d98da7aed41.jpg"},50065:function(e,i,s){s.d(i,{Z:function(){return l},a:function(){return r}});var n=s(67294);let t={},a=n.createContext(t);function r(e){let i=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),n.createElement(a.Provider,{value:i},e.children)}}}]);