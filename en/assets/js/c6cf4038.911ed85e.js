"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["22957"],{37523:function(e,s,n){n.r(s),n.d(s,{default:()=>d,frontMatter:()=>r,metadata:()=>a,assets:()=>c,toc:()=>h,contentTitle:()=>l});var a=JSON.parse('{"id":"contrastive-learning/examplar-cnn/index","title":"[14.06] Exemplar CNN","description":"Finding Invariant Features","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/contrastive-learning/1406-examplar-cnn/index.md","sourceDirName":"contrastive-learning/1406-examplar-cnn","slug":"/contrastive-learning/examplar-cnn/","permalink":"/en/papers/contrastive-learning/examplar-cnn/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739239448000,"frontMatter":{"title":"[14.06] Exemplar CNN","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"Contrastive Learning (7)","permalink":"/en/papers/category/contrastive-learning-7"},"next":{"title":"[18.05] InstDisc","permalink":"/en/papers/contrastive-learning/instdisc/"}}'),i=n("85893"),t=n("50065");let r={title:"[14.06] Exemplar CNN",authors:"Z. Yuan"},l=void 0,c={},h=[{value:"Finding Invariant Features",id:"finding-invariant-features",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Experimental Setup",id:"experimental-setup",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Comparison with Other Methods",id:"comparison-with-other-methods",level:3},{value:"Ablation - Number of Proxy Categories",id:"ablation---number-of-proxy-categories",level:3},{value:"Ablation - Number of Samples",id:"ablation---number-of-samples",level:3},{value:"Ablation - Type of Image Augmentation",id:"ablation---type-of-image-augmentation",level:3},{value:"Ablation - Performance in Image Classification",id:"ablation---performance-in-image-classification",level:3},{value:"Filter Visualization",id:"filter-visualization",level:3},{value:"Conclusion",id:"conclusion",level:2}];function o(e){let s={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h2,{id:"finding-invariant-features",children:"Finding Invariant Features"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1406.6909",children:(0,i.jsx)(s.strong,{children:"Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks"})})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"While reading the paper, I noticed that many studies reference the ideas from this work, so let's take some time to look at it."}),"\n",(0,i.jsx)(s.p,{children:"The concept of the paper is simple, but many experimental results can inspire us with new ideas."}),"\n",(0,i.jsx)(s.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,i.jsx)(s.p,{children:"The advantage of supervised learning is also its drawback: it requires a large amount of labeled data. In some specific application scenarios, such data might be hard to obtain or expensive."}),"\n",(0,i.jsx)(s.p,{children:"Moreover, if we want to perform transfer learning between different tasks, such as switching from object classification to descriptor matching, the performance of supervised learning might not be ideal."}),"\n",(0,i.jsx)(s.p,{children:"Since it\u2019s not ideal, a simple idea arises: what if we don\u2019t use data labels?"}),"\n",(0,i.jsx)(s.p,{children:"But if we don't use data labels, how should we train the model?"}),"\n",(0,i.jsx)(s.p,{children:"The authors suggest: we need a proxy task."}),"\n",(0,i.jsx)(s.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,i.jsxs)(s.p,{children:["The authors propose a training method that does not require manual annotations, called ",(0,i.jsx)(s.strong,{children:"Exemplar-CNN"}),"."]}),"\n",(0,i.jsx)(s.p,{children:"The core concept is very simple:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Randomly extract image patches"}),": To focus more on areas with rich object information, image patches of size ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"32"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"32"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"32 \\times 32"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"32"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"32"})]})]})]})," are sampled from unlabeled images, weighted by the gradient magnitude."]}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsx)(s.p,{children:"This is not the gradient from backpropagation, but the gradient of the image. The authors believe that the gradient in the image can help us find more meaningful patches."})}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Apply various random transformations to the image patches"}),': Such as translation, scaling, rotation, color adjustment, contrast adjustment, etc., to generate multiple "deformed versions." These transformations explicitly define the invariances the network should learn (e.g., rotation invariance, color invariance).']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:'Form "proxy categories" and perform discriminative learning'}),': Each original image patch and its deformed versions are considered as "positive samples" in the same proxy category.']}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:'The system repeats this process across many images, generating many proxy categories, and trains a CNN to distinguish "which image patches (including all deformed versions) belong to the same proxy category and which belong to other proxy categories." This way, the model learns to distinguish different image patches and also becomes invariant to "various deformations of the same image."'}),"\n",(0,i.jsx)(s.p,{children:"The list of transformations used in the paper is as follows:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Translation"}),": Translations in the vertical or horizontal direction, not exceeding 20% of the image patch size."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Scaling"}),": Rescale the image patch proportionally, with a scale factor between 0.7 and 1.4."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Rotation"}),": Rotation angle within \xb120 degrees."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Contrast 1"}),": Multiply each pixel\u2019s projection in all principal component directions by a factor between 0.5 and 2 (independent for each principal direction, the same for all pixels in the patch)."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Contrast 2"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Increase the saturation and brightness (S and V components in HSV color space) by a factor between 0.25 and 4."}),"\n",(0,i.jsx)(s.li,{children:"Then multiply by a factor between 0.7 and 1.4, and add an offset between \u22120.1 and 0.1."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Color"}),": Apply an offset between \u22120.1 and 0.1 to the hue (H component in HSV color space)."]}),"\n"]}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsx)(s.p,{children:"In the ablation experiments section, the authors verify the effectiveness of these transformations."})}),"\n",(0,i.jsx)(s.p,{children:"The training data after cropping looks like this (using STL-10 as an example):"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"90%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"dataset",src:n(15268).Z+"",width:"1080",height:"418"})})})}),"\n",(0,i.jsx)(s.h3,{id:"experimental-setup",children:"Experimental Setup"}),"\n",(0,i.jsxs)(s.p,{children:["To compare ",(0,i.jsx)(s.strong,{children:"Exemplar-CNN"})," with previous unsupervised feature learning methods, the authors performed a series of experiments using the following datasets: STL-10, CIFAR-10, Caltech-101, and Caltech-256."]}),"\n",(0,i.jsx)(s.p,{children:"Details about the datasets, such as the amount of data, number of categories, image sizes, are as follows:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Number of categories"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"STL-10 and CIFAR-10: 10 categories."}),"\n",(0,i.jsx)(s.li,{children:"Caltech-101: 101 categories."}),"\n",(0,i.jsx)(s.li,{children:"Caltech-256: 256 categories."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Image size processing"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["CIFAR-10: Adjusted from ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"32"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"32"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"32 \\times 32"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"32"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"32"})]})]})]})," to ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"64"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"64"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"64 \\times 64"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"64"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"64"})]})]})]}),"."]}),"\n",(0,i.jsxs)(s.li,{children:["Caltech-101: Adjusted to ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"150"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"150"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"150 \\times 150"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"150"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"150"})]})]})]}),"."]}),"\n",(0,i.jsxs)(s.li,{children:["Caltech-256: Adjusted to ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"256"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"256"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"256 \\times 256"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"256"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"256"})]})]})]}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Next, the authors designed several network architectures as follows:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Small network (64c5-64c5-128f)"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Two convolutional layers (64 filters each)."}),"\n",(0,i.jsx)(s.li,{children:"One fully connected layer (128 units)."}),"\n",(0,i.jsx)(s.li,{children:"Final Softmax layer."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Medium network (64c5-128c5-256c5-512f)"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Three convolutional layers (64, 128, 256 filters)."}),"\n",(0,i.jsx)(s.li,{children:"One fully connected layer (512 units)."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Large network (92c5-256c5-512c5-1024f)"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Three convolutional layers (92, 256, 512 filters)."}),"\n",(0,i.jsx)(s.li,{children:"One fully connected layer (1024 units)."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Convolution and pooling details"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["All convolution filters correspond to ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"5"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"5"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"5 \\times 5"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"5"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"5"})]})]})]})," regions of the input."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"2"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"2"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"2 \\times 2"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"2"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"2"})]})]})]})," max pooling is applied after the first two convolutional layers."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Other techniques"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Dropout is used in the fully connected layers."}),"\n",(0,i.jsx)(s.li,{children:"Based on the Caffe implementation, training details are in Appendix B.2."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The testing method applies the network to images of any size, computes feature maps, and selects the appropriate pooling method based on the dataset:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"STL-10 and CIFAR-10: Quadrant max pooling (4 values per feature map)."}),"\n",(0,i.jsxs)(s.li,{children:["Caltech-101 and Caltech-256: Three-level spatial pyramid pooling (",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mn,{children:"4"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mn,{children:"16"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mn,{children:"21"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1+4+16=21"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"+"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"4"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"+"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"16"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"21"})]})]})]})," values)."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Finally, a one-vs-all linear SVM is used for classification."}),"\n",(0,i.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(s.h3,{id:"comparison-with-other-methods",children:"Comparison with Other Methods"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"90%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"result",src:n(92859).Z+"",width:"1738",height:"440"})})})}),"\n",(0,i.jsxs)(s.p,{children:["The table above compares the performance of ",(0,i.jsx)(s.strong,{children:"Exemplar-CNN"})," with other unsupervised feature learning methods on different datasets."]}),"\n",(0,i.jsxs)(s.p,{children:["The experimental results show that ",(0,i.jsx)(s.strong,{children:"Exemplar-CNN"})," achieves the best performance on all datasets, and its performance on the STL-10 dataset significantly outperforms all previously reported results, even surpassing supervised methods!"]}),"\n",(0,i.jsx)(s.h3,{id:"ablation---number-of-proxy-categories",children:"Ablation - Number of Proxy Categories"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"70%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"num_classes",src:n(87862).Z+"",width:"996",height:"688"})})})}),"\n",(0,i.jsx)(s.p,{children:"The authors first tested the impact of the number of proxy categories on performance."}),"\n",(0,i.jsxs)(s.p,{children:["The experimental design adjusted the number of proxy categories ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"N"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]})," from 50 to 32,000 and observed changes in classification accuracy."]}),"\n",(0,i.jsx)(s.p,{children:"As shown in the figure above, classification accuracy improves as the number of proxy categories increases, peaking around 8000 categories before stabilizing or declining. The authors believe that having too many categories causes excessive similarity between samples, making it difficult to distinguish them, which reduces classification performance."}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsx)(s.p,{children:"However, later research suggests that this issue arises because features were not normalized. If the features are normalized, classification performance improves as the number of categories increases."})}),"\n",(0,i.jsx)(s.h3,{id:"ablation---number-of-samples",children:"Ablation - Number of Samples"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"70%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"num_samples",src:n(53706).Z+"",width:"916",height:"664"})})})}),"\n",(0,i.jsxs)(s.p,{children:["Next, the authors explored the effect of the number of samples per proxy category ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"K"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"K"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"K"})]})})]}),"."]}),"\n",(0,i.jsxs)(s.p,{children:["They adjusted ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"K"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"K"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"K"})]})})]})," from 1 to 300 and observed changes in classification accuracy."]}),"\n",(0,i.jsx)(s.p,{children:"The results show that accuracy increases as the number of samples per category increases, stabilizing around 100 samples. This suggests that 100 samples are sufficient to approximate the target, and further increasing the number of samples has little impact on optimization."}),"\n",(0,i.jsx)(s.h3,{id:"ablation---type-of-image-augmentation",children:"Ablation - Type of Image Augmentation"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"70%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"augmentation",src:n(73290).Z+"",width:"1044",height:"596"})})})}),"\n",(0,i.jsx)(s.p,{children:"The authors further explored the effect of different types of image augmentations on performance."}),"\n",(0,i.jsx)(s.p,{children:'The experimental design fixed the original "seed" samples and adjusted the types of transformations applied to the proxy data, including scaling, rotation, translation, color change, contrast change, and so on.'}),"\n",(0,i.jsx)(s.p,{children:"The results show that translation, color change, and contrast change significantly affect performance, while scaling and rotation have smaller impacts."}),"\n",(0,i.jsx)(s.h3,{id:"ablation---performance-in-image-classification",children:"Ablation - Performance in Image Classification"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"90%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"result",src:n(31917).Z+"",width:"1536",height:"662"})})})}),"\n",(0,i.jsxs)(s.p,{children:["The table above shows the performance of ",(0,i.jsx)(s.strong,{children:"Exemplar-CNN"})," across different datasets."]}),"\n",(0,i.jsx)(s.p,{children:"As the network size increases, classification accuracy generally improves, indicating that larger networks can better learn and leverage data features. The number of proxy categories used for training has a significant impact on the results, suggesting that the selection and generation of proxy data are key in unsupervised learning."}),"\n",(0,i.jsx)(s.h3,{id:"filter-visualization",children:"Filter Visualization"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"70%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"visualization",src:n(99159).Z+"",width:"876",height:"708"})})})}),"\n",(0,i.jsxs)(s.p,{children:["The figure above shows the filters learned by the first layer of the ",(0,i.jsx)(s.code,{children:"64c5-64c5-128f"})," network during training on proxy data from different datasets, arranged as follows:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Top"}),": Proxy data from the STL-10 dataset."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Middle"}),": Proxy data from the CIFAR-10 dataset."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Bottom"}),": Proxy data from the Caltech-101 dataset."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Analyzing the information presented in the figure, we can observe that the features learned by the model vary based on the dataset:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"The STL-10 dataset, with higher image resolution and diverse objects, tends to have filters that focus on learning fine-grained features (e.g., edges and textures)."}),"\n",(0,i.jsx)(s.li,{children:"The CIFAR-10 dataset, with lower image resolution, tends to have filters that focus on coarser features, such as color blocks or simple shapes."}),"\n",(0,i.jsx)(s.li,{children:"The Caltech-101 dataset, with well-aligned images, leads the filters to learn local features related to specific objects."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Thus, even though the proxy data comes from different datasets, the network is still able to learn effective features for classification or matching."}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(s.p,{children:"This paper presents an unsupervised feature learning method based on a discriminative objective that does not rely on object category labels. The authors generate proxy labels through data augmentation and define the invariances the network must learn via image transformations."}),"\n",(0,i.jsxs)(s.p,{children:["Experimental results demonstrate that ",(0,i.jsx)(s.strong,{children:"Exemplar-CNN"})," significantly improves feature learning for classification accuracy, and the discriminative objective offers advantages over previous unsupervised learning objectives."]}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsx)(s.p,{children:"Early comparative studies in contrastive learning often cite this paper. Although there are many areas for improvement, it undoubtedly provided valuable insights for subsequent research and is worth a look."})})]})}function d(e={}){let{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},92859:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img1-e4e2eaeaeb2241fd102b70ec74b320b9.jpg"},87862:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img2-a2ad9a654f40049c536e78fd3b7e2a2d.jpg"},15268:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img3-8702023834cfffc896c0b092e90a9f65.jpg"},53706:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img4-258a8c87c6fe83d351a0ca4f6e0dd79a.jpg"},73290:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img5-c928fa4a35eaa3a3e58b1ca3a2cba384.jpg"},99159:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img6-2305d699b35b9ebf8bf5952befddd7a8.jpg"},31917:function(e,s,n){n.d(s,{Z:function(){return a}});let a=n.p+"assets/images/img7-02e2f27ee982a53593db1c1b889eac6b.jpg"},50065:function(e,s,n){n.d(s,{Z:function(){return l},a:function(){return r}});var a=n(67294);let i={},t=a.createContext(i);function r(e){let s=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:s},e.children)}}}]);