"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2570],{1710:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var s=i(74848),t=i(28453);const a={},r="[20.04] Oscar",o={id:"multimodality/oscar/index",title:"[20.04] Oscar",description:"The Anchors of Oscar",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/2004-oscar/index.md",sourceDirName:"multimodality/2004-oscar",slug:"/multimodality/oscar/",permalink:"/en/papers/multimodality/oscar/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1723770126e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[19.09] UNITER",permalink:"/en/papers/multimodality/uniter/"},next:{title:"[20.04] Pixel-BERT",permalink:"/en/papers/multimodality/pixelbert/"}},l={},c=[{value:"The Anchors of Oscar",id:"the-anchors-of-oscar",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"Oscar Model Design",id:"oscar-model-design",level:3},{value:"Pre-training Objectives",id:"pre-training-objectives",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Parameter Efficiency Comparison",id:"parameter-efficiency-comparison",level:3},{value:"Model Performance Comparison",id:"model-performance-comparison",level:3},{value:"Qualitative Study",id:"qualitative-study",level:3},{value:"Ablation Study",id:"ablation-study",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"2004-oscar",children:"[20.04] Oscar"}),"\n",(0,s.jsx)(n.h2,{id:"the-anchors-of-oscar",children:"The Anchors of Oscar"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2004.06165",children:"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"})})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"In the field of Vision-Language (V+L) research, the issue of ambiguity has always been a significant hurdle, particularly when multiple objects overlap in image regions. Extracting precise visual region features becomes particularly challenging. For example, when an image contains both a cat and a dog intertwined, their overlapping region features might become ambiguous, posing a challenge for semantic alignment. How to find a clear and meaningful representation in this intertwined region and accurately align it with language is the problem that Oscar seeks to address."}),"\n",(0,s.jsx)(n.p,{children:"The operation mechanism of Oscar can be simply explained: suppose there is an image containing an apple and a banana, with some overlap between the two. In a conventional scenario, directly extracting features from this overlapping visual region would yield a \u201cmixed\u201d information, making subsequent semantic alignment difficult."}),"\n",(0,s.jsx)(n.p,{children:'Oscar introduces a clever strategy by using object labels as anchors for semantic alignment. Here, the labels are "apple" and "banana".'}),"\n",(0,s.jsx)(n.p,{children:"In this framework, each training sample is defined as a triplet, including a word sequence, a set of object labels, and a set of image region features. By leveraging this strategy, even when visual region features of objects are ambiguous, the authors can still perform effective semantic alignment using object labels, providing a relatively stable foundation for subsequent V+L tasks."}),"\n",(0,s.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,s.jsx)(n.p,{children:"The authors clearly state two core issues related to Vision-Language Pre-training (VLP) models:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Ambiguity"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": When image regions of two or more objects overlap, the extracted visual region features may become ambiguous and hard to distinguish."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Example"}),': In Figure 2(a), the objects "dog" and "couch" have significant overlapping regions, making their visual features hard to distinguish.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resulting Problem"}),": This ambiguity can make it difficult for models to establish accurate cross-modal alignment when dealing with complex image-text matching tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Lack of Grounding"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Issue"}),": VLP is a weakly supervised learning problem, lacking explicit correspondences between regions or objects in images and words or phrases in texts."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Background"}),': Even if objects like "dog" and "couch" appear in both the image and the corresponding text, the absence of explicit alignment annotations can hinder the model from learning the semantic correspondences between objects and text units.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,s.jsx)(n.h3,{id:"oscar-model-design",children:"Oscar Model Design"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Oscar Model Design",src:i(27481).A+"",width:"1224",height:"484"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Triplet Input Representation (w, q, v)"})}),"\n",(0,s.jsx)(n.p,{children:"The Oscar model represents each image-text pairing as a triplet (w, q, v)."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"w (Word Sequence Encoding)"})}),"\n",(0,s.jsx)(n.p,{children:"This is the word sequence encoding derived from the text input, where each word or phrase is converted into vector representations."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"q (Word Sequence Encoding of Object Labels)"})}),"\n",(0,s.jsx)(n.p,{children:"This is the word sequence encoding of the object labels identified in the image, typically generated by an image recognition model, possibly based on Faster R-CNN."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"v (Set of Region Vectors)"})}),"\n",(0,s.jsx)(n.p,{children:"This is the set of feature vectors for the identified regions in the image, which may include visual semantics and positional information."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"Pause here for a moment."}),(0,s.jsx)(n.p,{children:"Before moving forward, remember the concepts of the w, q, v input representations. These will appear frequently throughout the paper as combinations of wqv are explored and discussed."})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Object Labels as Alignment Anchors"})}),"\n",(0,s.jsx)(n.p,{children:"Oscar uses object labels (q) as anchors to simplify learning the alignment between images and text. Since important objects in images are often mentioned in the corresponding text descriptions, using q enhances the model's understanding and learning of the associations between images and text. During training, the model aligns visual objects (which may be ambiguously represented in the visual space) to clear and unique entity representations in the language space, improving cross-modal alignment learning."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Shared Semantic Space and Attention Mechanism"})}),"\n",(0,s.jsx)(n.p,{children:"Using the BERT model, the model can relatively easily identify the alignment between q and w in the text. Based on this, the model allocates more attention to image regions related to text semantics. When querying with words related to the semantics of q, the model assigns higher attention weights to these specific image regions."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Generation of v and q"})}),"\n",(0,s.jsx)(n.p,{children:"Given an image with K object regions, the Oscar model uses Faster R-CNN to extract visual semantics from each region as (v\u2019, z), where v\u2019 is a P-dimensional vector (region features) and z is an R-dimensional vector (region positions)."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"v is formed by concatenating v\u2019 and z into a position-sensitive region feature vector, further transformed through linear projection to match the same vector dimension as word encodings."}),"\n",(0,s.jsx)(n.li,{children:"Simultaneously, the word sequence encoding of object labels q is also derived from the image using the same Faster R-CNN."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"pre-training-objectives",children:"Pre-training Objectives"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Oscar Pre-training Objectives",src:i(47214).A+"",width:"1024",height:"198"})}),"\n",(0,s.jsx)(n.p,{children:"Oscar model inputs can be viewed from two different perspectives: here, x is the modality perspective distinguishing text and image representations; x\u2019 is the dictionary perspective distinguishing two different semantic spaces in which the inputs are expressed."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Dictionary Perspective: Masked Token Loss (MTL)"})}),"\n",(0,s.jsx)(n.p,{children:"Different dictionaries are used to identify the semantic spaces of different subsequences. Simply put, object labels and word tokens share the same linguistic semantic space, while image region features reside in the visual semantic space. During pre-training, the authors use the \u201cMasked Token Loss\u201d (MTL) method."}),"\n",(0,s.jsx)(n.p,{children:"In each training iteration, approximately 15% of input tokens in the sequence are randomly masked (i.e., replaced with a special [MASK] token). The training goal is to predict these masked tokens based on surrounding tokens and all image features."}),"\n",(0,s.jsx)(n.p,{children:"This process is very similar to BERT's masked language model, as it recovers masked words or labels from their surrounding context. Meanwhile, additional image information helps the learned word encodings find their place in the visual context."}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"Suppose a sentence: \u201cThis is a cute dog,\u201d with an accompanying image of a dog."}),(0,s.jsx)(n.p,{children:"During pre-training, the word \u201cdog\u201d might be masked, turning the sentence into \u201cThis is a cute [MASK].\u201d"})]}),"\n",(0,s.jsx)(n.p,{children:"The model's task is to use the unmasked words and the image of the dog to predict the true content of the [MASK], which is \u201cdog.\u201d This process leverages visual information to help the model accurately predict masked words, as the visual data provides additional contextual clues."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Modality Perspective: Contrastive Loss"})}),"\n",(0,s.jsx)(n.p,{children:"To express each input triplet, the authors group [h\u2019, [q, v]] to represent the image modality, while (w) is viewed as the language modality."}),"\n",(0,s.jsx)(n.p,{children:"Here\u2019s an interesting experimental process: with a 50% probability, the authors replace (q) with a randomly drawn different label sequence from the dataset to create a set of \u201ccontaminated\u201d image representations. Then, because the encoder output at the special marker [CLS] is a fused visual-language representation of (h\u2019, w), a fully connected layer is used on top of it to predict whether the pair contains the original image representation (i.e., (y = 1)) or any \u201ccontaminated\u201d representation (i.e., (y = 0))."}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"If the above description is confusing, perhaps you can imagine this process as a game:"}),(0,s.jsx)(n.p,{children:"Your friend gives you a picture and some text descriptions."}),(0,s.jsx)(n.p,{children:"But there's a catch: the text descriptions might be incorrect (e.g., a picture of a red apple described as \u201ca blue backpack\u201d). Your task is to determine whether these descriptions are true. In Oscar\u2019s context, the model plays a similar game, using mathematical and machine learning techniques to determine if the given text descriptions truly match the picture."})]}),"\n",(0,s.jsx)(n.p,{children:"Throughout the cross-modal pre-training process, the authors use object labels as proxies for the image to adjust the BERT word encoding space."}),"\n",(0,s.jsx)(n.p,{children:"Specifically, we want the learned text representations to be similar to the corresponding images (or detected object labels from the images) and contrast against \u201ccontaminated\u201d representations."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(n.h3,{id:"parameter-efficiency-comparison",children:"Parameter Efficiency Comparison"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Oscar Parameter Efficiency Comparison",src:i(25259).A+"",width:"1024",height:"233"})}),"\n",(0,s.jsx)(n.p,{children:"The authors first discuss Oscar's performance and efficiency on vision-language (V+L) tasks, comparing Oscar\u2019s performance and parameter efficiency with three different types of existing state-of-the-art (SoTA) models. Oscar shows relatively high parameter efficiency and excellent performance on most tasks compared to other large models."}),"\n",(0,s.jsx)(n.h3,{id:"model-performance-comparison",children:"Model Performance Comparison"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Oscar Model Performance Comparison",src:i(78520).A+"",width:"794",height:"1024"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Overall Performance of Oscar Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Oscar demonstrates strong performance on most V+L (Vision-Language) tasks."}),"\n",(0,s.jsx)(n.li,{children:"In 7 tasks, Oscar outperforms all existing VLP (Vision-Language Pre-training) methods."}),"\n",(0,s.jsx)(n.li,{children:"It achieves new state-of-the-art (SoTA) results in 6 out of these 7 tasks."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Comparison with Other Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Compared to Neural State Machine (NSM), Oscar may slightly underperform on the GQA task but can be enhanced by incorporating NSM\u2019s structural priors."}),"\n",(0,s.jsx)(n.li,{children:"Compared to the multi-task model 12-in-1, OscarB performs better on most tasks, except for lower results on NLVR2 Test-P."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Method and Training Strategies"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"On captioning tasks, using Self-Critical Sequence Training (SCST) to further fine-tune Oscar demonstrates the ability to improve sequence-level learning."}),"\n",(0,s.jsx)(n.li,{children:"Part 2 (e) might show Oscar\u2019s improvement over other methods in BLEU@4 and CIDEr metrics (over 2 points and 10 points improvement, respectively)."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Demonstration of Generalization Ability"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The NoCaps experiment requires models to use only the COCO Captioning training set. Oscar adheres to this requirement, showcasing its strong performance and generalization ability with limited training data."}),"\n",(0,s.jsx)(n.li,{children:"Part 2 (f) might compare Oscar variants with the previous SoTA method UpDown, highlighting Oscar's advantages in different scenarios (in-domain or out-of-domain)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Oscar significantly simplifies the learning of semantic alignment between images and text by using object labels as anchors, which is a key factor in its high efficiency and strong performance. In some tasks or scenarios, Oscar\u2019s approach and model structure can be further enhanced by integrating other powerful techniques or prior knowledge."}),"\n",(0,s.jsx)(n.p,{children:"While Oscar demonstrates strong performance on most tasks, there may be room for optimization or limitations in certain specific tasks or metrics, such as performance on NLVR2 Test-P."}),"\n",(0,s.jsx)(n.h3,{id:"qualitative-study",children:"Qualitative Study"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Oscar Qualitative Study",src:i(82837).A+"",width:"1024",height:"402"})}),"\n",(0,s.jsx)(n.p,{children:"This study uses t-SNE to visualize the learned semantic feature space of image-text pairs in the COCO test set on a 2D map. Through analysis, the authors present several key points:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Intra-Class Consistency"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Using object labels significantly shortens the distance between visual and textual representations of the same object."}),"\n",(0,s.jsx)(n.li,{children:'In the Oscar model, the visual and textual representations of a specific object (e.g., "person" or "zebra") are much closer compared to the baseline methods.'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inter-Class Differentiation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adding labels brings semantically related object classes closer together, although they remain distinguishable."}),"\n",(0,s.jsx)(n.li,{children:'In the baseline methods, classes (e.g., animals, furniture, and vehicles) exhibit some mixing, while the method with added labels can more accurately distinguish these classes (e.g., "person," "zebra," "sheep," "bird," "chair," "sofa," "bench," "bus," "train," "truck," "motorcycle," and "car").'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Importance of Object Labels"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Object labels play a crucial role in alignment learning, serving as anchors to connect and regularize cross-modal feature learning."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ablation-study",children:"Ablation Study"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Oscar Ablation Study",src:i(90762).A+"",width:"1024",height:"279"})}),"\n",(0,s.jsx)(n.p,{children:"Several key points can be observed from the above figure:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Learning curves with object label fine-tuning converge faster and better on all tasks compared to VLP methods without labels."}),"\n",(0,s.jsx)(n.li,{children:"For VQA (Visual Question Answering) and image retrieval tasks, using labels for training can achieve the final performance of the baseline method in only half the training time."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These findings indicate that Oscar, utilizing object labels, exhibits superior and more efficient performance on these vision tasks, achieving or surpassing the performance of label-free methods in a shorter training time."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Oscar Ablation Study 1",src:i(58503).A+"",width:"694",height:"312"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Using object labels indeed enhances model performance. This conclusion is drawn by comparing fully attentive and partially attentive models (w-v), showing that adding object labels benefits the model."}),"\n",(0,s.jsx)(n.li,{children:"Region features provide more information than object labels when representing images, as seen in the comparison of w-v (relationship between object regions and text) and v-q (relationship between object labels and questions)."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The Oscar model significantly improves performance across multiple downstream tasks by using object labels. Training with object labels can achieve or exceed the final performance of the baseline in a shorter training time. Object labels and region features play important roles in the model's attention mechanism interaction, and using different object label sets during pre-training also shows an impact on model performance."}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"In essence, this is also a method of introducing a knowledge graph."}),"\n",(0,s.jsx)(n.p,{children:"Because labels are human-provided, although they offer clear guidance, is this guidance always correct? Is it sufficient? Could it limit the model\u2019s potential?"}),"\n",(0,s.jsx)(n.p,{children:"The Oscar model relies on the accuracy and quality of object labels to a certain extent. If the object labels generated are not precise or diverse enough, the model might learn incorrect or overly narrow features, affecting the pre-training effect and downstream task performance. After all, human language has infinite possibilities, but the label content is limited. Using limited concepts to achieve unlimited expansion is inherently a very challenging task."}),"\n",(0,s.jsx)(n.p,{children:"Nevertheless, Oscar enriches the field of multimodal pre-training models and demonstrates an effective new approach to integrating vision and language. Through carefully designed pre-training strategies and experimental verification, this research provides a solid foundation for subsequent researchers to explore more innovative ideas and applications, continually advancing the prospects of vision and language integration technology."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},27481:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/oscar_1-6d8ac3da7dfa1389b376a2f2889266fd.jpg"},47214:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/oscar_2-058cacfdaff0d5c927853c6ec488078b.jpg"},25259:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/oscar_3-2205e4e05aad8273a3812da6253e3d18.jpg"},78520:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/oscar_4-66bb9016d93cdcbe96464938d6bfe6ad.jpg"},82837:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/oscar_5-56c0c67caec1512a9ca3df536184915d.jpg"},90762:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/oscar_6-a5f3331ab13a775c61053f8343c38a59.jpg"},58503:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/oscar_7-315f2c2cfccf706f891c7e2bdfad28ee.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(96540);const t={},a=s.createContext(t);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);