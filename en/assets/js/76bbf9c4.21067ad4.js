"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["90685"],{74446:function(e,t,n){n.r(t),n.d(t,{frontMatter:()=>r,default:()=>d,contentTitle:()=>o,assets:()=>l,toc:()=>h,metadata:()=>i});var i=JSON.parse('{"id":"feature-fusion/unet/index","title":"[15.05] U-Net","description":"The Dawn of Integration","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/feature-fusion/1505-unet/index.md","sourceDirName":"feature-fusion/1505-unet","slug":"/feature-fusion/unet/","permalink":"/en/papers/feature-fusion/unet/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739242156000,"frontMatter":{"title":"[15.05] U-Net","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"Feature Fusion (10)","permalink":"/en/papers/category/feature-fusion-10"},"next":{"title":"[16.03] Hourglass","permalink":"/en/papers/feature-fusion/hourglass/"}}'),s=n(85893),a=n(50065);let r={title:"[15.05] U-Net",authors:"Z. Yuan"},o=void 0,l={},h=[{value:"The Dawn of Integration",id:"the-dawn-of-integration",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Discussion",id:"discussion",level:2},{value:"ISBI Cell Tracking Challenge 2015",id:"isbi-cell-tracking-challenge-2015",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){let t={a:"a",admonition:"admonition",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{id:"the-dawn-of-integration",children:"The Dawn of Integration"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/1505.04597",children:(0,s.jsx)(t.strong,{children:"U-Net: Convolutional Networks for Biomedical Image Segmentation"})})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:"In the early days of VGG, there were still many unmet needs."}),"\n",(0,s.jsx)(t.p,{children:"Researchers found that traditional CNN architectures couldn't provide the fine-grained details necessary to address the challenges of biomedical image segmentation."}),"\n",(0,s.jsx)(t.p,{children:"Thus, this work was born, which has become a classic in the field of image segmentation."}),"\n",(0,s.jsx)(t.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,s.jsx)(t.p,{children:"In contrast to the image classification field, where everyone is content with ImageNet, biomedical image segmentation researchers were not as fortunate. In this field, the amount of available data for training is extremely limited, not enough to support the training requirements of deep learning."}),"\n",(0,s.jsx)(t.p,{children:"The solution to this problem wasn't very clear. One approach was to slice the training data into multiple small pieces to generate more training samples. However, this resulted in another issue: the loss of contextual information, which in turn reduced segmentation accuracy."}),"\n",(0,s.jsx)(t.p,{children:"Around this time, another study proposed the fully convolutional network (FCN) architecture, which provided some inspiration to the authors."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/1411.4038",children:(0,s.jsx)(t.strong,{children:"[14.11] Fully Convolutional Networks for Semantic Segmentation"})})}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"fcn arch",src:n(53836).Z+"",width:"1028",height:"516"})})})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Perhaps this architecture could be applied to biomedical image segmentation, solving the problem of losing contextual information."}),"\n",(0,s.jsx)(t.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,s.jsx)(t.p,{children:"Using the entire image indeed solved the problem of losing contextual information, but the issue of insufficient data remained."}),"\n",(0,s.jsx)(t.p,{children:"Thus, the authors proposed the U-Net architecture, as shown in the diagram below:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"U-Net arch",src:n(73419).Z+"",width:"1224",height:"824"})})})}),"\n",(0,s.jsx)(t.p,{children:"By reusing high-resolution feature maps, the accuracy of segmentation was improved while reducing the model's dependency on large amounts of data."}),"\n",(0,s.jsx)(t.p,{children:"At this point, you can temporarily ignore the numbers, as the authors did not use padding in the convolutional layers. Hence, with each convolution layer, the size of the feature maps decreases. This might distract someone seeing the architecture for the first time, preventing them from appreciating the structure as a whole."}),"\n",(0,s.jsx)(t.p,{children:"Let's cut the image in half and first look at the left side:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"60%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"U-Net arch left",src:n(44632).Z+"",width:"2809",height:"1911"})})})}),"\n",(0,s.jsx)(t.p,{children:"This is what we commonly refer to as the Backbone, which can be freely swapped for different architectures. If you like MobileNet, use MobileNet; if you prefer ResNet, use ResNet."}),"\n",(0,s.jsx)(t.p,{children:"A basic Backbone design has five downsampling layers, corresponding to the five output layers in the image above."}),"\n",(0,s.jsx)(t.p,{children:"Next, let's look at the right side:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"60%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"U-Net arch right",src:n(64381).Z+"",width:"2809",height:"1911"})})})}),"\n",(0,s.jsx)(t.p,{children:"This is the Neck, characterized by upsampling from the lowest layer. The method can be simple interpolation or more complex deconvolution; in this paper, the authors used deconvolution."}),"\n",(0,s.jsx)(t.p,{children:"After upsampling, we obtain higher-resolution feature maps, which are then fused with the feature map from the previous layer. The fusion method can either be concatenation or addition; the authors used concatenation in this paper."}),"\n",(0,s.jsx)(t.p,{children:"After this process, we obtain a segmentation result with the same size as the original image. The number of channels in the output controls whether it\u2019s a binary or multi-class segmentation. For binary segmentation, only one channel is needed; for multi-class segmentation, multiple channels are required."}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:"If you opt for addition instead of concatenation, it leads to another classic architecture: FPN."}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/en/papers/feature-fusion/fpn/",children:(0,s.jsx)(t.strong,{children:"[16.12] FPN: The Pyramid Structure"})})}),"\n"]})]}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:"Another popular terminology would refer to the Backbone as the Encoder and the Neck as the Decoder."}),(0,s.jsx)(t.p,{children:"This is because the task here is also an image-to-image transformation, conceptually similar to the AutoEncoder design."})]}),"\n",(0,s.jsx)(t.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(t.h3,{id:"isbi-cell-tracking-challenge-2015",children:"ISBI Cell Tracking Challenge 2015"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"isbi",src:n(72343).Z+"",width:"1224",height:"376"})}),"\n",(0,s.jsx)(t.p,{children:"The authors applied U-Net to the ISBI 2014 and 2015 Cell Tracking Challenges:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"In the PhC-U373 dataset, it achieved 92% IOU, significantly surpassing the second place at 83%."}),"\n",(0,s.jsx)(t.li,{children:"In the DIC-HeLa dataset, it achieved 77.5% IOU, again greatly outperforming the second place at 46%."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"These results demonstrate that U-Net performs exceptionally well in different types of microscopy image segmentation tasks and outperforms existing methods by a large margin."}),"\n",(0,s.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(t.p,{children:"The design of U-Net preserves high-resolution feature maps and integrates contextual information, improving segmentation accuracy while reducing data requirements. This architecture is simple, scalable, and applicable to various image segmentation tasks, including cell segmentation, organ segmentation, and lesion detection."}),"\n",(0,s.jsx)(t.p,{children:"Compared to FPN, the concatenation structure results in a higher number of parameters and computational load, which can be a concern when there are restrictions on model size. Each architecture has its strengths, and it\u2019s valuable to learn different designs and choose the one most suitable for the task at hand."})]})}function d(e={}){let{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},73419:function(e,t,n){n.d(t,{Z:()=>i});let i=n.p+"assets/images/img1-f922eb2739ff565ac098d06463c3918c.jpg"},72343:function(e,t,n){n.d(t,{Z:()=>i});let i=n.p+"assets/images/img2-cebbdc59f99024adc1816027a335cb64.jpg"},53836:function(e,t,n){n.d(t,{Z:()=>i});let i=n.p+"assets/images/img3-bcda4d0d5aed45e91d91f8aff2fea782.jpg"},44632:function(e,t,n){n.d(t,{Z:()=>i});let i=n.p+"assets/images/img4-9f953f1151f6c2839e0bfeec27afcb2c.jpg"},64381:function(e,t,n){n.d(t,{Z:()=>i});let i=n.p+"assets/images/img5-acd222eac735a66798d7c5df998668f1.jpg"},50065:function(e,t,n){n.d(t,{Z:()=>o,a:()=>r});var i=n(67294);let s={},a=i.createContext(s);function r(e){let t=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);