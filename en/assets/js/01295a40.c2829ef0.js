"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1631],{18118:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var t=i(74848),s=i(28453);const r={},a="[21.05] MLP-Mixer",o={id:"mlp-mixer/index",title:"[21.05] MLP-Mixer",description:"Less Loss is Gain",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/2105-mlp-mixer/index.md",sourceDirName:"2105-mlp-mixer",slug:"/mlp-mixer/",permalink:"/en/papers/mlp-mixer/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1722741699e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[21.04] EfficientNet-V2",permalink:"/en/papers/efficientnet-v2/"},next:{title:"[21.06] BEiT",permalink:"/en/papers/beit/"}},l={},c=[{value:"Less Loss is Gain",id:"less-loss-is-gain",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Discussion",id:"discussion",level:3},{value:"Performance on ImageNet",id:"performance-on-imagenet",level:3},{value:"Visualization Analysis",id:"visualization-analysis",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",del:"del",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"2105-mlp-mixer",children:"[21.05] MLP-Mixer"}),"\n",(0,t.jsx)(n.h2,{id:"less-loss-is-gain",children:"Less Loss is Gain"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2105.01601",children:(0,t.jsx)(n.strong,{children:"MLP-Mixer: An all-MLP Architecture for Vision"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"The following content is compiled by ChatGPT-4, and has been manually proofread, edited, and supplemented."})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"After Transformers were introduced into the field of computer vision, many studies began exploring ways to make the exchange of information between patches more efficient."}),"\n",(0,t.jsx)(n.p,{children:'Thus, a new term emerged: "Token-Mixer."'}),"\n",(0,t.jsx)(n.p,{children:"As the name implies, the purpose of a Token-Mixer is to mix information between different patches to enhance the performance of the model."}),"\n",(0,t.jsx)(n.p,{children:"The traditional self-attention mechanism is one such implementation of a Token-Mixer."}),"\n",(0,t.jsx)(n.p,{children:"However, due to the high computational complexity of the original attention mechanism, the academic community began searching for alternatives."}),"\n",(0,t.jsx)(n.p,{children:"MLP-Mixer is one of these alternatives."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Token-Mixer is a term coined in later papers and is not a proprietary term introduced in this paper."})}),"\n",(0,t.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,t.jsx)(n.p,{children:"The authors believe: the complexity of self-attention is too high, so let's just get rid of it!"}),"\n",(0,t.jsx)(n.p,{children:"But what should we do after getting rid of it?"}),"\n",(0,t.jsx)(n.p,{children:"Hey! Look, there's a simple fully connected layer by the roadside! Let's use it!"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.del,{children:"And then the paper ends here."})," (Not really!)"]}),"\n",(0,t.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,t.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MLP-Mixer Model Architecture",src:i(24959).A+"",width:"1456",height:"772"})}),"\n",(0,t.jsx)(n.p,{children:"In this paper, the authors propose using two stages of fully connected layers, or MLPs (Multi-Layer Perceptrons), to exchange information."}),"\n",(0,t.jsx)(n.p,{children:"The image above might be hard to understand, but don't worry, let's break it down step by step:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Input an image of size 3 x 224 x 224 and divide it into 16 x 16 patches, with a dimension assumed to be 512."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Dimension change: [B x 3 x 224 x 224] -> [B x 512 x 14 x 14]."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Convert it into Transformer input format."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Dimension change: [B x 512 x 14 x 14] -> [B x 512 x 196]."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"First stage: Exchange information between patches."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch.nn as nn\n\npatch_mixer = nn.Linear(196, 196)\n\n# Input dimension: x = [B x 512 x 196]\nx_mix_patch = patch_mixer(x)\n\n# skip connection\n x = x + x_mix_patch\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Second stage: Exchange information between channels."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch.nn as nn\n\nchannel_mixer = nn.Linear(512, 512)\n\n# Input dimension: x = [B x 512 x 196]\n# Reshape first -> [B x 196 x 512]\nx_mix_channel = x.permute(0, 2, 1)\nx_mix_channel = channel_mixer(x_mix_channel)\n\n# Reshape back\nx_mix_channel = x_mix_channel.permute(0, 2, 1)\n\n# skip connection\nx = x + x_mix_channel\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This completes one operation of the MLP-Mixer."}),"\n",(0,t.jsx)(n.p,{children:"You'll notice that in the above process, the second stage is similar to the original self-attention mechanism, which also involves an MLP operation."}),"\n",(0,t.jsx)(n.p,{children:"The difference lies in the first stage, where instead of calculating the patch self-attention map, a fully connected layer is used directly."}),"\n",(0,t.jsx)(n.h3,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(n.h3,{id:"performance-on-imagenet",children:"Performance on ImageNet"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MLP-Mixer on ImageNet 1",src:i(2477).A+"",width:"1444",height:"560"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MLP-Mixer on ImageNet 2",src:i(57584).A+"",width:"1224",height:"696"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'The "ImNet" and "ReaL" columns represent the results on the original ImageNet validation set and the cleaned ReaL label set, respectively.'}),"\n",(0,t.jsx)(n.li,{children:"Avg 5 indicates the average performance across five downstream tasks (ImageNet, CIFAR-10, CIFAR-100, Pets, Flowers)."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"When pre-trained on ImageNet-21k with additional regularization, MLP-Mixer achieves 84.15% top-1 accuracy on ImageNet. This is strong performance, but slightly below other models. Without regularization, Mixer tends to overfit, consistent with observations for ViT."}),"\n",(0,t.jsx)(n.p,{children:"When training MLP-Mixer from scratch on ImageNet, Mixer-B/16 scores 76.4% at resolution 224, similar to a standard ResNet50 but lower than other state-of-the-art CNN/hybrid models like BotNet (84.7%) and NFNet (86.5%)."}),"\n",(0,t.jsx)(n.p,{children:"As the size of the upstream dataset increases, MLP-Mixer's performance improves significantly."}),"\n",(0,t.jsx)(n.p,{children:"Mixer-H/14 achieves 87.94% top-1 accuracy on ImageNet, outperforming BiT-ResNet152x4 by 0.5%, but trailing ViT-H/14 by 0.5%. Mixer-H/14 runs 2.5 times faster than ViT-H/14 and nearly twice as fast as BiT."}),"\n",(0,t.jsx)(n.p,{children:"In terms of precision-computation trade-offs, Mixer is competitive with more traditional neural network architectures. There is a clear correlation between total pre-training cost and downstream accuracy across architecture categories."}),"\n",(0,t.jsx)(n.h3,{id:"visualization-analysis",children:"Visualization Analysis"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MLP-Mixer Visualization",src:i(86278).A+"",width:"1852",height:"606"})}),"\n",(0,t.jsx)(n.p,{children:"It is generally observed that the first layer of CNNs tends to learn Gabor-like detectors, which act on local regions of the image pixels."}),"\n",(0,t.jsx)(n.p,{children:"In contrast, MLP-Mixer allows global information exchange between tokens in the MLP, raising the question:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Does it process information in a similar way?"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"To answer this, the authors visualized features from different layers of MLP-Mixer, showing various levels of feature extraction."}),"\n",(0,t.jsx)(n.p,{children:"In the image above, from left to right, are the first, second, and third layers of MLP-Mixer. We can see:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Some learned features act on the entire image, while others act on smaller regions."}),"\n",(0,t.jsx)(n.li,{children:"Deeper layers seem to have less distinguishable structures."}),"\n",(0,t.jsx)(n.li,{children:"Similar to CNNs, there are many feature detectors with opposite phases."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Less loss is gain!"}),"\n",(0,t.jsx)(n.p,{children:"Compared to the original self-attention mechanism, MLP-Mixer significantly reduces computational complexity. Even if the performance is slightly worse, it still has a strong market appeal!"})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},24959:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/img1-78c8d6489cfcb34be173bdfb58f51d97.jpg"},57584:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/img2-4d3bfe5dfac398843b2a64bc8dd8b2a2.jpg"},2477:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/img3-08541f3cd9b07db2792604d1748364d9.jpg"},86278:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/img4-972e48efecf9e99daf8744617bcbe123.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(96540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);