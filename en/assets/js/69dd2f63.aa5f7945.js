"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["29866"],{37060:function(e,s,n){n.r(s),n.d(s,{frontMatter:()=>r,toc:()=>d,default:()=>h,metadata:()=>i,assets:()=>c,contentTitle:()=>l});var i=JSON.parse('{"id":"model-training-guide/cifar100_training_demo","title":"CIFAR-100","description":"Sleepless Nights of Overfitting","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/model-training-guide/cifar100_training_demo.md","sourceDirName":"model-training-guide","slug":"/model-training-guide/cifar100_training_demo","permalink":"/en/docs/model-training-guide/cifar100_training_demo","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1752716509000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Model Training Guide","permalink":"/en/docs/model-training-guide/"},"next":{"title":"Otter Style","permalink":"/en/docs/model-training-guide/otter-style"}}'),a=n(74848),t=n(84429);let r={},l="CIFAR-100",c={},d=[{value:"Sleepless Nights of Overfitting",id:"sleepless-nights-of-overfitting",level:2},{value:"Download This Project",id:"download-this-project",level:2},{value:"Build the Training Environment (Ubuntu 22.04/24.04)",id:"build-the-training-environment-ubuntu-22042404",level:2},{value:"Setting Up Docker Environment",id:"setting-up-docker-environment",level:3},{value:"Download and Build the Image",id:"download-and-build-the-image",level:3},{value:"Constructing the Dataset",id:"constructing-the-dataset",level:2},{value:"Writing the Dataset Class",id:"writing-the-dataset-class",level:2},{value:"First Model: Acc = 44.26%",id:"first-model-acc--4426",level:2},{value:"Key Configuration Explanation",id:"key-configuration-explanation",level:3},{value:"Adjusting Training Hyperparameters",id:"adjusting-training-hyperparameters",level:2},{value:"Common solutions include:",id:"common-solutions-include",level:3},{value:"Data Augmentation: Acc = 36.48%",id:"data-augmentation-acc--3648",level:2},{value:"Strong Regularization: Acc = 40.12%",id:"strong-regularization-acc--4012",level:2},{value:"Label Smoothing: Acc = 44.81%",id:"label-smoothing-acc--4481",level:2},{value:"Ultimately, Data Is Still Insufficient",id:"ultimately-data-is-still-insufficient",level:2},{value:"Pretrained Weights: Acc = 56.70%",id:"pretrained-weights-acc--5670",level:2},{value:"Margin Loss: Acc = 57.92%",id:"margin-loss-acc--5792",level:2},{value:"Why Margin?",id:"why-margin",level:3},{value:"What is Margin Loss?",id:"what-is-margin-loss",level:3},{value:"Relation to Geometric Space",id:"relation-to-geometric-space",level:3},{value:"Experimental Results",id:"experimental-results",level:3},{value:"Enlarged Input Images: Acc = 79.57%",id:"enlarged-input-images-acc--7957",level:2},{value:"Enlarged Model Capacity: Acc = 61.76%",id:"enlarged-model-capacity-acc--6176",level:2},{value:"Enlarged Input Image: Acc = 81.21%",id:"enlarged-input-image-acc--8121",level:2},{value:"Knowledge Distillation: Acc = 57.37%",id:"knowledge-distillation-acc--5737",level:2},{value:"Experimental Results",id:"experimental-results-1",level:3},{value:"Experiment and Results Summary",id:"experiment-and-results-summary",level:2},{value:"And More",id:"and-more",level:2}];function o(e){let s={a:"a",admonition:"admonition",annotation:"annotation",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",msubsup:"msubsup",msup:"msup",mtext:"mtext",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.header,{children:(0,a.jsx)(s.h1,{id:"cifar-100",children:"CIFAR-100"})}),"\n",(0,a.jsx)(s.h2,{id:"sleepless-nights-of-overfitting",children:"Sleepless Nights of Overfitting"}),"\n",(0,a.jsx)(s.p,{children:"This project provides a concise PyTorch training example using the CIFAR-100 image classification task, allowing beginners to quickly get started. It also includes flexible adjustment and extension sample code, making it easier for you to adapt to different experimental needs."}),"\n",(0,a.jsx)(s.p,{children:"Without further ado, let's get started."}),"\n",(0,a.jsx)(s.admonition,{type:"info",children:(0,a.jsxs)(s.p,{children:["The code for this project is available on GitHub: ",(0,a.jsx)(s.a,{href:"https://github.com/DocsaidLab/cifar100_training_demo",children:(0,a.jsx)(s.strong,{children:"cifar100_training_demo"})}),"."]})}),"\n",(0,a.jsx)(s.h2,{id:"download-this-project",children:"Download This Project"}),"\n",(0,a.jsx)(s.p,{children:"The initial preparation is complete. Please use the following command to get the code:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"git clone https://github.com/DocsaidLab/cifar100-training-demo.git\n"})}),"\n",(0,a.jsx)(s.h2,{id:"build-the-training-environment-ubuntu-22042404",children:"Build the Training Environment (Ubuntu 22.04/24.04)"}),"\n",(0,a.jsxs)(s.admonition,{type:"tip",children:[(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Why Use Docker?"})}),(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Consistency"}),': Ensures "it works on my machine, it works on yours."']}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"No Pollution"}),": All dependencies are packaged inside the image, so it won't mess up your existing Python/conda setup."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Easy Reproducibility"}),": If something goes wrong, ",(0,a.jsx)(s.code,{children:"docker rm"})," + ",(0,a.jsx)(s.code,{children:"docker run"})," resets the entire environment instantly.\n(If you are more familiar with venv/conda, you can set up the environment yourself; this project primarily uses Docker.)"]}),"\n"]})]}),"\n",(0,a.jsx)(s.h3,{id:"setting-up-docker-environment",children:"Setting Up Docker Environment"}),"\n",(0,a.jsx)(s.p,{children:"This section is detailed in our basic toolbox project. Please refer to:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.a,{href:"https://docsaid.org/en/docs/capybara/advance",children:(0,a.jsx)(s.strong,{children:"Docsaid Capybara #Advanced"})})}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"download-and-build-the-image",children:"Download and Build the Image"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"cd cifar100-training-demo\nbash docker/build.bash\n"})}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["Base image: ",(0,a.jsx)(s.code,{children:"nvcr.io/nvidia/pytorch:25.03-py3"})]}),"\n",(0,a.jsxs)(s.li,{children:["Version details: ",(0,a.jsx)(s.a,{href:"https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-25-03.html#rel-25-03",children:(0,a.jsx)(s.strong,{children:"PyTorch Release 25.03"})})]}),"\n",(0,a.jsx)(s.li,{children:"The first build downloads about ~20 GB and takes roughly 5\u201320 minutes, depending on your internet speed."}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"After completion, you can verify the image with:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"docker images | grep cifar100_train\n"})}),"\n",(0,a.jsx)(s.h2,{id:"constructing-the-dataset",children:"Constructing the Dataset"}),"\n",(0,a.jsxs)(s.p,{children:["In PyTorch, the CIFAR-100 dataset is already built into ",(0,a.jsx)(s.code,{children:"torchvision"}),", so we can use it directly:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"from torchvision.datasets import CIFAR100\n\ntrain_dataset = CIFAR100(root='data/', train=True, download=True)\ntest_dataset = CIFAR100(root='data/', train=False, download=True)\n"})}),"\n",(0,a.jsx)(s.p,{children:"But wait!"}),"\n",(0,a.jsx)(s.p,{children:"Since this is for practice, why not try downloading and constructing the dataset yourself? This way, you can better control the data processing flow."}),"\n",(0,a.jsx)(s.p,{children:"First, download the CIFAR-100 dataset from the official website and unzip it:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"wget https://www.cs.toronto.edu/\\~kriz/cifar-100-python.tar.gz\ntar xvf cifar-100-python.tar.gz\n"})}),"\n",(0,a.jsxs)(s.p,{children:["After running this, you will see a folder named ",(0,a.jsx)(s.code,{children:"cifar-100-python"})," in your working directory containing training and testing data."]}),"\n",(0,a.jsx)(s.p,{children:"The folder structure looks roughly like this:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-text",children:"cifar-100-python/\n\u251C\u2500\u2500 train\n\u251C\u2500\u2500 test\n\u251C\u2500\u2500 meta\n\u2514\u2500\u2500 file.txt~\n"})}),"\n",(0,a.jsxs)(s.p,{children:["These are not image files but Python pickle files packed for easy loading. Therefore, when using them, we need to load the data with the ",(0,a.jsx)(s.code,{children:"pickle"})," module."]}),"\n",(0,a.jsx)(s.h2,{id:"writing-the-dataset-class",children:"Writing the Dataset Class"}),"\n",(0,a.jsx)(s.p,{children:"Once you have the dataset, we need to write a PyTorch dataset class to read this data."}),"\n",(0,a.jsxs)(s.p,{children:["Here is a simple implementation of a ",(0,a.jsx)(s.code,{children:"CIFAR100DatasetSimple"})," class:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"import pickle\n\nimport capybara as cb\nimport numpy as np\n\nDIR = cb.get_curdir(__file__)\n\nclass CIFAR100DatasetSimple:\n\n    def __init__(\n        self,\n        root: str=None,\n        mode: str='train',\n        image_size: int=32,\n        return_tensor: bool=False,\n        image_aug_ratio: float=0.5,\n    ):\n\n        if mode not in ['train', 'test']:\n            raise ValueError(\"mode must be either 'train' or 'test'\")\n\n        if root is None:\n            self.root = DIR / 'cifar-100-python'\n        else:\n            self.root = root\n\n        self.image_size = image_size\n        self.return_tensor = return_tensor\n\n        # Load data file\n        with open(f'{self.root}/{mode}', 'rb') as f:\n            data = pickle.load(f, encoding='bytes')\n            self.images = data[b'data']\n            self.labels = data[b'fine_labels']\n            self.filenames = data[b'filenames']\n\n        # reshape: (N, 3, 32, 32)\n        self.images = self.images.reshape(-1, 3, 32, 32)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n        label = self.labels[idx]\n\n        img = np.transpose(img, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n        img = cb.imresize(img, size=self.image_size)\n\n        if self.return_tensor:\n            img = np.transpose(img, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n            img = img.astype(np.float32) / 255.  # Simple normalization to [0, 1]\n            label = np.array(label, dtype=np.int64)\n            return img, label\n\n        return img, label\n"})}),"\n",(0,a.jsx)(s.p,{children:"This class has several features:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:["You can specify the output image size (",(0,a.jsx)(s.code,{children:"image_size"}),"), default is 32."]}),"\n",(0,a.jsxs)(s.li,{children:["You can choose whether to convert images to PyTorch Tensor format (",(0,a.jsx)(s.code,{children:"return_tensor"}),")."]}),"\n",(0,a.jsxs)(s.li,{children:["You can specify the dataset mode (",(0,a.jsx)(s.code,{children:"mode"}),"), either training (",(0,a.jsx)(s.code,{children:"train"}),") or testing (",(0,a.jsx)(s.code,{children:"test"}),")."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"More complex features can be added later. For now, let's train the first baseline model."}),"\n",(0,a.jsx)(s.h2,{id:"first-model-acc--4426",children:"First Model: Acc = 44.26%"}),"\n",(0,a.jsxs)(s.p,{children:["You can find some preset configuration files in the ",(0,a.jsx)(s.code,{children:"config"})," folder. We use these config files to control the training process. The first config file we use is ",(0,a.jsx)(s.code,{children:"resnet18_baseline.yaml"}),", which uses the well-known ResNet-18 as the base model."]}),"\n",(0,a.jsx)(s.p,{children:"Before training, go back to the parent directory:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"cd ..\n"})}),"\n",(0,a.jsx)(s.p,{children:"Then you can start training with the following command:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"bash cifar100-training-demo/docker/train.bash resnet18_baseline\n"})}),"\n",(0,a.jsx)(s.p,{children:"Since this is the first model, let's take a close look at the parameter configuration."}),"\n",(0,a.jsx)(s.h3,{id:"key-configuration-explanation",children:"Key Configuration Explanation"}),"\n",(0,a.jsxs)(s.p,{children:["In ",(0,a.jsx)(s.code,{children:"config/resnet18_baseline.yaml"}),", the main configurations are as follows:"]}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Batch Size"}),": Set to 250, which evenly divides the 50,000 training samples, simplifying the training cycle."]}),"\n",(0,a.jsx)(s.admonition,{type:"tip",children:(0,a.jsx)(s.p,{children:"Generally, we choose batch sizes that are multiples of 32 to better utilize GPU computing resources. However, since we are working with a small dataset, we can ignore this constraint and directly use 250. This way, each epoch only requires 200 iterations (50,000 / 250 = 200)."})}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Image Size"}),": Set to 32, matching the original CIFAR-100 image size. Unless otherwise specified, this size will be used in subsequent experiments."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Model Configuration"})}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:"model:\n  name: CIFAR100ModelBaseline\n  backbone:\n    name: Backbone\n    options:\n      name: timm_resnet18\n      pretrained: False\n      features_only: True\n  head:\n    name: Baseline\n    options:\n      num_classes: 100\n"})}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["Uses ",(0,a.jsx)(s.code,{children:"timm_resnet18"})," without pretrained weights (",(0,a.jsx)(s.code,{children:"pretrained=False"}),"), making it easier to understand the model\u2019s learning process from scratch."]}),"\n",(0,a.jsxs)(s.li,{children:["The ",(0,a.jsx)(s.code,{children:"Baseline"})," head converts backbone outputs into predictions for 100 classes."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Training Epochs"}),": Set to 200. Multiple trials showed minimal improvement beyond 200 epochs."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Optimizer"}),": Uses ",(0,a.jsx)(s.code,{children:"AdamW"})," with a learning rate (",(0,a.jsx)(s.code,{children:"lr"}),") of 0.001, yielding relatively stable overall training performance."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Weight Decay"}),": Set to 0.0001; since the small model already has some inherent regularization, this value is moderately low."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.p,{children:"Ultimately, this model reached 44.26% test accuracy at epoch 186."}),"\n",(0,a.jsx)(s.p,{children:"However, the training accuracy already hit 100%, which is a classic sign of overfitting."}),"\n",(0,a.jsx)(s.h2,{id:"adjusting-training-hyperparameters",children:"Adjusting Training Hyperparameters"}),"\n",(0,a.jsx)(s.p,{children:"\u201COverfitting\u201D means the model memorizes the training data perfectly but fails to generalize to other data."}),"\n",(0,a.jsx)(s.p,{children:"This phenomenon is especially common on small datasets like CIFAR-100 because of many classes but few samples; the model easily memorizes details instead of learning general rules."}),"\n",(0,a.jsx)(s.h3,{id:"common-solutions-include",children:"Common solutions include:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Reducing Model Capacity"}),": Use a smaller model to lower the risk of overfitting."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Data Augmentation"}),": Random cropping, flipping, brightness adjustment, etc., so the model sees more diverse images and improves generalization."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Regularization"}),": Apply Dropout, Weight Decay, and other techniques to keep the model \u201Crestrained\u201D during learning."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Early Stopping"}),": Stop training early when validation accuracy stops improving to avoid overfitting."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Using Pretrained Models"}),": If allowed, fine-tune models pretrained on large datasets (e.g., ImageNet) instead of training from scratch."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Adjust Learning Rate and Batch Size"}),": Both overly high or low learning rates and batch sizes can destabilize training."]}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.p,{children:"We won\u2019t discuss Early Stopping here; we simply run all 200 epochs and report the best score."}),"\n",(0,a.jsx)(s.p,{children:"Data augmentation is a common technique, so let\u2019s try that next."}),"\n",(0,a.jsx)(s.h2,{id:"data-augmentation-acc--3648",children:"Data Augmentation: Acc = 36.48%"}),"\n",(0,a.jsx)(s.p,{children:"We try improving the model\u2019s generalization using data augmentation."}),"\n",(0,a.jsxs)(s.p,{children:["Here we introduce the ",(0,a.jsx)(s.code,{children:"albumentations"})," library and apply some basic augmentations:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"import albumentations as A\n\nclass DefaultImageAug:\n\n    def __init__(self, p=0.5):\n        self.aug = A.OneOf([\n            A.ShiftScaleRotate(),\n            A.CoarseDropout(),\n            A.ColorJitter(),\n            A.HorizontalFlip(),\n            A.VerticalFlip(),\n        ], p=p)\n\n    def __call__(self, img: np.ndarray):\n        img = self.aug(image=img)['image']\n        return img\n"})}),"\n",(0,a.jsx)(s.p,{children:"The selected augmentations include:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ShiftScaleRotate"}),": Randomly translate, scale, and rotate the image."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"CoarseDropout"}),": Randomly mask parts of the image to simulate missing data."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ColorJitter"}),": Randomly adjust brightness, contrast, and saturation."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"HorizontalFlip"}),": Random horizontal flip."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"VerticalFlip"}),": Random vertical flip."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Experience shows these augmentations effectively improve model generalization."}),"\n",(0,a.jsxs)(s.p,{children:["Then, we add this augmentation to ",(0,a.jsx)(s.code,{children:"config/resnet18_augment.yaml"}),":"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:"dataset:\n  train_options:\n    name: CIFAR100AugDataset\n    options:\n      mode: train\n      return_tensor: True\n      image_aug_ratio: 1.0\n  valid_options:\n    name: CIFAR100AugDataset\n    options:\n      mode: test\n      return_tensor: True\n"})}),"\n",(0,a.jsx)(s.p,{children:"However, the result was disappointing."}),"\n",(0,a.jsx)(s.p,{children:"The test accuracy dropped to only 36.48%, much lower than the previous 44.26%."}),"\n",(0,a.jsx)(s.p,{children:"This is because, for low-resolution 32\xd732 images like CIFAR-100, applying too strong augmentations at once (e.g., \xb145\xb0 rotation, large occlusions, or vertical flips) severely distorts the original semantics, preventing the model from stably learning basic features."}),"\n",(0,a.jsx)(s.h2,{id:"strong-regularization-acc--4012",children:"Strong Regularization: Acc = 40.12%"}),"\n",(0,a.jsx)(s.p,{children:"Next, we try improving generalization via regularization."}),"\n",(0,a.jsx)(s.p,{children:"Generally, CNNs already have some built-in regularization due to convolution\u2019s translation invariance and parameter sharing. Compared to Transformers, which tend to overfit early in training, CNNs usually don\u2019t require strong extra regularization."}),"\n",(0,a.jsx)(s.p,{children:"Still, we give it a try."}),"\n",(0,a.jsxs)(s.p,{children:["We increase ",(0,a.jsx)(s.code,{children:"weight_decay"})," to 0.1 to observe its effect on learning and generalization."]}),"\n",(0,a.jsxs)(s.p,{children:["In ",(0,a.jsx)(s.code,{children:"config/resnet18_baseline_wd01.yaml"}),", modify ",(0,a.jsx)(s.code,{children:"weight_decay"}),":"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:"optimizer:\n  name: AdamW\n  options:\n    lr: 0.001\n    betas: [0.9, 0.999]\n    weight_decay: 0.1\n    amsgrad: False\n"})}),"\n",(0,a.jsx)(s.p,{children:"As expected, test accuracy dropped to 40.12%, below the original 44.26%."}),"\n",(0,a.jsx)(s.p,{children:"This reflects a common phenomenon:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"For small datasets like CIFAR-100, overly strong regularization can suppress the model\u2019s ability to fit the training distribution sufficiently, causing it to converge prematurely before learning discriminative features, ultimately hurting generalization."}),"\n"]}),"\n",(0,a.jsx)(s.h2,{id:"label-smoothing-acc--4481",children:"Label Smoothing: Acc = 44.81%"}),"\n",(0,a.jsx)(s.p,{children:"Now we try Label Smoothing to improve generalization."}),"\n",(0,a.jsx)(s.p,{children:"Label Smoothing converts one-hot class labels into smoothed distributions to reduce overfitting."}),"\n",(0,a.jsxs)(s.p,{children:["We configure this in ",(0,a.jsx)(s.code,{children:"config/resnet18_baseline_lbsmooth.yaml"}),":"]}),"\n",(0,a.jsxs)(s.p,{children:["It\u2019s simple to use\u2014just add ",(0,a.jsx)(s.code,{children:"label_smoothing"})," to the loss function:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n"})}),"\n",(0,a.jsx)(s.p,{children:"Experiment results show the model reached 44.81% test accuracy at epoch 59, surpassing the previous 44.26%, and achieving this accuracy more than 100 epochs earlier."}),"\n",(0,a.jsx)(s.p,{children:"This shows Label Smoothing effectively reduces overfitting and improves generalization for this task."}),"\n",(0,a.jsx)(s.h2,{id:"ultimately-data-is-still-insufficient",children:"Ultimately, Data Is Still Insufficient"}),"\n",(0,a.jsx)(s.p,{children:"By this point in the experiments, we can draw a realistic conclusion:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Some problems cannot be solved by model design or hyperparameter tuning alone."})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Take CIFAR-100 as an example: although it has a decent number of samples, the resolution is low, semantic information is sparse, and each class has limited samples. These data characteristics make it difficult for the model to learn discriminative features with strong generalization ability."}),"\n",(0,a.jsxs)(s.p,{children:["From a practical perspective, the most direct solution is: ",(0,a.jsx)(s.strong,{children:"increase the data."})]}),"\n",(0,a.jsx)(s.p,{children:"However, data collection is often a costly endeavor."}),"\n",(0,a.jsxs)(s.p,{children:["In many application scenarios, data is hard to obtain, and labeling is time-consuming and labor-intensive. This has long been a core bottleneck for deep learning deployment. Therefore, a more common and pragmatic choice in practice is: ",(0,a.jsx)(s.strong,{children:"Transfer Learning"}),"."]}),"\n",(0,a.jsx)(s.p,{children:"Through transfer learning, we do not need to train a model from scratch. Instead, we leverage models pretrained on large-scale datasets (such as ImageNet) as backbones, then fine-tune them on the target task."}),"\n",(0,a.jsx)(s.p,{children:"This strategy has multiple advantages:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Faster Convergence"}),": Initial weights already contain semantic features, helping the model quickly find a learning direction."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Improved Performance"}),": Even with limited target data, the model can fully utilize general representations."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Reduced Overfitting"}),": Pretrained models provide a stable starting point, resulting in better generalization."]}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["Next, we will test using pretrained models provided by ",(0,a.jsx)(s.code,{children:"timm"})," to see how this works in practice."]}),"\n",(0,a.jsxs)(s.admonition,{type:"info",children:[(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"What is timm?"})}),(0,a.jsx)(s.p,{children:"This package is well-known in the deep learning community."}),(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.code,{children:"timm"})," is a PyTorch model library that offers a large collection of pretrained models and utilities, enabling users to quickly implement various deep learning tasks. It includes many state-of-the-art model architectures and training techniques, especially suitable for image classification, object detection, and related tasks."]}),(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.a,{href:"https://github.com/huggingface/pytorch-image-models",children:(0,a.jsx)(s.strong,{children:"Timm GitHub"})})}),"\n"]})]}),"\n",(0,a.jsx)(s.h2,{id:"pretrained-weights-acc--5670",children:"Pretrained Weights: Acc = 56.70%"}),"\n",(0,a.jsxs)(s.p,{children:["Continuing from the Baseline settings, we temporarily avoid using ",(0,a.jsx)(s.code,{children:"label_smoothing"})," or other regularization techniques, focusing solely on the backbone\u2019s pretrained weights."]}),"\n",(0,a.jsxs)(s.p,{children:["This time, we use the ",(0,a.jsx)(s.code,{children:"resnet18_pretrained.yaml"})," configuration file, where the main adjustment is in the backbone section by setting ",(0,a.jsx)(s.code,{children:"pretrained"})," to ",(0,a.jsx)(s.code,{children:"True"})," to enable ImageNet pretrained weights."]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:"model:\n  name: CIFAR100ModelBaseline\n  backbone:\n    name: Backbone\n    options:\n      name: timm_resnet18\n      pretrained: True\n      features_only: True\n  head:\n    name: Baseline\n    options:\n      num_classes: 100\n"})}),"\n",(0,a.jsxs)(s.p,{children:["At epoch 112, the model achieved 56.70% accuracy on the test set, an improvement of ",(0,a.jsx)(s.strong,{children:"12.44%"})," compared to the original 44.26%."]}),"\n",(0,a.jsx)(s.p,{children:"This is a significant effect and more effective than all previous hyperparameter tuning tricks!"}),"\n",(0,a.jsxs)(s.p,{children:['However, transfer learning is not a panacea. When the pretrained data and target task differ greatly, the model may fail to transfer effectively and even cause so-called "',(0,a.jsx)(s.strong,{children:"Negative Transfer"}),'." For example, applying an image pretrained model to natural language tasks yields almost no positive benefit.']}),"\n",(0,a.jsx)(s.p,{children:"In our case, CIFAR-100 is a standard image classification task closely related to ImageNet\u2019s context, so transfer learning performs quite well."}),"\n",(0,a.jsx)(s.h2,{id:"margin-loss-acc--5792",children:"Margin Loss: Acc = 57.92%"}),"\n",(0,a.jsx)(s.p,{children:"At this point, our strategy must shift."}),"\n",(0,a.jsxs)(s.p,{children:["If relying on the conventional cross-entropy loss no longer improves accuracy, we can try ",(0,a.jsx)(s.strong,{children:"actively increasing training difficulty"})," to force the model to learn more discriminative feature representations. This is exactly the problem Margin Loss addresses."]}),"\n",(0,a.jsx)(s.h3,{id:"why-margin",children:"Why Margin?"}),"\n",(0,a.jsxs)(s.p,{children:["In traditional classification, cross-entropy encourages the model to raise the logit score of the correct class but ",(0,a.jsx)(s.strong,{children:"does not enforce a sufficient margin (gap) between the correct and incorrect class scores"}),". In other words, as long as the correct class is highest, it doesn\u2019t matter by how much."]}),"\n",(0,a.jsx)(s.p,{children:"While sufficient for classification, this design often leads to ambiguous decision boundaries and unstable generalization when samples are close, data noisy, or classes similar."}),"\n",(0,a.jsx)(s.p,{children:"Margin Loss is designed to solve this:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Not only be correct, but be confidently correct."})}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"what-is-margin-loss",children:"What is Margin Loss?"}),"\n",(0,a.jsx)(s.p,{children:"The core idea of Margin Loss is:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"In logit or feature space, increase the distance between positive and negative samples while reducing intra-class variance."})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Common Margin Losses include:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Large Margin Softmax (L-Softmax)"})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"ArcFace / CosFace / SphereFace"})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Triplet Loss / Contrastive Loss"})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"These methods typically add an angular or magnitude margin before softmax, encouraging embeddings with clearer class boundaries. Below is a conceptual illustration of angular margin:"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"margin_loss",src:n(24467).A+"",width:"1224",height:"344"})}),"\n",(0,a.jsx)(s.p,{children:"The figure shows Margin Loss pulling features of the same class closer and pushing different classes farther apart, improving classification confidence and stability."}),"\n",(0,a.jsx)(s.h3,{id:"relation-to-geometric-space",children:"Relation to Geometric Space"}),"\n",(0,a.jsx)(s.p,{children:"When implementing these losses, features are often projected onto a unit hypersphere by applying L2 normalization, constraining embeddings to lie on a sphere of radius 1."}),"\n",(0,a.jsx)(s.p,{children:"Benefits include:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Removing feature length interference to focus on direction (angle)"})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Easier to control margin\u2019s effect on angle"})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Mathematically converting classification to an angular classification problem"})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Hence, many Margin-based methods apply margin on cosine similarity rather than raw logits."}),"\n",(0,a.jsx)(s.h3,{id:"experimental-results",children:"Experimental Results"}),"\n",(0,a.jsxs)(s.p,{children:["Using the pretrained ResNet-18 backbone, we add Margin Loss settings in ",(0,a.jsx)(s.code,{children:"config/resnet18_pretrained_arcface.yaml"}),"."]}),"\n",(0,a.jsxs)(s.p,{children:["We tested two implementations, ",(0,a.jsx)(s.code,{children:"ArcFace"})," and ",(0,a.jsx)(s.code,{children:"CosFace"}),", with different margin settings."]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"class ArcFace(nn.Module):\n\n    def __init__(self, s=64.0, m=0.5):\n        super(ArcFace, self).__init__()\n        self.s = s\n        self.margin = m\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.theta = math.cos(math.pi - m)\n        self.sinmm = math.sin(math.pi - m) * m\n        self.easy_margin = False\n\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n        index = torch.where(labels != -1)[0]\n        target_logit = logits[index, labels[index].view(-1)]\n        with torch.no_grad():\n            target_logit.arccos_()\n            logits.arccos_()\n            final_target_logit = target_logit + self.margin\n            logits[index, labels[index].view(-1)] = final_target_logit\n            logits.cos_()\n        logits = logits * self.s\n        return logits\n\n\nclass CosFace(nn.Module):\n\n    def __init__(self, s=64.0, m=0.40):\n        super(CosFace, self).__init__()\n        self.s = s\n        self.m = m\n\n    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n        index = torch.where(labels != -1)[0]\n        logits[index, labels[index].view(-1)] -= self.m\n        logits *= self.s\n        return logits\n"})}),"\n",(0,a.jsx)(s.p,{children:"After several experiments, we found little difference in performance between them, but ArcFace scored slightly higher."}),"\n",(0,a.jsx)(s.p,{children:"Therefore, we report ArcFace results: at epoch 199, the model achieved 57.92% accuracy on the test set. This is a 1.22% improvement over the standard Softmax loss."}),"\n",(0,a.jsx)(s.p,{children:"This shows Margin Loss has real value in enhancing the model\u2019s discriminative power, especially when classes are similar, effectively reducing overfitting and improving generalization."}),"\n",(0,a.jsx)(s.h2,{id:"enlarged-input-images-acc--7957",children:"Enlarged Input Images: Acc = 79.57%"}),"\n",(0,a.jsx)(s.p,{children:"Maintaining the Margin Loss settings, we next try enlarging the input image size to see if accuracy can be further improved."}),"\n",(0,a.jsxs)(s.p,{children:["In ",(0,a.jsx)(s.code,{children:"config/resnet18_pretrained_arcface_224x224.yaml"}),", we set ",(0,a.jsx)(s.code,{children:"image_size"})," to 224:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:"global_settings:\n  image_size: [224, 224]\n"})}),"\n",(0,a.jsxs)(s.p,{children:["With larger input images, the test set accuracy reached a peak of 79.57% at epoch 29, improving ",(0,a.jsx)(s.strong,{children:"21.65%"})," over the previous 57.92%."]}),"\n",(0,a.jsx)(s.p,{children:"This result is surprising:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsxs)(s.strong,{children:["Simply enlarging the original ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"32"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"32"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"32 \\times 32"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(s.span,{className:"mord",children:"32"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"32"})]})]})]})," images to ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"224"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"224"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"224 \\times 224"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(s.span,{className:"mord",children:"224"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"224"})]})]})]})," can greatly boost model performance?"]})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Reasons include:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Resolution Aligned with Pretraining Habits"})}),"\n",(0,a.jsxs)(s.p,{children:["ResNet-50 and most ImageNet models are pretrained on ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"224"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"224"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"224 \\times 224"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(s.span,{className:"mord",children:"224"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"224"})]})]})]})," inputs. Feeding ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"32"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"32"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"32 \\times 32"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(s.span,{className:"mord",children:"32"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"32"})]})]})]})," directly means convolution kernels \u201Csee through\u201D the entire image at once, compressing hierarchical features and making details indistinguishable. Enlarging resolution lets convolutions extract textures and local structures at appropriate receptive fields."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Spatial Sampling Points Greatly Increase"})}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mn,{children:"32"}),(0,a.jsx)(s.mn,{children:"2"})]})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"32^2"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8141em"}}),(0,a.jsx)(s.span,{className:"mord",children:"3"}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord",children:"2"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.8141em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"2"})})]})})})})})]})]})})]})," \u2192 ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mn,{children:"224"}),(0,a.jsx)(s.mn,{children:"2"})]})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"224^2"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8141em"}}),(0,a.jsx)(s.span,{className:"mord",children:"22"}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord",children:"4"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.8141em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"2"})})]})})})})})]})]})})]})," means a ",(0,a.jsx)(s.strong,{children:"49-fold"})," increase in pixels. Even after bilinear interpolation smoothing, the model captures more edges, textures, and color distributions, enhancing discriminative signals."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Avoid Early Signal Distortion and Aliasing"})}),"\n",(0,a.jsx)(s.p,{children:"At low resolution, object details are averaged out by multiple stride/pooling layers; enlarging images preserves key features before downsampling. Higher resolution reduces improper folding of high-frequency signals caused by convolution strides, maintaining feature stability."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.p,{children:"Although accuracy significantly improves, some issues remain."}),"\n",(0,a.jsx)(s.p,{children:"First, computation greatly increases: training time rises from about 3 minutes to roughly 2 hours (on RTX 4090)."}),"\n",(0,a.jsx)(s.p,{children:"Second, the model reaches about 80% accuracy within the first 30 epochs, indicating it learns most features early on. Subsequent gains are minor, suggesting the dataset information is largely exhausted, and further hundreds of epochs yield little new value."}),"\n",(0,a.jsx)(s.h2,{id:"enlarged-model-capacity-acc--6176",children:"Enlarged Model Capacity: Acc = 61.76%"}),"\n",(0,a.jsx)(s.p,{children:"What if we keep the input image size the same but increase the model capacity?"}),"\n",(0,a.jsx)(s.p,{children:"Generally, larger models can theoretically learn richer features but are also more prone to overfitting."}),"\n",(0,a.jsxs)(s.p,{children:["Since we introduced Margin Loss earlier, which should reduce overfitting risk, we can try increasing the model capacity. Here, we use the ",(0,a.jsx)(s.code,{children:"resnet50_pretrained_arcface.yaml"})," config, changing the backbone to ResNet-50 while keeping the input image size at 32\xd732."]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:"model:\n  name: CIFAR100ModelMargin\n  backbone:\n    name: Backbone\n    options:\n      name: timm_resnet50\n      pretrained: True\n      features_only: True\n  head:\n    name: MarginHead\n    options:\n      hid_dim: 512\n      num_classes: 100\n"})}),"\n",(0,a.jsx)(s.p,{children:"Training results show that at epoch 199, the model reached 61.76% test accuracy\u2014an improvement of 3.84% over the previous 57.92%\u2014at the cost of nearly doubling the number of parameters."}),"\n",(0,a.jsx)(s.p,{children:"This suggests that when input size cannot be increased, enlarging model capacity still effectively boosts performance, especially with Margin Loss helping the model better learn class boundaries."}),"\n",(0,a.jsx)(s.h2,{id:"enlarged-input-image-acc--8121",children:"Enlarged Input Image: Acc = 81.21%"}),"\n",(0,a.jsx)(s.p,{children:"Finally, we enlarged both the model and the input image size to see if further gains are possible."}),"\n",(0,a.jsxs)(s.p,{children:["In ",(0,a.jsx)(s.code,{children:"config/resnet50_pretrained_arcface_224x224.yaml"}),", we set ",(0,a.jsx)(s.code,{children:"image_size"})," to 224:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:"global_settings:\n  image_size: [224, 224]\n"})}),"\n",(0,a.jsx)(s.p,{children:"With larger input images, the model reached over 80% accuracy within the first five epochs, ultimately achieving 81.21% test accuracy at epoch 174."}),"\n",(0,a.jsx)(s.p,{children:"This is close to the ResNet-18 + 224\xd7224 result but with nearly double the parameters. Evidently, this dataset has reached a saturation point where increasing model capacity no longer brings significant performance gains."}),"\n",(0,a.jsx)(s.h2,{id:"knowledge-distillation-acc--5737",children:"Knowledge Distillation: Acc = 57.37%"}),"\n",(0,a.jsx)(s.p,{children:"Keeping ResNet-18 and input size at 32\xd732, to further improve performance, knowledge distillation (KD) can be applied to transfer the discriminative ability learned by a large teacher model trained at high resolution to a lightweight student model."}),"\n",(0,a.jsx)(s.p,{children:"The core idea is to use the teacher model\u2019s learned knowledge during training to guide the student model, improving its generalization and convergence efficiency."}),"\n",(0,a.jsx)(s.p,{children:"Unlike traditional supervised learning, KD not only relies on hard labels (ground truth) but also uses the teacher\u2019s output probability distributions (soft labels) as additional supervision. These soft labels encode relative relationships between classes, helping the student learn a more discriminative feature space."}),"\n",(0,a.jsx)(s.p,{children:"The distillation loss is defined as:"}),"\n",(0,a.jsx)(s.span,{className:"katex-display",children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{mathvariant:"script",children:"L"}),(0,a.jsx)(s.mtext,{children:"distill"})]}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"\u03B1"}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"}),(0,a.jsx)(s.mtext,{children:"\u2009"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{mathvariant:"script",children:"L"}),(0,a.jsx)(s.mtext,{children:"CE"})]}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{separator:"true",children:","}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"p"}),(0,a.jsx)(s.mi,{children:"s"})]}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"}),(0,a.jsx)(s.mo,{children:"+"}),(0,a.jsx)(s.mi,{children:"\u03B1"}),(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mi,{children:"T"}),(0,a.jsx)(s.mn,{children:"2"})]}),(0,a.jsx)(s.mo,{children:"\u22C5"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{mathvariant:"normal",children:"K"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"L"})]}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsxs)(s.msubsup,{children:[(0,a.jsx)(s.mi,{children:"p"}),(0,a.jsx)(s.mi,{children:"t"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mi,{children:"T"}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"})]})]}),(0,a.jsx)(s.mtext,{children:"\u2009"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsx)(s.mtext,{children:"\u2009"}),(0,a.jsxs)(s.msubsup,{children:[(0,a.jsx)(s.mi,{children:"p"}),(0,a.jsx)(s.mi,{children:"s"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mi,{children:"T"}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"})]})]}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\mathcal{L}_{\\text{distill}} = (1 - \\alpha)\\,\\mathcal{L}_{\\text{CE}}(y, p_s) + \\alpha T^2 \\cdot \\mathrm{KL}(p_t^{(T)} \\,||\\, p_s^{(T)})"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathcal",children:"L"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3361em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord text mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"distill"})})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord",children:"1"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03B1"}),(0,a.jsx)(s.span,{className:"mclose",children:")"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathcal",children:"L"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3283em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord text mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"CE"})})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,a.jsx)(s.span,{className:"mpunct",children:","}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1514em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"s"})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mclose",children:")"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"+"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8641em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03B1"}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"T"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.8641em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"2"})})]})})})})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22C5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1.2948em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord",children:(0,a.jsx)(s.span,{className:"mord mathrm",children:"KL"})}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"1.0448em"},children:[(0,a.jsxs)(s.span,{style:{top:"-2.4542em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"t"})})]}),(0,a.jsxs)(s.span,{style:{top:"-3.2198em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mopen mtight",children:"("}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.13889em"},children:"T"}),(0,a.jsx)(s.span,{className:"mclose mtight",children:")"})]})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.2458em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsx)(s.span,{className:"mord",children:"\u2223\u2223"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.938em"},children:[(0,a.jsxs)(s.span,{style:{top:"-2.453em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"s"})})]}),(0,a.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mopen mtight",children:"("}),(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.13889em"},children:"T"}),(0,a.jsx)(s.span,{className:"mclose mtight",children:")"})]})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.247em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mclose",children:")"})]})]})]})}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{mathvariant:"script",children:"L"}),(0,a.jsx)(s.mtext,{children:"CE"})]})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\mathcal{L}_{\\text{CE}}"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathcal",children:"L"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3283em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord text mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"CE"})})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]})]})})]}),": Cross-entropy loss between student predictions and true labels."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{mathvariant:"normal",children:"K"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"L"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\mathrm{KL}"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord",children:(0,a.jsx)(s.span,{className:"mord mathrm",children:"KL"})})]})})]}),": Kullback\u2013Leibler divergence between teacher and student probability distributions at temperature ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"T"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"T"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"T"})]})})]}),"."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"\u03B1"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\alpha"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03B1"})]})})]}),": Balancing factor between true labels and distillation signal (commonly 0.5\u20130.9)."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"T"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"T"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"T"})]})})]}),": Temperature parameter, which smooths logits to highlight inter-class differences."]}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.p,{children:"In this experiment, the teacher model is a ResNet-50 pretrained and trained on 224\xd7224 inputs, and the student model is ResNet-18 with the original CIFAR-100 size 32\xd732. The teacher is frozen during training, providing soft labels as auxiliary supervision only."}),"\n",(0,a.jsx)(s.p,{children:"Training architecture and flow:"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"60%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"teacher-student",src:n(75177).A+"",width:"2177",height:"1521"})})})}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsx)(s.li,{children:"Pretrain the teacher model and obtain its logit outputs."}),"\n",(0,a.jsx)(s.li,{children:"Apply softmax with temperature on both teacher and student logits to get soft labels."}),"\n",(0,a.jsx)(s.li,{children:"Train the student model using the KD loss."}),"\n",(0,a.jsx)(s.li,{children:"During deployment, only keep the student model; the teacher is not involved."}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"experimental-results-1",children:"Experimental Results"}),"\n",(0,a.jsxs)(s.p,{children:["In ",(0,a.jsx)(s.code,{children:"config/resnet18_pretrained_arcface_kd.yaml"}),", KD parameters are set. We load the pretrained ResNet-50 teacher model trained with 224\xd7224 inputs:"]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-yaml",children:'common:\n  batch_size: 250\n  image_size: [32, 32]\n  is_restore: True\n  restore_ind: "2025-05-26-00-49-22"\n  restore_ckpt: "epoch=177-step=35600.ckpt"\n  preview_batch: 1000\n'})}),"\n",(0,a.jsx)(s.p,{children:"Results show accuracy around 57.37%, similar to Margin Loss baseline."}),"\n",(0,a.jsx)(s.p,{children:"The teacher model was not as helpful as expected on this dataset."}),"\n",(0,a.jsx)(s.p,{children:"Possible reasons include:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Insufficient Student Capacity"}),": ResNet-18\u2019s representational space is much smaller than ResNet-50\u2019s, and mimicking the teacher\u2019s fine-grained decision boundaries may be too challenging."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Input Resolution Gap"}),": The teacher was trained on 224\xd7224 inputs, but the student only sees 32\xd732, making it hard for the student to capture teacher-learned features."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"There may be other factors, but these two are the main considerations."}),"\n",(0,a.jsx)(s.h2,{id:"experiment-and-results-summary",children:"Experiment and Results Summary"}),"\n",(0,a.jsx)(s.p,{children:"Below is a table summarizing each experiment\u2019s configuration and final accuracy on the CIFAR-100 test set:"}),"\n",(0,a.jsx)("div",{style:{whiteSpace:"nowrap",overflowX:"auto",fontSize:"1rem",lineHeight:"0.8",justifyContent:"center",display:"flex"},children:(0,a.jsxs)(s.table,{children:[(0,a.jsx)(s.thead,{children:(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.th,{children:"Config File"}),(0,a.jsx)(s.th,{children:"Accuracy"}),(0,a.jsx)(s.th,{children:"Description"})]})}),(0,a.jsxs)(s.tbody,{children:[(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_baseline.yaml"})}),(0,a.jsx)(s.td,{children:"44.26%"}),(0,a.jsx)(s.td,{children:"ResNet-18, no pretraining, AdamW (lr=0.001), WD=0.0001"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_augment.yaml"})}),(0,a.jsx)(s.td,{children:"36.48%"}),(0,a.jsx)(s.td,{children:"Added Albumentations data augmentation (rotation, dropout, flips)"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_baseline_wd01.yaml"})}),(0,a.jsx)(s.td,{children:"40.12%"}),(0,a.jsx)(s.td,{children:"ResNet-18, no pretraining, weight decay set to 0.1"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_baseline_lbsmooth.yaml"})}),(0,a.jsx)(s.td,{children:"44.81%"}),(0,a.jsx)(s.td,{children:"ResNet-18, no pretraining, label smoothing=0.1"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_pretrained.yaml"})}),(0,a.jsx)(s.td,{children:"56.70%"}),(0,a.jsxs)(s.td,{children:["ResNet-18, ",(0,a.jsx)(s.strong,{children:"with ImageNet pretrained weights"})]})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_pretrained_arcface.yaml"})}),(0,a.jsx)(s.td,{children:"57.92%"}),(0,a.jsx)(s.td,{children:"ResNet-18 pretrained + Margin Loss (ArcFace)"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_pretrained_arcface_224x224.yaml"})}),(0,a.jsx)(s.td,{children:"79.57%"}),(0,a.jsx)(s.td,{children:"ResNet-18 pretrained + Margin Loss, input enlarged to 224 \xd7 224"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet50_pretrained_arcface.yaml"})}),(0,a.jsx)(s.td,{children:"61.76%"}),(0,a.jsx)(s.td,{children:"ResNet-50 pretrained + Margin Loss, input size remains 32 \xd7 32"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet50_pretrained_arcface_224x224.yaml"})}),(0,a.jsx)(s.td,{children:"81.21%"}),(0,a.jsx)(s.td,{children:"ResNet-50 pretrained + Margin Loss, 224 \xd7 224 input"})]}),(0,a.jsxs)(s.tr,{children:[(0,a.jsx)(s.td,{children:(0,a.jsx)(s.code,{children:"resnet18_pretrained_arcface_kd.yaml"})}),(0,a.jsx)(s.td,{children:"57.37%"}),(0,a.jsx)(s.td,{children:"Knowledge Distillation (Teacher: ResNet-50 224 \xd7 224; Student: ResNet-18 32 \xd7 32)"})]})]})]})}),"\n",(0,a.jsx)(s.h2,{id:"and-more",children:"And More"}),"\n",(0,a.jsx)(s.p,{children:"So far, we focused on experiments based on ResNet-18 with fixed input size 32\xd732."}),"\n",(0,a.jsxs)(s.p,{children:["However, improving CIFAR-100 accuracy is not limited to these approaches. In fact, the ",(0,a.jsx)(s.a,{href:"https://paperswithcode.com/sota/image-classification-on-cifar-100",children:"Paper with Code"})," leaderboard shows top CIFAR-100 results exceeding 96%."]}),"\n",(0,a.jsx)(s.p,{children:"These models often combine several strategies such as:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Using large-scale Vision Transformer (ViT) architectures or custom CNNs"}),"\n",(0,a.jsx)(s.li,{children:"High-resolution inputs"}),"\n",(0,a.jsx)(s.li,{children:"Pretrained transfer learning and advanced augmentation techniques (e.g., RandAugment, MixUp, CutMix)"}),"\n",(0,a.jsx)(s.li,{children:"Longer training schedules with Cosine Annealing or One-Cycle learning rate policies"}),"\n",(0,a.jsx)(s.li,{children:"Modern regularization methods like Label Smoothing and Sharpness-Aware Minimization"}),"\n",(0,a.jsx)(s.li,{children:"Multi-model distillation and ensemble methods for final inference"}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"These techniques may not be feasible for every development scenario, especially in resource-constrained deployments. But one clear takeaway is:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Performance limits stem from the overall training strategy design, not just the model itself."})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"If you are also using CIFAR-100 for experiments, don\u2019t hesitate to try different architectures and strategy combinations."}),"\n",(0,a.jsx)(s.p,{children:"Finally, wishing you great success and joy on your deep learning journey!"})]})}function h(e={}){let{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(o,{...e})}):o(e)}},24467:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/margin_loss-b283f3b59c90cefaf835e28158d95e28.jpg"},75177:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/teacher_student-679db074dcb29b820f92030ae5b29e21.jpg"},84429:function(e,s,n){n.d(s,{R:()=>r,x:()=>l});var i=n(96540);let a={},t=i.createContext(a);function r(e){let s=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);