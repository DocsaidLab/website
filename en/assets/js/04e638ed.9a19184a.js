"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[6652],{94913:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>c,frontMatter:()=>a,metadata:()=>s,toc:()=>h});var i=t(74848),r=t(28453);const a={slug:"file-crawler-python-implementation",title:"Python Implementation of a Web File Downloader",authors:"Zephyr",image:"/en/img/2024/0923.webp",tags:["Python","File Crawler"],description:"Implement a simple web file downloader in Python."},l=void 0,s={permalink:"/en/blog/file-crawler-python-implementation",source:"@site/i18n/en/docusaurus-plugin-content-blog/2024/09-23-file-crawler/index.md",title:"Python Implementation of a Web File Downloader",description:"Implement a simple web file downloader in Python.",date:"2024-09-23T00:00:00.000Z",tags:[{inline:!0,label:"Python",permalink:"/en/blog/tags/python"},{inline:!0,label:"File Crawler",permalink:"/en/blog/tags/file-crawler"}],readingTime:1.775,hasTruncateMarker:!0,authors:[{name:"Zephyr",title:"Engineer",url:"https://github.com/zephyr-sh",imageURL:"https://github.com/zephyr-sh.png",key:"Zephyr",page:null}],frontMatter:{slug:"file-crawler-python-implementation",title:"Python Implementation of a Web File Downloader",authors:"Zephyr",image:"/en/img/2024/0923.webp",tags:["Python","File Crawler"],description:"Implement a simple web file downloader in Python."},unlisted:!1,nextItem:{title:"Automatically Count Articles in Docusaurus Sidebar",permalink:"/en/blog/customized-docusaurus-sidebars-auto-count"}},o={authorsImageUrls:[void 0]},h=[{value:"Install Required Packages",id:"install-required-packages",level:2},{value:"The Code",id:"the-code",level:2},{value:"Running the Script",id:"running-the-script",level:2}];function d(e){const n={code:"code",h2:"h2",hr:"hr",img:"img",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("figure",{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"title",src:t(12703).A+"",width:"1024",height:"1024"}),"\n",(0,i.jsx)("figcaption",{children:"Cover Image: Automatically generated by GPT-4 after reading this article"})]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"We came across a webpage containing hundreds of PDF file links."}),"\n",(0,i.jsx)(n.p,{children:"As engineers, if we were to download them manually, it would be highly inefficient, right?"}),"\n",(0,i.jsx)(n.p,{children:"So, what we need here is a small script that will help us download all the files."}),"\n",(0,i.jsx)(n.h2,{id:"install-required-packages",children:"Install Required Packages"}),"\n",(0,i.jsx)(n.p,{children:"First, you need to install the necessary packages. If you haven't installed them yet, you can do so using the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install requests beautifulsoup4 urllib3\n"})}),"\n",(0,i.jsx)(n.h2,{id:"the-code",children:"The Code"}),"\n",(0,i.jsx)(n.p,{children:"Without further ado, since the script is already written, let's dive straight into the code!"}),"\n",(0,i.jsx)(n.p,{children:"The parts highlighted are the ones you\u2019ll need to modify yourself. Adjust the script according to your needs."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'{13,16} title="file_crawler.py"',children:'import os\nfrom urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Simulating a browser\'s headers\nheaders = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"\n}\n\n# Web page URL\nurl = "put_your_url_here"\n\n# Target file format\ntarget_format = ".pdf"\n\n# Send an HTTP GET request with headers\nresponse = requests.get(url, headers=headers)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Use BeautifulSoup to parse the HTML\n    soup = BeautifulSoup(response.text, "html.parser")\n\n    # Find all <a> tags and filter those with href attributes matching the target format\n    target_links = []\n    for link in soup.find_all("a"):\n        href = link.get("href")\n        if href and href.endswith(target_format):  # Specify the file format you want to download\n            target_links.append(urljoin(url, href))\n\n    # Create a folder to save the files\n    os.makedirs("downloads", exist_ok=True)\n\n    # Download each file\n    for url in target_links:\n        file_name = url.split("/")[-1]  # Extract the filename from the URL\n        file_path = os.path.join("downloads", file_name)\n\n        # Send a request to download the file\n        response = requests.get(url, headers=headers)  # Add headers again\n        if response.status_code == 200:\n            with open(file_path, "wb") as f:\n                f.write(response.content)\n            print(f"Downloaded: {file_name}")\n        else:\n            print(f"Failed to download: {url}")\nelse:\n    print(f"Unable to access the webpage, status code: {response.status_code}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"running-the-script",children:"Running the Script"}),"\n",(0,i.jsx)(n.p,{children:"Once you\u2019re done, you can simply run the script to download all the files matching the target format."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python file_crawler.py\n"})})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},12703:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/0923-f0f893ffdaddcbc2c95fb74b47244394.webp"},28453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>s});var i=t(96540);const r={},a=i.createContext(r);function l(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);