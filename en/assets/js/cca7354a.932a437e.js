"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[6992],{19451:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var s=i(74848),t=i(28453);const r={},a="[21.04] EfficientNet-V2",o={id:"efficientnet-v2/index",title:"[21.04] EfficientNet-V2",description:"Deep convolutional creep",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/2104-efficientnet-v2/index.md",sourceDirName:"2104-efficientnet-v2",slug:"/efficientnet-v2/",permalink:"/en/papers/efficientnet-v2/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1722483445e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[21.02] PVT",permalink:"/en/papers/pvt/"},next:{title:"[21.05] MLP-Mixer",permalink:"/en/papers/mlp-mixer/"}},l={},c=[{value:"Deep convolutional creep",id:"deep-convolutional-creep",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Progressive Learning",id:"progressive-learning",level:3},{value:"Another Round of NAS",id:"another-round-of-nas",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Where to Place Fused-MBConv?",id:"where-to-place-fused-mbconv",level:3},{value:"Network Search Results",id:"network-search-results",level:3},{value:"Speeding Up Training",id:"speeding-up-training",level:3},{value:"Results on ImageNet",id:"results-on-imagenet",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"2104-efficientnet-v2",children:"[21.04] EfficientNet-V2"}),"\n",(0,s.jsx)(n.h2,{id:"deep-convolutional-creep",children:"Deep convolutional creep"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2104.00298",children:(0,s.jsx)(n.strong,{children:"EfficientNetV2: Smaller Models and Faster Training"})})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"The following content has been compiled by ChatGPT-4, and has been manually proofread, edited, and supplemented."})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"After proposing the first generation of EfficientNet, the authors felt there was still room for improvement."}),"\n",(0,s.jsx)(n.p,{children:"Thus, they introduced EfficientNetV2, a smaller and faster model while maintaining high efficiency."}),"\n",(0,s.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,s.jsx)(n.p,{children:"During the development of the original EfficientNet, the authors discovered several issues:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Using very large image sizes (e.g., 800x800) resulted in very slow training speeds."}),"\n",(0,s.jsx)(n.li,{children:"Placing depthwise convolutions in the early layers of the network led to slow training speeds."}),"\n",(0,s.jsx)(n.li,{children:"Following the traditional ResNet approach of uniformly expanding the number of channels at each stage did not yield optimal results."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Additionally, past research at Google showed that depthwise convolutions train significantly slower than regular convolutions."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2003.02838",children:(0,s.jsx)(n.strong,{children:"[20.03] Accelerator-aware Neural Network Design using AutoML"})})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"fused-conv",src:i(84937).A+"",width:"1224",height:"768"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This paper noted that depthwise convolutions, although they significantly reduce computational costs, fail to achieve hardware acceleration effectively."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Recognizing these problems, the authors sought to improve upon them."}),"\n",(0,s.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,s.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"efficientnet-v2-arch",src:i(11230).A+"",width:"1068",height:"944"})}),"\n",(0,s.jsx)(n.p,{children:"Depthwise convolutions have fewer parameters and FLOPs compared to regular convolutions, but they often fail to fully utilize modern accelerators."}),"\n",(0,s.jsxs)(n.p,{children:["Therefore, the authors proposed replacing the ",(0,s.jsx)(n.code,{children:"MBConv"})," modules used in EfficientNet-V1 with ",(0,s.jsx)(n.code,{children:"Fused-MBConv"})," modules."]}),"\n",(0,s.jsx)(n.p,{children:"Specifically, as shown in the figure, this involved removing depthwise convolutions and using standard convolutions for channel expansion instead."}),"\n",(0,s.jsx)(n.h3,{id:"progressive-learning",children:"Progressive Learning"}),"\n",(0,s.jsx)(n.p,{children:"While many previous works dynamically changed image sizes during training, they often resulted in reduced accuracy."}),"\n",(0,s.jsx)(n.p,{children:"The authors hypothesized that the accuracy drop stemmed from unbalanced regularization:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When training with different image sizes, the regularization strength should also be adjusted accordingly, rather than using fixed regularization as in previous work."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Indeed, larger models require stronger regularization to combat overfitting. For example, EfficientNet-B7 uses more dropout and stronger data augmentation than B0."}),"\n",(0,s.jsx)(n.p,{children:"The authors posited that even for the same network, smaller image sizes imply a smaller network capacity, necessitating weaker regularization. Conversely, larger image sizes increase computational load and network capacity, thus requiring stronger regularization to prevent overfitting."}),"\n",(0,s.jsx)(n.p,{children:"To test this hypothesis, the authors conducted the following experiment on ImageNet:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"progressive-resizing",src:i(59688).A+"",width:"1224",height:"252"})}),"\n",(0,s.jsx)(n.p,{children:"The table shows that for an input size of 128, less regularization yields better results, whereas for an input size of 300, more regularization is better."}),"\n",(0,s.jsx)(n.p,{children:"These results validate the authors' hypothesis, prompting them to adaptively adjust regularization and image size during training, improving upon previous progressive learning methods."}),"\n",(0,s.jsx)(n.h3,{id:"another-round-of-nas",children:"Another Round of NAS"}),"\n",(0,s.jsx)(n.p,{children:"After modifying the basic modules, the authors conducted a new round of Neural Architecture Search (NAS)."}),"\n",(0,s.jsx)(n.p,{children:"In this phase, they discovered that in EfficientNet-V1 training, increasing the image size significantly led to high memory consumption and slow training speeds. To address this, they modified the scaling rules, capping the maximum image size at 480 pixels."}),"\n",(0,s.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(n.h3,{id:"where-to-place-fused-mbconv",children:"Where to Place Fused-MBConv?"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"fused-conv-position",src:i(39564).A+"",width:"1518",height:"362"})}),"\n",(0,s.jsxs)(n.p,{children:["The placement of ",(0,s.jsx)(n.code,{children:"Fused-MBConv"})," is also crucial."]}),"\n",(0,s.jsxs)(n.p,{children:["The table shows that replacing all blocks with ",(0,s.jsx)(n.code,{children:"Fused-MBConv"})," significantly increases parameters and FLOPs while slowing down speed."]}),"\n",(0,s.jsx)(n.p,{children:"Placing it in stages 1-3 increases FLOPs but still enhances speed and model performance."}),"\n",(0,s.jsx)(n.h3,{id:"network-search-results",children:"Network Search Results"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"efficientnet-v2-arch",src:i(93911).A+"",width:"1124",height:"448"})}),"\n",(0,s.jsx)(n.p,{children:"Compared to the first generation, the second generation models differ in several ways:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Using ",(0,s.jsx)(n.code,{children:"Fused-MBConv"})," in the early layers."]}),"\n",(0,s.jsx)(n.li,{children:"EfficientNet-V2 prefers a smaller expansion ratio for MBConv."}),"\n",(0,s.jsx)(n.li,{children:"EfficientNet-V2 favors smaller 3x3 convolutional kernel sizes but adds more layers to compensate for the loss in the receptive field."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"speeding-up-training",children:"Speeding Up Training"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"efficientnet-v2-speedup",src:i(99450).A+"",width:"1104",height:"796"})}),"\n",(0,s.jsx)(n.p,{children:"The authors compared the training speeds of EfficientNet-V2 and other models."}),"\n",(0,s.jsx)(n.p,{children:"As shown in the table, EfficientNet-V2 trains much faster than other models."}),"\n",(0,s.jsx)(n.h3,{id:"results-on-imagenet",children:"Results on ImageNet"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"efficientnet-v2-imagenet",src:i(22053).A+"",width:"1200",height:"1012"})}),"\n",(0,s.jsx)(n.p,{children:"EfficientNetV2 models significantly outperform previous ConvNet and Transformer models in terms of speed and accuracy."}),"\n",(0,s.jsx)(n.p,{children:"Notably, EfficientNetV2-M achieves accuracy comparable to EfficientNet-B7 with the same computational resources and 11 times faster training speed."}),"\n",(0,s.jsx)(n.p,{children:"EfficientNetV2 models also outperform the latest RegNet and ResNeSt models in terms of accuracy and inference speed."}),"\n",(0,s.jsx)(n.p,{children:"While Vision Transformers perform well on ImageNet, EfficientNetV2 remains advantageous in accuracy and training efficiency with improved training methods and proper ConvNet design. Specifically, EfficientNetV2-L achieves 85.7% top-1 accuracy, surpassing the ViT-L/16 (21k) model pretrained on a larger dataset, ImageNet21k."}),"\n",(0,s.jsx)(n.p,{children:"In terms of inference speed, EfficientNetV2 models also excel, as training speed usually correlates with inference speed."}),"\n",(0,s.jsx)(n.p,{children:"Compared to ViT-L/16 (21k), EfficientNetV2-L (21k) improves top-1 accuracy by 1.5% (85.3% vs. 86.8%), uses 2.5 times fewer parameters, 3.6 times fewer FLOPs, and has 6-7 times faster training and inference speeds."}),"\n",(0,s.jsx)(n.p,{children:"The authors' key observations include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Scaling data size is more effective than merely scaling model size for high accuracy. When top-1 accuracy exceeds 85%, further improving accuracy by scaling model size becomes challenging. However, additional pretraining on ImageNet21k can significantly enhance accuracy."}),"\n",(0,s.jsx)(n.li,{children:"Pretraining on ImageNet21k is highly efficient. Despite the dataset being 10 times larger, EfficientNetV2's pretraining completes in two days using 32 TPU cores, significantly faster than ViT's several weeks."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Future research suggests using the public ImageNet21k dataset for large-scale model pretraining."}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"For engineers frequently involved in model development, this paper offers valuable insights:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Using ",(0,s.jsx)(n.code,{children:"Fused-MBConv"})," in the first three stages of convolutional network architectures can enhance model performance."]}),"\n",(0,s.jsx)(n.li,{children:"Adapting regularization strength according to changes in image size during training can improve model accuracy."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These methods can help train models more efficiently and solve practical problems in the field."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},11230:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/img1-6e5cae525f51f9c61dfc1e43474f759b.jpg"},84937:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/img2-0cd2555797065bd5a767110dd09fb9e4.jpg"},39564:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/img3-757fce95353a74edb5e3674fc4300c25.jpg"},93911:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/img4-2d65c501f56893b0a5da5078a16fa846.jpg"},99450:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/img5-4c5de7fd00ada2ef06bc1a71e71afb1c.jpg"},22053:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/img6-72a79d306e6a27b6df238c5ff4be7228.jpg"},59688:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/img7-1d36d1bf92e6040d7a1a2c63d26eec1a.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);