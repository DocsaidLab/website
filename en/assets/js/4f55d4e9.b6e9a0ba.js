"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5469],{3235:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var t=i(74848),s=i(28453);const a={},r="[20.06] ERNIE-ViL",o={id:"multimodality/ernie-vil/index",title:"[20.06] ERNIE-ViL",description:"The Double-Edged Sword of Knowledge",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/2006-ernie-vil/index.md",sourceDirName:"multimodality/2006-ernie-vil",slug:"/multimodality/ernie-vil/",permalink:"/en/papers/multimodality/ernie-vil/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1726039819e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[20.04] Pixel-BERT",permalink:"/en/papers/multimodality/pixelbert/"},next:{title:"[20.06] VILLA",permalink:"/en/papers/multimodality/villa/"}},l={},d=[{value:"The Double-Edged Sword of Knowledge",id:"the-double-edged-sword-of-knowledge",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"ERNIE-ViL Model Design",id:"ernie-vil-model-design",level:3},{value:"Dual-Stream Cross-Modal Network",id:"dual-stream-cross-modal-network",level:3},{value:"Scene Graph Prediction Tasks",id:"scene-graph-prediction-tasks",level:3},{value:"Semantic Alignment",id:"semantic-alignment",level:3},{value:"Encoding Methods",id:"encoding-methods",level:3},{value:"Pre-Training Tasks",id:"pre-training-tasks",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Advantage of Out-of-Domain Training Data",id:"advantage-of-out-of-domain-training-data",level:3},{value:"Importance of Scene Graph Prediction",id:"importance-of-scene-graph-prediction",level:3},{value:"Cloze Tests",id:"cloze-tests",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"2006-ernie-vil",children:"[20.06] ERNIE-ViL"})}),"\n",(0,t.jsx)(n.h2,{id:"the-double-edged-sword-of-knowledge",children:"The Double-Edged Sword of Knowledge"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2006.16934",children:"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"You might have heard various stories about BERT and its successors, and you could probably point out a few differences. For instance:"}),"\n",(0,t.jsx)(n.p,{children:"ERNIE models integrate rich prior knowledge, such as entity knowledge graphs, during pre-training, resulting in better semantic understanding. Additionally, ERNIE employs phrase-level masking. Besides the common Masked Language Model (MLM) task, ERNIE incorporates other pre-training tasks to enhance the model's representation capabilities."}),"\n",(0,t.jsx)(n.p,{children:"Since ERNIE optimizes BERT, it's logical for researchers to apply this optimization approach to other domains, which is indeed what they have done."}),"\n",(0,t.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,t.jsx)(n.p,{children:"The authors focus on the issues of vision-language pre-training models, particularly the shortcomings of current models in detailed semantic alignment."}),"\n",(0,t.jsx)(n.p,{children:"Current models often fail to distinguish between common words and those that describe detailed semantics, such as objects, attributes, and relationships between objects. This limitation hampers these models' ability to effectively represent and capture fine-grained semantics in real-world scenes."}),"\n",(0,t.jsx)(n.p,{children:"The key points defined and addressed by the authors are:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Shortcomings of Current Vision-Language Pre-Training Models"})}),"\n",(0,t.jsx)(n.p,{children:"Existing models often rely on random masking and subword prediction methods without effectively distinguishing between common words and those describing detailed semantics like objects, attributes, and relationships."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Importance of Detailed Semantic Alignment"})}),"\n",(0,t.jsx)(n.p,{children:"Current approaches often overlook the importance of constructing detailed semantic alignments across visual and linguistic constructs, which means models may fail to capture and represent fine semantic differences in real-world scenarios."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Data Source Challenges for Vision-Language Pre-Training"})}),"\n",(0,t.jsx)(n.p,{children:"Unlike textual pre-training models, vision-language models require high-quality, well-aligned image-text data, which are usually hard to obtain."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,t.jsx)(n.h3,{id:"ernie-vil-model-design",children:"ERNIE-ViL Model Design"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ERNIE-ViL Model Architecture",src:i(88085).A+"",width:"1680",height:"776"})}),"\n",(0,t.jsx)(n.h3,{id:"dual-stream-cross-modal-network",children:"Dual-Stream Cross-Modal Network"}),"\n",(0,t.jsx)(n.p,{children:"The dual-stream cross-modal Transformer structure ensures that information from both modalities can be effectively combined, providing a comprehensive, unified vision-language representation."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Dual-Stream Structure"})}),"\n",(0,t.jsx)(n.p,{children:'"Dual-stream" means there are two separate data streams or pathways. In the context of ERNIE-ViL, these two streams are text (or language) and images. Each modality has its own Transformer structure, allowing the model to focus on specific features of each modality instead of mixing them together.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Processing Text and Image Data"})}),"\n",(0,t.jsx)(n.p,{children:"Given the significant differences in the nature of visual and linguistic data (e.g., text is sequential, while images are two-dimensional), processing them separately allows the model to focus on the unique properties of each modality and use specially designed methods to parse and learn from these data."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Cross-Modal Transformer Blocks"})}),"\n",(0,t.jsx)(n.p,{children:"The purpose of these blocks is to facilitate interaction and alignment between the visual and linguistic data. Once text and images are processed by their respective Transformer structures, the cross-modal blocks work to fuse the information from both modalities using attention mechanisms and other specific strategies to find the correlations and contexts between text and images."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'The main goal of the dual-stream structure is to ensure that the model understands the relationship between the text and the image. For example, if the text says "red ball," the model should be able to identify the red spherical object in the image.'}),"\n",(0,t.jsx)(n.h3,{id:"scene-graph-prediction-tasks",children:"Scene Graph Prediction Tasks"}),"\n",(0,t.jsx)(n.p,{children:"A scene graph itself is not a trainable model but a data structure or representation used to describe the objects in an image, the relationships between objects, and the attributes of objects."}),"\n",(0,t.jsx)(n.p,{children:'A scene graph is a visual representation that describes the presence of objects in an image, their relationships, and specific attributes. For instance, for an image with "a red apple on a table," the scene graph would include: the object "apple," the object "table," the attribute "red" for the apple, and the relationship "on" between the apple and the table.'}),"\n",(0,t.jsx)(n.p,{children:"Three major prediction tasks:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Prediction"}),': The goal here is to predict or identify specific objects mentioned in the text. For example, in the sentence "The red apple is on the table," it should identify "apple" and "table" as the main objects.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Attribute Prediction"}),': This focuses on the specific characteristics or descriptions of the objects. In the example above, the attribute of the object "apple" is "red." The goal is to identify and predict these attributes.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Relationship Prediction"}),': This aims to identify the relationships between objects. In our example, the relationship between the apple and the table is "on."']}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By training ERNIE-ViL on these three prediction tasks, the model learns to deeply model the semantics between the visual (image) and linguistic (text description) modalities. This means the model not only identifies objects and their descriptions in the image but also understands the relationships between objects, providing richer and deeper vision-language representations."}),"\n",(0,t.jsx)(n.h3,{id:"semantic-alignment",children:"Semantic Alignment"}),"\n",(0,t.jsx)(n.p,{children:"A core objective of ERNIE-ViL is to ensure deep semantic alignment between the visual (image) and linguistic (text) modalities. Semantic alignment can be understood as the model's deep understanding of the relationship between objects in an image and their semantic descriptions in the text."}),"\n",(0,t.jsx)(n.p,{children:'For example, if an image shows a cat on a car, the model should not only recognize the "cat" and "car" in the image but also understand the relationship "on" between them. Such semantic understanding ensures that the model can make the correct connections between the description and the image.'}),"\n",(0,t.jsx)(n.p,{children:"The object, attribute, and relationship prediction tasks play a crucial role in this process. These tasks require the model to not only identify objects and their attributes in the image but also understand the relationships between objects, helping to establish stronger cross-modal understanding."}),"\n",(0,t.jsx)(n.h3,{id:"encoding-methods",children:"Encoding Methods"}),"\n",(0,t.jsx)(n.p,{children:"Encoding is a technique in machine learning used to transform high-dimensional input data into lower-dimensional forms, which helps the model understand and process the data better. ERNIE-ViL uses advanced encoding techniques to handle its input text and image data:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Sentence Encoding"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'The WordPiece method is used to decompose sentences. This method breaks down words into smaller units or fragments. For example, "playing" can be decomposed into "play" and "ing."'}),"\n",(0,t.jsx)(n.li,{children:"Each generated subword is encoded based on a combination of various information sources, including: the original word encoding (based on word semantics), segment encoding (distinguishing different sentences or paragraphs), and sequence position encoding (determining the position of the word in the sentence)."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Image Encoding"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'A pre-trained object detector is used to identify major objects and features in the image. For instance, it can recognize "cat" or "car" in an image.'}),"\n",(0,t.jsx)(n.li,{children:'For each identified object or region, the model also encodes its positional information in the image. This helps the model understand the relative positions and relationships between objects, such as the "cat" being "on" the "car."'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"pre-training-tasks",children:"Pre-Training Tasks"}),"\n",(0,t.jsx)(n.p,{children:"In deep learning, pre-training tasks are a common strategy designed to train models in advance using large amounts of unlabelled data, enabling them to generalize better on subsequent specific tasks. ERNIE-ViL proposes a series of pre-training tasks tailored for vision-language models, including object prediction, attribute prediction, relationship prediction, and masked language modeling. Below we delve into the characteristics and significance of each pre-training task:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Object Prediction"})}),"\n",(0,t.jsx)(n.p,{children:"Objects are the core elements of a visual scene, crucial for understanding the scene. Imagine a picture with just an apple and a table; if the apple is masked, the main element of the picture is hidden, making it difficult for the model to understand. In the object prediction pre-training task, some objects are masked in this way, requiring the model to predict based on other visible visual and textual information. This forces the model to learn to establish connections between images and text and improve its overall understanding of the scene."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Attribute Prediction"})}),"\n",(0,t.jsx)(n.p,{children:'Besides basic concepts, objects also have many related attributes, such as color, size, and shape. For instance, "red" in "red apple" is an attribute of the apple. The attribute prediction task requires the model to predict these attributes of masked objects, enabling it to describe and understand objects in images more precisely, rather than just basic classification.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Relationship Prediction"})}),"\n",(0,t.jsx)(n.p,{children:'Relationships between objects provide more contextual information. For example, in "the apple is on the table," "on" describes the relative position between the apple and the table. In the relationship prediction task, the model learns how to identify these relationships in the image and correctly map them to language descriptions, providing a deeper understanding of the visual scene.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Masked Language Modeling (MLM)"})}),"\n",(0,t.jsx)(n.p,{children:'MLM is a textual pre-training task. For example, in the sentence "The apple is red," the word "red" is masked, and the model is asked to fill in this missing part. ERNIE-ViL uses this strategy to learn syntactic and semantic information from text, enhancing its language processing capabilities for subsequent tasks.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Masked Region Modeling and Image-Text Matching"})}),"\n",(0,t.jsx)(n.p,{children:'These tasks focus on the image part, which we previously discussed as ITM (Image-Text Matching) and MRM (Masked Region Modeling). For example, an image may contain a "red apple" and a "wooden table." The model might mask the apple part and then try to predict the masked part based on the table and the related text description. These pre-training tasks enhance the model\'s understanding of single modalities and strengthen cross-modal connections, helping the model better integrate visual and linguistic information in real-world scenarios.'}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(n.p,{children:"ERNIE-ViL's experimental results demonstrate its outstanding performance across various vision-language tasks, particularly when compared to other state-of-the-art cross-modal pre-training models. Here are the authors' main observations and discussions:"}),"\n",(0,t.jsx)(n.h3,{id:"advantage-of-out-of-domain-training-data",children:"Advantage of Out-of-Domain Training Data"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ERNIE-ViL Performance on Different Datasets",src:i(20746).A+"",width:"1024",height:"656"})}),"\n",(0,t.jsx)(n.p,{children:"Based on the data from Table 1, the ERNIE-ViL model shows excellent performance across multiple vision-language tasks, especially when pre-trained on large out-of-domain datasets like CC and SBU."}),"\n",(0,t.jsx)(n.p,{children:"Here are some key highlights:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Compared to other methods pre-trained on the same out-of-domain datasets, ERNIE-ViL achieves the best performance across five major areas."}),"\n",(0,t.jsx)(n.li,{children:"In vision reasoning, ERNIE-ViL-large shows a 6.60% improvement over VLBERT-large on the VCR task and a 1.74% improvement on the VQA task."}),"\n",(0,t.jsx)(n.li,{children:"For visual grounding tasks, ERNIE-ViL-large achieves a 2.40% improvement on the RefCOCO+ testA and testB sections compared to VLBERT-large."}),"\n",(0,t.jsx)(n.li,{children:"In cross-modal retrieval, ERNIE-ViLbase shows a 2.94% improvement in image retrieval and a 0.50% improvement in text retrieval tasks, surpassing Unicoder-VL-base."}),"\n",(0,t.jsx)(n.li,{children:"When pre-trained on all out-of-domain and in-domain datasets, ERNIE-ViL-large still outperforms other state-of-the-art models like UNITER, OSCAR, and VILLA across multiple tasks."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"importance-of-scene-graph-prediction",children:"Importance of Scene Graph Prediction"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ERNIE-ViL Performance on Various Tasks",src:i(59751).A+"",width:"1024",height:"151"})}),"\n",(0,t.jsx)(n.p,{children:"The Scene Graph Prediction (SGP) task plays a crucial role in the model's performance. The experiments reveal:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Introduction of Scene Graph Prediction"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"ERNIE-ViL shows significant performance improvements after introducing the SGP task, highlighting its value during pre-training."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Improvements in Specific Tasks"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"In foundational referencing expressions, especially those requiring deep semantic alignment, SGP improves the model's accuracy on RefCOCO+ by 0.69%."}),"\n",(0,t.jsx)(n.li,{children:"For image retrieval tasks, the model's R@1 on the Flickr30K dataset improves by 2.22%."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Impact of ERNIE 2.0"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The textual initialization from ERNIE 2.0 further enhances the model's performance, particularly in visual reasoning tasks like VCR. This is likely because ERNIE 2.0 learns more commonsense knowledge during pre-training."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The inclusion of the SGP task significantly enhances ERNIE-ViL's performance, especially in tasks requiring detailed semantic alignment, such as foundational referencing expressions and cross-modal retrieval. This underscores the importance of modeling scene graphs for understanding the connections between images and text."}),"\n",(0,t.jsx)(n.h3,{id:"cloze-tests",children:"Cloze Tests"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"ERNIE-ViL Performance on Cloze Tests",src:i(35964).A+"",width:"1024",height:"398"})}),"\n",(0,t.jsx)(n.p,{children:"The authors use cloze tests conditioned on the visual modality to evaluate the impact of the SGP task. These tests require the model to infer hidden detailed semantic tags based on visual and textual context."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset Construction"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Randomly selected 15,000 image-text pairs from the Flickr30K dataset. Selected 5,000 object, attribute, and relationship tags as hidden targets."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluation Metrics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Used top-1 accuracy (ACC@1) and top-5 accuracy (ACC@5) as evaluation criteria."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Comparison Results (from Table 3)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The model pre-trained with the SGP task shows significant improvements in ACC@1 compared to the model without SGP: 1.20% improvement for objects, 3.08% for relationships, and 1.84% for attributes."}),"\n",(0,t.jsx)(n.li,{children:"Both models' text parameters are initialized based on BERT."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Observations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"In some cases, the model without the SGP task fails to make correct predictions as it lacks detailed semantic alignment and cannot distinguish between common words and detailed semantic words during pre-training. In other scenarios, the model can make predictions but with lower confidence compared to the model pre-trained with the SGP task."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The authors' cloze test results show that introducing the SGP task enables the ERNIE-ViL model to learn detailed semantic alignments across modalities more effectively."}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"The ERNIE framework, though effectively extracting knowledge during multi-modal learning, demands significant computational and storage resources. This means it requires more computing power, which can be limiting in certain devices or scenarios. Additionally, while ERNIE leverages pre-learned knowledge, its generalization ability in rare cross-modal scenarios still needs improvement."}),"\n",(0,t.jsx)(n.p,{children:'Extending to ERNIE-ViL, this model successfully incorporates the Scene Graph Prediction (SGP) task to optimize cross-modal detailed semantic alignment but also introduces new challenges. For example, if the relationship between objects in the scene graph, such as "tree" and "person," is mistakenly labeled as "sitting on" instead of "standing by," this misunderstanding could lead to failure in downstream tasks.'}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"The accuracy and completeness of the scene graph directly affect model performance!"})}),"\n",(0,t.jsx)(n.p,{children:'This adds uncertainty in practical applications. Moreover, consider a photo showing a blurred silhouette or overlapping objects; the model may struggle to determine whether it is a "person" or a "shadow" or another object. In such cases, the model may face difficulties interpreting complex or ambiguous scenes, limiting its applicability in scenarios requiring precise object recognition or scene interpretation.'}),"\n",(0,t.jsx)(n.p,{children:'The use of scene graphs implicitly means: "The way humans understand knowledge" is better than "The way models understand knowledge," so let the model learn how humans perceive the world.'}),"\n",(0,t.jsx)(n.p,{children:"But is this approach truly beneficial?"}),"\n",(0,t.jsx)(n.p,{children:'We encourage you to think about its advantages and disadvantages. Once you understand these, you will better grasp the opportunities and challenges presented by the "knowledge enhancement" techniques used in this paper.'}),"\n",(0,t.jsx)(n.p,{children:"Despite these challenges, ERNIE-ViL still shows impressive advantages in the realm of cross-modal pre-training. It not only optimizes detailed semantic alignment but also opens new directions, such as integrating scene graphs extracted from images and using graph neural networks (GNN) to incorporate more structured knowledge. This not only symbolizes technological progress but also provides new research directions and thinking spaces for future researchers."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},88085:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/ernie_vil_1-7aa05503f954d682d9f2c55c1fb92360.jpg"},20746:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/ernie_vil_2-366b8fa885506457759c6a0514053983.jpg"},59751:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/ernie_vil_3-2005f7d64bb742d62b5a52bc76588a78.jpg"},35964:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/ernie_vil_4-8d270533806f2e78d8a1e81e82dcfcfd.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(96540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);