"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7588],{81159:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var t=i(74848),s=i(28453);const a={},r="[21.04] MDETR",o={id:"multimodality/mdetr/index",title:"[21.04] MDETR",description:"The Art of Continuity",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/2104-mdetr/index.md",sourceDirName:"multimodality/2104-mdetr",slug:"/multimodality/mdetr/",permalink:"/en/papers/multimodality/mdetr/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1722902191e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[21.03] CLIP",permalink:"/en/papers/multimodality/clip/"},next:{title:"[21.07] ALBEF",permalink:"/en/papers/multimodality/albef/"}},l={},d=[{value:"The Art of Continuity",id:"the-art-of-continuity",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"MDETR Model Design",id:"mdetr-model-design",level:3},{value:"Training Method",id:"training-method",level:3},{value:"Contrastive Alignment",id:"contrastive-alignment",level:3},{value:"All Losses",id:"all-losses",level:3},{value:"Dataset",id:"dataset",level:3},{value:"Technical Details",id:"technical-details",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Downstream Task Performance Analysis",id:"downstream-task-performance-analysis",level:3},{value:"Few-Shot Capabilities?",id:"few-shot-capabilities",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"2104-mdetr",children:"[21.04] MDETR"}),"\n",(0,t.jsx)(n.h2,{id:"the-art-of-continuity",children:"The Art of Continuity"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2104.12763",children:(0,t.jsx)(n.strong,{children:"MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"The following content is summarized by ChatGPT-4 and has been manually checked, edited, and supplemented with additional explanations."})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"In recent years, object detection has been at the forefront of computer vision, serving as the core of many advanced multi-modal understanding systems. However, traditional methods treat detection systems as black boxes, conducting fixed concept image detection, which clearly has inherent limitations."}),"\n",(0,t.jsx)(n.p,{children:"A significant issue is the inability to effectively utilize multi-modal context for collaborative training, limiting downstream models to accessing only detected objects. Moreover, these detection systems are often frozen, meaning they lack further refinement and adaptability. Most importantly, the vocabulary of these detection systems is severely limited, making them blind to novel concept combinations expressed in free-form text."}),"\n",(0,t.jsx)(n.p,{children:"Simply put, the authors of this paper aim to replace the object detection architecture in VL models."}),"\n",(0,t.jsx)(n.p,{children:"You might remember a previous paper that attempted to replace the detection architecture with ViT, which didn\u2019t turn out well."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Portal: ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/en/papers/multimodality/vilt/",children:"ViLT: Passing the Baton"})})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'Since replacing with ViT wasn\u2019t very successful (because ViT isn\u2019t specialized for object detection), this time the authors used another "specialist" object detection model, DETR, for the replacement.'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Portal: ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/en/papers/object-detection/detr/",children:"DETR: The Pioneer Across Fields"})})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,t.jsx)(n.p,{children:"Most advanced multi-modal understanding systems currently rely on object detection as a core component, but these designs are fraught with various issues:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Limitations in Collaborative Training"})}),"\n",(0,t.jsx)(n.p,{children:"In multi-modal systems, collaborative training means using data from multiple input sources (e.g., images, text, audio) simultaneously for model training. If a system component cannot collaborate with other parts for such training, it might fail to fully leverage all available information."}),"\n",(0,t.jsx)(n.p,{children:"Imagine a model with both image and audio inputs that only trains the image detector independently, ignoring the audio input. If the audio provides important information about objects in the image, the model might fail to identify them correctly."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Detection Scope Limitation"})}),"\n",(0,t.jsx)(n.p,{children:"The main purpose of detection systems is to identify specific objects in images. However, if these systems focus solely on known objects and ignore other parts of the image, they might miss important contextual information. For instance, in an image with multiple people and a dog, the detector might only identify the people and dog, ignoring the park setting, which provides important context."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Model Fixation"})}),"\n",(0,t.jsx)(n.p,{children:'Once a model is trained and "frozen," it no longer updates or learns. This hinders its ability to adapt and optimize in new situations or with new data. For example, a detector trained on summer images might perform poorly on winter images with snow or people in heavy coats if it cannot be fine-tuned.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Vocabulary Limitation"})}),"\n",(0,t.jsx)(n.p,{children:"Object detection systems identify specific classes or attributes based on their training data. If they encounter new objects or concepts not seen in the training data, they might fail to recognize them."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Lack of End-to-End Design"})}),"\n",(0,t.jsx)(n.p,{children:"End-to-end systems allow continuous learning and optimization from input to output without intermediate steps. If the detector is not end-to-end, its collaborative training with other tasks might be limited. Mathematically speaking, this system cannot be differentiated, and without differentiation, it cannot be optimized."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,t.jsx)(n.h3,{id:"mdetr-model-design",children:"MDETR Model Design"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MDETR Architecture",src:i(8548).A+"",width:"1024",height:"338"})}),"\n",(0,t.jsx)(n.p,{children:"The model is straightforward: for the text part, it uses a modified RoBERTa encoder. After generating text feature encodings, they are concatenated and fed into the original DETR architecture."}),"\n",(0,t.jsx)(n.p,{children:"The overall structure includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Main Convolutional Encoder"}),": The image is first encoded by a convolutional backbone and flattened."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Spatial Information"}),": By adding 2D positional encoding to the flattened vectors, the model retains spatial information."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Text Encoding"}),": The text is encoded using a pretrained Transformer language model, producing a sequence of hidden vectors of the same size as the input."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modality-Specific Linear Projections"}),": Modality-specific linear projections are applied to image and text features to project them into a shared encoding space."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Encoder"}),": The concatenated sequence of image and text features is fed into a joint Transformer encoder, the core part of the model."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"training-method",children:"Training Method"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Soft Token Prediction"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Soft Token Prediction",src:i(51123).A+"",width:"1224",height:"380"})}),"\n",(0,t.jsx)(n.p,{children:'The idea behind soft token prediction is intriguing. Soft token prediction focuses on predicting the "range" of each matched object in the original text rather than predicting the class label of each detected object. This is the main difference between this method and standard object detection.'}),"\n",(0,t.jsx)(n.p,{children:'Suppose the description sentence is "a black cat and a white dog." When the model detects a black animal, it tries to predict its association with the phrase "black cat." This is not just about an individual token or class label but about a range of tokens in the text that collectively describe a specific object.'}),"\n",(0,t.jsx)(n.p,{children:"The benefit of this approach is that it can handle multiple references to objects in the same text or cases where multiple objects correspond to the same text."}),"\n",(0,t.jsx)(n.h3,{id:"contrastive-alignment",children:"Contrastive Alignment"}),"\n",(0,t.jsx)(n.p,{children:'Contrastive alignment aims to ensure that the encoded representations of visual objects are close to their corresponding text tokens in the feature space. This alignment is stronger than just relying on positional information from "Soft Token Prediction" as it directly operates on feature representations.'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Contrastive Alignment",src:i(6358).A+"",width:"1024",height:"169"})}),"\n",(0,t.jsx)(n.p,{children:"Referencing the mathematical equation provided in the paper:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"L: The maximum number of tokens."}),"\n",(0,t.jsx)(n.li,{children:"N: The maximum number of objects."}),"\n",(0,t.jsx)(n.li,{children:"T+\u200bi: The set of tokens that should align with a given object oi\u200b."}),"\n",(0,t.jsx)(n.li,{children:"Oi+\u200b: The set of objects that should align with a given token ti\u200b."}),"\n",(0,t.jsx)(n.li,{children:"\u03c4: The temperature parameter, set to 0.07 directly."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The concept of the entire equation is simple: objects and text, align well, the closer the better."}),"\n",(0,t.jsx)(n.h3,{id:"all-losses",children:"All Losses"}),"\n",(0,t.jsx)(n.p,{children:"Training MDETR involves multiple loss functions. In addition to the contrastive loss mentioned above, it includes the original losses from the DETR paper, such as bipartite matching, bounding box prediction loss, L1 loss, GIoU, etc."}),"\n",(0,t.jsx)(n.h3,{id:"dataset",children:"Dataset"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CLEVR"}),": Used to evaluate the method's results."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flickr30k"}),": Used to build the composite dataset images."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MS COCO"}),": Used to build the composite dataset images."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Genome (VG)"}),": Used to build the composite dataset images."]}),"\n",(0,t.jsx)(n.li,{children:"Annotation data is sourced from referring expression datasets, VG regions, Flickr entities, and the GQA training balanced set."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"technical-details",children:"Technical Details"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pre-training Modulated Detection"})}),"\n",(0,t.jsx)(n.p,{children:"During the pre-training phase, the goal is to detect all objects referenced in the aligned free-form text."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Data Composition Technique and Its Importance"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"For each image, all relevant text annotations are retrieved from the mentioned datasets. If different annotations reference the same image, they are merged. To ensure the independence of the training and validation/test sets, all images that appear in the validation or test sets are removed from the training set."}),"\n",(0,t.jsx)(n.li,{children:"This algorithm combines sentences, ensuring the overlap between combined phrases or text blocks is minimal (GIoU \u2264 0.5). GIoU is a metric used to assess the overlap between two rectangular regions. The total length of the combined sentence is limited to fewer than 250 characters. Using this method, a large dataset of 1.3 million aligned image-text pairs is formed."}),"\n",(0,t.jsxs)(n.li,{children:["There are two main reasons for using this data composition technique:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Data efficiency: By packing more information into a single training sample, the data can be utilized more effectively."}),"\n",(0,t.jsxs)(n.li,{children:["Better learning signal:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"When the model learns, it needs to identify and resolve ambiguities between multiple references to the same object class in the text."}),"\n",(0,t.jsx)(n.li,{children:'In a single-sentence context, the task of "Soft Token Prediction" becomes relatively simple, as the model can usually easily predict the subject or core meaning of the sentence without much reliance on the image.'}),"\n",(0,t.jsx)(n.li,{children:"By combining multiple sentences, the model is forced to explore the associations between images and text more deeply, thereby improving its prediction capabilities."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Model Architecture Choices"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The text encoder uses a pretrained RoBERTa-base with 12 Transformer encoder layers."}),"\n",(0,t.jsxs)(n.li,{children:["Visual backbones include:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ResNet-101"}),": Obtained from Torchvision and pretrained on ImageNet."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"EfficientNet Series"}),": EfficientNetB3 and EfficientNetB5 were used. EfficientNetB3 achieved 84.1% top-1 accuracy on ImageNet, while EfficientNetB5 achieved 86.1%."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Additionally, a model trained on large amounts of unlabeled data using the Noisy-Student pseudo-labeling technique was used."}),"\n",(0,t.jsx)(n.li,{children:"Training details: Using 32 V100 GPUs, pretraining was conducted for 40 epochs with an effective batch size of 64, taking approximately one week."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(n.h3,{id:"downstream-task-performance-analysis",children:"Downstream Task Performance Analysis"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Phrase Grounding"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Phrase Grounding",src:i(29053).A+"",width:"972",height:"872"})}),"\n",(0,t.jsx)(n.p,{children:"The authors used the Flickr30k dataset with specific training/validation/test splits. For evaluation, they adopted two different protocols to address the issue of multiple objects being referenced by a single phrase, each with its pros and cons."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"ANY-BOX Protocol"})}),"\n",(0,t.jsx)(n.p,{children:"Under this protocol, if a given phrase references multiple different objects in the image, the predicted bounding box is considered correct if it intersects with any of the ground truth boxes above a set threshold, typically 0.5 IoU. This means the prediction is correct as long as the model correctly identifies any of the referenced objects. However, this method does not evaluate if the model has found all referenced instances."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"MERGED-BOXES Protocol"})}),"\n",(0,t.jsx)(n.p,{children:"Under this protocol, if a phrase references multiple objects in the image, all ground truth boxes associated with the phrase are first merged into a single bounding box that encompasses all of them. Then, the merged box is used to calculate IoU as usual. This means the model's prediction needs to match the merged box rather than individual ground truth boxes. The issue with this method is that it may lose detailed understanding of each instance, especially when these instances are far apart in the image, making the merged box unreasonably large."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Results Comparison"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Under the ANY-BOX setting, MDETR achieved an 8.5 point improvement in Recall@1 on the validation set compared to the current state-of-the-art techniques, without using any additional pre-training data."}),"\n",(0,t.jsx)(n.li,{children:"With pre-training and using the same backbone, MDETR further improved its performance on the test set by 12.1 points."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Referring Expression Comprehension"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Referring Expression Comprehension",src:i(60960).A+"",width:"1024",height:"271"})}),"\n",(0,t.jsx)(n.p,{children:"Most previous research and methods for this task involve ranking a set of pre-extracted image-related bounding boxes. These boxes are obtained using pretrained object detectors."}),"\n",(0,t.jsx)(n.p,{children:"In contrast, this paper aims for a more challenging goal: training the model to predict bounding boxes directly given a referring expression and the corresponding image, rather than simply ranking pre-extracted boxes."}),"\n",(0,t.jsx)(n.p,{children:'The model in this paper is pretrained to identify every object referenced in the text during pre-training. For example, for the caption "a woman in a blue dress standing next to a rose bush," the model is trained to predict boxes for all referenced objects (woman, blue dress, and rose bush). However, when it comes to referring expressions, the task is only to return a single bounding box representing the object referenced by the entire expression. To adapt to this change, the model was fine-tuned on these three specific datasets.'}),"\n",(0,t.jsx)(n.p,{children:"The results shown in the table indicate significant improvements over state-of-the-art methods across all datasets."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Visual Question Answering"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Visual Question Answering",src:i(51279).A+"",width:"1024",height:"368"})}),"\n",(0,t.jsx)(n.p,{children:"This model architecture can also be applied to the VQA task but requires some design modifications."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Design"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Query Types: In addition to the 100 detection queries, query types for question types and queries for predicting question types are introduced. In GQA, these question types are defined as REL, OBJ, GLOBAL, CAT, and ATTR."}),"\n",(0,t.jsx)(n.li,{children:"Training: Pretraining was conducted for 40 epochs, followed by fine-tuning on the unbalanced GQA split for 125 epochs and final fine-tuning on the balanced split for 10 epochs."}),"\n",(0,t.jsx)(n.li,{children:"Loss Strategy: During the first 125 epochs, both detection loss and QA loss were trained, with QA loss given more weight."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The model utilizes object queries as learned encodings input into the decoder for object detection. During inference, specific parts of the model predict the question type and derive the answer from that part."}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:"Unlike commonly used VQA v2, this paper uses GQA."}),(0,t.jsx)(n.p,{children:"GQA and VQA v2 are widely used datasets for Visual Question Answering (VQA). While both focus on answering questions given an image, there are key differences:"}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Data Scale and Source"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"GQA: Based on the Visual Genome dataset, containing approximately 22 million question-answer pairs."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"VQA v2: An improved version of the original VQA dataset, containing about 1.2 million question-answer pairs, based on MS COCO and Abstract Scenes datasets."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Nature of Questions and Answers"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"GQA: Focuses on complex and compositional questions, often involving multiple objects and their relationships. Answers are usually more descriptive and can be multi-word responses."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:'VQA v2: More diverse, with questions ranging from very simple (e.g., "What color is this?") to more complex. Answers are usually one or two words.'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Data Imbalance"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"GQA: Designed to address some imbalance issues in VQA that might lead models to guess answers without truly understanding the image content."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"VQA v2: Introduced challenging counterexample images to address data bias issues found in the original dataset."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Scene Graphs"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"GQA: Contains rich scene graphs, detailing the types, attributes, and relationships of objects in the images."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"VQA v2: Does not include built-in scene graphs, but researchers can use other data sources or techniques to provide this information."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Task Goals"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"GQA: In addition to basic VQA tasks, GQA focuses on multi-modal reasoning, pushing models to deeply understand image content and question context."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"VQA v2: Primarily focuses on basic VQA tasks, aiming to improve model performance and address data bias issues."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),(0,t.jsx)(n.p,{children:"In summary, GQA tends to provide more complex questions and answers, with richer object and relationship descriptions, while VQA v2 is more diverse and focuses on addressing data bias issues."})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Performance Comparison"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Performance Comparison",src:i(74898).A+"",width:"860",height:"576"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Using a model with a ResNet-101 backbone, its performance exceeds that of LXMERT and VL-T5."}),"\n",(0,t.jsx)(n.li,{children:"The model's performance even surpasses OSCAR, which uses more pre-training data."}),"\n",(0,t.jsx)(n.li,{children:"The MDETR model with an EfficientNet-B5 backbone achieves even higher performance, as shown in the table."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"few-shot-capabilities",children:"Few-Shot Capabilities?"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Few-Shot Capabilities",src:i(57609).A+"",width:"812",height:"400"})}),"\n",(0,t.jsx)(n.p,{children:"Inspired by the success of CLIP in zero-shot image classification, the authors explored how to fine-tune the pretrained MDETR model for object detection with a small amount of labeled data. Unlike CLIP, MDETR's pretraining dataset does not ensure all target classes are balanced. This means that in its dataset, no bounding boxes are aligned with text, leading the model always to predict a box for the given text."}),"\n",(0,t.jsx)(n.p,{children:"Due to this design, MDETR cannot be evaluated in a true zero-shot transfer setting. Therefore, the authors chose an alternative strategy, evaluating in a few-shot setting. The LVIS dataset was chosen for this experiment, containing 1.2k categories, most of which have very few training samples, exhibiting a long-tail distribution."}),"\n",(0,t.jsx)(n.p,{children:"To adapt to this distribution, MDETR's training strategy was: for each positive class, it used the image and the class name as the training instance while using all annotations for that class. For negative classes, only the class name and empty annotations were provided. During inference, MDETR queried for each possible class name and merged all detected boxes across text prompts."}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:"Example:"}),(0,t.jsx)(n.p,{children:'Assume we have a simple dataset with three classes: "dog," "cat," and "fish."'}),(0,t.jsx)(n.p,{children:"Our labeled data includes:"}),(0,t.jsx)(n.p,{children:'An image showing a dog, labeled "dog."\nAnother image showing a cat and a fish, labeled "cat" and "fish," respectively.\nBased on MDETR\'s training strategy:'}),(0,t.jsx)(n.p,{children:"For the first image:"}),(0,t.jsx)(n.p,{children:'Since it contains a "dog," this image and the text "dog" are used as the training instance with the label "dog."\nSince there are no "cats" or "fish" in the image, the class names "cat" and "fish" are provided without labels (i.e., empty labels).\nFor the second image:'}),(0,t.jsx)(n.p,{children:'Since it contains a "cat" and a "fish," the image, and the texts "cat" and "fish" are used as training instances with the labels "cat" and "fish."\nSince there are no "dogs" in the image, the class name "dog" is provided without a label (i.e., empty label).\nDuring inference, MDETR queries each class name "dog," "cat," and "fish" and merges the results detected across text prompts. For example, if it detects a box when querying "dog" and "fish" but none when querying "cat," the final result will contain boxes for "dog" and "fish."'})]}),"\n",(0,t.jsx)(n.p,{children:"The authors fine-tuned MDETR on three subsets of the LVIS dataset (1%, 10%, and 100% of images). The results were compared with two baselines: a Mask-RCNN trained on the full LVIS training set and a DETR pretrained on MSCOCO and fine-tuned on LVIS subsets. Surprisingly, even with only 1% of the data, MDETR leveraged its text pretraining to outperform fully fine-tuned DETR on rare categories."}),"\n",(0,t.jsx)(n.p,{children:"Moreover, a significant observation was that fine-tuning on the full training data led to a sharp drop in detection performance on rare objects, from 20.9 AP with 10% data to 7.5 AP with 100% data. This steep decline is likely due to extreme class imbalance in the data."}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:'One of the most appealing features of MDETR is its "complete differentiability."'}),"\n",(0,t.jsx)(n.p,{children:"This design allows the entire model to be trained end-to-end, fostering tighter model collaboration and potentially improving overall performance and training efficiency. Additionally, its practical performance on multiple datasets has demonstrated impressive results, establishing its position in the multi-modal learning domain."}),"\n",(0,t.jsx)(n.p,{children:"Furthermore, MDETR's versatility is a major highlight. Not only does it excel in modulated detection, but it has also proven its value in downstream applications such as few-shot detection and visual question answering."}),"\n",(0,t.jsx)(n.p,{children:"MDETR provides a pathway, with its non-reliance on black-box object detectors potentially inspiring more researchers to create accurate and efficient models."}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:"Why didn't previous papers string Faster RCNN together? Complete differentiability, how great would that be?"}),(0,t.jsx)(n.p,{children:"While a fully differentiable model sounds appealing, it can also require more computational resources. Especially if you do it directly and brutally without clever design, there's a high probability that the model will retaliate against you:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"It won't train."})}),"\n"]}),(0,t.jsx)(n.p,{children:"When the entire model is differentiable, its internal structure is likely complex, meaning higher computational costs and more training difficulty. Researchers might need to spend more time tuning hyperparameters, which might not be feasible for every team."})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8548:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_1-9fe82970ec37a8117f878d76e4271882.jpg"},51123:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_2-f522827e2c5a6a2fd3d51965193cbabb.jpg"},6358:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_3-4c1659405e91212323e7fdfafd0ef89b.jpg"},29053:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_4-c8426ce0c50ebdd7a392e1127821d2fb.jpg"},60960:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_5-363d71e92179f27e926b68e38e1cde4d.jpg"},51279:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_6-fb30a864ffbf83db984cd2dfdc55dc52.jpg"},74898:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_7-e8febec9925a06c95bcae4030fb3b9df.jpg"},57609:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/mdetr_8-b119933acbc0bffe8251f1e7133e3b5c.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(96540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);