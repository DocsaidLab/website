"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([["67115"],{5314:function(e,n,i){i.r(n),i.d(n,{metadata:()=>t,contentTitle:()=>o,default:()=>h,assets:()=>l,toc:()=>d,frontMatter:()=>r});var t=JSON.parse('{"id":"text-spotting/got/index","title":"[24.09] GOT","description":"All-encompassing OCR","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/text-spotting/2409-got/index.md","sourceDirName":"text-spotting/2409-got","slug":"/text-spotting/got/","permalink":"/en/papers/text-spotting/got/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1733839479000,"frontMatter":{"title":"[24.09] GOT","authors":"Zephyr"},"sidebar":"papersSidebar","previous":{"title":"[21.05] ABCNet v2","permalink":"/en/papers/text-spotting/abcnet-v2/"},"next":{"title":"Transformers (16)","permalink":"/en/papers/category/transformers-16"}}'),a=i("85893"),s=i("50065");let r={title:"[24.09] GOT",authors:"Zephyr"},o=void 0,l={},d=[{value:"All-encompassing OCR",id:"all-encompassing-ocr",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Problem Solving",id:"problem-solving",level:3},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Phase 1 Training",id:"phase-1-training",level:3},{value:"Training Data",id:"training-data",level:4},{value:"Phase 2 Training",id:"phase-2-training",level:3},{value:"Training Data",id:"training-data-1",level:4},{value:"Phase 3 Training",id:"phase-3-training",level:3},{value:"Training Strategy",id:"training-strategy",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Plain Text Document OCR Performance",id:"plain-text-document-ocr-performance",level:3},{value:"Scene Text OCR Performance",id:"scene-text-ocr-performance",level:3},{value:"Formatted Document OCR Performance",id:"formatted-document-ocr-performance",level:3},{value:"Fine-grained OCR Performance",id:"fine-grained-ocr-performance",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){let n={a:"a",admonition:"admonition",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"all-encompassing-ocr",children:"All-encompassing OCR"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2409.01704",children:(0,a.jsx)(n.strong,{children:"General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model"})})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.p,{children:"This is an end-to-end OCR paper proposed by China's Megvii Technology."}),"\n",(0,a.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,a.jsx)(n.p,{children:"OCR (Optical Character Recognition) is a widely used technology, with traditional systems typically relying on a multi-modular pipeline design, involving detection, cropping, and recognition modules. Each module has its specific function, and models are often developed for specific tasks, which can result in suboptimal overall performance and complicated maintenance."}),"\n",(0,a.jsx)(n.p,{children:'The authors of this paper believe that the models based on these pipeline systems belong to the "OCR 1.0" category. They propose an "All-In-One" model with the following features:'}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"It is a unified end-to-end model."}),"\n",(0,a.jsx)(n.li,{children:"It addresses the bottlenecks faced by traditional and LVLM models in OCR tasks."}),"\n",(0,a.jsx)(n.li,{children:"It can handle various generalized OCR tasks."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["This model is named ",(0,a.jsx)(n.strong,{children:"GOT-OCR2.0"}),"!"]}),"\n",(0,a.jsx)(n.h3,{id:"problem-solving",children:"Problem Solving"}),"\n",(0,a.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"model arch",src:i(38492).Z+"",width:"1224",height:"908"})}),"\n",(0,a.jsx)(n.p,{children:"The figure above not only illustrates the model architecture but also the training process."}),"\n",(0,a.jsx)(n.p,{children:"The model architecture consists of three components:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Vision Encoder"}),": This is the image encoding layer responsible for feature extraction from images."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Linear"}),": This is the linear transformation layer that bridges the encoder and decoder feature dimensions."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Text Decoder"}),": This is the text decoding layer, which takes the image information and generates text."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The training process is divided into three phases:"}),"\n",(0,a.jsx)(n.h3,{id:"phase-1-training",children:"Phase 1 Training"}),"\n",(0,a.jsx)(n.p,{children:"The authors used the ViTDet architecture as the encoder, benefiting from its local attention mechanism that significantly reduces the computational cost for high-resolution images. The encoder has around 80 million parameters, and the model was trained using 5 million image-text pairs."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2203.16527",children:(0,a.jsx)(n.strong,{children:"[22.03] Exploring Plain Vision Transformer Backbones for Object Detection"})})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The encoder's last two layers follow the ",(0,a.jsx)(n.strong,{children:"Vary"})," setup, transforming a 1024\xd71024\xd73 input image into 256\xd71024 image tokens. These tokens are projected through a 1024\xd7768 linear layer to match the dimension of the language model (OPT-125M)."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2312.06109",children:(0,a.jsx)(n.strong,{children:"[23.12] Vary: Scaling up the vision vocabulary for large vision-language models"})})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"During preprocessing, images of varying shapes are resized to a 1024\xd71024 square, balancing the need for different aspect ratios."}),"\n",(0,a.jsx)(n.h4,{id:"training-data",children:"Training Data"}),"\n",(0,a.jsx)(n.p,{children:"The training data is categorized into two types:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Natural Scene Data"})}),"\n",(0,a.jsx)(n.p,{children:"English images were sampled from the Laion-2B dataset, while Chinese images were taken from the Wukong dataset. The text in these scenes was pseudo-labeled using the PaddleOCR tool. In this step, the authors collected 2 million data samples, evenly split between English and Chinese."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Document-level Data"})}),"\n",(0,a.jsx)(n.p,{children:"The authors collected open-source PDF-type documents from Common Crawl and extracted dense text content using the Fitz Python package. They acquired 1.2 million full-page image-text pairs and 800,000 image fragments of line and paragraph-level data by cropping PDF images based on parsed bounding boxes."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Finally, the text annotation was processed in two ways:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Removing the bounding box and assembling the text content in a top-to-bottom, left-to-right sequence."}),"\n",(0,a.jsx)(n.li,{children:"Cropping text regions from the original image based on bounding boxes, stored as image fragments, resulting in 1 million fragment-type image-text pairs."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"phase-2-training",children:"Phase 2 Training"}),"\n",(0,a.jsx)(n.p,{children:'After pretraining the vision encoder, the model\'s knowledge was expanded through "multi-task joint training."'}),"\n",(0,a.jsx)(n.p,{children:"At this stage, the authors replaced the OPT-125M model used in Phase 1 with the Qwen-0.5B model, which has 500 million parameters and contains multilingual priors. The linear embedding layer's dimensions were adjusted to 1024\xd71024 to match Qwen-0.5B's input channels. GOT adopts an encoder-decoder paradigm, with a total parameter count of approximately 580 million."}),"\n",(0,a.jsx)(n.p,{children:"The input image for GOT is 1024\xd71024 pixels, compressed into 256 tokens. The decoder references these tokens to predict OCR results, with a maximum length of up to 8K."}),"\n",(0,a.jsx)(n.h4,{id:"training-data-1",children:"Training Data"}),"\n",(0,a.jsx)(n.p,{children:"In this phase, the authors aimed to inject more OCR knowledge by exploring various data generation methods and engines:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Pure OCR Data"})}),"\n",(0,a.jsx)(n.p,{children:"80% of the data from section 3.2.2 was used as pure OCR data, and a handwritten text recognition subtask was added, covering multiple languages and handwriting styles. Datasets included Chinese CASIA-HWDB2, English IAM, and Norwegian NorHand-v3."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mathpix-markdown Format Data"})}),"\n",(0,a.jsx)(n.p,{children:"To maintain high readability of outputs, especially for mathematical formulas and tables, the authors used various methods to collect a large amount of formatted data:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Math Formulas"}),": Extracted .tex source files from Arxiv, obtaining around 1 million formula segments, which were converted to Mathpix format and rendered into PNG images."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Molecular Formulas"}),": Extracted 2 million smiles sources from ChEMBL_25 and generated about 1 million molecular formula image-text pairs using Mathpix and rdkit.Chem tools."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Tables"}),": Extracted 300,000 table sources from .tex files and rendered them into images using LATEX."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Full-page Data"}),": Obtained 500,000 pairs of English Markdown PDF-text data and 500,000 pairs of Chinese Markdown data using the Nougat method. Additionally, the authors directly labeled 200,000 internal data points, including books, papers, and financial reports, using Mathpix."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Broader OCR Data"})}),"\n",(0,a.jsx)(n.p,{children:"To enable GOT to handle more general optical character recognition tasks, the authors collected data for three related challenge tasks:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sheet Music"}),": Rendered approximately 500,000 single-system sheet music data points using the GrandStaff dataset."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Geometric Figures"}),": Constructed basic geometric figures (e.g., circles, rectangles, triangles, simple function curves) using TikZ, resulting in about 1 million geometric TikZ data points."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Charts"}),": Referenced OneChart, rendering chart data using Matplotlib and Pyecharts, generating 2 million chart data points, with half from Matplotlib and the other half from Pyecharts."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"phase-3-training",children:"Phase 3 Training"}),"\n",(0,a.jsx)(n.p,{children:"After Phase 2, the model is already capable of handling various OCR tasks. In the final phase, the authors aimed to add more functionality to the model, introducing three new features:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Fine-grained Interactive OCR Data Engine"})}),"\n",(0,a.jsx)(n.p,{children:"Fine-grained OCR is a highly interactive feature that allows for region-level visual perception controlled by spatial coordinates or color. Users can specify regions of interest (RoI) by adding bounding boxes (box-guided OCR) or color-coded text (color-guided OCR) in the query prompt, avoiding unnecessary character output."}),"\n",(0,a.jsx)(n.p,{children:"Natural scene fine-grained OCR data comes from open datasets such as RCTW, ReCTS, ShopSign, and COCO-Text. These datasets provide text bounding boxes, used to generate fine-grained OCR data directly. Document-level fine-grained data is obtained by filtering out scanned-format PDF files and using the Fitz/PDFminer Python packages to parse the rest."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Large Image OCR with Multi-cropping Data Engine"})}),"\n",(0,a.jsx)(n.p,{children:"GOT supports a 1024\xd71024 input resolution, which is sufficient for most common OCR tasks such as scene OCR or A4 page PDF OCR. For cases requiring extremely large images (e.g., two-page PDF spreads), GOT uses a large sliding window to achieve dynamic resolution."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Batch OCR for Multi-page PDF Files"})}),"\n",(0,a.jsx)(n.p,{children:'GOT\u2019s multi-page OCR feature can directly process multi-page PDFs in batches, eliminating the need for a "for loop." This ensures that researchers do not have to worry about annotation interruptions caused by pagination.'}),"\n",(0,a.jsx)(n.p,{children:"To implement this feature, the authors randomly selected 2-8 pages from Mathpix-format PDF data and merged them into a single OCR task. The overall length was limited to 8K to ensure efficient processing. The authors generated about 200,000 multi-page OCR data points, with many containing mixed English and Chinese pages."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"training-strategy",children:"Training Strategy"}),"\n",(0,a.jsx)(n.p,{children:"The authors used 8\xd78 L40s GPUs for training, adopting a three-phase training approach:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pretraining Phase"}),": The entire model was trained using the AdamW optimizer, with a global batch size of 128, over 3 epochs. The initial learning rate was 1e-4, and the maximum token length was set to 4096."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Joint Training Phase"}),": The maximum token length was increased to 6000, using the same optimizer setup as in pretraining, and trained for 1 epoch."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Post-training Phase"}),": The maximum token length was extended to 8192, with an initial learning rate of 2e-5, and trained for 1 epoch to support multi-block/page OCR functionality."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,a.jsx)(n.h3,{id:"plain-text-document-ocr-performance",children:"Plain Text Document OCR Performance"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"result plain text",src:i(70277).Z+"",width:"1592",height:"582"})}),"\n",(0,a.jsx)(n.p,{children:"As shown in the table, the main evaluation metrics are Edit Distance, F1 Score, Precision, Recall, BLEU, and METEOR, with word-level segmentation used for calculations. GOT (580M) exhibits outstanding performance in pure text OCR tasks, demonstrating its excellent PDF text recognition capabilities."}),"\n",(0,a.jsx)(n.h3,{id:"scene-text-ocr-performance",children:"Scene Text OCR Performance"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"result scene text",src:i(63451).Z+"",width:"1698",height:"422"})}),"\n",(0,a.jsx)(n.p,{children:"The authors collected 400 natural images, split evenly between Chinese and English, as a benchmark test for scene text OCR. Due to the shorter text in scene images, character-level segmentation was used to calculate the metrics."}),"\n",(0,a.jsx)(n.p,{children:"GOT also performed remarkably well in natural images, showcasing its superior performance in most fundamental OCR tasks."}),"\n",(0,a.jsx)(n.h3,{id:"formatted-document-ocr-performance",children:"Formatted Document OCR Performance"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"result formatted text",src:i(37612).Z+"",width:"1630",height:"556"})}),"\n",(0,a.jsx)(n.p,{children:"The data source consists of 90 sample pages, generated using Mathpix pseudo-labeling and manually corrected errors. GOT provided satisfactory results at a single resolution (1024\xd71024), while the dynamic resolution strategy further improved its performance in small text formulas and tables."}),"\n",(0,a.jsx)(n.h3,{id:"fine-grained-ocr-performance",children:"Fine-grained OCR Performance"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"result fine-grained text",src:i(71280).Z+"",width:"1734",height:"504"})}),"\n",(0,a.jsx)(n.p,{children:"GOT demonstrated exceptional performance in fine-grained OCR tasks (including box-guided and color-guided OCR), surpassing Fox, which indicates its strong interactive OCR capabilities."}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"GOT is relatively simple in structure and focuses specifically on pure OCR tasks, demonstrating excellent performance across multiple tasks. Its ability to integrate various OCR tasks, including document-level text recognition, scene text recognition, fine-grained interactive OCR, formatted document recognition, and more general character recognition, makes it a highly flexible model with future potential."}),"\n",(0,a.jsxs)(n.p,{children:["The model is available on Hugging Face: ",(0,a.jsx)(n.a,{href:"https://huggingface.co/stepfun-ai/GOT-OCR2_0",children:(0,a.jsx)(n.strong,{children:"GOT-OCR2.0"})})]}),"\n",(0,a.jsxs)(n.p,{children:["You can test its capabilities on the online demo: ",(0,a.jsx)(n.a,{href:"https://huggingface.co/spaces/stepfun-ai/GOT_official_online_demo",children:(0,a.jsx)(n.strong,{children:"Online Demo"})})]}),"\n",(0,a.jsxs)(n.admonition,{type:"tip",children:[(0,a.jsx)(n.p,{children:"We conducted our own test, and when inputting a PDF research paper, GOT-OCR2.0 was able to directly extract the text content, supporting multiple languages with impressive results!"}),(0,a.jsx)(n.p,{children:"However, when the model encounters an unfamiliar format, there might be some issues. For instance, when we input an image of a passport from the MIDV-2020 dataset, GOT-OCR2.0 couldn't fully comprehend the passport's format and could only extract some scattered text."}),(0,a.jsx)(n.p,{children:"To use the model effectively, you need to prepare your own fine-tuning data or use the data engines provided by the authors for fine-tuning."}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.img,{alt:"demo1",src:i(94495).Z+"",width:"1224",height:"202"}),"\n",(0,a.jsx)(n.img,{alt:"demo2",src:i(2612).Z+"",width:"1226",height:"544"}),"\n",(0,a.jsx)(n.img,{alt:"demo3",src:i(43039).Z+"",width:"1226",height:"512"})]})]})]})}function h(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},38492:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img1-8fe3ac3eaa4df3d486f252735ec08c04.jpg"},94495:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img10-3d8d3d200b4dbb00015645894195a562.jpg"},70277:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img3-93e4051ccfe35688c4927472f61471dc.jpg"},63451:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img4-7e97753001d86d74b4994c047431285c.jpg"},37612:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img5-efc1583f8bc55b4d70681da498c187b3.jpg"},71280:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img6-195f23a569ed4790a2d9f8c3414a8452.jpg"},43039:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img8-a39a035eff7450ac55c9f3a46234b42c.jpg"},2612:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img9-679f66a079d7622c87eae8d642deee19.jpg"},50065:function(e,n,i){i.d(n,{Z:function(){return o},a:function(){return r}});var t=i(67294);let a={},s=t.createContext(a);function r(e){let n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);