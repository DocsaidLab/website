"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[22177],{96207:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"text-recognition/yatr/index","title":"[21.07] YATR","description":"Fake Can\'t Replace the Real","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/text-recognition/2107-yatr/index.md","sourceDirName":"text-recognition/2107-yatr","slug":"/text-recognition/yatr/","permalink":"/en/papers/text-recognition/yatr/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1731164874000,"frontMatter":{},"sidebar":"papersSidebar","previous":{"title":"[21.05] ViTSTR","permalink":"/en/papers/text-recognition/vitstr/"},"next":{"title":"[21.09] TrOCR","permalink":"/en/papers/text-recognition/trocr/"}}');var a=n(74848),r=n(28453);const s={},o="[21.07] YATR",l={},h=[{value:"Fake Can&#39;t Replace the Real",id:"fake-cant-replace-the-real",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Discussion",id:"discussion",level:2},{value:"The Importance of Real Data",id:"the-importance-of-real-data",level:3},{value:"Testing with ViTSTR",id:"testing-with-vitstr",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const t={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"2107-yatr",children:"[21.07] YATR"})}),"\n",(0,a.jsx)(t.h2,{id:"fake-cant-replace-the-real",children:"Fake Can't Replace the Real"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/2107.13938",children:(0,a.jsx)(t.strong,{children:"Why You Should Try the Real Data for Scene Text Recognition"})})}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:"In the field of Scene Text Recognition (STR), the majority of models rely heavily on synthetic data for training."}),"\n",(0,a.jsx)(t.p,{children:"The standard approach is to begin with pre-training on synthetic data, and then either evaluate the model directly on validation datasets or fine-tune it using a small amount of real data before proceeding with evaluation."}),"\n",(0,a.jsx)(t.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,a.jsx)(t.p,{children:"There\u2019s no denying the importance of real-world data. Synthetic data must be generated based on specific rules\u2014such as selecting particular fonts, adding noise, or choosing backgrounds."}),"\n",(0,a.jsx)(t.p,{children:"Where there are rules, there are exceptions. These exceptions are often things that synthetic data cannot cover, which can impact the model\u2019s ability to generalize. In the real world, synthetic data is the exception; real data is the rule."}),"\n",(0,a.jsx)(t.p,{children:"Would you trust a model's generalization ability if it\u2019s trained on exceptions instead of the rule?"}),"\n",(0,a.jsx)(t.p,{children:"The authors of this paper argue that a combination of synthetic and real data provides the best training material for STR models."}),"\n",(0,a.jsx)(t.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,a.jsx)(t.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"80%"},children:(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"model architecture",src:n(74250).A+"",width:"1224",height:"552"})})})}),"\n",(0,a.jsx)(t.p,{children:"The authors reference the approach used in WWWSTR:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"/en/papers/text-recognition/wwwstr/",children:(0,a.jsx)(t.strong,{children:"[19.04] WWWSTR: Data and Model Analysis"})})}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"However, they do not separate the last two stages; in this architecture, the final stage of text recognition incorporates both sequence modeling and the final prediction in a single step."}),"\n",(0,a.jsx)(t.p,{children:"For image rectification, the model employs the Thin Plate Spline (TPS) technique from ASTER:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://ieeexplore.ieee.org/document/8395027",children:(0,a.jsx)(t.strong,{children:"[18.06] ASTER: An Attentional Scene Text Recognizer with Flexible Rectification"})})}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"The TPS spatial transformation mechanism consists of three components:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Localization Network"}),": First, it generates the spatial transformation parameters for the input image."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Grid Generator"}),": Based on the predicted parameters, it creates a set of sampling grid points, marking the locations where the input image should be sampled, producing the transformed output."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Sampler"}),": Using the grid and the input image, it generates the final output image."]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"The strength of TPS lies in its design, which doesn\u2019t require character-level annotation, making it adaptable to various text recognition architectures. Its parameters can be learned in an end-to-end manner."}),"\n",(0,a.jsx)(t.p,{children:"For the backbone, ResNeXt-101, a variant of ResNet with enhanced expressive capability, is used."}),"\n",(0,a.jsx)(t.p,{children:"Finally, for the text recognition head, a 2D attention map with a GRU decoder is employed."}),"\n",(0,a.jsx)(t.admonition,{type:"tip",children:(0,a.jsx)(t.p,{children:"Notably, this paper does not introduce any significant innovations in model architecture; the primary focus is instead on the training data."})}),"\n",(0,a.jsx)(t.h2,{id:"discussion",children:"Discussion"}),"\n",(0,a.jsx)(t.h3,{id:"the-importance-of-real-data",children:"The Importance of Real Data"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"80%"},children:(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"real data importance",src:n(49973).A+"",width:"1140",height:"248"})})})}),"\n",(0,a.jsx)(t.p,{children:"The authors conducted training experiments using both synthetic and real datasets, keeping all conditions consistent: ResNeXt-101 as the backbone network, a case-insensitive training and testing mode, and no use of the TPS module in this experiment."}),"\n",(0,a.jsx)(t.p,{children:"The results, shown in the table above, reveal that training exclusively with synthetic datasets fails to produce a robust text recognition model. Conversely, when training solely with real datasets, overall accuracy decreases. The authors suggest this might be due to the greater challenges posed by real-world datasets, indicating that models trained on real data may benefit from longer training cycles."}),"\n",(0,a.jsx)(t.p,{children:"The best-performing combination was MJSynth + OpenImagesV5, which the authors identify as the optimal training dataset mix."}),"\n",(0,a.jsx)(t.h3,{id:"testing-with-vitstr",children:"Testing with ViTSTR"}),"\n",(0,a.jsx)(t.p,{children:"To further validate the importance of real data, the authors experimented with ViTSTR, a Transformer-based text recognition model."}),"\n",(0,a.jsx)(t.p,{children:"You can refer to our earlier discussion of ViTSTR:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"/en/papers/text-recognition/vitstr/",children:(0,a.jsx)(t.strong,{children:"[21.05] ViTSTR: Encoder-Only Model"})})}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Using both synthetic and real datasets for training, they obtained the following results:"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"80%"},children:(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"ViTSTR",src:n(45656).A+"",width:"1224",height:"180"})})})}),"\n",(0,a.jsx)(t.p,{children:"The results show that ViTSTR models trained with real datasets demonstrated a significant improvement across all metrics, with accuracy increasing by about 5% on each validation set. Although the original ViTSTR paper employed various data augmentation techniques, these were insufficient to offset the absence of real data."}),"\n",(0,a.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(t.p,{children:"Combining synthetic and real data enhances the generalization capabilities of text recognition models."}),"\n",(0,a.jsx)(t.p,{children:"While this conclusion may seem intuitive, knowing the exact impact through empirical results is invaluable. If anyone asks, \u201cWhy use real data, and what specific difference does it make?\u201d this paper provides a concrete, evidence-based answer."})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},74250:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/img1-6d9f0af447ff1574f371d8596f42ac90.jpg"},49973:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/img2-28ed4f6f62a85bab143819434488f056.jpg"},45656:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/img3-f780738fc4731405f6a23ac34aca372a.jpg"},28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>o});var i=n(96540);const a={},r=i.createContext(a);function s(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);