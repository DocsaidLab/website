"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["81647"],{10734:function(e,i,n){n.r(i),n.d(i,{frontMatter:()=>r,default:()=>h,contentTitle:()=>o,assets:()=>l,toc:()=>d,metadata:()=>t});var t=JSON.parse('{"id":"multimodality/vilbert/index","title":"[19.08] ViLBERT","description":"Interwoven Layers in the Beginning","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/1908-vilbert/index.md","sourceDirName":"multimodality/1908-vilbert","slug":"/multimodality/vilbert/","permalink":"/en/papers/multimodality/vilbert/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739242156000,"frontMatter":{"title":"[19.08] ViLBERT","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[19.08] LXMERT","permalink":"/en/papers/multimodality/lxmert/"},"next":{"title":"[19.08] VisualBERT","permalink":"/en/papers/multimodality/visualbert/"}}'),a=n(85893),s=n(50065);let r={title:"[19.08] ViLBERT",authors:"Z. Yuan"},o=void 0,l={},d=[{value:"Interwoven Layers in the Beginning",id:"interwoven-layers-in-the-beginning",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Approach",id:"approach",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Pre-training Tasks",id:"pre-training-tasks",level:3},{value:"Discussion",id:"discussion",level:2},{value:"How Effective is ViLBERT?",id:"how-effective-is-vilbert",level:3},{value:"Impact of Model Depth on Performance",id:"impact-of-model-depth-on-performance",level:3},{value:"Effect of Pre-training Data",id:"effect-of-pre-training-data",level:3},{value:"Conclusion",id:"conclusion",level:2}];function c(e){let i={a:"a",admonition:"admonition",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.h2,{id:"interwoven-layers-in-the-beginning",children:"Interwoven Layers in the Beginning"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.a,{href:"https://arxiv.org/abs/1908.02265",children:(0,a.jsx)(i.strong,{children:"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"})})}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.p,{children:"Each year in academia brings numerous developments, and 2019 was especially eventful, with the wave of advancements still unfolding."}),"\n",(0,a.jsx)(i.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,a.jsx)(i.p,{children:"Current methods primarily train visual and language models separately before attempting to combine them."}),"\n",(0,a.jsx)(i.p,{children:"This approach often leads to suboptimal results, as models struggle to generalize well when visual and language data are limited or biased. While models may be capable of recognizing objects in images or understanding language independently, creating meaningful connections between these two modalities remains a significant challenge."}),"\n",(0,a.jsx)(i.p,{children:"The authors aim to depart from traditional learning approaches, designing a method to help the model simultaneously learn from both images and text, thus fostering a closer association between them."}),"\n",(0,a.jsx)(i.h2,{id:"approach",children:"Approach"}),"\n",(0,a.jsx)(i.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"85%"},children:(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"ViLBERT Model Architecture",src:n(51072).Z+"",width:"1596",height:"328"})})})}),"\n",(0,a.jsx)(i.p,{children:"Inspired by BERT, the authors sought to develop a similar model that could learn joint representations of language and visual content from paired data."}),"\n",(0,a.jsx)(i.p,{children:"In BERT\u2019s original architecture, only language data is processed. The authors wondered if some of these inputs could be replaced with image data, enabling the model to simultaneously learn connections between images and text."}),"\n",(0,a.jsx)(i.p,{children:"As shown above, the authors propose a dual-stream architecture: one stream is dedicated to visual processing, and the other to language. These streams interact through cross-attention mechanisms, allowing variable network depth for each modality and enabling cross-modal connections at different layers."}),"\n",(0,a.jsx)(i.p,{children:"The image input is derived from bounding boxes and visual features extracted by a pre-trained object detector, which are then combined with visual position encodings."}),"\n",(0,a.jsx)(i.h3,{id:"pre-training-tasks",children:"Pre-training Tasks"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"70%"},children:(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"ViLBERT Pre-training Tasks",src:n(70875).Z+"",width:"1024",height:"813"})})})}),"\n",(0,a.jsx)(i.p,{children:"As illustrated above, the authors propose two main pre-training tasks: Masked Language Modeling (MLM) for multimodal learning and multimodal alignment prediction."}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Multimodal MLM"}),": This task aims to enable the model to reconstruct masked inputs."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Multimodal Alignment Prediction"}),": This task requires the model to determine whether the image and text are aligned\u2014whether the text accurately describes the image."]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"The model is trained on the Conceptual Captions dataset, which consists of 3.3 million image-caption pairs automatically extracted from web images. Despite some noise and incomplete captions, this dataset provides a rich variety of visual content."}),"\n",(0,a.jsx)(i.p,{children:"The experimental setup includes:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Language model initialization using BERT-BASE, which was pre-trained on BookCorpus and English Wikipedia."}),"\n",(0,a.jsx)(i.li,{children:"Visual features derived from a pre-trained Faster R-CNN."}),"\n",(0,a.jsx)(i.li,{children:"Transformer and co-attentional Transformer blocks in the visual stream with 1024 neurons and 8 attention heads."}),"\n",(0,a.jsx)(i.li,{children:"Training on 8 TitanX GPUs for 10 epochs."}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"discussion",children:"Discussion"}),"\n",(0,a.jsx)(i.h3,{id:"how-effective-is-vilbert",children:"How Effective is ViLBERT?"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"ViLBERT Performance",src:n(8194).Z+"",width:"1024",height:"264"})}),"\n",(0,a.jsx)(i.p,{children:"As shown above, the ViLBERT model achieves impressive performance across various vision-language tasks."}),"\n",(0,a.jsx)(i.p,{children:"It performs well not only in pre-trained settings (compared to single-stream and ViLBERT models) but also in non-pre-trained settings (compared to single-stream\u2020 and ViLBERT\u2020 variants). In particular, ViLBERT demonstrates significant benefits in tasks such as VQA and RefCOCO+."}),"\n",(0,a.jsx)(i.p,{children:"Using ViLBERT in pre-training tasks leads to improvements of 2% to 13% across various tasks (comparing ViLBERT and ViLBERT\u2020), indicating that both ViLBERT and single-stream models benefit from pre-training."}),"\n",(0,a.jsx)(i.p,{children:"ViLBERT\u2019s fine-tuning strategy surpasses state-of-the-art task-specific models across four established tasks. Notably, it sets a new benchmark in VCR, RefCOCO+, and image retrieval, with improvements of 7-10 percentage points. Moreover, expanding the model to these tasks is relatively straightforward, requiring only an additional classifier per task."}),"\n",(0,a.jsx)(i.h3,{id:"impact-of-model-depth-on-performance",children:"Impact of Model Depth on Performance"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"ViLBERT Depth Analysis",src:n(13484).Z+"",width:"1024",height:"194"})}),"\n",(0,a.jsx)(i.p,{children:"As shown above, the authors examine how model depth affects ViLBERT\u2019s performance."}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"VQA and Image Retrieval"}),": Both tasks benefit from deeper ViLBERT models, with performance increasing monotonically with depth, peaking at a depth of 6 layers."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Zero-Shot Image Retrieval"}),": Performance also improves significantly with model depth, suggesting that deeper models may be more suitable for this task."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"VCR and RefCOCO+"}),": These tasks, in contrast, seem to perform better with shallower models, indicating that different tasks may require different model depths for optimal performance."]}),"\n"]}),"\n",(0,a.jsx)(i.admonition,{type:"tip",children:(0,a.jsx)(i.p,{children:"In today\u2019s era of extremely deep models\u2014GPT-3 with its 96 layers, we\u2019re looking at you!\u2014even 2 or 8 layers would still be considered relatively small-scale architectures."})}),"\n",(0,a.jsx)(i.h3,{id:"effect-of-pre-training-data",children:"Effect of Pre-training Data"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.img,{alt:"ViLBERT Pre-training Data Analysis",src:n(93804).Z+"",width:"1024",height:"196"})}),"\n",(0,a.jsx)(i.p,{children:"When pre-trained on random subsets of 25% and 50% of the Conceptual Captions dataset, the model\u2019s accuracy increases as the amount of data increases."}),"\n",(0,a.jsx)(i.p,{children:"This finding supports a common belief in deep learning: more training data typically enhances model performance, as it allows the model more opportunities to learn and extract useful features and patterns."}),"\n",(0,a.jsx)(i.p,{children:"Additionally, this suggests that ViLBERT could potentially benefit from even larger datasets during pre-training, making it a promising direction for future research and applications to further optimize model performance."}),"\n",(0,a.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(i.p,{children:"In the early days of multimodal research, the development of dual-stream architecture marked a major breakthrough, offering a more nuanced perspective on understanding and capturing relationships between images and text."}),"\n",(0,a.jsx)(i.p,{children:"The introduction of ViLBERT represents a milestone in this field, as it attempts to deeply integrate visual content with language, establishing close connections between the two and achieving outstanding performance on various vision-language tasks."})]})}function h(e={}){let{wrapper:i}={...(0,s.a)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},70875:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/arch_vilbert-16638bbbd4cdd4384236b9bb29fcfc8c.jpg"},51072:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img1-3ea8f2e040e4911e3c13d16a03ac1291.jpg"},8194:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/vil_bert_table1-4bccb527b41c66e7cda171e364f53d1b.jpg"},13484:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/vil_bert_table2-662499c8a2ff88bd94a328e1e4893697.jpg"},93804:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/vil_bert_table3-087a820ee7c2681bf93473ca37e16ab2.jpg"},50065:function(e,i,n){n.d(i,{Z:()=>o,a:()=>r});var t=n(67294);let a={},s=t.createContext(a);function r(e){let i=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:i},e.children)}}}]);