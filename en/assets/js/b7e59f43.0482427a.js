"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["84939"],{20640:function(e,n,t){t.r(n),t.d(n,{frontMatter:()=>o,toc:()=>c,default:()=>h,metadata:()=>i,assets:()=>l,contentTitle:()=>a});var i=JSON.parse('{"id":"object-detection/yolov11/index","title":"[24.10] YOLOv11","description":"Engineering-Optimized Edition","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/object-detection/2410-yolov11/index.md","sourceDirName":"object-detection/2410-yolov11","slug":"/object-detection/yolov11/","permalink":"/en/papers/object-detection/yolov11/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1756052813000,"frontMatter":{"title":"[24.10] YOLOv11","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[24.05] YOLOv10","permalink":"/en/papers/object-detection/yolov10/"},"next":{"title":"[24.12] YOLO-Tiny","permalink":"/en/papers/object-detection/yolo-tiny/"}}'),s=t(74848),r=t(84429);let o={title:"[24.10] YOLOv11",authors:"Z. Yuan"},a,l={},c=[{value:"Engineering-Optimized Edition",id:"engineering-optimized-edition",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Problem Solving",id:"problem-solving",level:2},{value:"Backbone: Minor Replacement",id:"backbone-minor-replacement",level:3},{value:"Neck: Minor Feature Fusion Tweaks",id:"neck-minor-feature-fusion-tweaks",level:3},{value:"Head: Extended Output Layer",id:"head-extended-output-layer",level:3},{value:"Multi-Task Integration",id:"multi-task-integration",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Conclusion",id:"conclusion",level:3}];function d(e){let n={a:"a",admonition:"admonition",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"engineering-optimized-edition",children:"Engineering-Optimized Edition"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2410.17725",children:(0,s.jsx)(n.strong,{children:"YOLOv11: An Overview of the Key Architectural Enhancements"})})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"It's rare for Ultralytics to publish a paper whenever they release a new architecture."}),"\n",(0,s.jsx)(n.p,{children:"In any case, we should give them some encouragement."}),"\n",(0,s.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,s.jsxs)(n.p,{children:["In the realm of object detection, the name ",(0,s.jsx)(n.strong,{children:"YOLO"})," has been around for a decade."]}),"\n",(0,s.jsxs)(n.p,{children:['Back in 2015, Redmon came up with the slogan: "',(0,s.jsx)(n.strong,{children:"You Only Look Once"}),'." He decisively ditched the mainstream two-stage detection architecture, boldly redefining detection as a ',(0,s.jsx)(n.strong,{children:"regression problem"}),"\u2014combining classification and localization in a single shot."]}),"\n",(0,s.jsx)(n.p,{children:'Since then, each generation of YOLO has become synonymous with "real-time detection."'}),"\n",(0,s.jsx)(n.p,{children:"However, as the YOLO family evolved, two core issues have become apparent:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The Performance-Speed Dilemma:"})," Small models are fast but inaccurate; large models are accurate but slow."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-task Integration:"})," Tasks like detection, segmentation, pose estimation, and rotated bounding boxes are often required in real-world applications, but past versions could never truly integrate them all."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["So, the Ultralytics team has tried to push YOLO forward into an ",(0,s.jsx)(n.strong,{children:"all-in-one vision system"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Oh? Is that really possible? Let\u2019s take a look."}),"\n",(0,s.jsx)(n.h2,{id:"problem-solving",children:"Problem Solving"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"90%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"model_arch",src:t(72265).A+"",width:"1224",height:"536"})})})}),"\n",(0,s.jsx)(n.p,{children:"YOLOv11 continues the design foundations of YOLOv9 and YOLOv10. The overall architecture still follows the classic three-stage structure:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Backbone \u2192 Neck \u2192 Head"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"There are no radical overhauls here, but with several local tweaks, they attempt to achieve more stable performance in both speed and accuracy."}),"\n",(0,s.jsx)(n.h3,{id:"backbone-minor-replacement",children:"Backbone: Minor Replacement"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"C2f \u2192 C3k2"})}),"\n",(0,s.jsxs)(n.p,{children:["The previously common C2f block is now replaced by the new ",(0,s.jsx)(n.strong,{children:"C3k2 block"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:['The purpose of this change is purely to pursue "',(0,s.jsx)(n.strong,{children:"faster and more efficient"}),'" performance: breaking large convolutions into smaller ones, using smaller kernels to reduce computation and the number of parameters. In practice, C3k2 delivers a better latency-accuracy tradeoff, providing more efficiency without sacrificing too much accuracy.']}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"Let\u2019s visualize the architecture\u2014the overall structure remains mostly the same."}),(0,s.jsx)(n.p,{children:"Essentially, the stacked Bottleneck modules have been replaced by the C3k module."}),(0,s.jsxs)("div",{style:{display:"flex",gap:"2rem",flexWrap:"wrap"},children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{style:{textAlign:"center"},children:" C2f"}),(0,s.jsx)(n.mermaid,{value:"%%{init: {'theme':'base','themeVariables':{\n'fontFamily':'ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, Helvetica, Arial',\n'primaryColor':'#EEF2FF','primaryTextColor':'#0f172a','primaryBorderColor':'#4338ca',\n'lineColor':'#64748b','tertiaryColor':'#F1F5F9'\n}}}%%\n%% -------- C2f\uFF1AFast Dual-Conv Bottleneck --------\nflowchart TD\n    X[\"x<br/>C_in \xd7 H \xd7 W\"]\n    C1[\"1\xd71 Conv<br/>BN + Act<br/>C_in \u2192 2h\"]\n    SPLIT{{split<br/>2h \u2192 h,h}}\n    Y0[\"y0<br/>h \xd7 H \xd7 W\"]\n    Y1[\"y1<br/>h \xd7 H \xd7 W\"]\n\n    subgraph STACK[\"Bottleneck_f \xd7 n\"]\n        direction LR\n        Z1[\"z\u2081\"] --\x3e Z2[\"z\u2082\"] --\x3e Zn[\"z\u2099\"]\n    end\n\n    CAT[\"Concat<br/>y0, y1, z\u2081\u2026z\u2099<br/>(2+n)\xb7h\"]\n    C2[\"1\xd71 Conv<br/>BN + Act<br/>(2+n)\xb7h \u2192 C_out\"]\n    OUT[\"out<br/>C_out \xd7 H \xd7 W\"]\n\n    X --\x3e C1 --\x3e SPLIT\n    SPLIT --\x3e|skip| Y0 --\x3e CAT\n    SPLIT --\x3e|stack| Y1 --\x3e Z1\n    Zn --\x3e CAT\n    CAT --\x3e C2 --\x3e OUT\n\n    classDef tensor fill:#ffffff,stroke:#64748b,stroke-width:1px,rx:6,ry:6;\n    classDef op     fill:#eef2ff,stroke:#4338ca,stroke-width:1px,rx:6,ry:6;\n    classDef fuse   fill:#ecfccb,stroke:#65a30d,stroke-width:1px,rx:6,ry:6;\n    class X,Y0,Y1,Z1,Z2,Zn tensor\n    class C1,SPLIT,STACK,C2 op\n    class CAT fuse"})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{style:{textAlign:"center"},children:" C3k2"}),(0,s.jsx)(n.mermaid,{value:"%%{init: {'theme':'base','themeVariables':{\n'fontFamily':'ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, Helvetica, Arial',\n'primaryColor':'#EEF2FF','primaryTextColor':'#0f172a','primaryBorderColor':'#4338ca',\n'lineColor':'#64748b','tertiaryColor':'#F1F5F9'\n}}}%%\n%% -------- C3k2\uFF1AC2f \u9AA8\u67B6 + C3k \u4F5C\u70BA\u5167\u90E8\u5806\u758A\u55AE\u5143 --------\nflowchart TD\n    X[\"x<br/>C_in \xd7 H \xd7 W\"]\n    C1[\"1\xd71 Conv<br/>BN + Act<br/>C_in \u2192 2h\"]\n    SPLIT{{split<br/>2h \u2192 h,h}}\n    Y0[\"y0<br/>h \xd7 H \xd7 W\"]\n    Y1[\"y1<br/>h \xd7 H \xd7 W\"]\n\n    subgraph STACK[\"C3k \xd7 n\"]\n        direction LR\n        Z1[\"z\u2081\"] --\x3e Z2[\"z\u2082\"] --\x3e Zn[\"z\u2099\"]\n    end\n\n    CAT[\"Concat<br/>y0, y1, z\u2081\u2026z\u2099<br/>(2+n)\xb7h\"]\n    C2[\"1\xd71 Conv<br/>BN + Act<br/>(2+n)\xb7h \u2192 C_out\"]\n    OUT[\"out<br/>C_out \xd7 H \xd7 W\"]\n\n    X --\x3e C1 --\x3e SPLIT\n    SPLIT --\x3e|skip| Y0 --\x3e CAT\n    SPLIT --\x3e|stack| Y1 --\x3e Z1\n    Zn --\x3e CAT\n    CAT --\x3e C2 --\x3e OUT\n\n    classDef tensor fill:#ffffff,stroke:#64748b,stroke-width:1px,rx:6,ry:6;\n    classDef op     fill:#eef2ff,stroke:#4338ca,stroke-width:1px,rx:6,ry:6;\n    classDef fuse   fill:#ecfccb,stroke:#65a30d,stroke-width:1px,rx:6,ry:6;\n    class X,Y0,Y1,Z1,Z2,Zn tensor\n    class C1,SPLIT,STACK,C2 op\n    class CAT fuse"})]})]})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"SPPF + C2PSA"})}),"\n",(0,s.jsxs)(n.p,{children:["After the SPPF module, a ",(0,s.jsx)(n.strong,{children:"C2PSA (Cross Stage Partial with Spatial Attention)"})," is added."]}),"\n",(0,s.jsx)(n.p,{children:"This allows the model to focus a bit more on critical areas of the image, which is helpful for small objects or occluded items."}),"\n",(0,s.jsx)(n.p,{children:'However, this type of attention design isn\u2019t particularly novel\u2014it\'s more of a "keeping up with the trend" kind of choice.'}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"As usual, let\u2019s also visualize this new C2PSA module."}),(0,s.jsxs)("div",{style:{display:"flex",gap:"2rem",flexWrap:"wrap"},children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{style:{textAlign:"center"},children:" C2\u2011PSA"}),(0,s.jsx)(n.mermaid,{value:"%%{init: {'theme':'base','themeVariables':{\n'fontFamily':'ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, Helvetica, Arial',\n'primaryColor':'#EEF2FF','primaryTextColor':'#0f172a','primaryBorderColor':'#4338ca',\n'lineColor':'#64748b','tertiaryColor':'#F1F5F9'\n}}}%%\n%% -------- C2\u2011PSA\uFF1A1\xd71 \u6295\u5F71 \u2192 split \u2192 PSA\xd7n \u2192 concat \u2192 1\xd71 fuse --------\nflowchart TD\n    X[\"x<br/>C \xd7 H \xd7 W\"]\n    C1[\"1\xd71 Conv<br/>BN + Act<br/>C \u2192 2h\"]\n    SPLIT{{split<br/>2h \u2192 h, h}}\n    Y0[\"y0<br/>h \xd7 H \xd7 W\"]\n    Y1[\"y1<br/>h \xd7 H \xd7 W\"]\n\n    subgraph STACK[\"PSA \xd7 n\"]\n        direction LR\n        P1[\"PSA\u2081\"] --\x3e P2[\"PSA\u2082\"] --\x3e Pn[\"PSA\u2099\"]\n    end\n\n    CAT[\"Concat<br/>2h\"]\n    C2[\"1\xd71 Conv<br/>BN + Act<br/>2h \u2192 C\"]\n    OUT[\"out<br/>C \xd7 H \xd7 W\"]\n\n    X --\x3e C1 --\x3e SPLIT\n    SPLIT --\x3e|skip| Y0 --\x3e CAT\n    SPLIT --\x3e|stack| Y1 --\x3e P1\n    Pn --\x3e CAT\n    CAT --\x3e C2 --\x3e OUT\n\n    classDef tensor fill:#ffffff,stroke:#64748b,stroke-width:1px,rx:6,ry:6;\n    classDef op     fill:#eef2ff,stroke:#4338ca,stroke-width:1px,rx:6,ry:6;\n    classDef fuse   fill:#ecfccb,stroke:#65a30d,stroke-width:1px,rx:6,ry:6;\n    class X,Y0,Y1,P1,P2,Pn tensor\n    class C1,SPLIT,STACK,C2 op\n    class CAT fuse"})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{style:{textAlign:"center"},children:" PSA\uFF08Position\u2011Sensitive Attention\uFF09"}),(0,s.jsx)(n.mermaid,{value:"%%{init: {'theme':'base','themeVariables':{\n'fontFamily':'ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, Helvetica, Arial',\n'primaryColor':'#EEF2FF','primaryTextColor':'#0f172a','primaryBorderColor':'#4338ca',\n'lineColor':'#64748b','tertiaryColor':'#F1F5F9'\n}}}%%\n%% -------- PSA\uFF1AMHSA \u2192 Add(res, \u53EF\u9078) \u2192 FFN \u2192 Add(res, \u53EF\u9078) --------\nflowchart TD\n    U[\"u<br/>h \xd7 H \xd7 W\"]\n    MHSA[\"MHSA\uFF08ConvAttention\uFF09<br/>QKV \u2192 Attn \u2192 PE \u2192 Proj\"]\n    ADD1[\"Add\"]\n    FFN[\"FFN<br/>1\xd71 Conv + Act \u2192 1\xd71 Conv (no Act)\"]\n    ADD2[\"Add\"]\n    Z[\"z\uFF08Output\uFF09<br/>h \xd7 H \xd7 W\"]\n\n    U --\x3e MHSA --\x3e ADD1 --\x3e FFN --\x3e ADD2 --\x3e Z\n    U --\x3e|skip| ADD1\n    ADD1 --\x3e|skip| ADD2\n\n    classDef tensor fill:#ffffff,stroke:#64748b,stroke-width:1px,rx:6,ry:6;\n    classDef op     fill:#eef2ff,stroke:#4338ca,stroke-width:1px,rx:6,ry:6;\n    classDef fuse   fill:#ecfccb,stroke:#65a30d,stroke-width:1px,rx:6,ry:6;\n    class U,Z tensor\n    class MHSA,FFN op\n    class ADD1,ADD2 fuse"})]}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{style:{textAlign:"center"},children:" ConvAttention\uFF08PSA \u5167\u90E8 MHSA \u7D30\u7BC0\uFF09"}),(0,s.jsx)(n.mermaid,{value:"%%{init: {'theme':'base','themeVariables':{\n'fontFamily':'ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, Helvetica, Arial',\n'primaryColor':'#EEF2FF','primaryTextColor':'#0f172a','primaryBorderColor':'#4338ca',\n'lineColor':'#64748b','tertiaryColor':'#F1F5F9'\n}}}%%\n%% -------- ConvAttention\uFF1AQKV \u6295\u5F71 \u2192 \u6CE8\u610F\u529B \u2192 \u4F4D\u7F6E\u7DE8\u78BC \u2192 \u8F38\u51FA\u6295\u5F71 --------\nflowchart TD\n    X2[\"x<br/>C x H x W\"]\n    QKV[\"1x1 Conv (QKV proj)<br/>no Act\"]\n    SPLITQKV{{split<br/>Q: H*d_k<br/>K: H*d_k<br/>V: H*d_v}}\n    ATTN[\"Scaled Dot-Product<br/>softmax( transpose(Q) * K / sqrt(d_k) )\"]\n    WSUM[\"Weighted Sum<br/>V * Attn^T\"]\n    RESHAPE[\"Reshape<br/>B x C x H x W\"]\n    PE[\"3x3 Depth-wise Conv<br/>Positional Enc. (no Act)\"]\n    ADDPE[\"Add<br/>positional enc.\"]\n    PROJ[\"1x1 Conv (output proj)<br/>no Act\"]\n    Y2[\"y<br/>C x H x W\"]\n\n    X2 --\x3e QKV --\x3e SPLITQKV\n    SPLITQKV --\x3e|Q| ATTN\n    SPLITQKV --\x3e|K| ATTN\n    SPLITQKV --\x3e|V| WSUM\n    ATTN --\x3e WSUM --\x3e RESHAPE --\x3e ADDPE --\x3e PROJ --\x3e Y2\n    SPLITQKV --\x3e|V| PE --\x3e ADDPE\n\n    classDef tensor fill:#ffffff,stroke:#64748b,stroke-width:1px,rx:6,ry:6;\n    classDef op     fill:#eef2ff,stroke:#4338ca,stroke-width:1px,rx:6,ry:6;\n    classDef fuse   fill:#ecfccb,stroke:#65a30d,stroke-width:1px,rx:6,ry:6;\n    class X2,Y2 tensor\n    class QKV,ATTN,WSUM,RESHAPE,PE,PROJ op\n    class SPLITQKV op\n    class ADDPE fuse"})]})]})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"neck-minor-feature-fusion-tweaks",children:"Neck: Minor Feature Fusion Tweaks"}),"\n",(0,s.jsx)(n.p,{children:"The neck\u2019s role is still to bring together features at different resolutions."}),"\n",(0,s.jsxs)(n.p,{children:["Here, YOLOv11 also switches to the ",(0,s.jsx)(n.strong,{children:"C3k2 block"}),", combined with C2PSA, to make the fusion process slightly more efficient."]}),"\n",(0,s.jsxs)(n.p,{children:["Overall, these changes make YOLOv11\u2019s ",(0,s.jsx)(n.strong,{children:"speed-accuracy curve"})," look a bit better than v8 or v9."]}),"\n",(0,s.jsx)(n.h3,{id:"head-extended-output-layer",children:"Head: Extended Output Layer"}),"\n",(0,s.jsxs)(n.p,{children:["For the head, YOLOv11 doesn\u2019t drastically change the structure\u2014just continues to use the ",(0,s.jsx)(n.strong,{children:"C3k2 block"})," and ",(0,s.jsx)(n.strong,{children:"CBS (Conv-BN-SiLU)"}),", finally outputting bounding boxes, objectness scores, and classification results."]}),"\n",(0,s.jsx)(n.h3,{id:"multi-task-integration",children:"Multi-Task Integration"}),"\n",(0,s.jsx)(n.p,{children:'Compared to previous versions, the most significant change in YOLOv11 isn\u2019t in the detection architecture itself, but in its official positioning as a "multi-task framework."'}),"\n",(0,s.jsx)(n.p,{children:"As shown in the table below:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"90%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"multitask",src:t(29182).A+"",width:"1224",height:"568"})})})}),"\n",(0,s.jsx)(n.p,{children:"Beyond standard object detection, Ultralytics now includes segmentation, pose estimation, rotated bounding boxes (OBB), and classification\u2014all in a unified system, with the same inference, validation, training, and export interfaces."}),"\n",(0,s.jsx)(n.p,{children:"In practical terms, this integration is meaningful. For researchers or industry developers, it used to be necessary to rely on separate toolchains or scattered implementations for detection, segmentation, pose, and so on. Now, with YOLOv11\u2019s unified version, it can all be done in one place."}),"\n",(0,s.jsx)(n.p,{children:"However, note that YOLOv11 isn\u2019t the state-of-the-art in accuracy for every task; more often, it maintains a \u201Cusable and consistent\u201D standard."}),"\n",(0,s.jsx)(n.p,{children:"In other words, its strength isn\u2019t peak performance in a single task, but rather the convergence of various computer vision needs into a single entry point, forming a relatively stable multi-task ecosystem. This positioning also aligns with Ultralytics\u2019 product strategy, making the YOLO series applicable across a broader range of real-world scenarios."}),"\n",(0,s.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"90%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"benchmark",src:t(77019).A+"",width:"1224",height:"504"})})})}),"\n",(0,s.jsx)(n.p,{children:"Looking at the official benchmark, YOLOv11 does appear neater on the speed-accuracy curve than previous generations."}),"\n",(0,s.jsx)(n.p,{children:"Models of various sizes (n, s, m, x) achieve slightly higher mAP on COCO than YOLOv8, YOLOv9, and YOLOv10, while maintaining relatively low latency. Notably, YOLOv11m achieves accuracy close to YOLOv8l, but with fewer parameters and lower FLOPs, demonstrating real effort in efficiency optimization."}),"\n",(0,s.jsx)(n.p,{children:"In the high-latency range, large models like YOLOv11x can reach about 54.5% mAP@50\u201395 with inference times around 13ms; in the low-latency range, small models like YOLOv11s can still maintain around 47% mAP within 2\u20136ms. This distribution allows YOLOv11 to offer suitable model sizes for various real-time requirements\u2014a practical convenience."}),"\n",(0,s.jsx)(n.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"After reading this paper, I think YOLOv11\u2019s contribution is more about engineering refinements than methodological breakthroughs."}),"\n",(0,s.jsx)(n.p,{children:"By switching the backbone, neck, and head entirely to the C3k2 block, it does bring better parameter efficiency and inference speed, with solid results in low-latency scenarios. Still, these changes don\u2019t alter YOLO\u2019s fundamental paradigm\u2014they\u2019re just optimizations of existing designs."}),"\n",(0,s.jsx)(n.p,{children:"Strictly speaking, the magnitude of these improvements doesn\u2019t quite justify being called a \u201Cnext-generation architecture\u201D..."}),"\n",(0,s.jsx)(n.p,{children:"But hey, it\u2019s released, so let\u2019s accept it."})]})}function h(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},72265:function(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/img1-e629b94e319015380b40a4a014c39e04.jpg"},29182:function(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/img2-123e54b138860773908b33da01a27cd6.jpg"},77019:function(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/img3-1c9ae6524a31d16752ec2dfd127bdd9b.jpg"},84429:function(e,n,t){t.d(n,{R:()=>o,x:()=>a});var i=t(96540);let s={},r=i.createContext(s);function o(e){let n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);