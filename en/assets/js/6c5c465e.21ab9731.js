"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["55497"],{74147:function(n,e,t){t.r(e),t.d(e,{metadata:()=>i,contentTitle:()=>d,default:()=>u,assets:()=>c,toc:()=>a,frontMatter:()=>r});var i=JSON.parse('{"id":"docsaidkit/funcs/onnxengine/onnxengine","title":"ONNXEngine","description":"ONNXEngine(modelpath int = 0, backend Dict[str, Any] = {}, provideroption: Dict[str, Any] = {})","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/docsaidkit/funcs/onnxengine/onnxengine.md","sourceDirName":"docsaidkit/funcs/onnxengine","slug":"/docsaidkit/funcs/onnxengine/","permalink":"/en/docs/docsaidkit/funcs/onnxengine/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1713493370000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"ONNXEngine","permalink":"/en/docs/category/onnxengine-1"},"next":{"title":"get_onnx_metadata","permalink":"/en/docs/docsaidkit/funcs/onnxengine/get_onnx_metadata"}}'),o=t("85893"),s=t("50065");let r={sidebar_position:1},d="ONNXEngine",c={},a=[];function l(n){let e={a:"a",blockquote:"blockquote",code:"code",h1:"h1",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"onnxengine",children:"ONNXEngine"})}),"\n",(0,o.jsxs)(e.blockquote,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsxs)(e.a,{href:"https://github.com/DocsaidLab/DocsaidKit/blob/main/docsaidkit/onnxengine/engine.py",children:["ONNXEngine(model_path: Union[str, Path], gpu_id: int = 0, backend: Union[str, int, Backend] = Backend.cpu, session_option: Dict[str, Any] = ",", provider_option: Dict[str, Any] = ",")"]})}),"\n"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Description"})}),"\n",(0,o.jsx)(e.p,{children:"Initialize the ONNX model inference engine."}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Parameters"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"model_path"})," (",(0,o.jsx)(e.code,{children:"Union[str, Path]"}),"): The file name or byte string of the serialized ONNX or ORT format model."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"gpu_id"})," (",(0,o.jsx)(e.code,{children:"int"}),"): GPU ID. Default is 0."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"backend"})," (",(0,o.jsx)(e.code,{children:"Union[str, int, Backend]"}),"): Inference backend, can be ",(0,o.jsx)(e.code,{children:"Backend.cpu"})," or ",(0,o.jsx)(e.code,{children:"Backend.cuda"}),". Default is ",(0,o.jsx)(e.code,{children:"Backend.cpu"}),"."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"session_option"})," (",(0,o.jsx)(e.code,{children:"Dict[str, Any]"}),"): Parameters for onnxruntime.SessionOptions to set session options. Default is ",(0,o.jsx)(e.code,{children:"{}"}),". For detailed configuration, please refer to: ",(0,o.jsx)(e.a,{href:"https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.SessionOptions",children:"SessionOptions"}),"."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"provider_option"})," (",(0,o.jsx)(e.code,{children:"Dict[str, Any]"}),"): Parameters for onnxruntime.provider_options. Default is ",(0,o.jsx)(e.code,{children:"{}"}),". For detailed configuration, please refer to: ",(0,o.jsx)(e.a,{href:"https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#configuration-options",children:"CUDAExecutionProvider"}),"."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Inference"})}),"\n",(0,o.jsx)(e.p,{children:"When loading the model, this function loads information from the ONNX file and gives a dictionary for input and output values, which includes shapes and data types for input and output."}),"\n",(0,o.jsxs)(e.p,{children:["Therefore, when you call an ",(0,o.jsx)(e.code,{children:"ONNXEngine"})," instance, you can directly use this dictionary to obtain the output results."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Example"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import docsaidkit as D\n\nmodel_path = 'model.onnx'\nengine = D.ONNXEngine(model_path)\nprint(engine)\n\n# Inferencing\n# Assuming the model has two inputs and two outputs and named:\n#   'input1', 'input2', 'output1', 'output2'.\ninput_data = {\n    'input1': np.random.randn(1, 3, 224, 224).astype(np.float32),\n    'input2': np.random.randn(1, 3, 224, 224).astype(np.float32),\n}\n\noutputs = engine(**input_data)\n\noutput_data1 = outputs['output1']\noutput_data2 = outputs['output2']\n"})}),"\n"]}),"\n"]})]})}function u(n={}){let{wrapper:e}={...(0,s.a)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(l,{...n})}):l(n)}},50065:function(n,e,t){t.d(e,{Z:function(){return d},a:function(){return r}});var i=t(67294);let o={},s=i.createContext(o);function r(n){let e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function d(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);