"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4234],{57244:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var s=n(74848),t=n(28453);const a={},r="[22.09] FRVT-Twins",o={id:"face-recognition/frvt-distinguishing-twins/index",title:"[22.09] FRVT-Twins",description:"Report on Twin Identification Accuracy",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/face-recognition/2209-frvt-distinguishing-twins/index.md",sourceDirName:"face-recognition/2209-frvt-distinguishing-twins",slug:"/face-recognition/frvt-distinguishing-twins/",permalink:"/en/papers/face-recognition/frvt-distinguishing-twins/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1725678376e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[18.01] CosFace",permalink:"/en/papers/face-recognition/cosface/"},next:{title:"[23.09] TIVC",permalink:"/en/papers/face-recognition/tivc/"}},l={},c=[{value:"Report on Twin Identification Accuracy",id:"report-on-twin-identification-accuracy",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Datasets",id:"datasets",level:2},{value:"Twins Day Dataset 2010-2018",id:"twins-day-dataset-2010-2018",level:3},{value:"Immigration-related Images",id:"immigration-related-images",level:3},{value:"Dataset Limitations",id:"dataset-limitations",level:2},{value:"Small Population",id:"small-population",level:3},{value:"Variations in Identification Codes",id:"variations-in-identification-codes",level:3},{value:"Incorrect/Missing Metadata",id:"incorrectmissing-metadata",level:3},{value:"Imbalanced Data",id:"imbalanced-data",level:3},{value:"Racial Imbalance",id:"racial-imbalance",level:3},{value:"Algorithm Performance Report",id:"algorithm-performance-report",level:2},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"Identical Twins",id:"identical-twins",level:3},{value:"Algorithms Capable of Identifying Identical Twins",id:"algorithms-capable-of-identifying-identical-twins",level:3},{value:"Comprehensive Analysis",id:"comprehensive-analysis",level:2},{value:"Score Distributions",id:"score-distributions",level:3},{value:"Age Effects",id:"age-effects",level:3},{value:"Longitudinal Effects",id:"longitudinal-effects",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Using Higher Resolution Images",id:"using-higher-resolution-images",level:3}];function d(e){const i={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"2209-frvt-twins",children:"[22.09] FRVT-Twins"})}),"\n",(0,s.jsx)(i.h2,{id:"report-on-twin-identification-accuracy",children:"Report on Twin Identification Accuracy"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8439.pdf",children:(0,s.jsx)(i.strong,{children:"FRVT: Face Recognition Verification Accuracy on Distinguishing Twins (NIST.IR.8439)"})})}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.p,{children:"This is not a paper but a technical report."}),"\n",(0,s.jsx)(i.p,{children:"The NIST conducted research on distinguishing twins within the scope of the FRVT (Face Recognition Vendor Test)."}),"\n",(0,s.jsx)(i.p,{children:"This is an intriguing issue because the high similarity between twins can pose significant challenges for facial recognition systems."}),"\n",(0,s.jsx)(i.p,{children:'In fact, "distinguishing twins" is an exceedingly challenging task for facial recognition systems.'}),"\n",(0,s.jsx)(i.admonition,{type:"tip",children:(0,s.jsxs)(i.p,{children:["For more information about NIST, you can refer to ",(0,s.jsx)(i.a,{href:"https://docsaid.org/blog/nist-and-frvt/",children:(0,s.jsx)(i.strong,{children:"NIST & FRVT"})}),"."]})}),"\n",(0,s.jsx)(i.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(i.p,{children:"Facial recognition technology is increasingly used in both public and private sectors for identity verification, transaction authorization, and access control."}),"\n",(0,s.jsx)(i.p,{children:"Over the past decade, accuracy in the FRVT evaluations has significantly improved, supporting these applications."}),"\n",(0,s.jsx)(i.p,{children:"Since the COVID-19 pandemic, some algorithms have even been able to recognize individuals wearing masks."}),"\n",(0,s.jsx)(i.p,{children:"However, despite these advancements, the identification of twins remains problematic."}),"\n",(0,s.jsx)(i.p,{children:"This report documents the results of the latest algorithms in this regard."}),"\n",(0,s.jsx)(i.h2,{id:"datasets",children:"Datasets"}),"\n",(0,s.jsx)(i.h3,{id:"twins-day-dataset-2010-2018",children:"Twins Day Dataset 2010-2018"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.a,{href:"https://biic.wvu.edu/data-sets/twins-day-dataset-2010-1015",children:(0,s.jsx)(i.strong,{children:"Download"})})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"This dataset, from West Virginia University's Center for Identification Technology Research, includes images from Twins Days from 2010 to 2018."}),"\n",(0,s.jsx)(i.p,{children:"These collections were conducted under IRB protocols with informed consent from each participant."}),"\n",(0,s.jsx)(i.p,{children:"The image sizes vary by year:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"2010: 2848x4288"}),"\n",(0,s.jsx)(i.li,{children:"2011: 3744x5616"}),"\n",(0,s.jsx)(i.li,{children:"2012 to 2018: 3300x4400 and 2400x3200."}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The images are high-quality frontal portraits, meeting NIST SAP50 specifications (head and shoulder composition) and SAP51 specifications (head composition)."}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Sample Image: Different Individuals"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"different",src:n(73695).A+"",width:"1224",height:"404"})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Sample Image: Same Individual"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"same",src:n(6320).A+"",width:"1224",height:"484"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"immigration-related-images",children:"Immigration-related Images"}),"\n",(0,s.jsx)(i.p,{children:"This dataset includes real-time captured images, primarily from webcam images and some visa application images."}),"\n",(0,s.jsx)(i.p,{children:"Webcam images are taken by operators under time constraints, leading to variations in roll, pitch, and yaw angles, with sometimes overexposed faces due to bright backgrounds. The average inter-pupil distance is 38 pixels, not conforming to ISO/IEC 19794-5 full-frontal image types."}),"\n",(0,s.jsx)(i.p,{children:"Some subjects also have visa application images captured in interview environments using dedicated equipment and lighting. These images conform to ISO/IEC 19794-5 standards with a capture size of 300x300 and nearly frontal poses."}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Example Image"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"img3",src:n(51213).A+"",width:"1020",height:"402"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"dataset-limitations",children:"Dataset Limitations"}),"\n",(0,s.jsx)(i.h3,{id:"small-population",children:"Small Population"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The Twins Days dataset is relatively small compared to other NIST FRVT 1:1 datasets, containing over 5900 images."}),"\n",(0,s.jsx)(i.li,{children:"Due to inconsistent or missing metadata, many images and twins were excluded, including some incorrect matches (e.g., A and B are twins, but B and C are also recorded as twins)."}),"\n",(0,s.jsx)(i.li,{children:"Most of the immigration-related dataset images were retained, including 152 pairs of twins and 2,478 images."}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"variations-in-identification-codes",children:"Variations in Identification Codes"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Twins Days usually assigns the same identification code to the same participants each year, but sometimes a new code is issued."}),"\n",(0,s.jsx)(i.li,{children:"This results in some high-threshold non-twin comparisons that are actually correct matches being incorrectly marked as errors."}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"incorrectmissing-metadata",children:"Incorrect/Missing Metadata"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Metadata for Twins Days collections from 2011 to 2016 is partially or entirely missing."}),"\n",(0,s.jsx)(i.li,{children:"Incorrect metadata, such as twin type or birthdate mismatches, led to the exclusion of many images."}),"\n",(0,s.jsx)(i.li,{children:"The immigration-related image dataset lacks information to distinguish fraternal from identical twins."}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"imbalanced-data",children:"Imbalanced Data"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Twins Days images have an uneven distribution of twin types: 2.8% fraternal opposite-sex, 6.7% fraternal same-sex, and 90.5% identical twins."}),"\n",(0,s.jsx)(i.li,{children:"Age group distribution is also uneven, with most images belonging to the 20-39 age range, and very few from the 40-59 and 60+ age groups."}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"racial-imbalance",children:"Racial Imbalance"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The Twins Days dataset includes racial identity: 85% White, 10% African American, and 5% others."}),"\n",(0,s.jsx)(i.li,{children:"The number of racial groups is too numerous for meaningful analysis based on race."}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"algorithm-performance-report",children:"Algorithm Performance Report"}),"\n",(0,s.jsx)(i.p,{children:"With FRVT being open to global participation, many algorithms are involved."}),"\n",(0,s.jsx)(i.p,{children:"This report documents the results of 478 algorithms submitted to the FRVT 1:1 test from 2019 to mid-February 2022."}),"\n",(0,s.jsx)(i.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"False Match Rate (FMR)"})}),"\n",(0,s.jsx)(i.p,{children:"The False Match Rate (FMR) is the proportion of incorrect matches among all attempts, where faces of different people are wrongly recognized as the same person."}),"\n",(0,s.jsx)(i.p,{children:"This is a crucial performance metric in facial recognition systems, especially in high-security applications like banking or airport security."}),"\n",(0,s.jsx)(i.p,{children:"Setting FMR = 0.0001 means only one incorrect match in every 10,000 attempts."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"False Non-Match Rate (FNMR)"})}),"\n",(0,s.jsx)(i.p,{children:"The False Non-Match Rate (FNMR) is the proportion of incorrect non-matches among all attempts, where faces of the same person are wrongly recognized as different people."}),"\n",(0,s.jsx)(i.p,{children:"This is also an important performance metric, particularly in applications concerning user convenience and experience."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"FNMR @ FMR = 0.0001"})}),"\n",(0,s.jsx)(i.p,{children:'When you see "FNMR @ FMR = 0.0001," it means observing the False Non-Match Rate (FNMR) when the False Match Rate (FMR) is set to 0.0001.'}),"\n",(0,s.jsx)(i.p,{children:"This measures how well the facial recognition system performs under extremely low false match conditions, ensuring effective recognition of the same person under stringent false match settings."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"identical-twins",children:"Identical Twins"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"For the Twins Days data, the FMR for identical twins usually exceeds 0.99, indicating these twins are almost always incorrectly recognized as each other by algorithms."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"FMRs below 0.99 generally result from two situations:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"System Errors"}),": Algorithms with high Failure to Enroll (FTE) rates failing to extract features from many or all images."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Poor Algorithms"}),": Systems with high False Non-Match Rates (FNMR), producing low scores, meaning the algorithm sees everything as different people."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"algorithms-capable-of-identifying-identical-twins",children:"Algorithms Capable of Identifying Identical Twins"}),"\n",(0,s.jsx)(i.p,{children:"Some algorithms show high accuracy in recognizing identical twins, not always mistakenly identifying identical twins as the same person."}),"\n",(0,s.jsx)(i.p,{children:"These algorithms include:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Algorithms"}),": aigen-001, aigen-002, beyneai-000, glory-004, mobai-000, and iqface-001."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Performance Metrics"}),":","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"FNMR (False Non-Match Rate)"}),": \u2264 0.02"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"FTE (Failure to Enroll Rate)"}),": \u2264 0.02"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"FMR (False Match Rate)"}),": \u2264 0.7"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Specifically, aigen-002 is noteworthy, with an FMR of 0.475 for identical twins, meaning this algorithm correctly distinguishes twins about half the time. While this FMR is much higher than the ideal standard (FMR = 0.0001), it is significantly lower than the 0.98 and 0.99 FMRs of most other algorithms, indicating better performance in distinguishing identical twins."}),"\n",(0,s.jsx)(i.h2,{id:"comprehensive-analysis",children:"Comprehensive Analysis"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"result",src:n(35014).A+"",width:"1224",height:"628"})}),"\n",(0,s.jsx)(i.h3,{id:"score-distributions",children:"Score Distributions"}),"\n",(0,s.jsx)(i.p,{children:"The figure above shows score distributions for two accurate and representative algorithms across different photo types."}),"\n",(0,s.jsx)(i.p,{children:'The highest scores come from "mated twins," indicating comparisons between photos of the same individual taken on the same day.'}),"\n",(0,s.jsx)(i.p,{children:'The next highest scores are from "mated mugshot," ordinary face images used as a control group.'}),"\n",(0,s.jsx)(i.p,{children:'Comparisons between identical twins (labeled "non-mated identical twins") also receive similarly high scores.'}),"\n",(0,s.jsx)(i.p,{children:'Scores for same-sex fraternal twins (labeled "non-mated fraternal same-sex twins") are also close to these high scores.'}),"\n",(0,s.jsx)(i.p,{children:"These scores are all above the threshold set in the report, designed to yield a false match rate (FMR=0.0001) for a set of unrelated face pairs."}),"\n",(0,s.jsxs)(i.admonition,{type:"tip",children:[(0,s.jsx)(i.p,{children:"Although twin scores are slightly lower than same individual scores, these results are still far above the model threshold, highlighting the challenge of distinguishing twins from non-twins for algorithms."}),(0,s.jsx)(i.p,{children:"Additionally, the report notes that simply raising the threshold to reduce false matches between twins is ineffective, as the issue lies in the fundamental design of the algorithms, not the threshold settings."})]}),"\n",(0,s.jsx)(i.h3,{id:"age-effects",children:"Age Effects"}),"\n",(0,s.jsx)(i.p,{children:"Analyzing different age groups of twins, both datasets show a decrease in similarity scores among unrelated twins in older age groups."}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Twins Days dataset"})," categorizes twins into four age groups: 0-19, 20-39, 40-59, and 60+. Analysis shows lower similarity scores in the oldest age group for both fraternal and identical same-sex twins compared to the youngest age group."]}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Immigration-related dataset"})," includes all same-sex twins, divided into three age groups: 0-19, 20-39, and 40-59. In this dataset, similarity scores for the 0-19 age group are closer to those of related twins, while scores for the 40-59 age group are significantly lower, diverging more from related twins' scores."]}),"\n",(0,s.jsx)(i.p,{children:"Despite lower similarity scores in older age groups, these scores still exceed the algorithm thresholds, indicating algorithms still consider unrelated twins as matches. Therefore, algorithms fail to effectively distinguish between fraternal and identical twins."}),"\n",(0,s.jsx)(i.admonition,{type:"tip",children:(0,s.jsx)(i.p,{children:"The failure to distinguish fraternal twins is particularly surprising, as fraternal twins have different genomes and should exhibit different facial features. This suggests algorithms may also struggle to accurately differentiate siblings with close birth years."})}),"\n",(0,s.jsx)(i.h3,{id:"longitudinal-effects",children:"Longitudinal Effects"}),"\n",(0,s.jsx)(i.p,{children:"Despite improvements in facial recognition algorithm performance, the ability to distinguish same-sex twins has not improved. Algorithms that perform well on general face data do not show improved or decreased performance when comparing same-sex fraternal and identical twins in the Twins Days dataset."}),"\n",(0,s.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(i.p,{children:"Considering NIST's Face Recognition Vendor Test (FRVT) performance and its ability to distinguish between identical and fraternal twins, current facial recognition technology faces significant challenges, particularly in distinguishing genetically similar identical twins."}),"\n",(0,s.jsx)(i.p,{children:"Despite lower genetic similarity and potential greater facial differences among fraternal twins, algorithms still struggle to differentiate them, which could lead to catastrophic consequences in high-accuracy-required scenarios."}),"\n",(0,s.jsx)(i.p,{children:"For future improvements, the report suggests one main point: utilizing higher resolution images to enhance facial recognition system performance."}),"\n",(0,s.jsx)(i.h3,{id:"using-higher-resolution-images",children:"Using Higher Resolution Images"}),"\n",(0,s.jsx)(i.p,{children:"Higher resolution images can provide more detailed facial skin features, such as skin texture and pore patterns, which are unique to individuals and could help distinguish even genetically identical twins."}),"\n",(0,s.jsxs)(i.admonition,{type:"tip",children:[(0,s.jsx)(i.p,{children:"The effectiveness of this approach has been demonstrated by a 2004 patented algorithm (US Patent: US7369685B2)."}),(0,s.jsx)(i.p,{children:"This algorithm accurately distinguished twins by analyzing skin texture visible in high-resolution images."})]}),"\n",(0,s.jsx)(i.p,{children:"To achieve this, future research and development should focus on improving the quality of image capture, ensuring an inter-pupil distance of at least 120 pixels, and conforming to ISO standard frontal portraits for skin texture-based analysis. Additionally, mainstream neural network models often downsample input images to improve processing speed, but this loses crucial detail information, affecting recognition accuracy (especially in highly similar cases like twins)."}),"\n",(0,s.jsx)(i.p,{children:"These are the directions for future improvements, and we hope to see better performance in recognizing twins in future FRVTs."})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},73695:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/img1-dfa583b2ef01c9ed77b3e35eb46a84e0.jpg"},6320:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/img2-bda0f61a05a0592d5668920b7feaaaae.jpg"},51213:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/img3-a3286ebe3d17a383b5e6b0513f88aa58.jpg"},35014:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/img4-1adafa7b621a164d28b0c32ddb10b599.jpg"},28453:(e,i,n)=>{n.d(i,{R:()=>r,x:()=>o});var s=n(96540);const t={},a=s.createContext(t);function r(e){const i=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:i},e.children)}}}]);