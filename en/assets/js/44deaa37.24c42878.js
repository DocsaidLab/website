"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["62733"],{71120:function(e,n,i){i.r(n),i.d(n,{frontMatter:()=>s,default:()=>h,contentTitle:()=>o,assets:()=>l,toc:()=>c,metadata:()=>t});var t=JSON.parse('{"id":"docclassifier/intro","title":"Introduction","description":"In past project experiences, classification models have been some of the most common machine learning tasks.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/docclassifier/intro.md","sourceDirName":"docclassifier","slug":"/docclassifier/intro","permalink":"/en/docs/docclassifier/intro","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1736245697000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"DocClassifier","permalink":"/en/docs/docclassifier/"},"next":{"title":"Installation","permalink":"/en/docs/docclassifier/installation"}}'),r=i(85893),a=i(50065);let s={sidebar_position:1},o="Introduction",l={},c=[{value:"Metric Learning",id:"metric-learning",level:2},{value:"Application Scenarios",id:"application-scenarios",level:3},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Why Not Contrastive Learning?",id:"why-not-contrastive-learning",level:2},{value:"Benefits of Contrastive Learning",id:"benefits-of-contrastive-learning",level:3},{value:"But There Are Drawbacks",id:"but-there-are-drawbacks",level:3},{value:"Final Thoughts",id:"final-thoughts",level:2}];function d(e){let n={a:"a",admonition:"admonition",blockquote:"blockquote",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,r.jsx)(n.p,{children:"In past project experiences, classification models have been some of the most common machine learning tasks."}),"\n",(0,r.jsx)(n.p,{children:"Classification models are not difficult to implement. First, we build a backbone, then map the final output to multiple specific categories. Finally, we evaluate the model\u2019s performance using several metrics, such as accuracy, recall, F1-score, and so on."}),"\n",(0,r.jsx)(n.p,{children:"Although this may sound straightforward, in practical applications, we often encounter some issues. Let\u2019s take the topic of this project as an example:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Category Definition"})}),"\n",(0,r.jsx)(n.p,{children:"In classification tasks, if the categories we define are highly similar, the model may have difficulty distinguishing between them. For example, \u201CCompany A insurance document\u201D and \u201CCompany B insurance document.\u201D Both categories belong to documents from a company, and their differences may be minimal, making it hard for the model to distinguish between the two."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Data Imbalance"})}),"\n",(0,r.jsx)(n.p,{children:"In most scenarios, data collection can be the most challenging problem, especially when dealing with documents containing personal information. Data imbalance further leads to poor prediction performance for minority categories."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Data Augmentation"})}),"\n",(0,r.jsx)(n.p,{children:"In our daily life, we are surrounded by a large number of documents, and we constantly want to add more document categories."}),"\n",(0,r.jsx)(n.p,{children:"However, every time we add a new category, the entire model needs to be retrained or fine-tuned, which incurs high costs, including data collection, labeling, retraining, reevaluation, deployment, and so on. All of these processes must be repeated."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sub-labels for Categories"})}),"\n",(0,r.jsx)(n.p,{children:"Customers' needs are often unpredictable."}),"\n",(0,r.jsx)(n.p,{children:"Let\u2019s assume there\u2019s a customer who first defines a document type, let\u2019s call it Document A. Then, the customer wants to add more sub-labels for Document A, such as:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Damaged Document A"}),"\n",(0,r.jsx)(n.li,{children:"Glare Document A"}),"\n",(0,r.jsx)(n.li,{children:"First-generation format Document A"}),"\n",(0,r.jsx)(n.li,{children:"Second-generation format Document A"}),"\n",(0,r.jsx)(n.li,{children:"..."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Let\u2019s not even discuss the fact that adding each sub-label would require retraining the model."}),"\n",(0,r.jsx)(n.p,{children:"From the perspective of model engineering, it\u2019s \u201Cirrational\u201D to treat these labels as independent categories since they all belong to Document A. Likewise, it\u2019s \u201Cunreasonable\u201D to treat them as a multi-class problem because sub-labels corresponding to different document formats may vary."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsx)(n.p,{children:"So you might think: Since we can\u2019t solve the problem, let\u2019s solve the person who raised it!"}),(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"No, you can\u2019t!"}),"\n"]}),(0,r.jsx)(n.p,{children:"This is a machine learning problem."})]}),"\n",(0,r.jsx)(n.h2,{id:"metric-learning",children:"Metric Learning"}),"\n",(0,r.jsxs)(n.p,{children:["Stepping out of the document classification problem, you\u2019ll realize that what we\u2019re really discussing here is ",(0,r.jsx)(n.strong,{children:"Metric Learning"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"The primary goal of metric learning is to learn the optimal distance measure to evaluate the similarity between samples. In traditional machine learning, metric learning typically involves mapping data from the original feature space to a new feature space, where similar objects are closer together and dissimilar objects are farther apart. This is usually achieved by learning a distance function that better reflects the true similarity between samples."}),"\n",(0,r.jsxs)(n.p,{children:["To summarize in one sentence: ",(0,r.jsx)(n.strong,{children:"Metric learning is a method for learning similarity"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"application-scenarios",children:"Application Scenarios"}),"\n",(0,r.jsxs)(n.p,{children:["A well-known application of metric learning is ",(0,r.jsx)(n.strong,{children:"Face Recognition"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"As mentioned earlier, the number of faces continues to grow, and we can\u2019t always retrain the model. Therefore, by using a metric learning framework, we can learn a better distance function to improve the accuracy of face recognition."}),"\n",(0,r.jsx)(n.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,r.jsx)(n.p,{children:"Although not every classification problem is suited to elevate to the level of metric learning, in this project, the weapon of metric learning can indeed help us overcome the obstacles mentioned earlier."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Obstacle 1: Category Definition"})}),"\n",(0,r.jsx)(n.p,{children:"The goal of our learning is to learn a better distance function, which helps us better distinguish similar categories. Therefore, we no longer need to define categories. The objects we want to classify will ultimately only become registered data."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Obstacle 2: Data Imbalance"})}),"\n",(0,r.jsx)(n.p,{children:"We no longer need to collect massive amounts of data because our model doesn\u2019t rely on large samples. We only need one sample, which serves as our registered data, and other parts can be trained using different training data."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Obstacle 3: Category Expansion"})}),"\n",(0,r.jsx)(n.p,{children:"Expanding categories only requires registering new data, without the need to retrain the model. This design greatly reduces the training cost."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Obstacle 4: Sub-labels for Categories"})}),"\n",(0,r.jsx)(n.p,{children:"This issue can be well addressed within the metric learning framework. We can treat sub-labels as new registered data, which will not affect the original model. The distance between sub-labels and the main label in the feature space may be close, but not identical, thus effectively distinguishing these two categories."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:["We initially introduced the metric learning framework: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2203.15565",children:(0,r.jsx)(n.strong,{children:"PartialFC"})}),", which combines technologies like ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1801.09414",children:(0,r.jsx)(n.strong,{children:"CosFace"})})," and ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1801.07698",children:(0,r.jsx)(n.strong,{children:"ArcFace"})}),", enabling precise classification without the need for pre-defined categories."]}),"\n",(0,r.jsxs)(n.p,{children:["Next, in further experiments, we introduced the ",(0,r.jsx)(n.a,{href:"https://www.image-net.org/",children:(0,r.jsx)(n.strong,{children:"ImageNet-1K dataset"})})," and ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2103.00020",children:(0,r.jsx)(n.strong,{children:"CLIP model"})}),". We used ImageNet-1K as the base, treating each image as a category. This operation expanded the number of categories to about 1.3 million, providing the model with richer visual variations and increasing data diversity."]}),"\n",(0,r.jsx)(n.p,{children:"In the TPR@FPR=1e-4 benchmark, the performance improved by about 4.1% (77.2% -> 81.3%) compared to the baseline model. By introducing the CLIP model on top of ImageNet-1K, and conducting knowledge distillation during training, the performance improved by an additional 4.6% (81.3% -> 85.9%) in the same benchmark."}),"\n",(0,r.jsx)(n.p,{children:"In the latest experiments, we combined BatchNorm and LayerNorm, achieving encouraging results. On top of the CLIP distillation model, the TPR@FPR=1e-4 performance improved by about 4.4% (85.9% -> 90.3%)."}),"\n",(0,r.jsx)(n.h2,{id:"why-not-contrastive-learning",children:"Why Not Contrastive Learning?"}),"\n",(0,r.jsx)(n.p,{children:"Contrastive Learning and Metric Learning are both methods for learning the similarity between samples."}),"\n",(0,r.jsx)(n.p,{children:"So why did we choose not to use contrastive learning this time?"}),"\n",(0,r.jsx)(n.p,{children:"It\u2019s not because it\u2019s not good; we just believe that at this stage, metric learning is a better fit."}),"\n",(0,r.jsx)(n.h3,{id:"benefits-of-contrastive-learning",children:"Benefits of Contrastive Learning"}),"\n",(0,r.jsx)(n.p,{children:"The greatest advantage of contrastive learning is its ability to handle unlabeled data well. For scenarios where data labeling is difficult or the dataset is enormous, it\u2019s essentially a \u201Clifesaver.\u201D"}),"\n",(0,r.jsx)(n.p,{children:"Moreover, it excels at learning general features, which can be applied not only to classification tasks but also across tasks, such as object detection, semantic segmentation, and so on."}),"\n",(0,r.jsx)(n.h3,{id:"but-there-are-drawbacks",children:"But There Are Drawbacks"}),"\n",(0,r.jsx)(n.p,{children:"First, contrastive learning heavily relies on the design of negative samples. If the selection of negative samples is poor, either too simple or too complex, the model\u2019s training performance may suffer."}),"\n",(0,r.jsx)(n.p,{children:"Additionally, contrastive learning has high resource requirements because it needs a large number of negative samples to help the model understand \u201Cwhat is different,\u201D leading to high computational costs. This is especially true when large training batches are required to provide enough negative samples, posing a challenge for our hardware resources."}),"\n",(0,r.jsx)(n.p,{children:"Furthermore, contrastive learning is limited by its self-supervised design for unlabeled data, so it\u2019s difficult for the model to learn highly precise features (such as an error rate of one in ten thousand). This is evident in the leaderboard of face recognition, where metric learning methods still dominate."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:"In conclusion, we chose \u201Cmetric learning\u201D to solve the problem. In the future, we\u2019ll allocate time to explore the application of contrastive learning and may even combine the strengths of both methods, allowing the model to learn general features while possessing powerful similarity judgment abilities."}),"\n",(0,r.jsx)(n.h2,{id:"final-thoughts",children:"Final Thoughts"}),"\n",(0,r.jsx)(n.p,{children:"In testing, our model demonstrated over 90% accuracy at a TPR@FPR=1e-4 error rate, with no need to retrain when adding new category types."}),"\n",(0,r.jsx)(n.p,{children:"In simple terms, we\u2019ve essentially transferred the operation process from a face recognition system over to this!"}),"\n",(0,r.jsx)(n.p,{children:"During the development, we often jokingly asked ourselves, \u201CCan this really work?\u201D As mentioned earlier, the first-generation framework (first author) had some effect but was still unstable. By the time this project was released, it was already the third-generation model (second author), and the overall performance showed significant improvement, making it a solid result."}),"\n",(0,r.jsx)(n.p,{children:"Compared to our previous \u201Cstandard\u201D projects, this one is full of fun."}),"\n",(0,r.jsx)(n.p,{children:"Therefore, we\u2019ve decided to release the project\u2019s framework and experimental results, hoping it will inspire you. If you also find a new application scenario from the design concepts of this project, feel free to share it with us."})]})}function h(e={}){let{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},50065:function(e,n,i){i.d(n,{Z:()=>o,a:()=>s});var t=i(67294);let r={},a=t.createContext(r);function s(e){let n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);