"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[9714],{60921:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var t=i(74848),s=i(28453);const a={},r="[21.01] VinVL",o={id:"multimodality/vinvl/index",title:"[21.01] VinVL",description:"Revisiting Oscar",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/multimodality/2101-vinvl/index.md",sourceDirName:"multimodality/2101-vinvl",slug:"/multimodality/vinvl/",permalink:"/en/papers/multimodality/vinvl/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1721322974e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[20.12] UNIMO",permalink:"/en/papers/multimodality/unimo/"},next:{title:"[21.02] ViLT",permalink:"/en/papers/multimodality/vilt/"}},l={},c=[{value:"Revisiting Oscar",id:"revisiting-oscar",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Object Detection Pretraining",id:"object-detection-pretraining",level:3},{value:"Attribute Injection",id:"attribute-injection",level:3},{value:"Improving Efficiency",id:"improving-efficiency",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Performance on Downstream Tasks",id:"performance-on-downstream-tasks",level:3},{value:"How Important Are Visual Features?",id:"how-important-are-visual-features",level:3},{value:"Impact of Visual Concept Diversity",id:"impact-of-visual-concept-diversity",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"2101-vinvl",children:"[21.01] VinVL"}),"\n",(0,t.jsx)(n.h2,{id:"revisiting-oscar",children:"Revisiting Oscar"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2101.00529",children:(0,t.jsx)(n.strong,{children:"VinVL: Revisiting Visual Representations in Vision-Language Models"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"The following content was compiled by ChatGPT-4, edited and supplemented with manual corrections and explanations."})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"After examining Oscar, further research promptly followed."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Portal: ",(0,t.jsx)(n.a,{href:"/en/papers/multimodality/oscar/",children:(0,t.jsx)(n.strong,{children:"Oscar"})})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This paper acknowledges Oscar's contributions but also highlights its shortcomings:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"It lacks a thorough exploration of visual representations."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'So, what constitutes a "thorough" exploration? Let\'s delve into this paper to find out.'}),"\n",(0,t.jsx)(n.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"VinVL demo",src:i(69541).A+"",width:"1224",height:"416"})}),"\n",(0,t.jsx)(n.p,{children:"The authors focus on the field of Visual-Language Pretraining (VLP), specifically on enhancing the visual representations generated by Object Detection (OD) models. The effectiveness and richness of visual features are crucial for the performance of models in various Vision-Language (VL) tasks. Here are the main points of concern:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Need for Rich Visual Representations"})}),"\n",(0,t.jsx)(n.p,{children:"Current VLP methods heavily rely on visual representations provided by object detection models. The authors argue that while these models offer valuable visual information for VL tasks, there is room for improvement, particularly in handling complex, diverse, and semantically rich image scenes."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Effectiveness of Cross-Modal Fusion Models"})}),"\n",(0,t.jsx)(n.p,{children:"VLP typically consists of two components: a pretrained object detection model and a cross-modal fusion model, which combines visual and language features. While much research focuses on improving the cross-modal model, the authors emphasize that stronger, semantically richer visual representations are equally critical."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Limitations of Existing Object Detection Models"})}),"\n",(0,t.jsx)(n.p,{children:'The authors note that the widely used OD models are often treated as a "black box" in literature and may have limitations in their training datasets and objectives, impacting their performance in various VL tasks.'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Diversity of Visual Objects and Concepts"})}),"\n",(0,t.jsx)(n.p,{children:"In many practical applications, models must recognize and understand a wide range of visual objects and concepts. Current models, especially those trained on datasets like Open Images, may not fully capture and express this rich diversity of visual content."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"VinVL Model Design",src:i(47392).A+"",width:"1224",height:"484"})}),"\n",(0,t.jsx)(n.p,{children:"In this paper, the authors focus on optimizing visual features within the existing Oscar framework, without proposing a new structure."}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language models typically consist of two main components: a Vision module (which translates images into semantic representations) and a VL module (which integrates visual and language inputs). Here\u2019s a breakdown of how the paper aims to improve the Vision module, particularly through enhancements in object detection pretraining and the incorporation of attribute information:"}),"\n",(0,t.jsx)(n.h3,{id:"object-detection-pretraining",children:"Object Detection Pretraining"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Data Processing"})}),"\n",(0,t.jsx)(n.p,{children:"The authors use four major public datasets (COCO, OpenImagesV5, Objects365V1, and Visual Genome) for pretraining the object detection model. They employ strategies like class-aware sampling and balancing for imbalanced datasets to create a unified, large corpus."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Model Architecture"})}),"\n",(0,t.jsx)(n.p,{children:"While the Feature Pyramid Network (FPN) performs better in object detection tasks, the C4 model offers more effective region features for VL tasks."}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What is the C4 model?"})}),(0,t.jsx)(n.p,{children:"The C4 model refers to features extracted from the CNN architecture at 1/16 resolution. This terminology was adopted from a referenced paper, where it was argued that features at this scale enhance VL task performance compared to FPN."})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pretraining Strategy"})}),"\n",(0,t.jsx)(n.p,{children:"The authors combine classical and experiential methods to enhance the model's performance."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Freezing initial layers (including the first convolutional layer, the first residual block, and all batch normalization layers) to preserve low-level visual features during early training."}),"\n",(0,t.jsx)(n.li,{children:"Applying data augmentation techniques like horizontal flipping and multi-scale training to expand the dataset and improve model generalization."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'The object detection model\'s basic structure is the "X152-C4 architecture," with initial weights based on the ImageNet-5K dataset, trained over 1.8 million iterations with a batch size of 16 images.'}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"attribute-injection",children:"Attribute Injection"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Pretrained Model and Attribute Branch"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The pretrained OD model is enhanced with a new branch to predict object attributes, thereby enabling simultaneous object detection and attribute prediction."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Fine-Tuning on Visual Genome (VG) Dataset"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Attributes: The VG dataset provides 524 attribute categories (e.g., color, shape, size)."}),"\n",(0,t.jsx)(n.li,{children:"Fine-Tuning Strategy: After adding the attribute branch, the model is fine-tuned on the VG dataset to enhance attribute prediction capabilities."}),"\n",(0,t.jsx)(n.li,{children:"Attribute Loss Weight Adjustment: Increasing the attribute loss weight from 0.5 to 1.25 during fine-tuning to emphasize the importance of attribute learning."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This approach allows the model to not only detect objects but also predict multiple attributes on the VG dataset, demonstrating significant advantages over previous models."}),"\n",(0,t.jsx)(n.h3,{id:"improving-efficiency",children:"Improving Efficiency"}),"\n",(0,t.jsx)(n.p,{children:"Object detection models face computational challenges due to the diversity of visual features and attributes, especially during feature extraction."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Non-Maximum Suppression (NMS) Challenge"})}),"\n",(0,t.jsx)(n.p,{children:"The diversity of visual objects and attributes requires extensive computational resources for class-aware NMS during post-processing, slowing down feature extraction."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Efficiency Strategies"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Using class-agnostic NMS: Performing a single NMS operation to reduce computational complexity and maintain high operational efficiency."}),"\n",(0,t.jsx)(n.li,{children:"Adjusting Convolutional Layer Settings: Replacing dilated convolutions with non-dilated ones to further optimize computation."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These adjustments significantly improve the speed of the region feature extraction process without compromising accuracy in VL downstream tasks."}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What is Dilated Convolution?"})}),(0,t.jsx)(n.p,{children:"Dilated Convolution, also known as atrous convolution, introduces gaps within the convolution kernel to expand its receptive field. This technique allows each output feature to be calculated from a wider range of the input feature map, capturing more extensive information while increasing computational complexity."})]}),"\n",(0,t.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(n.h3,{id:"performance-on-downstream-tasks",children:"Performance on Downstream Tasks"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"VinVL results",src:i(8289).A+"",width:"2554",height:"450"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"To evaluate model parameter efficiency, state-of-the-art models (SoTA) are categorized into three groups:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"SoTAS: Small models that achieved the best performance before Transformer-based VLP models."}),"\n",(0,t.jsx)(n.li,{children:"SoTAB: The best-performing VLP models comparable in size to BERT Base."}),"\n",(0,t.jsx)(n.li,{children:"SoTAL: The best-performing VLP models comparable in size to BERT Large."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"OSCAR+ and VinVL outperform previous SoTA across seven VL tasks, often by a significant margin."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"VQA: The OSCAR+B model outperforms the best model on the VQA leaderboard."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"GQA: The OSCAR+B with VinVL is the first VLP model to surpass Neural State Machine (NSM)."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Image Captioning: The OSCAR+B model ranks first on the COCO Image Captioning leaderboard, outperforming 263 models."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"NoCaps: Without any VLP, BERT-based models with new visual features (VinVL) surpass human CIDEr performance. Adding VIVO pretraining further improves performance, setting new SoTA."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"how-important-are-visual-features",children:"How Important Are Visual Features?"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"VinVL ablation",src:i(46938).A+"",width:"1024",height:"248"})}),"\n",(0,t.jsx)(n.p,{children:"The authors compare the performance of two different visual models (R101-C4 and X152-C4) and various VLP methods (no VLP and OSCAR) on the VQA task. VinVL and other VLP methods are used for pretraining."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Model Comparison and Performance Improvement"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"OSCAR vs. OSCAR+: Using R101-C4 features, the OSCAR model serves as a baseline. Switching to X152-C4 features in the OSCAR+ model increases absolute accuracy from 72.38 to 74.90."}),"\n",(0,t.jsx)(n.li,{children:"Contribution Analysis: OSCAR+ pretraining contributes 5% to accuracy, while improved visual features from visual pretraining contribute 95%."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Importance of Visual Representations"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visual representations play a crucial role in VLP and downstream tasks, with both visual models and VLP methods significantly contributing to improved results."}),"\n",(0,t.jsx)(n.li,{children:"The additive gains from VinVL and VLP highlight the independent enhancements provided by visual pretraining and VLP."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"impact-of-visual-concept-diversity",children:"Impact of Visual Concept Diversity"}),"\n",(0,t.jsx)(n.p,{children:"The paper explores how the diversity of visual concepts, particularly object and attribute vocabularies, affects VL models, especially in the VQA task."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Richness of Visual Concept Vocabulary"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"VinVL concepts",src:i(10775).A+"",width:"1024",height:"117"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A richer object vocabulary correlates positively with VQA results. Models trained on datasets with more diverse object categories, such as VG w/o attr, perform better than those with fewer categories like VG-obj and ImageNet."}),"\n",(0,t.jsx)(n.li,{children:'Object Vocabulary and Semantic Completeness: Certain visual concepts (e.g., "sky" and "water") are crucial for VQA tasks, highlighting the advantage of comprehensive object vocabularies.'}),"\n",(0,t.jsx)(n.li,{children:"Attributes play a critical role in VQA results. Models trained to recognize attributes (e.g., on VG or 4Sets\u2192VG datasets) perform significantly better than those without such training."}),"\n",(0,t.jsx)(n.li,{children:"Even smaller visual models (e.g., R50-C4) benefit from visual pretraining, demonstrating its overall effectiveness."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Impact of Rich Visual Semantics on VL Tasks"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"VinVL concepts",src:i(29580).A+"",width:"1024",height:"228"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"COCO ground truth (GT-Obj and GT-Obj&Stuff) excels in object localization but has limited vocabulary. Conversely, VG-trained models may be less effective in object localization but offer richer vocabularies, benefiting VQA tasks."}),"\n",(0,t.jsx)(n.li,{children:"Visual Language (VL) OD tasks rely more on rich visual semantics than typical OD tasks. VL tasks require visual semantics to align with the rich semantics of the language modality, underscoring the importance of a comprehensive visual vocabulary in VL tasks."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Although VinVL attempts to incorporate more visual features, it still adheres to the concept of categories. The selected objects and attributes might not be sufficiently rich or may carry inherent biases, limiting the model's ability to capture all visual concepts fully and affecting its broader application potential."}),"\n",(0,t.jsx)(n.p,{children:"Additionally, the large scale and complexity of the model pose significant challenges. The authors seem aware of this and have included a section on improving efficiency to guide users on how to enhance inference speed."}),"\n",(0,t.jsx)(n.p,{children:"VinVL's large architecture and complex computation demand substantial resources, which limits its applicability and widespread adoption in resource-constrained environments."}),"\n",(0,t.jsx)(n.p,{children:"Finally, the integration of attributes and visual concepts presents another significant challenge. Training models across multiple data sources to cover a wide range of visual concepts and attributes requires ensuring data consistency and leveraging their advantages fully, which remains a major hurdle in developing a highly consistent and broadly applicable model."}),"\n",(0,t.jsx)(n.p,{children:"Despite these challenges, VinVL demonstrates strong capabilities and potential in various VL tasks, particularly in object recognition and understanding visual attributes. Its exceptional performance across multiple tasks highlights its value as a visual-language pretraining model."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},47392:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/oscar_1-6d8ac3da7dfa1389b376a2f2889266fd.jpg"},69541:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vinvl_1-180f3412eee38d3bcc51ccd5cc4978b5.jpg"},46938:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vinvl_2-3cd4d49647d0e11187fdc28d5927e4e6.jpg"},10775:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vinvl_3-bd817e374bbf85ef6720716f011889a6.jpg"},29580:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vinvl_4-886501fd32b64428c1db5ebbc844220d.jpg"},8289:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vinvl_result-d7e21928f2c9ad9d61e7e30b37ca5d3b.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(96540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);