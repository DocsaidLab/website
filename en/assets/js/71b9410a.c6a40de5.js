"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["88110"],{48449:function(e,s,n){n.r(s),n.d(s,{default:()=>h,frontMatter:()=>r,metadata:()=>t,assets:()=>c,toc:()=>o,contentTitle:()=>l});var t=JSON.parse('{"id":"object-detection/yolov3/index","title":"[18.04] YOLO-V3","description":"Introducing Multi-Scale Detection","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/object-detection/1804-yolov3/index.md","sourceDirName":"object-detection/1804-yolov3","slug":"/object-detection/yolov3/","permalink":"/en/papers/object-detection/yolov3/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1747981730000,"frontMatter":{"title":"[18.04] YOLO-V3","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[17.08] RetinaNet","permalink":"/en/papers/object-detection/retinanet/"},"next":{"title":"[20.05] DETR","permalink":"/en/papers/object-detection/detr/"}}'),i=n("85893"),a=n("50065");let r={title:"[18.04] YOLO-V3",authors:"Z. Yuan"},l=void 0,c={},o=[{value:"Introducing Multi-Scale Detection",id:"introducing-multi-scale-detection",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Problem Solution",id:"problem-solution",level:2},{value:"Updated Backbone",id:"updated-backbone",level:3},{value:"Class Prediction",id:"class-prediction",level:3},{value:"Multi-Scale Detection",id:"multi-scale-detection",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Performance on COCO",id:"performance-on-coco",level:3},{value:"Attempts That Didn&#39;t Work",id:"attempts-that-didnt-work",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){let s={a:"a",admonition:"admonition",annotation:"annotation",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h2,{id:"introducing-multi-scale-detection",children:"Introducing Multi-Scale Detection"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1804.02767",children:(0,i.jsx)(s.strong,{children:"YOLOv3: An Incremental Improvement"})})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"The third generation of YOLO is not a formal research paper; as stated by the author, it is more of a technical report."}),"\n",(0,i.jsx)(s.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,i.jsx)(s.p,{children:"Recently, there have been numerous advancements in the field of object detection."}),"\n",(0,i.jsx)(s.p,{children:"The author felt it was time to integrate these new ideas into YOLO and make some improvements. The bounding box prediction part remains unchanged and continues to use the design from YOLOv2. We won't delve into that here."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"/en/papers/object-detection/yolov2/",children:(0,i.jsx)(s.strong,{children:"[16.12] YOLO-V2: Expanding a Large Number of Categories"})})}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"problem-solution",children:"Problem Solution"}),"\n",(0,i.jsx)(s.p,{children:"The author implemented a series of improvements."}),"\n",(0,i.jsx)(s.h3,{id:"updated-backbone",children:"Updated Backbone"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"yolov3 backbone",src:n(62843).Z+"",width:"1060",height:"1080"})}),"\n",(0,i.jsx)(s.p,{children:"The author replaced the Darknet-19 network architecture with Darknet-53. This updated network also uses consecutive 3x3 and 1x1 convolutional layers, with added residual connections. It is a deeper network, trained on ImageNet. Due to its 53 convolutional layers, the author named it Darknet-53."}),"\n",(0,i.jsx)(s.p,{children:"The table below shows the performance of Darknet-53 on ImageNet. It can be seen that the top-1 accuracy is comparable to ResNet-101, around 77.1%, but with a 50% increase in inference speed."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"yolov3 imagenet",src:n(31432).Z+"",width:"1092",height:"256"})}),"\n",(0,i.jsx)(s.h3,{id:"class-prediction",children:"Class Prediction"}),"\n",(0,i.jsx)(s.p,{children:"Instead of using softmax for class prediction, logistic regression is used to predict the probability of each class."}),"\n",(0,i.jsx)(s.p,{children:"This means the model does not have to make a hard decision about the class but can instead assign probabilities to multiple classes, making it easier to extend to unknown classes."}),"\n",(0,i.jsx)(s.h3,{id:"multi-scale-detection",children:"Multi-Scale Detection"}),"\n",(0,i.jsx)(s.p,{children:"Inspired by concepts from FPN and RetinaNet, the author introduced multi-scale detection in YOLOv3."}),"\n",(0,i.jsx)(s.p,{children:"Anchors are still found using K-means clustering, but now three different scales of feature maps are used: 1/32, 1/16, and 1/8. Three different sized boxes are predicted at each scale, resulting in a total of 4 bounding box offsets, 1 object confidence score, and 80 class probabilities."}),"\n",(0,i.jsxs)(s.p,{children:["The prediction tensor size is ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"N"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"N"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"["}),(0,i.jsx)(s.mn,{children:"3"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mn,{children:"4"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mn,{children:"80"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"]"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"N \\times N \\times [3 \\times (4 + 1 + 80)]"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mopen",children:"["}),(0,i.jsx)(s.span,{className:"mord",children:"3"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord",children:"4"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"+"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"+"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord",children:"80"}),(0,i.jsx)(s.span,{className:"mclose",children:")]"})]})]})]}),"."]}),"\n",(0,i.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(s.h3,{id:"performance-on-coco",children:"Performance on COCO"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"yolov3 coco",src:n(87301).Z+"",width:"1224",height:"760"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"yolov3 coco",src:n(42471).Z+"",width:"1718",height:"624"})}),"\n",(0,i.jsx)(s.p,{children:'The author admitted that, not wanting to spend a lot of time retraining other models, he "borrowed" an image and a table from the RetinaNet paper. He then added the YOLOv3 results to the table, showing that YOLOv3 performs well in terms of speed and accuracy.'}),"\n",(0,i.jsxs)(s.p,{children:["From the table, we can see that YOLOv3 excels in ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"A"}),(0,i.jsxs)(s.msub,{children:[(0,i.jsx)(s.mi,{children:"P"}),(0,i.jsx)(s.mn,{children:"50"})]})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"AP_{50}"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"A"}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"P"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,i.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.1389em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"50"})})})]})}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(s.span,{})})})]})})]})]})})]}),", significantly outperforming the SSD architecture, indicating strong performance in object detection. However, as the IOU threshold increases, YOLOv3's performance drops, suggesting difficulty in producing high-precision bounding boxes."]}),"\n",(0,i.jsx)(s.p,{children:"In the past, YOLO architectures performed poorly in detecting small objects. However, YOLOv3 has improved significantly in this aspect due to the new multi-scale detection, although its performance in detecting medium and large objects has declined."}),"\n",(0,i.jsx)(s.h3,{id:"attempts-that-didnt-work",children:"Attempts That Didn't Work"}),"\n",(0,i.jsx)(s.p,{children:"The author also mentioned some methods tried during training that did not yield significant results."}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"Predicting anchor box center points (x, y) using linear activation: Tried but found no significant improvement."}),"\n",(0,i.jsx)(s.li,{children:"Linear x, y predictions instead of logistic: Performance dropped with linear activation."}),"\n",(0,i.jsx)(s.li,{children:"Using Focal Loss: Tried to address class imbalance but resulted in a 2-point drop in performance, the reason for which remains unclear."}),"\n",(0,i.jsx)(s.li,{children:"Dual IOU threshold: Tried using the Faster R-CNN approach with objects considered positive if IOU > 0.7 and negative if IOU < 0.3, but found it ineffective."}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(s.p,{children:"Based on previous research, it's difficult for humans to distinguish between detection boxes with IOUs of 0.3 and 0.5 by eye. Therefore, the author believes that striving for higher scores on these validation datasets may not be very meaningful."}),"\n",(0,i.jsx)(s.p,{children:"The performance of YOLO v3 is considered sufficient to handle most real-world problems. The focus should be on enhancing model safety and ensuring these high-performance models are used correctly, reducing potential harm to the world."}),"\n",(0,i.jsx)(s.admonition,{type:"tip",children:(0,i.jsx)(s.p,{children:"The research report is quite candidly written. If you're curious about the author's original statements, you might want to check out the original text."})})]})}function h(e={}){let{wrapper:s}={...(0,a.a)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},62843:function(e,s,n){n.d(s,{Z:function(){return t}});let t=n.p+"assets/images/img1-9672fce22992313c38af925c2ed767ad.jpg"},31432:function(e,s,n){n.d(s,{Z:function(){return t}});let t=n.p+"assets/images/img2-31fe8eda6728507e8e4da7bc927a1ea3.jpg"},42471:function(e,s,n){n.d(s,{Z:function(){return t}});let t=n.p+"assets/images/img3-eefc6e56cce7276a1eede78c07be54f1.jpg"},87301:function(e,s,n){n.d(s,{Z:function(){return t}});let t=n.p+"assets/images/img4-ab48402879f03d0badea6ba52d56a2ea.jpg"},50065:function(e,s,n){n.d(s,{Z:function(){return l},a:function(){return r}});var t=n(67294);let i={},a=t.createContext(i);function r(e){let s=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:s},e.children)}}}]);