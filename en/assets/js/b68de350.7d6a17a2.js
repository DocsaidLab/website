"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3684],{65575:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>a,toc:()=>c});var r=i(74848),s=i(28453);const t={},o="[22.01] ConvNeXt",a={id:"convnext/index",title:"[22.01] ConvNeXt",description:"Making Convolutions Great Again",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/2201-convnext/index.md",sourceDirName:"2201-convnext",slug:"/convnext/",permalink:"/en/papers/convnext/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1724031812e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[21.11] PoolFormer",permalink:"/en/papers/poolformer/"},next:{title:"[22.03] RepLKNet",permalink:"/en/papers/replknet/"}},l={},c=[{value:"Making Convolutions Great Again",id:"making-convolutions-great-again",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Redesigning Network Architecture",id:"redesigning-network-architecture",level:3},{value:"Introducing Modern Training Techniques",id:"introducing-modern-training-techniques",level:3},{value:"Macro Design",id:"macro-design",level:3},{value:"ResNeXt-ify",id:"resnext-ify",level:3},{value:"Inverted Bottleneck",id:"inverted-bottleneck",level:3},{value:"Large Kernel Convolution",id:"large-kernel-convolution",level:3},{value:"Micro Design",id:"micro-design",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Summary of Model Configurations",id:"summary-of-model-configurations",level:3},{value:"Performance on ImageNet",id:"performance-on-imagenet",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"2201-convnext",children:"[22.01] ConvNeXt"}),"\n",(0,r.jsx)(n.h2,{id:"making-convolutions-great-again",children:"Making Convolutions Great Again"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2201.03545",children:(0,r.jsx)(n.strong,{children:"A ConvNet for the 2020s"})})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:"Looking back at the 2010s, deep learning made significant strides, profoundly impacting multiple fields."}),"\n",(0,r.jsx)(n.p,{children:"At the heart of this progress was the revival of neural networks, particularly Convolutional Neural Networks (ConvNet)."}),"\n",(0,r.jsx)(n.p,{children:"Over the past decade, the field of visual recognition has successfully transitioned from handcrafted features to the design of ConvNet architectures."}),"\n",(0,r.jsx)(n.p,{children:'Although the concept of ConvNet dates back to the 1980s, it wasn\'t until 2012 that we truly witnessed its potential in visual feature learning, marked by the emergence of AlexNet, which heralded the "ImageNet moment" in computer vision.'}),"\n",(0,r.jsx)(n.p,{children:"Since then, the field has advanced rapidly with landmark works such as:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1409.1556",children:(0,r.jsx)(n.strong,{children:"[14.09] VGG"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1409.4842",children:(0,r.jsx)(n.strong,{children:"[14.09] GoogLeNet"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1512.03385",children:(0,r.jsx)(n.strong,{children:"[15.12] ResNet"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1608.06993",children:(0,r.jsx)(n.strong,{children:"[16.08] DenseNet"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1704.04861",children:(0,r.jsx)(n.strong,{children:"[17.04] MobileNet"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1905.11946",children:(0,r.jsx)(n.strong,{children:"[19.05] EfficientNet"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2003.13678",children:(0,r.jsx)(n.strong,{children:"[20.03] RegNet"})})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These seminal works emphasized efficiency and scalability, popularizing many practical design principles."}),"\n",(0,r.jsx)(n.p,{children:"However, everything changed with the advent of ViT."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2010.11929",children:(0,r.jsx)(n.strong,{children:"[20.10] ViT"})})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,r.jsx)(n.p,{children:'The introduction of Vision Transformers (ViT) fundamentally disrupted network architecture design. Besides the initial "slicing" layer that splits the image into multiple patches, ViT did not introduce any image-specific inductive bias, making minimal modifications to the original NLP Transformer.'}),"\n",(0,r.jsx)(n.p,{children:"One significant advantage of ViT is its scalability: as the model and dataset sizes grow, its performance significantly surpasses standard ResNet."}),"\n",(0,r.jsx)(n.p,{children:'To close the performance gap, hierarchical Transformers adopted hybrid strategies. For example, the "sliding window" strategy reintroduced local attention mechanisms, making Transformers behave more like ConvNet.'}),"\n",(0,r.jsx)(n.p,{children:"Swin Transformer is a milestone in this regard, proving that Transformers can serve as a general-purpose vision backbone, achieving state-of-the-art performance across various computer vision tasks beyond image classification."}),"\n",(0,r.jsx)(n.p,{children:"The success and rapid adoption of Swin Transformer also indicate that the essence of convolutions remains irreplaceable."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2103.14030",children:(0,r.jsx)(n.strong,{children:"[21.03] Swin Transformer"})})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"From these advancements, it becomes apparent that many of the improvements in Vision Transformers are essentially restoring the strengths of convolutions."}),"\n",(0,r.jsx)(n.p,{children:"However, these attempts come at a cost: implementing sliding window self-attention can be expensive, and advanced methods like cyclic shifting can enhance efficiency but make system design more complex."}),"\n",(0,r.jsx)(n.p,{children:"Ironically, ConvNet already possesses many of the required attributes, albeit in a simple, straightforward manner."}),"\n",(0,r.jsx)(n.p,{children:"The only reason ConvNet lost momentum is that Transformers outperformed them in many vision tasks."}),"\n",(0,r.jsx)(n.p,{children:"The authors argue: it shouldn't be this way."}),"\n",(0,r.jsx)(n.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,r.jsx)(n.h3,{id:"redesigning-network-architecture",children:"Redesigning Network Architecture"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"roadmap",src:i(94784).A+"",width:"3199",height:"2475"})}),"\n",(0,r.jsx)(n.p,{children:"For simplicity, the authors started with ResNet-50 and used the similarly sized Swin Transformer as a reference for comparison."}),"\n",(0,r.jsx)(n.h3,{id:"introducing-modern-training-techniques",children:"Introducing Modern Training Techniques"}),"\n",(0,r.jsx)(n.p,{children:"ViT not only brought new network architectures but also introduced many modern training techniques. Therefore, without changing anything else, the authors first applied ViT's training techniques to ConvNet."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Increased training time from 90 epochs to 300 epochs."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Used the AdamW optimizer."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Employed image augmentation techniques:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1710.09412",children:(0,r.jsx)(n.strong,{children:"Mixup"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1905.04899",children:(0,r.jsx)(n.strong,{children:"CutMix"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1909.13719",children:(0,r.jsx)(n.strong,{children:"RandAugment"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1708.04896",children:(0,r.jsx)(n.strong,{children:"Random Erasing"})})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Applied regularization methods:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1608.06993",children:(0,r.jsx)(n.strong,{children:"Stochastic Depth"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1512.00567",children:(0,r.jsx)(n.strong,{children:"Label Smoothing"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"With the above training techniques, the performance of ResNet-50 improved by 2.7%, from 76.1% to 78.8%."})}),"\n",(0,r.jsx)(n.h3,{id:"macro-design",children:"Macro Design"}),"\n",(0,r.jsx)(n.p,{children:"This part considers two design factors:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Stage Compute Ratio"})}),"\n",(0,r.jsx)(n.p,{children:"In ResNet, the cross-stage compute distribution design is mainly empirical."}),"\n",(0,r.jsx)(n.p,{children:"For instance, ResNet's stage-4 is designed to be compatible with downstream tasks (like object detection) where detector heads operate on a 14\xd714 feature plane."}),"\n",(0,r.jsx)(n.p,{children:"Similarly, Swin-T follows the same principle but with a slightly different stage compute ratio of 1:1:3:1. For larger Swin Transformers, the ratio is 1:1:9:1."}),"\n",(0,r.jsx)(n.p,{children:"To maintain similar FLOPs to Swin-T, the authors adjusted ResNet-50's block numbers per stage from (3, 4, 6, 3) to (3, 3, 9, 3)."}),"\n",(0,r.jsxs)(n.admonition,{type:"info",children:[(0,r.jsx)(n.p,{children:"Research related to stage compute ratio:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1905.13214",children:(0,r.jsx)(n.strong,{children:"[19.05] On network design spaces for visual recognition"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2003.13678",children:(0,r.jsx)(n.strong,{children:"[20.03] Designing network design spaces"})})}),"\n"]})]}),"\n",(0,r.jsx)(n.p,{children:"From this point onward, the model will use this stage compute ratio."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This adjustment increased the model's accuracy from 78.8% to 79.4%."})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Stem Structure Design"})}),"\n",(0,r.jsx)(n.p,{children:"In ConvNet, the stem refers to the initial layer of input."}),"\n",(0,r.jsx)(n.p,{children:"Typically, it is used for downsampling to quickly reduce the spatial size of the input."}),"\n",(0,r.jsx)(n.p,{children:"In ViT, the stem is a patchify layer that splits the image into a series of patches using a large kernel convolution of 16\xd716."}),"\n",(0,r.jsx)(n.p,{children:"In Swin-T, the stem is a 4x4 large convolution with a stride of 4."}),"\n",(0,r.jsx)(n.p,{children:"Here, the authors adopted the same design as Swin-T and adjusted the ResNet-50's stem to a 4\xd74 non-overlapping convolution."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This design increased the model's accuracy from 79.4% to 79.5%."})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"resnext-ify",children:"ResNeXt-ify"}),"\n",(0,r.jsx)(n.p,{children:"In this section, the authors adopted the ResNeXt design philosophy, which offers a better FLOPs/accuracy trade-off compared to a standard ResNet."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1611.05431",children:(0,r.jsx)(n.strong,{children:"Aggregated Residual Transformations for Deep Neural Networks"})})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The core design of ResNeXt is grouped convolution, where convolutional filters are divided into different groups."}),"\n",(0,r.jsx)(n.p,{children:'The guiding principle of ResNeXt is "use more groups, expand width," applying grouped convolution in the 3\xd73 convolution layer.'}),"\n",(0,r.jsx)(n.p,{children:"In this paper, the authors used depthwise convolution, a special form of grouped convolution where the number of groups equals the number of channels."}),"\n",(0,r.jsx)(n.p,{children:"Depthwise convolution is akin to the weighted sum operation in self-attention, operating on each channel individually, mixing information only in the spatial dimension."}),"\n",(0,r.jsx)(n.p,{children:"Using depthwise convolution effectively reduces the network's FLOPs but is expected to lower accuracy."}),"\n",(0,r.jsx)(n.p,{children:"Following ResNeXt's strategy, the authors increased the network width to match the same channel numbers as Swin-T (from 64 to 96)."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This improved the network performance to 80.5% while increasing FLOPs (5.3G)."})}),"\n",(0,r.jsx)(n.h3,{id:"inverted-bottleneck",children:"Inverted Bottleneck"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"inverted-bottleneck",src:i(85103).A+"",width:"1224",height:"432"})}),"\n",(0,r.jsx)(n.p,{children:"Each block in Transformers has an inverted bottleneck structure, typically with an expansion ratio of 4."}),"\n",(0,r.jsx)(n.p,{children:"Later, MobileNetV2 also promoted this concept, with the difference being the addition of a 3x3 depthwise convolution after expansion."}),"\n",(0,r.jsx)(n.p,{children:"In the figure above, (a) is the basic structure of ResNeXt, while (b) is the basic structure of MobileNetV2. (c) is another option, moving the 3x3 depthwise convolution to the front, preparing for the next chapter on exploring large kernel convolutions."}),"\n",(0,r.jsxs)(n.admonition,{type:"info",children:[(0,r.jsx)(n.p,{children:"Using the design in (b), the model's accuracy increased to 80.6%."}),(0,r.jsx)(n.p,{children:"In larger systems like ResNet-200 / Swin-B, more gains were achieved, increasing from 81.9% to 82.6%."})]}),"\n",(0,r.jsx)(n.h3,{id:"large-kernel-convolution",children:"Large Kernel Convolution"}),"\n",(0,r.jsx)(n.p,{children:"One important characteristic of ViT is its non-local self-attention, granting the network global receptive fields in each layer."}),"\n",(0,r.jsx)(n.p,{children:"This contrasts with traditional ConvNet, which uses smaller kernels (like 3\xd73, popularized by VGGNet)."}),"\n",(0,r.jsx)(n.p,{children:"To explore the application of large kernels, the article proposed moving the depthwise convolution layer upward, akin to placing the MHSA block before the MLP layer in Transformers."}),"\n",(0,r.jsx)(n.p,{children:"Such structural adjustments can effectively reduce FLOPs and optimize performance."}),"\n",(0,r.jsx)(n.p,{children:"After these adjustments, the study tested different kernel sizes (from 3\xd73 to 11\xd711):"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"3x3: 79.9%"}),"\n",(0,r.jsx)(n.li,{children:"7x7: 80.6%"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The authors found that increasing the kernel size to 7\xd77 improved network performance from 79.9% to 80.6%, with FLOPs remaining unchanged."}),"\n",(0,r.jsx)(n.p,{children:"Further increasing kernel size (beyond 7\xd77) showed no additional performance gains in the ResNet-200 model, indicating performance saturation at the 7\xd77 size."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"These steps did not enhance performance, but changing from (b) to (c) mimicked the style of Transformer self-attention."})}),"\n",(0,r.jsx)(n.h3,{id:"micro-design",children:"Micro Design"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Replacing ReLU with GELU"})}),"\n",(0,r.jsx)(n.p,{children:"Rectified Linear Unit (ReLU) has been the preferred activation function in ConvNet due to its simplicity and computational efficiency."}),"\n",(0,r.jsx)(n.p,{children:"Gaussian Error Linear Unit (GELU), a smoother variant of ReLU, has gradually gained favor in state-of-the-art Transformer models, such as Google's BERT and OpenAI's GPT-2."}),"\n",(0,r.jsx)(n.p,{children:"The authors replaced ReLU with GELU in ConvNet, and the model's accuracy remained unchanged."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Although performance did not improve, the authors still preferred GELU."})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Reducing Activation Functions"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"activation",src:i(86341).A+"",width:"1096",height:"1080"})}),"\n",(0,r.jsx)(n.p,{children:"In Transformer architectures, particularly in the MLP block, typically only one activation function is used."}),"\n",(0,r.jsx)(n.p,{children:"In contrast, the practice in ResNet modules is to use activation functions after each convolution layer, even after small 1\xd71 convolution layers."}),"\n",(0,r.jsx)(n.p,{children:"The authors removed all GELU activation layers in the residual network block except for one between the two 1\xd71 convolution layers, mimicking the Transformer block's style."}),"\n",(0,r.jsx)(n.p,{children:"This adjustment improved performance, increasing accuracy by 0.7%, reaching 81.3%, comparable to Swin-T's performance."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This design increased the model's accuracy from 80.6% to 81.3%."})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Reducing Normalization Layers"})}),"\n",(0,r.jsx)(n.p,{children:"Transformers generally have fewer normalization layers, whereas in ConvNet, Batch Normalization appears after each convolution layer."}),"\n",(0,r.jsx)(n.p,{children:"Therefore, the authors removed two additional Batch Normalization layers, retaining them only before the 1x1 convolutions."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This design increased the model's accuracy from 81.3% to 81.4%."})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Replacing BatchNorm with LayerNorm"})}),"\n",(0,r.jsx)(n.p,{children:"Transformers use simpler Layer Normalization, achieving good performance across different application scenarios."}),"\n",(0,r.jsx)(n.p,{children:"Directly replacing BN with LN in the original ResNet results in suboptimal performance."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2105.07576",children:(0,r.jsx)(n.strong,{children:'[21.05] Rethinking "batch" in batchnorm'})})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"With all the modifications in network architecture and training techniques, the authors revisited the impact of replacing BN with LN, finding no difficulty in training the ConvNet model using LN."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This design increased the model's accuracy from 81.4% to 81.5%."})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Downsampling Layer"})}),"\n",(0,r.jsx)(n.p,{children:"In the ResNet architecture, spatial downsampling is performed at the beginning of each stage using 3\xd73 convolutions with a stride of 2 and 1\xd71 convolutions with a stride of 2 in the shortcut connections."}),"\n",(0,r.jsx)(n.p,{children:"Compared to ResNet, Swin Transformers add separate downsampling layers between stages."}),"\n",(0,r.jsx)(n.p,{children:"Further investigation showed that adding normalization layers where spatial resolution changes can help stabilize training."}),"\n",(0,r.jsx)(n.p,{children:"This includes multiple Layer Normalization (LN) layers used in Swin Transformers: one before each downsampling layer, one after the stem, and one after the final global average pooling."}),"\n",(0,r.jsx)(n.p,{children:"Based on these results, the authors decided to use separate downsampling layers in their final model, naming it ConvNeXt."}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"This design increased the model's accuracy from 81.5% to 82%."})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,r.jsx)(n.h3,{id:"summary-of-model-configurations",children:"Summary of Model Configurations"}),"\n",(0,r.jsx)(n.p,{children:'These models represent a "modernized" upgrade to the ResNet architecture, with different variants primarily differing in the number of channels (C) and the number of blocks per stage (B).'}),"\n",(0,r.jsx)(n.p,{children:"The specific configurations for each variant are as follows:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ConvNeXt-T"})," (Tiny):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Channels C = (96, 192, 384, 768)"}),"\n",(0,r.jsx)(n.li,{children:"Blocks per stage B = (3, 3, 9, 3)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ConvNeXt-S"})," (Small):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Channels C = (96, 192, 384, 768)"}),"\n",(0,r.jsx)(n.li,{children:"Blocks per stage B = (3, 3, 27, 3)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ConvNeXt-B"})," (Base):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Channels C = (128, 256, 512, 1024)"}),"\n",(0,r.jsx)(n.li,{children:"Blocks per stage B = (3, 3, 27, 3)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ConvNeXt-L"})," (Large):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Channels C = (192, 384, 768, 1536)"}),"\n",(0,r.jsx)(n.li,{children:"Blocks per stage B = (3, 3, 27, 3)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ConvNeXt-XL"})," (Extra Large):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Channels C = (256, 512, 1024, 2048)"}),"\n",(0,r.jsx)(n.li,{children:"Blocks per stage B = (3, 3, 27, 3)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The number of channels doubles with each new stage, following a design philosophy similar to ResNet and Swin Transformers."}),"\n",(0,r.jsx)(n.p,{children:"This hierarchical design allows different model sizes to provide flexible and efficient performance across varying dataset sizes or complexities."}),"\n",(0,r.jsx)(n.h3,{id:"performance-on-imagenet",children:"Performance on ImageNet"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"imagenet",src:i(32321).A+"",width:"2575",height:"2475"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ImageNet-1K Performance Comparison"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ConvNeXt vs. ConvNet Benchmarks"}),": ConvNeXt competes with strong ConvNet benchmarks (like RegNet and EfficientNet) in terms of accuracy and inference throughput."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Comparison with Swin Transformers"}),": ConvNeXt generally outperforms Swin Transformers under similar complexity conditions, e.g., ConvNeXt-T outperforms Swin-T by 0.8%. ConvNeXt also demonstrates higher throughput without relying on specialized modules like sliding windows or relative positional bias."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Specific Model Performance"}),": For instance, ConvNeXt-B shows a significant advantage in FLOPs/throughput when resolution increases (from 224\xb2 to 384\xb2) compared to Swin-B, with a 0.6% accuracy improvement (85.1% vs. 84.5%) and 12.5% higher inference throughput."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ImageNet-22K Performance Comparison"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Impact of Large-Scale Pre-training"}),": While it's generally believed that Vision Transformers might perform better with large-scale pre-training, the results show that a well-designed ConvNet (like ConvNeXt) can match or even surpass similarly sized Swin Transformers after large-scale pre-training, with slightly higher throughput."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance of ConvNeXt-XL"}),": When further scaled up to ConvNeXt-XL, accuracy reaches 87.8%, demonstrating the scalability of the ConvNeXt architecture."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advantages of the ConvNeXt Architecture"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"On ImageNet-1K, EfficientNetV2-L achieved the best performance under progressive training programs."}),"\n",(0,r.jsx)(n.li,{children:"With ImageNet-22K pre-training, ConvNeXt can outperform EfficientNetV2, further proving the importance of large-scale training."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"In the 2020s, with the rise of hierarchical Vision Transformers like Swin Transformers, traditional ConvNet seemed to be viewed as less superior."}),"\n",(0,r.jsx)(n.p,{children:"However, the ConvNeXt model presented in this paper demonstrates that maintaining structural simplicity and efficiency can still compete with state-of-the-art Vision Transformers across various computer vision benchmarks."}),"\n",(0,r.jsx)(n.p,{children:"This battle is just beginning!"})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},94784:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/img1-b31f804197da634ebeee43c8ace55419.jpg"},85103:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/img2-a194d27930094ae1e36ef1476ca55abc.jpg"},86341:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/img3-85ad8b25b47e67d8c67d3ebcd89ba0b7.jpg"},32321:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/img4-189f56cd65d8f2dd9b959ae84cc4c667.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(96540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);