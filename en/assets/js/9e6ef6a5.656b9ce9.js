"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["84060"],{57755:function(e,n,t){t.r(n),t.d(n,{default:()=>h,frontMatter:()=>o,metadata:()=>i,assets:()=>c,toc:()=>l,contentTitle:()=>r});var i=JSON.parse('{"id":"classic-cnns/inception-v1/index","title":"[14.09] GoogLeNet","description":"Also Known as Inception-V1","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/classic-cnns/1409-inception-v1/index.md","sourceDirName":"classic-cnns/1409-inception-v1","slug":"/classic-cnns/inception-v1/","permalink":"/en/papers/classic-cnns/inception-v1/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1733839479000,"frontMatter":{"title":"[14.09] GoogLeNet","authors":"Zephyr"},"sidebar":"papersSidebar","previous":{"title":"[12.09] AlexNet","permalink":"/en/papers/classic-cnns/alexnet/"},"next":{"title":"[14.09] VGG","permalink":"/en/papers/classic-cnns/vgg/"}}'),s=t("85893"),a=t("50065");let o={title:"[14.09] GoogLeNet",authors:"Zephyr"},r=void 0,c={},l=[{value:"Also Known as Inception-V1",id:"also-known-as-inception-v1",level:2},{value:"Problem Solved",id:"problem-solved",level:2},{value:"The Inception Module",id:"the-inception-module",level:3},{value:"Configuration Parameters",id:"configuration-parameters",level:3},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Performance at ILSVRC 2014",id:"performance-at-ilsvrc-2014",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){let n={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mn:"mn",mo:"mo",mrow:"mrow",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"also-known-as-inception-v1",children:"Also Known as Inception-V1"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1409.4842",children:(0,s.jsx)(n.strong,{children:"Going deeper with convolutions"})})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:'GoogLeNet employs the Inception architecture, inspired by the "Network in Network" model and a popular meme, "we need to go deeper."'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1312.4400",children:(0,s.jsx)(n.strong,{children:"[13.12] Network in Network"})})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"It\u2019s rare to see a research paper sincerely citing a meme as part of its motivation."}),"\n",(0,s.jsx)(n.p,{children:"But since the authors boldly wrote this, we\u2019ll share it as it is:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"60%"},children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{alt:"meme",src:t(60897).Z+"",width:"428",height:"847"}),"\n",(0,s.jsxs)("figcaption",{children:["Source: ",(0,s.jsx)(n.a,{href:"https://knowyourmeme.com/memes/we-need-to-go-deeper",children:"Know your meme: We need to go deeper"})]})]})})}),"\n",(0,s.jsx)(n.p,{children:"The \u201Cdepth\u201D of the Inception architecture has two meanings: on one hand, it introduces a new organizational structure called the \u201CInception module,\u201D and on the other hand, it achieves deeper network architectures."}),"\n",(0,s.jsx)(n.p,{children:'The name "GoogLeNet" is a tribute to Yann LeCun\'s LeNet-5 network. This specific version of the Inception architecture was used in the ILSVRC14 competition.'}),"\n",(0,s.jsx)(n.h2,{id:"problem-solved",children:"Problem Solved"}),"\n",(0,s.jsx)(n.h3,{id:"the-inception-module",children:"The Inception Module"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"model arch",src:t(19389).Z+"",width:"1330",height:"394"})}),"\n",(0,s.jsx)(n.p,{children:"The core idea of the Inception architecture is to approximate and cover the optimal sparse structure by using existing dense components."}),"\n",(0,s.jsx)(n.p,{children:"In the module, 1\xd71, 3\xd73, and 5\xd75 convolution filters are used to process features with different spatial distributions. The outputs of all layers are concatenated into a single output vector, which serves as the input for the next stage. To avoid excessively large convolution filters, the size of these filters is constrained more for convenience than necessity."}),"\n",(0,s.jsx)(n.p,{children:"Since pooling operations are crucial in convolutional neural networks, each Inception module includes a parallel pooling path, and the pooling layer's output is concatenated with the output from the convolutional layers, enriching the feature representation further."}),"\n",(0,s.jsx)(n.p,{children:"To prevent computational demands from skyrocketing, Inception architecture uses 1\xd71 convolutions for dimensionality reduction. These convolutions serve as both a dimensionality reduction tool and a non-linear activation layer. This approach significantly reduces the computational resource requirements, especially before performing 3\xd73 and 5\xd75 convolutions, thus reducing computational complexity."}),"\n",(0,s.jsx)(n.p,{children:"As the Inception modules are stacked, the abstraction level of features increases layer by layer, and the concentration of higher-level feature spaces gradually decreases, meaning the ratio of 3\xd73 and 5\xd75 convolutions increases as the network deepens."}),"\n",(0,s.jsx)(n.h3,{id:"configuration-parameters",children:"Configuration Parameters"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"model setting",src:t(52140).Z+"",width:"1458",height:"778"})}),"\n",(0,s.jsx)(n.p,{children:"The table above lists some important configuration parameters of GoogLeNet."}),"\n",(0,s.jsxs)(n.p,{children:["The receptive field size of the GoogLeNet network is ",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsxs)(n.mrow,{children:[(0,s.jsx)(n.mn,{children:"224"}),(0,s.jsx)(n.mo,{children:"\xd7"}),(0,s.jsx)(n.mn,{children:"224"})]}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"224 \\times 224"})]})})}),(0,s.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,s.jsx)(n.span,{className:"mord",children:"224"}),(0,s.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.jsx)(n.span,{className:"mbin",children:"\xd7"}),(0,s.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.6444em"}}),(0,s.jsx)(n.span,{className:"mord",children:"224"})]})]})]}),', processing RGB images with mean subtraction. "#3\xd73 reduce" and "#5\xd75 reduce" indicate the number of 1\xd71 filters used before the 3\xd73 and 5\xd75 convolutions, while "pool proj" refers to the number of 1\xd71 filters in the projection layer following the max-pooling operation.']}),"\n",(0,s.jsx)(n.p,{children:"The depth of the GoogLeNet architecture, when only considering layers with parameters, is 22 layers, and 27 layers if pooling layers are included. The overall network contains around 100 independent building modules, with the exact number depending on the machine learning infrastructure used. An average pooling layer is used before the classifier, and in practice, an additional linear layer is added, which makes the network easier to adapt to other label sets."}),"\n",(0,s.jsx)(n.p,{children:"Replacing the fully connected layers with average pooling improved top-1 accuracy by around 0.6%. However, despite removing the fully connected layers, using Dropout remains essential, with a dropout ratio of 70%."}),"\n",(0,s.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"model",src:t(98556).Z+"",width:"4395",height:"16384"})}),"\n",(0,s.jsx)(n.p,{children:"Due to the depth of the network, effectively backpropagating gradients to all layers is a design concern. To address this, the authors introduced auxiliary classifiers, placed on top of the outputs of Inception modules (4a) and (4d)."}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"Softmax0"})," and ",(0,s.jsx)(n.code,{children:"Softmax1"})," in the image above are the auxiliary classifiers."]})}),"\n",(0,s.jsx)(n.p,{children:"The design of the auxiliary classifiers includes a 5\xd75 average pooling layer, a 1\xd71 convolution layer (with 128 filters), a fully connected layer with 1024 units, a Dropout layer with a 70% dropout rate, and a linear layer. These classifiers help reinforce the backpropagation of gradient signals during training, improving the discriminative power of lower-level features. During training, the loss from these auxiliary classifiers is weighted by 0.3 and added to the total loss, but the auxiliary networks are removed during inference."}),"\n",(0,s.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(n.h3,{id:"performance-at-ilsvrc-2014",children:"Performance at ILSVRC 2014"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"ILSVRC 2014",src:t(82355).Z+"",width:"1224",height:"560"})}),"\n",(0,s.jsx)(n.p,{children:"The task of the ILSVRC 2014 competition was to classify images into one of the 1,000 leaf node categories in the Imagenet hierarchy."}),"\n",(0,s.jsx)(n.p,{children:"The training set contained around 1.2 million images, the validation set 50,000 images, and the test set 100,000 images. Each image was associated with a ground truth label, and the evaluation of performance was based on prediction results."}),"\n",(0,s.jsx)(n.p,{children:"The authors did not use any external data for training during the competition, relying solely on the data provided in the training set. Additionally, several techniques were employed during testing to enhance performance:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Seven independent versions of the GoogLeNet model were trained, and predictions were made using an ensemble method."}),"\n",(0,s.jsx)(n.li,{children:"Multiple crops of the input were used, and the predictions from all independent classifiers were averaged to produce the final result."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The final submission in the challenge achieved a top-5 error rate of 6.67% on both the validation and test sets, ranking first."}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"The design of GoogLeNet demonstrates the importance of balancing computational resources and model performance through the innovative Inception architecture. It also shows the potential of sparse architectures, paving the way for future research in automatically designing more sparse and refined neural network structures."}),"\n",(0,s.jsx)(n.p,{children:"The scalability of this method has made it a cornerstone in modern deep learning research, particularly in contexts that emphasize a balance between efficiency and performance. This approach offers crucial insights for future studies and applications."}),"\n",(0,s.jsx)(n.p,{children:"This article is just the beginning of the Inception series. Later iterations, such as Inception-V2, V3, and V4, introduced various improvements and optimizations. We will explore these in due time."})]})}function h(e={}){let{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},19389:function(e,n,t){t.d(n,{Z:function(){return i}});let i=t.p+"assets/images/img1-c06d228e821fcac31bc1fad9459779db.jpg"},52140:function(e,n,t){t.d(n,{Z:function(){return i}});let i=t.p+"assets/images/img2-9c9f04c47b3c4a65b307618945137970.jpg"},98556:function(e,n,t){t.d(n,{Z:function(){return i}});let i=t.p+"assets/images/img3-fdeb4da7660fc55ba4968b277a00e7ae.jpg"},82355:function(e,n,t){t.d(n,{Z:function(){return i}});let i=t.p+"assets/images/img4-76d02660b55e705217853c40169f77f4.jpg"},60897:function(e,n,t){t.d(n,{Z:function(){return i}});let i=t.p+"assets/images/img5-97c0dd4a24e64069281b730d00c09554.jpg"},50065:function(e,n,t){t.d(n,{Z:function(){return r},a:function(){return o}});var i=t(67294);let s={},a=i.createContext(s);function o(e){let n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);