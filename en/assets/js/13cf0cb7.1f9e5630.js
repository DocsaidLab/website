"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["1511"],{6682:function(e,s,n){n.r(s),n.d(s,{frontMatter:()=>r,toc:()=>c,default:()=>d,metadata:()=>i,assets:()=>l,contentTitle:()=>o});var i=JSON.parse('{"id":"face-antispoofing/three-d-mad/index","title":"[14.05] 3DMAD","description":"The Real Mask","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/face-antispoofing/1405-three-d-mad/index.md","sourceDirName":"face-antispoofing/1405-three-d-mad","slug":"/face-antispoofing/three-d-mad/","permalink":"/en/papers/face-antispoofing/three-d-mad/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1743738248000,"frontMatter":{"title":"[14.05] 3DMAD","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[12.09] LBP","permalink":"/en/papers/face-antispoofing/lbp/"},"next":{"title":"[16.12] rPPG","permalink":"/en/papers/face-antispoofing/rppg/"}}'),t=n(74848),a=n(84429);let r={title:"[14.05] 3DMAD",authors:"Z. Yuan"},o,l={},c=[{value:"The Real Mask",id:"the-real-mask",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Morpho Dataset",id:"morpho-dataset",level:3},{value:"3D Mask Attack Database (3DMAD)",id:"3d-mask-attack-database-3dmad",level:3},{value:"Discussion",id:"discussion",level:2},{value:"3D Mask Attack Security Analysis",id:"3d-mask-attack-security-analysis",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let s={a:"a",admonition:"admonition",annotation:"annotation",blockquote:"blockquote",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mrow:"mrow",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.h2,{id:"the-real-mask",children:"The Real Mask"}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://infoscience.epfl.ch/server/api/core/bitstreams/1694cd6d-ff27-4323-8993-c76232d5aa14/content",children:(0,t.jsx)(s.strong,{children:"Spoofing Face Recognition with 3D Masks"})})}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"Do you remember what malicious attacks we\u2019ve seen before?"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Print Attack"}),": The attacker prints out a photo of the target user and uses it to bypass the face recognition system\u2019s security."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Replay Attack"}),": The attacker records a video of the target user and then replays it to deceive the face recognition system."]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"You say the resolution is too low? No problem, just use a 4K monitor to play it."}),"\n",(0,t.jsx)(s.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(s.p,{children:"While we were still focusing on these 2D attacks, 3D attacks have already arrived."}),"\n",(0,t.jsx)(s.p,{children:"The authors of this paper point out that with the advancements in 3D reconstruction and printing technology, past research has been based on the assumption of 2D attack types, which is no longer valid."}),"\n",(0,t.jsx)(s.p,{children:"This is because we can easily reconstruct a 3D model from a 2D photo and print it out, creating a mask that closely resembles the target user's facial structure."}),"\n",(0,t.jsx)(s.p,{children:"This mask can easily produce:"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Depth of Field Effect"}),": It can bypass depth detection systems."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Facial Expressions"}),": It can bypass active liveness detection systems."]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"It\u2019s neither printing nor video replay. It can directly bypass all defense systems based on 2D attacks."}),"\n",(0,t.jsx)(s.p,{children:"Uh oh! We're in trouble."}),"\n",(0,t.jsx)(s.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,t.jsx)(s.p,{children:"In response to the challenge of 3D mask attacks, the authors propose a straightforward solution:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.strong,{children:"First, create a representative 3D mask attack dataset, and then train the next generation of defense systems based on this data."})}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"This strategy, starting from data implementation, helps clarify attack patterns and the weaknesses of recognition models in real-world scenarios."}),"\n",(0,t.jsx)(s.h3,{id:"morpho-dataset",children:"Morpho Dataset"}),"\n",(0,t.jsx)(s.p,{children:"Morpho is a private dataset collected by MORPHO company in the TABULA RASA project. Although it is not publicly available, it is still one of the few datasets containing high-precision 3D mask attack samples and has been a key resource in early FAS research."}),"\n",(0,t.jsx)(s.p,{children:"The dataset includes:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"207 real access samples"}),"\n",(0,t.jsx)(s.li,{children:"199 3D mask attack samples"}),"\n",(0,t.jsx)(s.li,{children:"Provides both 2D color images and 3D facial models"}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"These masks were created for 16 specific users. A structured light 3D scanner was used to construct the facial models, which were then 3D printed in grayscale materials to create realistic-looking masks. During the recording process, each user was filmed between 9 to 15 times, with a total of 20 subjects' data. Coordinates of the two eye corners and the tip of the nose were annotated in each frame."}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Features and limitations are as follows:"})}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"The masks have extremely high shape accuracy, being a precise replica of the real face."}),"\n",(0,t.jsx)(s.li,{children:"All samples were captured in a single session to avoid environmental variation."}),"\n",(0,t.jsx)(s.li,{children:"The subjects were required to remain still, making it harder for attackers to obtain scanning data."}),"\n",(0,t.jsx)(s.li,{children:"The dataset is a private resource and cannot be used as a public research benchmark."}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"Below are sample images from the dataset, with the leftmost image showing the target user\u2019s grayscale face image, the middle showing the depth map, and the rightmost showing the grayscale image of the 3D mask. All of these samples are from the Morpho dataset."}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"60%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"morpho dataset",src:n(72611).A+"",width:"544",height:"476"})})})}),"\n",(0,t.jsx)(s.h3,{id:"3d-mask-attack-database-3dmad",children:"3D Mask Attack Database (3DMAD)"}),"\n",(0,t.jsxs)(s.p,{children:["Traditionally, creating 3D masks required a 3D scanner, ",(0,t.jsx)(s.strong,{children:"requiring close proximity and cooperation from the user"}),', which made it difficult to implement in real-world attack scenarios. Although scanning accuracy has improved, obtaining "high-quality 3D facial data" without authorization is still very challenging. Therefore, in early research, the prevalence of 3D mask attacks was limited.']}),"\n",(0,t.jsxs)(s.p,{children:["However, in recent years, the ",(0,t.jsx)(s.strong,{children:"commercialization of 3D printing and facial modeling technologies"})," has rapidly developed, lowering the technical barriers and difficulty of creating masks. We only need to obtain a 2D facial photo of the user (such as a social media profile picture) with minimal risk of detection, making it a more practical threat."]}),"\n",(0,t.jsxs)(s.p,{children:["To simulate this ",(0,t.jsx)(s.strong,{children:"low-cost, highly accessible"})," attack mode, the authors used a third-party service called ",(0,t.jsx)(s.strong,{children:(0,t.jsx)(s.a,{href:"https://www.thatsmyface.com/",children:"ThatsMyFace.com"})})," to create the masks."]}),"\n",(0,t.jsx)(s.p,{children:"The production steps are as follows:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Image Collection"}),": Each subject needs to provide ",(0,t.jsx)(s.strong,{children:"one front-facing photo"})," and ",(0,t.jsx)(s.strong,{children:"two side photos"})," (left and right), totaling three high-quality 2D facial images. These images will be the input for 3D facial reconstruction."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Model Reconstruction and Mask Printing"}),": After uploading the images to ThatsMyFace, the platform will automatically reconstruct the three photos into a 3D facial model. The user can preview the model on the website and place an order to customize the mask."]}),"\n",(0,t.jsx)(s.p,{children:"The authors created two types of masks for each user:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Wearable Full-Size Mask"}),": Made from hard resin, with eye holes and nostrils, simulating a real attack."]}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"all masks",src:n(40881).A+"",width:"752",height:"480"})})})}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Paper Craft Assembly Mask"}),": A lower-cost option, intended only for display and future reference."]}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"70%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"3d mask",src:n(26548).A+"",width:"816",height:"480"})})})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"The dataset includes the uploaded images (front and side photos), paper craft mask files, and STL model files for printing, all of which are included for future research use."}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"After completing the 3D masks, the authors designed a rigorous data recording process to simulate real access and attack behaviors and collect color and depth images for model training."}),"\n",(0,t.jsx)(s.p,{children:"The entire recording was done using the Microsoft Kinect for Xbox 360, a device capable of capturing both RGB and depth images, which is invaluable for detecting and analyzing 3D mask attacks."}),"\n",(0,t.jsx)(s.p,{children:"The features of Kinect are as follows:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"30 frames per second"}),": Simultaneously captures RGB and depth images."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Image Resolution"}),":","\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Color images: 640 \xd7 480 pixels, 24-bit (3 \xd7 8 bits)"}),"\n",(0,t.jsx)(s.li,{children:"Depth images: 640 \xd7 480 pixels, 11-bit (1 \xd7 11 bits)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Supports manual annotation"}),": Allows manual marking of eye positions on color images."]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"This design enables researchers to analyze attack samples from a multi-modal perspective, which not only helps develop more robust defense algorithms but also allows for comparing the recognition abilities of 2D and 3D models in different scenarios."}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"To simulate real-world behaviors in various contexts, the 3DMAD recording was scheduled in three different sessions. This design takes into account the impact of time, roles, and attack variables to effectively build data diversity."}),"\n",(0,t.jsx)(s.p,{children:"The recording arrangements for each user are as follows:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Session 1 and Session 2"})," (Real Access):"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:'Filmed two weeks apart to reflect the "temporal difference" challenge in biometric recognition.'}),"\n",(0,t.jsx)(s.li,{children:"Five video segments per session, each 10 seconds long."}),"\n",(0,t.jsx)(s.li,{children:"The user faces the camera and interacts normally."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Session 3"})," (Mask Attack):"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["A ",(0,t.jsx)(s.strong,{children:"single attacker"})," wears the previously made 3D mask to simulate an attack."]}),"\n",(0,t.jsx)(s.li,{children:"Five video segments per mask, simulating different attack actions and angles."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Overall Scale"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["17 users \xd7 3 sessions \xd7 5 video segments = ",(0,t.jsx)(s.strong,{children:"255 video segments in total"})]}),"\n",(0,t.jsxs)(s.li,{children:["Each video segment lasts 10 seconds, totaling ",(0,t.jsx)(s.strong,{children:"76,500 frames"})," of RGB and depth data."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"The actual recording samples are shown in the example below, corresponding from left to right to the three sessions mentioned above:"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"3d mask",src:n(2626).A+"",width:"932",height:"484"})})})}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"To ensure the quality and consistency of subsequent model training, the authors tightly controlled the data annotation and recording environment:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Eye Position Annotation"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"In each video segment, the eye position is manually annotated every 60 frames."}),"\n",(0,t.jsx)(s.li,{children:"The remaining frames are filled in automatically using linear interpolation, ensuring that every frame has annotated coordinates."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Recording Environment Design"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"The user faces the camera directly."}),"\n",(0,t.jsx)(s.li,{children:"The background is uniform (without distracting patterns)."}),"\n",(0,t.jsx)(s.li,{children:"Lighting is sufficient to avoid facial shadows or overexposure."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"This environment setup maximizes the removal of background and lighting interference, focusing on recognizing the difference between the mask and the real face."}),"\n",(0,t.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(s.admonition,{type:"tip",children:(0,t.jsx)(s.p,{children:"Please remember the definitions of these two indicators, otherwise the following charts will be difficult to understand!"})}),"\n",(0,t.jsxs)(s.p,{children:["To systematically evaluate the vulnerability of face recognition systems to mask attacks, the authors in this study proposed several benchmark experiments, using ",(0,t.jsx)(s.strong,{children:"Equal Error Rate (EER)"})," and ",(0,t.jsx)(s.strong,{children:"Spoofing False Acceptance Rate (SFAR)"})," as the two main evaluation metrics:"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"EER"})," represents the error rate at which the false acceptance and false rejection probabilities are equal during the verification task. The ",(0,t.jsx)(s.strong,{children:"lower"})," the EER, the more accurate the verification system."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"SFAR"})," is a metric specifically designed for spoofing attacks, quantifying the rate at which ",(0,t.jsx)(s.strong,{children:"attack samples pass the verification threshold"}),", reflecting the system's resistance to spoofing attacks. The ",(0,t.jsx)(s.strong,{children:"lower"})," the SFAR, the more secure the system."]}),"\n"]}),"\n",(0,t.jsx)(s.h3,{id:"3d-mask-attack-security-analysis",children:"3D Mask Attack Security Analysis"}),"\n",(0,t.jsxs)(s.admonition,{type:"info",children:[(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"In this paper, the reference [22] refers to:"})}),(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"https://ieeexplore.ieee.org/abstract/document/6638076",children:(0,t.jsx)(s.strong,{children:"[13.03] On the vulnerability of face recognition systems to spoofing mask attacks"})})}),"\n"]})]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"3d mask attack",src:n(94335).A+"",width:"1626",height:"178"})}),"\n",(0,t.jsx)(s.p,{children:"The table above shows the verification error rate (EER) and mask attack success rate (SFAR) of three algorithms (LBP-2D, LBP-2.5D, TPS-3D) under different datasets and preprocessing strategies, divided into four settings:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"(1): Results reported in reference [22] (limited to Morpho dataset, SFAR not provided)"}),"\n",(0,t.jsx)(s.li,{children:"(2): Preprocessed images provided by reference [22], tested with the authors' implemented algorithm (Morpho)"}),"\n",(0,t.jsx)(s.li,{children:"(3): Full reconstruction experiment with self-developed preprocessing flow and algorithm (Morpho)"}),"\n",(0,t.jsx)(s.li,{children:"(4): Custom preprocessing and algorithm tested on the 3DMAD dataset (including verification and attack)"}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"The data in the table shows that the authors successfully reproduced the EER results from reference [22] in (2), with minimal differences, indicating that the algorithm implementation and settings were consistent, with good reproducibility and reference value."}),"\n",(0,t.jsx)(s.p,{children:"In the 2D mode, even after changing the preprocessing method (from (2) to (3)), the EER did not significantly change, indicating that the cropping and geometric normalization processes were consistent. However, the SFAR significantly increased, indicating that whether the mask edges appeared in the image had a significant impact on attack success."}),"\n",(0,t.jsx)(s.p,{children:"In 2.5D and 3D modes, omitting smoothing and hole-filling processing (as in reference [22]) caused both EER and SFAR to deteriorate simultaneously. This shows that preprocessing quality severely affects the model's stability and resistance to attacks."}),"\n",(0,t.jsx)(s.p,{children:"A horizontal comparison of different datasets shows that, under the same algorithm, the overall performance of 3DMAD is worse than Morpho. The possible reasons include:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"The depth quality obtained by Kinect is inferior to the laser scanner used in Morpho."}),"\n",(0,t.jsx)(s.li,{children:"The masks in 3DMAD are reconstructed from 2D images, which results in lower shape accuracy, affecting the spoofing effectiveness."}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"The authors further analyzed the 17 masks in 3DMAD one by one and compared the EER and SFAR of four algorithms (LBP, TPS, ISV, ICP) in 2D / 2.5D / 3D modes. The results are shown in the figure below:"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"analysis",src:n(79984).A+"",width:"1392",height:"770"})})})}),"\n",(0,t.jsx)(s.p,{children:"The analysis shows that the SFAR varies greatly between different masks, ranging from 0% to 98.4%. These discrepancies mainly come from:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"The accuracy of mask shape production"}),"\n",(0,t.jsx)(s.li,{children:"The realism and restoration of facial textures"}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"This suggests that a single average value is insufficient to reflect the overall attack risk, and analyzing each mask individually is necessary to evaluate spoofing capabilities."}),"\n",(0,t.jsx)(s.p,{children:"Further observations also found significant differences in attack performance of the same mask under different modes. For example:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Mask #11 had an SFAR of 96.68% in the ISV-2D mode, but only 7.52% in 2.5D mode."}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"This indicates that:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Masks with strong texture information may have high attack potential in 2D mode."}),"\n",(0,t.jsx)(s.li,{children:"However, if the 3D structure is insufficient, it is difficult to launch effective attacks in 2.5D or 3D modes."}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"Similarly, under the same mask and mode, different algorithms may have drastically different sensitivities to attacks. For example:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Mask #17 had an SFAR of 76.32% using ISV-2D, but only 4.44% when switched to LBP-2D."}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"This suggests that the characteristics of the algorithm itself (such as discriminability and generalization ability) directly impact its resistance to mask attacks."}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"These observations highlight a key fact:"}),"\n",(0,t.jsxs)(s.blockquote,{children:["\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"There is a nonlinear relationship between the verification accuracy (EER) and anti-spoofing capability (SFAR) of face recognition systems, which may even be inversely proportional."})}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"This phenomenon is more pronounced in high-performance models like ISV and ICP."}),"\n",(0,t.jsxs)(s.p,{children:["The underlying reason may be that high-performance models have strong generalization capabilities, which allow them to tolerate a wide variety of real user appearance variations (such as expressions, angles, lighting, etc.). However, this generalization ability can also cause the model to ",(0,t.jsx)(s.strong,{children:"accidentally identify a mask with a similar appearance to a real user"}),", thereby reducing security."]}),"\n",(0,t.jsx)(s.p,{children:"This is extremely important in practice, reminding us that when designing recognition systems, we must not only aim for a low EER but also evaluate the SFAR to ensure the system has resistance to spoofing attacks. Otherwise, even if the system performs excellently on the test set, it may still be exposed to high-risk environments in real-world deployment."}),"\n",(0,t.jsxs)(s.admonition,{type:"tip",children:[(0,t.jsx)(s.p,{children:"In addition to this discussion section, the paper also includes some performance analyses based on the algorithms at the time."}),(0,t.jsx)(s.p,{children:"However, these analyses have limited reference value for modern deep learning models, so they are not discussed here."})]}),"\n",(0,t.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(s.p,{children:'3DMAD is one of the earliest systematic works to explore the "threat of 3D masks to face recognition systems."'}),"\n",(0,t.jsxs)(s.p,{children:["The authors, through the Morpho and 3DMAD datasets, designed various verification and anti-spoofing experiments and analyzed the performance differences of different modalities (2D / 2.5D / 3D), features (LBP variants), and classifiers (SVM, LDA, ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"\u03C7"}),(0,t.jsx)(s.mn,{children:"2"})]})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\chi^2"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"1.0085em",verticalAlign:"-0.1944em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",children:"\u03C7"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.8141em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"2"})})]})})})})})]})]})})]}),")."]}),"\n",(0,t.jsx)(s.p,{children:"Overall, the study is comprehensive in data construction, experimental design, and indicator evaluation, providing reference value for the then emerging field of 3D spoofing. The analysis, particularly regarding the relationship between mask production accuracy and model stability, is insightful."}),"\n",(0,t.jsx)(s.p,{children:"However, from the perspective of 2025, there are some obvious limitations in this research:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"The evaluation mainly relies on closed datasets, lacking generalized tests for cross-dataset and unknown attacks."}),"\n",(0,t.jsx)(s.li,{children:"Model training relies on known attack samples, which differs from the real-world scenario where attack types are difficult to predict."}),"\n",(0,t.jsx)(s.li,{children:"Under changes in data distribution, some results show instability, indicating that the model's adaptability to mask diversity is still limited."}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"Nevertheless, the study still holds value in terms of method design and analytical perspectives, laying a preliminary foundation for future research on 3D mask attack defenses. With the continued development of deep learning and sensing technologies, these issues remain worth exploring and expanding."})]})}function d(e={}){let{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},72611:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/img1-60a266eece1f09dc279accd1f0535831.jpg"},26548:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/img2-0788f771751abacaa0e488cd9016b6d4.jpg"},40881:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/img3-afce73aa3c527e66790bbe4aea35c861.jpg"},2626:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/img4-e9fb27c48d5ab34ecbd46de25a734b08.jpg"},94335:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/img5-227109784b9984d577ed7eeb878b5352.jpg"},79984:function(e,s,n){n.d(s,{A:()=>i});let i=n.p+"assets/images/img6-fe045c9564ad4dfec1e7cbd3d28ef8c5.jpg"},84429:function(e,s,n){n.d(s,{R:()=>r,x:()=>o});var i=n(96540);let t={},a=i.createContext(t);function r(e){let s=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:s},e.children)}}}]);