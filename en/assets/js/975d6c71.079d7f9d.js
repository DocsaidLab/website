"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["5649"],{77044:function(e,s,n){n.r(s),n.d(s,{frontMatter:()=>r,default:()=>d,toc:()=>h,metadata:()=>a,assets:()=>c,contentTitle:()=>l});var a=JSON.parse('{"id":"reparameterization/repvit/index","title":"[23.07] RepViT","description":"Revisiting Mobile CNN from the ViT Perspective","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/reparameterization/2307-repvit/index.md","sourceDirName":"reparameterization/2307-repvit","slug":"/reparameterization/repvit/","permalink":"/en/papers/reparameterization/repvit/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739242156000,"frontMatter":{"title":"[23.07] RepViT","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[23.05] VanillaNet","permalink":"/en/papers/reparameterization/vanillanet/"},"next":{"title":"Retail Product (2)","permalink":"/en/papers/category/retail-product-2"}}'),i=n(85893),t=n(50065);let r={title:"[23.07] RepViT",authors:"Z. Yuan"},l=void 0,c={},h=[{value:"Revisiting Mobile CNN from the ViT Perspective",id:"revisiting-mobile-cnn-from-the-vit-perspective",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution Approach",id:"solution-approach",level:2},{value:"Redesigning the Architecture",id:"redesigning-the-architecture",level:3},{value:"Reparameterizing the Structure",id:"reparameterizing-the-structure",level:3},{value:"Reducing Expansion Ratio and Increasing Width",id:"reducing-expansion-ratio-and-increasing-width",level:3},{value:"Adjusting the Stem Structure",id:"adjusting-the-stem-structure",level:3},{value:"Adjusting the Downsampling Structure",id:"adjusting-the-downsampling-structure",level:3},{value:"Simple Classifier",id:"simple-classifier",level:3},{value:"Adjusting Stage Ratios",id:"adjusting-stage-ratios",level:3},{value:"Kernel Size Selection",id:"kernel-size-selection",level:3},{value:"Squeeze-and-Excitation (SE) Layer Decision",id:"squeeze-and-excitation-se-layer-decision",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Performance on ImageNet-1K",id:"performance-on-imagenet-1k",level:3},{value:"Ablation Study - Reparameterized Structure",id:"ablation-study---reparameterized-structure",level:3},{value:"Ablation Study - Impact of SE Layers",id:"ablation-study---impact-of-se-layers",level:3},{value:"Conclusion",id:"conclusion",level:2}];function o(e){let s={a:"a",admonition:"admonition",annotation:"annotation",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h2,{id:"revisiting-mobile-cnn-from-the-vit-perspective",children:"Revisiting Mobile CNN from the ViT Perspective"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2307.09283",children:(0,i.jsx)(s.strong,{children:"RepViT: Revisiting Mobile CNN From ViT Perspective"})})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"Research on reparameterization is thriving and gradually spreading to different model architectures."}),"\n",(0,i.jsx)(s.p,{children:"The goal of this paper is to reparameterize MobileNet-V3, but unlike previous works, this study approaches it from the perspective of Vision Transformers (ViTs)."}),"\n",(0,i.jsx)(s.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,i.jsx)(s.p,{children:"Research on lightweight vision models is primarily divided into two camps:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"CNN-Based Research"}),": Represented by models like the MobileNet series, which reduce the number of parameters through depthwise separable convolutions and enhance the model's expressiveness using inverted residual bottleneck structures."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"ViT-Based Research"}),": Represented by models like MobileViT, Mobileformer, and FastViT, which use the MetaFormer architecture to maintain model expressiveness."]}),"\n"]}),"\n",(0,i.jsxs)(s.admonition,{type:"tip",children:[(0,i.jsx)(s.p,{children:"For those unfamiliar with MetaFormer, you might find these references useful:"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"/en/papers/vision-transformers/poolformer/",children:(0,i.jsx)(s.strong,{children:"[21.11] PoolFormer: You Need a Meta!"})})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"/en/papers/vision-transformers/caformer/",children:(0,i.jsx)(s.strong,{children:"[22.10] CAFormer: MetaFormer User Manual"})})}),"\n"]})]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"Is there an insurmountable gap between these two camps?"}),"\n",(0,i.jsx)(s.p,{children:"This paper aims to bridge this gap by redesigning MobileNet-V3 and rethinking it from the ViT perspective."}),"\n",(0,i.jsx)(s.h2,{id:"solution-approach",children:"Solution Approach"}),"\n",(0,i.jsx)(s.h3,{id:"redesigning-the-architecture",children:"Redesigning the Architecture"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"model arch",src:n(72740).Z+"",width:"1224",height:"996"})}),"\n",(0,i.jsx)(s.p,{children:"Initially, we ignore constraints like the number of parameters and FLOPs because the primary focus is Latency!"}),"\n",(0,i.jsx)(s.p,{children:"The goal is to be fast, faster, and extremely fast!"}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"We start with MobileNet-V3 and take inspiration from recent lightweight ViTs, which typically use DeiT's training methods."}),"\n",(0,i.jsx)(s.p,{children:"Specifically, the training methods used include:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"AdamW optimizer"}),"\n",(0,i.jsx)(s.li,{children:"Cosine learning rate decay, training for 300 epochs"}),"\n",(0,i.jsx)(s.li,{children:"Knowledge distillation using RegNetY16GF"}),"\n",(0,i.jsx)(s.li,{children:"Data augmentation methods: Mixup, RandAugment, and Random Erasing"}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"All models are trained using these standardized methods."}),"\n",(0,i.jsxs)(s.admonition,{type:"tip",children:[(0,i.jsx)(s.p,{children:"For those unfamiliar with DeiT, you might find this reference helpful:"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"/en/papers/vision-transformers/deit/",children:(0,i.jsx)(s.strong,{children:"[20.12] DeiT: Distillation ViT"})})}),"\n"]})]}),"\n",(0,i.jsx)(s.h3,{id:"reparameterizing-the-structure",children:"Reparameterizing the Structure"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"reparameterization",src:n(56787).Z+"",width:"1224",height:"704"})}),"\n",(0,i.jsx)(s.p,{children:"Next, the structure of MobileNet-V3 is adjusted."}),"\n",(0,i.jsx)(s.p,{children:"In MetaFormer, we know that the success of Transformers comes from separating information exchange into two parts:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Token-level information exchange"}),", which corresponds to global information exchange in images."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Feature-level information exchange"}),", which corresponds to channel-level information exchange in images."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"In the original MobileNet structure, these two aspects are coupled together:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["First, ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"1"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1 \\times 1"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"})]})]})]}),' convolutions handle "channel" information exchange.']}),"\n",(0,i.jsxs)(s.li,{children:["Next, ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"3"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"3"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"3 \\times 3"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"})]})]})]}),' depthwise convolutions handle "global" information exchange.']}),"\n",(0,i.jsxs)(s.li,{children:["Followed by a ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"S"}),(0,i.jsx)(s.mi,{children:"E"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"SE"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"SE"})]})})]}),' layer, which handles both "global and channel" information exchange.']}),"\n",(0,i.jsxs)(s.li,{children:["Finally, another ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"1"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1 \\times 1"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"})]})]})]}),' convolution handles "channel" information exchange.']}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The authors separate these aspects in the new structure:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:['Start with "global" information exchange using ',(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"3"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"3"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"3 \\times 3"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"})]})]})]})," depthwise convolutions, and to improve inference speed, use a reparameterized structure."]}),"\n",(0,i.jsxs)(s.li,{children:["Then apply the ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"S"}),(0,i.jsx)(s.mi,{children:"E"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"SE"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"SE"})]})})]}),' layer for "global and channel" information exchange.']}),"\n",(0,i.jsxs)(s.li,{children:["Conclude with ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"1"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1 \\times 1"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"})]})]})]}),' convolutions for "channel" information exchange.']}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"This separation improves inference speed by 20%."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"The speed increases, but accuracy drops."}),"\n",(0,i.jsx)(s.p,{children:"Next, measures are taken to recover the lost accuracy."}),"\n",(0,i.jsx)(s.h3,{id:"reducing-expansion-ratio-and-increasing-width",children:"Reducing Expansion Ratio and Increasing Width"}),"\n",(0,i.jsx)(s.p,{children:"In typical ViTs, the expansion ratio in the channel mixer is often set to 4, making the hidden dimensions in the feed-forward network (FFN) four times the input dimensions; MobileNet-V3 uses a ratio of 6. This consumes significant computational resources and increases overall inference time."}),"\n",(0,i.jsx)(s.p,{children:"Previous research has shown that the FFN contains a lot of redundant information, and a lower expansion ratio is often sufficient."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2104.01136",children:(0,i.jsx)(s.strong,{children:"[21.04] LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference"})})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2104.10858",children:(0,i.jsx)(s.strong,{children:"[21.04] All Tokens Matter: Token Labeling for Training Better Vision Transformers"})})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/3b11c5cc84b6da2838db348b37dbd1a2-Abstract-Conference.html",children:(0,i.jsx)(s.strong,{children:"[22.12] SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization"})})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://openreview.net/forum?id=LzBBxCg-xpa",children:(0,i.jsx)(s.strong,{children:"[23.10] NViT: Vision Transformer Compression and Parameter Redistribution"})})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The authors decided to use an expansion ratio of 2 while increasing the model width to maintain expressiveness."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"This adjustment recovers accuracy and surpasses the original MobileNet-V3, achieving 73.5%."}),"\n",(0,i.jsx)(s.h3,{id:"adjusting-the-stem-structure",children:"Adjusting the Stem Structure"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"stem",src:n(4201).Z+"",width:"2360",height:"1430"})}),"\n",(0,i.jsxs)(s.p,{children:["ViTs typically use a patchify operation as the backbone, dividing the input image into non-overlapping patches, with simple stem structures often using a large ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"16"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"16"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"16 \\times 16"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"16"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"16"})]})]})]})," convolution. In hierarchical ViTs, a ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"4"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"4"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"4 \\times 4"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"4"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"4"})]})]})]})," convolution is used."]}),"\n",(0,i.jsxs)(s.p,{children:["In contrast, MobileNet-V3 has a complex stem structure with ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"3"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"3"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"3 \\times 3"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"})]})]})]})," convolutions, depthwise separable convolutions, and inverted residual bottlenecks, as shown in (a)."]}),"\n",(0,i.jsx)(s.p,{children:"Since the stem structure processes input images at the highest resolution, its complexity can create significant speed bottlenecks on mobile devices. As a trade-off, MobileNet-V3's stem structure uses a small number of filters (only 16 channels), limiting its expressiveness."}),"\n",(0,i.jsxs)(s.p,{children:["To address this, the authors use early convolutions with two ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"3"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"3"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"3 \\times 3"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"})]})]})]})," convolutions and a stride of 2 to reduce computation while increasing the number of filters to enhance expressiveness, as shown in (b)."]}),"\n",(0,i.jsx)(s.h3,{id:"adjusting-the-downsampling-structure",children:"Adjusting the Downsampling Structure"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"downsampling",src:n(71012).Z+"",width:"2365",height:"1430"})}),"\n",(0,i.jsx)(s.p,{children:"MobileNet-V3 uses a single stride-2 depthwise separable convolution for downsampling. While this design is efficient, it may lack sufficient network depth, leading to information loss and negatively impacting performance, as shown in (c)."}),"\n",(0,i.jsxs)(s.p,{children:["To address this, the authors use a more complex downsampling structure, as shown in (d). This structure includes stride-2 depthwise separable convolutions for downsampling and adjusting channel dimensions, a RepViT module at the front, and an FFN module after ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"1"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1 \\times 1"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"})]})]})]})," convolutions to retain more information."]}),"\n",(0,i.jsx)(s.h3,{id:"simple-classifier",children:"Simple Classifier"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"classifier",src:n(28830).Z+"",width:"2365",height:"1430"})}),"\n",(0,i.jsx)(s.p,{children:"In lightweight ViTs, the classifier typically consists of global average pooling and a linear layer, which is very fast on mobile devices."}),"\n",(0,i.jsxs)(s.p,{children:["In contrast, MobileNet-V3's classifier includes a ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"1"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"1 \\times 1"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"})]})]})]})," convolution and an additional linear layer to expand features into a higher-dimensional space, as shown in (e). While this design is crucial for MobileNet-V3, it increases inference time on mobile devices."]}),"\n",(0,i.jsx)(s.p,{children:"The authors abandon MobileNet-V3\u2019s design and opt for a simple global average pooling and linear layer combination for the final architecture."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"This design brings inference time back to 0.77 ms, but accuracy drops again!"}),"\n",(0,i.jsx)(s.h3,{id:"adjusting-stage-ratios",children:"Adjusting Stage Ratios"}),"\n",(0,i.jsx)(s.p,{children:"Past research has typically followed a 1:1:3:1 stage ratio, balancing model expressiveness and speed."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1905.13214",children:(0,i.jsx)(s.strong,{children:"[19.05] On Network Design Spaces for Visual Recognition"})})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2003.13678",children:(0,i.jsx)(s.strong,{children:"[20.03] Designing Network Design Spaces"})})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Recent studies suggest that more aggressive ratios might benefit smaller models. For example, Conv2Former-T and Conv2Former-S use ratios of 1:1:4:1 and 1:1:8:1, respectively."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/2211.11943",children:(0,i.jsx)(s.strong,{children:"[22.11] Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition"})})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The authors adopt a 1:1:7:1 ratio with deeper stages of 2:2:14:2, achieving a deeper network layout."}),"\n",(0,i.jsx)(s.p,{children:"This adjustment successfully recovers the lost accuracy, reaching 76.9%."}),"\n",(0,i.jsx)(s.h3,{id:"kernel-size-selection",children:"Kernel Size Selection"}),"\n",(0,i.jsxs)(s.p,{children:["In previous research, larger kernel convolutions (e.g., ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"5"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"5"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"5 \\times 5"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"5"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"5"})]})]})]})," or ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"7"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"7"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"7 \\times 7"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"7"})]})]})]}),") in small networks have been effective in improving model expressiveness."]}),"\n",(0,i.jsx)(s.p,{children:"However, on mobile devices, larger kernels do not benefit from acceleration, as hardware is generally not optimized for large kernels."}),"\n",(0,i.jsxs)(s.p,{children:["To ensure inference efficiency on mobile devices, the authors prioritize ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mn,{children:"3"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mn,{children:"3"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"3 \\times 3"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"3"})]})]})]})," convolutions across all modules."]}),"\n",(0,i.jsx)(s.p,{children:"This adjustment does not affect accuracy while reducing inference time to 0.89 ms."}),"\n",(0,i.jsx)(s.h3,{id:"squeeze-and-excitation-se-layer-decision",children:"Squeeze-and-Excitation (SE) Layer Decision"}),"\n",(0,i.jsx)(s.p,{children:"The SE module has been widely used in various network architectures to enhance model expressiveness."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"However, SE modules are quite slow!"})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"To balance the benefits of SE modules with speed, the authors use them sparingly:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"Applying SE modules selectively in the 1st, 3rd, 5th... blocks of each stage."})}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(s.h3,{id:"performance-on-imagenet-1k",children:"Performance on ImageNet-1K"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"imagenet performance",src:n(72736).Z+"",width:"1224",height:"752"})}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"imagenet performance 1",src:n(46229).Z+"",width:"1176",height:"860"})}),"\n",(0,i.jsx)(s.p,{children:"Experiments were conducted using standard 224\xd7224 pixel images for training and testing, evaluating the performance of various models with different training epochs (300 or 450 epochs)."}),"\n",(0,i.jsx)(s.p,{children:"Key results include:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Model Performance Comparison"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"The RepViT model shows excellent performance across various sizes, outperforming other state-of-the-art models. For instance, RepViT-M0.9's top-1 accuracy is 3.0% and 2.0% higher than EfficientFormerV2-S0 and FastViT-T8, respectively. Additionally, RepViT-M1.1 surpasses EfficientFormerV2-S1 by 1.7%."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Latency and Accuracy"}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"RepViT-M1.0 achieves over 80% top-1 accuracy with a 1.0 ms latency on an iPhone 12, marking a significant breakthrough in lightweight models."}),"\n",(0,i.jsx)(s.li,{children:"The largest model, RepViT-M2.3, achieves 83.7% accuracy with only 2.3 ms latency, demonstrating outstanding performance and efficient latency control."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Impact of Knowledge Distillation"}),":"]}),"\n",(0,i.jsx)(s.p,{children:"Even without knowledge distillation, RepViT models show significant performance advantages at various latency levels. For example, RepViT-M1.0\u2019s accuracy is 2.7% higher than MobileOne-S1 at 1.0 ms latency. For larger models, RepViT-M2.3 achieves 1.1% higher accuracy than PoolFormer-S36 with a 34.3% reduction in latency (from 3.5 ms to 2.3 ms)."}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"distillation",src:n(97705).Z+"",width:"984",height:"752"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"ablation-study---reparameterized-structure",children:"Ablation Study - Reparameterized Structure"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"ablation1",src:n(25997).Z+"",width:"1090",height:"244"})}),"\n",(0,i.jsx)(s.p,{children:"To validate the effectiveness of the reparameterized structure in RepViT, the authors conducted ablation studies by removing the multi-branch topology during training on ImageNet-1K."}),"\n",(0,i.jsx)(s.p,{children:"Results show that without the reparameterized structure, RepViT variants consistently experience performance drops."}),"\n",(0,i.jsx)(s.h3,{id:"ablation-study---impact-of-se-layers",children:"Ablation Study - Impact of SE Layers"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"ablation2",src:n(51959).Z+"",width:"940",height:"280"})}),"\n",(0,i.jsx)(s.p,{children:"To verify the advantage of using SE layers in a cross-block manner, the authors compared scenarios with all SE layers removed and where SE layers are used in each block."}),"\n",(0,i.jsx)(s.p,{children:"Results indicate that alternating SE layers in blocks offers a better trade-off between accuracy and latency."}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(s.p,{children:"This architecture is the first to achieve over 80% accuracy on ImageNet-1K with an inference speed of under 1 ms on the iPhone 12, marking a significant advancement."}),"\n",(0,i.jsx)(s.p,{children:"The authors have explored efficient designs for lightweight CNNs, combining innovative lightweight ViT structures to introduce the RepViT series."}),"\n",(0,i.jsx)(s.p,{children:"This lightweight CNN model, designed for resource-constrained mobile devices, demonstrates superior performance in various vision tasks. RepViT not only outperforms current state-of-the-art lightweight ViTs and CNNs in accuracy but also excels in latency."}),"\n",(0,i.jsx)(s.p,{children:"The authors hope RepViT will become a solid baseline for lightweight model design and inspire further research and innovation in this field."}),"\n",(0,i.jsxs)(s.admonition,{type:"tip",children:[(0,i.jsx)(s.p,{children:"While RepViT's parameter size may be a concern, many modern mobile devices have ample capacity. However, model size remains a critical factor for many clients, which can limit its adoption."}),(0,i.jsx)(s.p,{children:"If your development scenario is not constrained by model size and prioritizes accuracy and inference time, RepViT is an excellent choice."})]})]})}function d(e={}){let{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},46229:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img1-d56a38964cb61547d5a9fa3cd09ca21e.jpg"},72740:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img2-039e2f7cda7143bd23317ebdc6ffba8b.jpg"},56787:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img3-22a730bafd3310f012365a4efb243a36.jpg"},4201:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img4_1-b8f0be57e3bcf036644ea39fa74d19b6.jpg"},71012:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img4_2-d5b7e3e669723f729fa1b0c2475e7502.jpg"},28830:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img4_3-663d1f33d2671962921d87868268ef5c.jpg"},72736:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img5-ae194471867f2a9dfd0e85170e6c7709.jpg"},97705:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img6-4e505ad21550ff946aca6f334951df29.jpg"},25997:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img7-3ef86732d8d62c6443aa4edcbf821671.jpg"},51959:function(e,s,n){n.d(s,{Z:()=>a});let a=n.p+"assets/images/img8-92fb37d61c4d811a2f58b10cb0eed186.jpg"},50065:function(e,s,n){n.d(s,{Z:()=>l,a:()=>r});var a=n(67294);let i={},t=a.createContext(i);function r(e){let s=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:s},e.children)}}}]);