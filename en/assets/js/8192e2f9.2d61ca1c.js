"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1373],{2512:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var i=t(74848),s=t(28453);const r={slug:"impl-normalized-levenshtein-similarity",title:"Implementing ANLS",authors:"Zephyr",image:"/en/img/2024/0516.webp",tags:["pytorch","anls"],description:"Average Normalized Levenshtein Similarity"},o=void 0,a={permalink:"/en/blog/impl-normalized-levenshtein-similarity",source:"@site/i18n/en/docusaurus-plugin-content-blog/2024/05-16-impl-normalized-levenshtein-similarity/index.md",title:"Implementing ANLS",description:"Average Normalized Levenshtein Similarity",date:"2024-05-16T00:00:00.000Z",tags:[{inline:!0,label:"pytorch",permalink:"/en/blog/tags/pytorch"},{inline:!0,label:"anls",permalink:"/en/blog/tags/anls"}],readingTime:5.715,hasTruncateMarker:!0,authors:[{name:"Zephyr",title:"Engineer",url:"https://github.com/zephyr-sh",imageURL:"https://github.com/zephyr-sh.png",key:"Zephyr",page:null}],frontMatter:{slug:"impl-normalized-levenshtein-similarity",title:"Implementing ANLS",authors:"Zephyr",image:"/en/img/2024/0516.webp",tags:["pytorch","anls"],description:"Average Normalized Levenshtein Similarity"},unlisted:!1,prevItem:{title:"LaTeX Syntax Quick Reference",permalink:"/en/blog/latex-usage"},nextItem:{title:"Equivalent Basic Commands between Python and JS",permalink:"/en/blog/python-js-basic-command-equivalents"}},l={authorsImageUrls:[void 0]},c=[{value:"Import Necessary Libraries",id:"import-necessary-libraries",level:2},{value:"Implement Normalization Functionality",id:"implement-normalization-functionality",level:2},{value:"Implement the <code>reduction</code> Parameter",id:"implement-the-reduction-parameter",level:2},{value:"Complete Implementation",id:"complete-implementation",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("figure",{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"title",src:t(65475).A+"",width:"1024",height:"1024"}),"\n",(0,i.jsx)("figcaption",{children:"Cover Image: Automatically generated by GPT-4 after reading this article"})]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"Average Normalized Levenshtein Similarity, abbreviated as ANLS, is a metric used to compute the similarity between two strings."}),"\n",(0,i.jsx)(n.p,{children:"In natural language processing (NLP), it's often necessary to compare the similarity of two strings."}),"\n",(0,i.jsx)(n.p,{children:'Levenshtein Similarity is a common measure that assesses the "edit distance" between two strings, which is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other. However, Levenshtein Similarity itself isn\'t intuitive as it depends on the lengths of the strings.'}),"\n",(0,i.jsx)(n.p,{children:"To address this issue, we can normalize Levenshtein Similarity to the [0, 1] range, making it easier to understand and compare the similarity between different strings, known as Normalized Levenshtein Similarity (NLS)."}),"\n",(0,i.jsx)(n.p,{children:"As NLS refers to the similarity between sets of strings, we can further extend it to ANLS, which computes the average similarity among multiple sets of strings, thereby quantifying the performance of a model."}),"\n",(0,i.jsx)(n.p,{children:"And then..."}),"\n",(0,i.jsx)(n.p,{children:"I always struggle to find implementations that I like, so I decided to write one myself."}),"\n",(0,i.jsx)(n.h2,{id:"import-necessary-libraries",children:"Import Necessary Libraries"}),"\n",(0,i.jsxs)(n.p,{children:["First, we need to import some necessary libraries, especially the ",(0,i.jsx)(n.code,{children:"EditDistance"})," implemented by ",(0,i.jsx)(n.code,{children:"torchmetrics"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from typing import Any, Literal, Optional, Sequence, Union\n\nimport torch\nfrom torch import Tensor\nfrom torchmetrics.metric import Metric\nfrom torchmetrics.text import EditDistance\nfrom torchmetrics.utilities.data import dim_zero_cat\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Since ",(0,i.jsx)(n.code,{children:"EditDistance"})," can already compute the Levenshtein distance, we can directly use it to calculate the edit distance between two strings. However, ",(0,i.jsx)(n.code,{children:"EditDistance"})," doesn't provide normalization functionality, so we need to implement this part ourselves."]}),"\n",(0,i.jsx)(n.h2,{id:"implement-normalization-functionality",children:"Implement Normalization Functionality"}),"\n",(0,i.jsxs)(n.p,{children:["Here, we inherit the interface of ",(0,i.jsx)(n.code,{children:"torchmetrics.metric.Metric"}),", so we need to implement the ",(0,i.jsx)(n.code,{children:"update"})," and ",(0,i.jsx)(n.code,{children:"compute"})," methods:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class NormalizedLevenshteinSimilarity(Metric):\n\n    def __init__(\n        self,\n        substitution_cost: int = 1,\n        reduction: Optional[Literal["mean", "sum", "none"]] = "mean",\n        **kwargs: Any\n    ) -> None:\n        super().__init__(**kwargs)\n        self.edit_distance = EditDistance(\n            substitution_cost=substitution_cost,\n            reduction=None  # Set to None to get distances for all string pairs\n        )\n\n        # ...\n'})}),"\n",(0,i.jsx)(n.p,{children:"Here are a few key points:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Ensure that the input ",(0,i.jsx)(n.code,{children:"preds"})," and ",(0,i.jsx)(n.code,{children:"target"})," are lists of strings, otherwise the function will calculate on a character level."]}),"\n",(0,i.jsx)(n.li,{children:"Calculate the maximum length of each string, so that we can perform normalization."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def update(self, preds: Union[str, Sequence[str]], target: Union[str, Sequence[str]]) -> None:\n    """Update state with predictions and targets."""\n\n    if isinstance(preds, str):\n        preds = [preds]\n    if isinstance(target, str):\n        target = [target]\n\n    distances = self.edit_distance(preds, target)\n    max_lengths = torch.tensor([\n        max(len(p), len(t))\n        for p, t in zip(preds, target)\n    ], dtype=torch.float)\n\n    ratio = torch.where(\n        max_lengths == 0,\n        torch.zeros_like(distances).float(),\n        distances.float() / max_lengths\n    )\n\n    nls_values = 1 - ratio\n\n    # ...\n'})}),"\n",(0,i.jsxs)(n.h2,{id:"implement-the-reduction-parameter",children:["Implement the ",(0,i.jsx)(n.code,{children:"reduction"})," Parameter"]}),"\n",(0,i.jsxs)(n.p,{children:["We also need to accommodate the ",(0,i.jsx)(n.code,{children:"reduction"})," parameter, where if we specify ",(0,i.jsx)(n.code,{children:"mean"}),", it will be the common ANLS score."]}),"\n",(0,i.jsxs)(n.p,{children:["In addition to the usual ",(0,i.jsx)(n.code,{children:"mean"}),", we can also use ",(0,i.jsx)(n.code,{children:"sum"})," or ",(0,i.jsx)(n.code,{children:"none"})," to fulfill different needs."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def _compute(\n    self,\n    nls_score: Tensor,\n    num_elements: Union[Tensor, int],\n) -> Tensor:\n    """Compute the ANLS over state."""\n    if nls_score.numel() == 0:\n        return torch.tensor(0, dtype=torch.int32)\n    if self.reduction == "mean":\n        return nls_score.sum() / num_elements\n    if self.reduction == "sum":\n        return nls_score.sum()\n    if self.reduction is None or self.reduction == "none":\n        return nls_score\n\ndef compute(self) -> torch.Tensor:\n    """Compute the NLS over state."""\n    if self.reduction == "none" or self.reduction is None:\n        return self._compute(dim_zero_cat(self.nls_values_list), 1)\n    return self._compute(self.nls_score, self.num_elements)\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Here, it's noteworthy that when we specify ",(0,i.jsx)(n.code,{children:"reduction"})," as ",(0,i.jsx)(n.code,{children:"none"}),", we need to return all NLS values instead of computing the average. In this case, I referenced the implementation of ",(0,i.jsx)(n.code,{children:"torchmetrics.text.EditDistance"}),", using ",(0,i.jsx)(n.code,{children:"dim_zero_cat"})," to concatenate values in the list together, ensuring that the return value is a ",(0,i.jsx)(n.code,{children:"Tensor"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"complete-implementation",children:"Complete Implementation"}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["We have synchronized the code to ",(0,i.jsx)(n.a,{href:"https://github.com/DocsaidLab/DocsaidKit/blob/main/docsaidkit/torch/metrics/normalized_levenshtein_similarity.py",children:(0,i.jsx)(n.strong,{children:"DocsaidKit/.../normalized_levenshtein_similarity.py"})})]})}),"\n",(0,i.jsx)(n.p,{children:"The complete implementation is as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from typing import Any, Literal, Optional, Sequence, Union\n\nimport torch\nfrom torch import Tensor\nfrom torchmetrics.metric import Metric\nfrom torchmetrics.text import EditDistance\nfrom torchmetrics.utilities.data import dim_zero_cat\n\n\nclass NormalizedLevenshteinSimilarity(Metric):\n    """\n    Normalized Levenshtein Similarity (NLS) is a metric that computes the\n    normalized Levenshtein similarity between two sequences.\n    This metric is calculated as 1 - (levenshtein_distance / max_length),\n    where `levenshtein_distance` is the Levenshtein distance between the two\n    sequences and `max_length` is the maximum length of the two sequences.\n\n    NLS aims to provide a similarity measure for character sequences\n    (such as text), making it useful in areas like text similarity analysis,\n    Optical Character Recognition (OCR), and Natural Language Processing (NLP).\n\n    This class inherits from `Metric` and uses the `EditDistance` class to\n    compute the Levenshtein distance.\n\n    Inputs to the ``update`` and ``compute`` methods are as follows:\n\n    - ``preds`` (:class:`~Union[str, Sequence[str]]`):\n        Predicted text sequences or a collection of sequences.\n    - ``target`` (:class:`~Union[str, Sequence[str]]`):\n        Target text sequences or a collection of sequences.\n\n    Output from the ``compute`` method is as follows:\n\n    - ``nls`` (:class:`~torch.Tensor`): A tensor containing the NLS value.\n        Returns 0.0 when there are no samples; otherwise, it returns the NLS.\n\n    Args:\n        substitution_cost:\n            The cost of substituting one character for another. Default is 1.\n        reduction:\n            Method to aggregate metric scores.\n            Default is \'mean\', options are \'sum\' or None.\n\n            - ``\'mean\'``: takes the mean over samples, which is ANLS.\n            - ``\'sum\'``: takes the sum over samples\n            - ``None`` or ``\'none\'``: returns the score per sample\n\n        kwargs: Additional keyword arguments.\n\n    Example::\n        Multiple strings example:\n\n        >>> metric = NormalizedLevenshteinSimilarity(reduction=None)\n        >>> preds = ["rain", "lnaguaeg"]\n        >>> target = ["shine", "language"]\n        >>> metric(preds, target)\n        tensor([0.4000, 0.5000])\n        >>> metric = NormalizedLevenshteinSimilarity(reduction="mean")\n        >>> metric(preds, target)\n        tensor(0.4500)\n    """\n\n    def __init__(\n        self,\n        substitution_cost: int = 1,\n        reduction: Optional[Literal["mean", "sum", "none"]] = "mean",\n        **kwargs: Any\n    ) -> None:\n        super().__init__(**kwargs)\n        self.edit_distance = EditDistance(\n            substitution_cost=substitution_cost,\n            reduction=None  # Set to None to get distances for all string pairs\n        )\n\n        allowed_reduction = (None, "mean", "sum", "none")\n        if reduction not in allowed_reduction:\n            raise ValueError(\n                f"Expected argument `reduction` to be one of {allowed_reduction}, but got {reduction}")\n        self.reduction = reduction\n\n        if self.reduction == "none" or self.reduction is None:\n            self.add_state(\n                "nls_values_list",\n                default=[],\n                dist_reduce_fx="cat"\n            )\n        else:\n            self.add_state(\n                "nls_score",\n                default=torch.tensor(0.0),\n                dist_reduce_fx="sum"\n            )\n            self.add_state(\n                "num_elements",\n                default=torch.tensor(0),\n                dist_reduce_fx="sum"\n            )\n\n    def update(self, preds: Union[str, Sequence[str]], target: Union[str, Sequence[str]]) -> None:\n        """Update state with predictions and targets."""\n        if isinstance(preds, str):\n            preds = [preds]\n        if isinstance(target, str):\n            target = [target]\n\n        distances = self.edit_distance(preds, target)\n        max_lengths = torch.tensor([\n            max(len(p), len(t))\n            for p, t in zip(preds, target)\n        ], dtype=torch.float)\n\n        ratio = torch.where(\n            max_lengths == 0,\n            torch.zeros_like(distances).float(),\n            distances.float() / max_lengths\n        )\n\n        nls_values = 1 - ratio\n\n        if self.reduction == "none" or self.reduction is None:\n            self.nls_values_list.append(nls_values)\n        else:\n            self.nls_score += nls_values.sum()\n            self.num_elements += nls_values.shape[0]\n\n    def _compute(\n        self,\n        nls_score: Tensor,\n        num_elements: Union[Tensor, int],\n    ) -> Tensor:\n        """Compute the ANLS over state."""\n        if nls_score.numel() == 0:\n            return torch.tensor(0, dtype=torch.int32)\n        if self.reduction == "mean":\n            return nls_score.sum() / num_elements\n        if self.reduction == "sum":\n            return nls_score.sum()\n        if self.reduction is None or self.reduction == "none":\n            return nls_score\n\n    def compute(self) -> torch.Tensor:\n        """Compute the NLS over state."""\n        if self.reduction == "none" or self.reduction is None:\n            return self._compute(dim_zero_cat(self.nls_values_list), 1)\n        return self._compute(self.nls_score, self.num_elements)\n\n\nif __name__ == "__main__":\n    anls = NormalizedLevenshteinSimilarity(reduction=\'mean\')\n    preds = ["rain", "lnaguaeg"]\n    target = ["shine", "language"]\n    print(anls(preds, target))\n'})}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://lightning.ai/docs/torchmetrics/stable/text/edit.html",children:(0,i.jsx)(n.strong,{children:"torchmetrics.text.EditDistance"})})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},65475:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/0516-0a6a6660c35203eb7984bbb873ead9cc.webp"},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(96540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);