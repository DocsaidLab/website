"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([["61110"],{51125:function(e,t,n){n.r(t),n.d(t,{metadata:()=>i,contentTitle:()=>o,default:()=>c,assets:()=>l,toc:()=>d,frontMatter:()=>r});var i=JSON.parse('{"id":"text-recognition/clip4str/index","title":"[23.05] CLIP4STR","description":"The Blessing of Multimodality","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/text-recognition/2305-clip4str/index.md","sourceDirName":"text-recognition/2305-clip4str","slug":"/text-recognition/clip4str/","permalink":"/en/papers/text-recognition/clip4str/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1732059961000,"frontMatter":{},"sidebar":"papersSidebar","previous":{"title":"[22.07] PARSeq","permalink":"/en/papers/text-recognition/parseq/"},"next":{"title":"[23.06] DiffusionSTR","permalink":"/en/papers/text-recognition/diffusionstr/"}}'),s=n("85893"),a=n("50065");let r={},o="[23.05] CLIP4STR",l={},d=[{value:"The Blessing of Multimodality",id:"the-blessing-of-multimodality",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"CLIP",id:"clip",level:2},{value:"So, What Does This Mean?",id:"so-what-does-this-mean",level:3},{value:"Zero-Shot Testing",id:"zero-shot-testing",level:3},{value:"Problem Solving",id:"problem-solving",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Text Decoder",id:"text-decoder",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Comparison with Other Methods",id:"comparison-with-other-methods",level:3},{value:"Basic Ablation Study",id:"basic-ablation-study",level:3},{value:"How Should We Freeze CLIP?",id:"how-should-we-freeze-clip",level:3},{value:"Is Multimodality Really Useful?",id:"is-multimodality-really-useful",level:3},{value:"Can We Use PEFT?",id:"can-we-use-peft",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let t={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"2305-clip4str",children:"[23.05] CLIP4STR"})}),"\n",(0,s.jsx)(t.h2,{id:"the-blessing-of-multimodality",children:"The Blessing of Multimodality"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2305.14014",children:(0,s.jsx)(t.strong,{children:"CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained Vision-Language Model"})})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:"Contrastive learning has swept across various fields."}),"\n",(0,s.jsx)(t.p,{children:"Following contrastive learning, multimodal learning has become a major focus in recent years, giving rise to a new term:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.strong,{children:"VLM, Vision-Language Models."})}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"In the industry, almost everyone\u2014regardless of age or background\u2014can confidently chant: VLM!"}),"\n",(0,s.jsx)(t.p,{children:"With all the hype, it\u2019s only natural for researchers in the field of text recognition to join the action, right?"}),"\n",(0,s.jsx)(t.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,s.jsx)(t.p,{children:"Scene Text Recognition (STR) faces challenges when dealing with text that is rotated, curved, blurred, or occluded. Despite decades of work, STR\u2019s performance on these challenging cases remains less than ideal."}),"\n",(0,s.jsx)(t.p,{children:"It\u2019s time to shake things up."}),"\n",(0,s.jsx)(t.p,{children:"Since CLIP was introduced in 2021, its powerful cross-modal learning capabilities have made it a hot topic. CLIP\u2019s ability to perceive and interpret various text forms in natural images holds promise as a potential solution for STR."}),"\n",(0,s.jsx)(t.p,{children:"Wait a second\u2014what exactly is CLIP?"}),"\n",(0,s.jsx)(t.p,{children:"Why the sudden shift in style?"}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:"We\u2019ve already explored CLIP, so readers unfamiliar with it might want to check out our previous article:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/en/papers/multimodality/clip/",children:(0,s.jsx)(t.strong,{children:"[21.03] CLIP: Breaking the Dimensional Barrier"})})}),"\n"]})]}),"\n",(0,s.jsx)(t.h2,{id:"clip",children:"CLIP"}),"\n",(0,s.jsx)(t.p,{children:"We know you might not feel like diving into the details, so here\u2019s a brief overview of CLIP."}),"\n",(0,s.jsx)(t.p,{children:"Below is the architecture of CLIP:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"clip arch",src:n(7151).Z+"",width:"1726",height:"622"})}),"\n",(0,s.jsx)(t.p,{children:"Imagine we have a set of image-text pairs, where one pair could be a picture of a dog and the caption \u201Ca cute little dog.\u201D"}),"\n",(0,s.jsx)(t.p,{children:"In each training batch, CLIP processes multiple such pairs. The image encoder, possibly a ResNet or ViT, processes the images to obtain image features, while the text encoder, typically a Transformer, processes the text to obtain text features."}),"\n",(0,s.jsx)(t.p,{children:"The model then compares these features to ensure that the cosine similarity between correctly matched images and text (e.g., a dog image and \u201Ca cute little dog\u201D) is maximized, while the cosine similarity between mismatched pairs (e.g., a dog image and the text \u201Can apple\u201D) is minimized."}),"\n",(0,s.jsx)(t.p,{children:"It\u2019s a straightforward architecture!"}),"\n",(0,s.jsx)(t.p,{children:"Finally, stack 400 million image-text pairs, and start training!"}),"\n",(0,s.jsx)(t.h3,{id:"so-what-does-this-mean",children:"So, What Does This Mean?"}),"\n",(0,s.jsx)(t.p,{children:"What\u2019s the significance here?"}),"\n",(0,s.jsx)(t.p,{children:"Once trained, we can search for images using natural language directly, or conversely, provide an image and let the model generate a corresponding description. If images containing text, such as those in STR tasks, are present in this massive training set, then CLIP\u2019s architecture might establish associations between the appearance of text images and the meaning of the text."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.strong,{children:"Is there a link between text images and the text itself? That\u2019s precisely what STR aims to resolve!"})}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Additionally, CLIP\u2019s training data comes from real-world scenarios, meaning the features it produces can interpret the world from a broader perspective, unrestricted by the traditional STR training data. This can significantly enhance STR\u2019s performance."}),"\n",(0,s.jsx)(t.p,{children:"Most notably, for distorted, skewed, or heavily occluded text, CLIP\u2019s data includes \u201Cunimaginable\u201D varieties, so it may be able to detect these text features."}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"What does \u201Cunimaginable\u201D mean?"})}),(0,s.jsx)(t.p,{children:"You might not feel the impact of \u201C400 million\u201D image-text pairs, so let\u2019s put it in perspective: early STR datasets, such as SynthText, had around 800,000 image-text pairs. Union14M, proposed recently, is currently the largest STR dataset, with 14 million pairs\u201417 times the size of SynthText."}),(0,s.jsx)(t.p,{children:"CLIP\u2019s training dataset is 30 times the size of Union14M and 500 times that of SynthText."}),(0,s.jsx)(t.p,{children:"If you viewed one image per second, it would take you about 12 continuous years to go through the entire dataset."})]}),"\n",(0,s.jsx)(t.h3,{id:"zero-shot-testing",children:"Zero-Shot Testing"}),"\n",(0,s.jsx)(t.p,{children:"To validate this concept, the authors conducted zero-shot testing on CLIP."}),"\n",(0,s.jsx)(t.p,{children:"They wanted to assess CLIP\u2019s \u201Cunderstanding\u201D of text. In the image below, the leftmost column shows the input text, the middle column shows attention visualization results, and the rightmost column shows the text output:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"60%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"zero-shot",src:n(63458).Z+"",width:"728",height:"1058"})})})}),"\n",(0,s.jsx)(t.p,{children:"To our surprise, CLIP can recognize text!"}),"\n",(0,s.jsxs)(t.p,{children:["Moreover, when faced with occluded text, as shown in the example with ",(0,s.jsx)(t.code,{children:"+occluded"})," above, CLIP automatically considers the relationship between the text and the background, resulting in an output that splits the probability between \u201Ccat\u201D and \u201Chorse.\u201D"]}),"\n",(0,s.jsx)(t.h2,{id:"problem-solving",children:"Problem Solving"}),"\n",(0,s.jsx)(t.p,{children:'Since CLIP already has a certain degree of text understanding, our goal now is to "guide" it, focusing its capabilities on the STR task.'}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:"Unless we have sufficient computing power or an extremely large dataset, we usually avoid fine-tuning CLIP's parameters, as doing so could disrupt its multimodal learning capabilities."}),(0,s.jsx)(t.p,{children:"Generally, we freeze CLIP\u2019s parameters and stack a small, task-specific network on top to tackle our particular problem."})]}),"\n",(0,s.jsx)(t.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"model",src:n(173).Z+"",width:"1318",height:"308"})}),"\n",(0,s.jsx)(t.p,{children:"Assuming you\u2019re already familiar with CLIP, let\u2019s dive straight into this architecture."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:"CLIP has two branches: a text branch and an image branch."}),"\n",(0,s.jsx)(t.p,{children:'In the CLIP4STR architecture, the image is first fed into the "CLIP image branch" to obtain image features. These features are then passed into a custom \u201Cimage decoder\u201D to process the features and generate an initial text prediction.'}),"\n",(0,s.jsx)(t.p,{children:'As shown above, when an image is input, the model might first output something like "briiad." Since this output may be incorrect, it is then fed into the "CLIP text branch" to extract text features.'}),"\n",(0,s.jsx)(t.p,{children:'Finally, the text and image features are concatenated and fed into a custom "cross-modal decoder" to refine and produce the final text prediction.'}),"\n",(0,s.jsx)(t.p,{children:'In this setup, the "CLIP text branch" is frozen, meaning it is not trained further. Additionally, when concatenating the text and image features, gradients do not propagate back to the image branch, ensuring the image branch remains unaffected by the text branch.'}),"\n",(0,s.jsx)(t.p,{children:'Here, the primary role of the "CLIP text branch" functions more like a "spell checker" rather than a text generator.'}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:"The spell-checking concept here draws from ABINet. Readers interested in more details can refer to:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/en/papers/text-recognition/abinet/",children:(0,s.jsx)(t.strong,{children:"[21.03] ABINet: Thinking more!"})})}),"\n"]})]}),"\n",(0,s.jsx)(t.h3,{id:"text-decoder",children:"Text Decoder"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"decoder",src:n(3829).Z+"",width:"1636",height:"356"})}),"\n",(0,s.jsx)(t.p,{children:'Remember the "cross-modal decoder" module we mentioned earlier?'}),"\n",(0,s.jsx)(t.p,{children:'This module directly adopts the decoder structure from PARSeq. The difference is that, in PARSeq, the target for the second cross-attention layer comes from the output image features. In CLIP4STR, however, this target is the concatenated output features from both the "CLIP text branch" and the "CLIP image branch."'}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:"If you\u2019re unfamiliar with PARSeq, feel free to check out our previous article:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/en/papers/text-recognition/parseq/",children:(0,s.jsx)(t.strong,{children:"[22.07] PARSeq: Wrod oerdr dseon't mteartr for redaing"})})}),"\n"]})]}),"\n",(0,s.jsx)(t.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(t.h3,{id:"comparison-with-other-methods",children:"Comparison with Other Methods"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"compare",src:n(91362).Z+"",width:"1514",height:"928"})}),"\n",(0,s.jsx)(t.p,{children:"The table above shows CLIP4STR\u2019s performance across 11 STR benchmark datasets. Compared to other state-of-the-art (SOTA) methods, CLIP4STR achieves the latest SOTA results on 9 of these benchmark datasets."}),"\n",(0,s.jsx)(t.h3,{id:"basic-ablation-study",children:"Basic Ablation Study"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"ablation",src:n(53893).Z+"",width:"1004",height:"500"})})})}),"\n",(0,s.jsx)(t.p,{children:"The study begins with a baseline model that only includes a ViT-S encoder for the visual branch, with no pre-training."}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Introducing PSM (Permuted Sequence Modeling) and following PARSeq\u2019s training recipe brings a 0.7% accuracy improvement."}),"\n",(0,s.jsx)(t.li,{children:"Replacing the encoder with CLIP\u2019s image encoder ViT-B/16 shows no significant improvement, indicating a need for further adaptation."}),"\n",(0,s.jsx)(t.li,{children:"Adjusting training parameters by setting the patch size to 16\xd716, using a smaller learning rate for the encoder, a larger one for the decoder, and reducing training epochs to 16 rounds."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"At this point, the model has already surpassed previous SOTA performance."}),"\n",(0,s.jsxs)(t.ol,{start:"4",children:["\n",(0,s.jsx)(t.li,{children:"Adding a cross-modal branch further boosts the average accuracy by 0.4% across 9 benchmark datasets, demonstrating its effectiveness."}),"\n",(0,s.jsx)(t.li,{children:"Introducing a larger model, ViT-L/14, yields an additional 0.7% improvement in accuracy."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The CLIP-ViT-L/14 converges faster on STR than CLIP-ViT-B/16, requiring only 10 training epochs."}),"\n",(0,s.jsx)(t.h3,{id:"how-should-we-freeze-clip",children:"How Should We Freeze CLIP?"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.img,{alt:"freeze",src:n(82656).Z+"",width:"1016",height:"596"}),"\n",(0,s.jsxs)("figurecaption",{children:["#Params represents the number of trainable parameters, while ",(0,s.jsx)(t.code,{children:"token"})," indicates the use of pre-trained token embeddings only.",(0,s.jsx)("br",{}),"The top half shows the frozen text branch, and the bottom half shows the frozen image branch."]})]})})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:"Common freezing strategies include:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Freezing the CLIP text branch"}),": Freezing half of the layers, a standard practice for adapting large language models to new tasks."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Freezing the CLIP image branch"}),": This makes the image branch untrainable, which could impact final performance."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The authors conducted a series of experiments, showing that freezing the language model has minimal impact on performance, while freezing the image model significantly affects performance."}),"\n",(0,s.jsx)(t.p,{children:"Even with the pre-trained token embeddings of the CLIP text encoder fixed, the system can still achieve good results, indicating that semantic understanding in STR is relatively straightforward, focusing mainly on words and phrases rather than complex language."}),"\n",(0,s.jsx)(t.p,{children:"Freezing the image model, however, has a more substantial impact, possibly due to the domain gap between STR data and CLIP\u2019s pre-trained data. CLIP\u2019s pre-training primarily uses natural images, whereas STR data consists of cropped text images. Thus, a fully trainable image encoder is necessary in CLIP4STR to bridge this gap."}),"\n",(0,s.jsx)(t.h3,{id:"is-multimodality-really-useful",children:"Is Multimodality Really Useful?"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"multimodal",src:n(52727).Z+"",width:"1030",height:"300"})})})}),"\n",(0,s.jsx)(t.p,{children:"To verify whether multimodal pre-training truly benefits STR, the authors re-trained the \u201Cimage branch.\u201D"}),"\n",(0,s.jsx)(t.p,{children:"Three different image encoders were tested:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Randomly Initialized ViT"}),": No pre-training."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"ImageNet-1K Pre-trained ViT"}),": Pre-trained on ImageNet-1K."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"ImageNet-21K Pre-trained ViT"}),": Pre-trained on ImageNet-21K."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The table above shows that models pre-trained on image-text pairs perform best, followed by models trained from scratch."}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsx)(t.p,{children:"This aligns with findings from PARSeq, where pre-trained models did not perform as well in STR tasks!"})}),"\n",(0,s.jsx)(t.h3,{id:"can-we-use-peft",children:"Can We Use PEFT?"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"40%"},children:(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"peft",src:n(36606).Z+"",width:"736",height:"1008"})})})}),"\n",(0,s.jsx)(t.p,{children:"Aside from full fine-tuning, parameter-efficient fine-tuning (PEFT) methods for large pre-trained models are becoming increasingly popular. For example, CoOp trains only learnable prefix prompts, while CLIP-Adapter adds tunable linear layers on top of a frozen VLM."}),"\n",(0,s.jsx)(t.p,{children:"Given the success of PEFT methods in some tasks, the authors applied two PEFT methods to STR in this study:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"CLIP-Adapter"}),": Adds two linear layers on top of the frozen CLIP model, with a residual addition ratio of ",(0,s.jsxs)(t.span,{className:"katex",children:[(0,s.jsx)(t.span,{className:"katex-mathml",children:(0,s.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(t.semantics,{children:[(0,s.jsxs)(t.mrow,{children:[(0,s.jsx)(t.mi,{children:"\u03BB"}),(0,s.jsx)(t.mo,{children:"="}),(0,s.jsx)(t.mn,{children:"0.2"})]}),(0,s.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\lambda = 0.2"})]})})}),(0,s.jsxs)(t.span,{className:"katex-html","aria-hidden":"true",children:[(0,s.jsxs)(t.span,{className:"base",children:[(0,s.jsx)(t.span,{className:"strut",style:{height:"0.6944em"}}),(0,s.jsx)(t.span,{className:"mord mathnormal",children:"\u03BB"}),(0,s.jsx)(t.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.jsx)(t.span,{className:"mrel",children:"="}),(0,s.jsx)(t.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,s.jsxs)(t.span,{className:"base",children:[(0,s.jsx)(t.span,{className:"strut",style:{height:"0.6444em"}}),(0,s.jsx)(t.span,{className:"mord",children:"0.2"})]})]})]}),"."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"LST (Ladder Side-Tuning)"}),": Uses a ladder side network with the frozen CLIP model, where features are downsampled, then upsampled to match the original feature dimensions."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The results show that CLIP-Adapter outperforms the frozen model but does not reach the performance of a fully fine-tuned model. In contrast, LST yields a more significant improvement in accuracy, although it still lags behind the fully fine-tuned model."}),"\n",(0,s.jsx)(t.p,{children:"When training resources are limited, LST serves as a viable alternative."}),"\n",(0,s.jsxs)(t.admonition,{type:"tip",children:[(0,s.jsx)(t.p,{children:"For further reading on LLM tuning, we\u2019ve reviewed a few papers previously, which readers might find useful:"}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/en/papers/model-tuning/adapter/",children:(0,s.jsx)(t.strong,{children:"[19.02] Adapter: Saving 96% of Parameters"})})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/en/papers/model-tuning/prefix-tuning/",children:(0,s.jsx)(t.strong,{children:"[21.01] Prefix-Tuning: Is it the Same or Different?"})})}),"\n"]})]}),"\n",(0,s.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(t.p,{children:"Multimodal model architectures have achieved tremendous success across a variety of tasks in recent years. This study demonstrates the effectiveness of CLIP in the STR domain, achieving SOTA performance."}),"\n",(0,s.jsx)(t.p,{children:"The path forward is not an endpoint; rather, it\u2019s time for a new turn."})]})}function c(e={}){let{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},7151:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/arch_clip-ef4a34c3ec1c19ee45e598ca12ff5459.jpg"},63458:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img1-2ba494cd062c19cc621f189ae2a82e11.jpg"},173:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img2-3aec8f7219c6cfadb38427ba3303888d.jpg"},3829:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img3-74a938f2700cb2a8a96a4ad6b52bf3cd.jpg"},82656:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img4-e193358830ac6ebe5569f0f673262342.jpg"},52727:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img5-a851e7707296d893edd2e12a3a6aef70.jpg"},36606:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img6-682af120e9dbab8f7764248941de341e.jpg"},91362:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img7-f9a40bebab1f35e21005cc0a50ee1ae4.jpg"},53893:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img8-ce83fbd12915c3d5a04f47f605c7ff23.jpg"},50065:function(e,t,n){n.d(t,{Z:function(){return o},a:function(){return r}});var i=n(67294);let s={},a=i.createContext(s);function r(e){let t=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);