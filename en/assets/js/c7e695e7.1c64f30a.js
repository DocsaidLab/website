"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[96306],{5389:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var i=t(74848),s=t(28453);const a={sidebar_position:3},o="QuickStart",r={id:"docaligner/quickstart",title:"QuickStart",description:"We provide a simple model inference interface, which includes both pre-processing and post-processing logic.",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/docaligner/quickstart.md",sourceDirName:"docaligner",slug:"/docaligner/quickstart",permalink:"/en/docs/docaligner/quickstart",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:17230934e5,sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Installation",permalink:"/en/docs/docaligner/installation"},next:{title:"Advanced",permalink:"/en/docs/docaligner/advance"}},c={},d=[{value:"Model Inference",id:"model-inference",level:2},{value:"Output Results",id:"output-results",level:2},{value:"1. Draw Polygon",id:"1-draw-polygon",level:3},{value:"2. Get NumPy Image",id:"2-get-numpy-image",level:3},{value:"3. Extract Flattened Image",id:"3-extract-flattened-image",level:3},{value:"Why the Model Fail?",id:"why-the-model-fail",level:2},{value:"Document Size in the Image",id:"document-size-in-the-image",level:3},{value:"Missing Document Corners",id:"missing-document-corners",level:3},{value:"Blurry Document in the Image",id:"blurry-document-in-the-image",level:3},{value:"Unknown Document Types",id:"unknown-document-types",level:3},{value:"Model Visualization",id:"model-visualization",level:2},{value:"Contact Us",id:"contact-us",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"quickstart",children:"QuickStart"})}),"\n",(0,i.jsx)(n.p,{children:"We provide a simple model inference interface, which includes both pre-processing and post-processing logic."}),"\n",(0,i.jsxs)(n.p,{children:["First, you need to import the necessary dependencies and create the ",(0,i.jsx)(n.code,{children:"DocAligner"})," class."]}),"\n",(0,i.jsx)(n.h2,{id:"model-inference",children:"Model Inference"}),"\n",(0,i.jsxs)(n.p,{children:["Below is a simple example demonstrating how to use ",(0,i.jsx)(n.code,{children:"DocAligner"})," for model inference:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from docaligner import DocAligner\n\nmodel = DocAligner()\n"})}),"\n",(0,i.jsx)(n.p,{children:"Once the model is initialized, you need to prepare an image for inference:"}),"\n",(0,i.jsxs)(n.admonition,{type:"tip",children:[(0,i.jsxs)(n.p,{children:["You can use the test image provided by ",(0,i.jsx)(n.code,{children:"DocAligner"}),":"]}),(0,i.jsxs)(n.p,{children:["Download link: ",(0,i.jsx)(n.a,{href:"https://github.com/DocsaidLab/DocAligner/blob/main/docs/run_test_card.jpg",children:(0,i.jsx)(n.strong,{children:"run_test_card.jpg"})})]})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import docsaidkit as D\n\nimg = D.imread('path/to/run_test_card.jpg')\n"})}),"\n",(0,i.jsx)(n.p,{children:"Alternatively, you can read it directly from a URL:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nfrom skimage import io\n\nimg = io.imread('https://github.com/DocsaidLab/DocAligner/blob/main/docs/run_test_card.jpg?raw=true')\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"test_card",src:t(53912).A+"",width:"512",height:"512"})}),"\n",(0,i.jsxs)(n.p,{children:["Next, you can use the ",(0,i.jsx)(n.code,{children:"model"})," for inference:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"result = model(img)\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The inference result is encapsulated in a ",(0,i.jsx)(n.a,{href:"../docsaidkit/funcs/objects/document",children:(0,i.jsx)(n.strong,{children:"Document"})})," object, which contains the document's polygon, OCR text information, etc. In this module, we will only use the ",(0,i.jsx)(n.code,{children:"image"})," and ",(0,i.jsx)(n.code,{children:"doc_polygon"})," attributes. After obtaining the inference result, you can perform various post-processing operations."]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"DocAligner"})," is wrapped with ",(0,i.jsx)(n.code,{children:"__call__"}),", so you can directly call the instance for inference."]})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"DocAligner"})," has an auto-download feature for the model. When you use ",(0,i.jsx)(n.code,{children:"DocAligner"})," for the first time, it will automatically download the model."]})}),"\n",(0,i.jsx)(n.h2,{id:"output-results",children:"Output Results"}),"\n",(0,i.jsx)(n.h3,{id:"1-draw-polygon",children:"1. Draw Polygon"}),"\n",(0,i.jsx)(n.p,{children:"Draw and save the image with the document polygon."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# draw\nresult.draw_doc(\n    folder='path/to/save/folder',\n    name='output_image.jpg'\n)\n"})}),"\n",(0,i.jsx)(n.p,{children:"Or output it directly without specifying a path:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# The default output path is the current directory.\n# The default output filename is f"output_{D.now()}.jpg".\nresult.draw_doc()\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"output_image",src:t(36844).A+"",width:"512",height:"512"})}),"\n",(0,i.jsx)(n.h3,{id:"2-get-numpy-image",children:"2. Get NumPy Image"}),"\n",(0,i.jsxs)(n.p,{children:["If you have other requirements, you can use the ",(0,i.jsx)(n.code,{children:"gen_doc_info_image"})," method and then process it further."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"img = result.gen_doc_info_image()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-extract-flattened-image",children:"3. Extract Flattened Image"}),"\n",(0,i.jsxs)(n.p,{children:["If you know the original size of the document, you can use the ",(0,i.jsx)(n.code,{children:"gen_doc_flat_img"})," method to convert the document image from its polygon boundary to a rectangular image."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"H, W = 1080, 1920\nflat_img = result.gen_doc_flat_img(image_size=(H, W))\n"})}),"\n",(0,i.jsxs)(n.p,{children:["For an unknown image category, you can omit the ",(0,i.jsx)(n.code,{children:"image_size"})," parameter. In this case, the minimum rectangle based on the document polygon boundary will be calculated, and the dimensions of the minimum rectangle will be set as ",(0,i.jsx)(n.code,{children:"H"})," and ",(0,i.jsx)(n.code,{children:"W"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"flat_img = result.gen_doc_flat_img()\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsxs)(n.p,{children:["When your document appears highly skewed in the image, the calculated minimum rectangle might be significantly distorted. In such cases, it's recommended to manually set the ",(0,i.jsx)(n.code,{children:"image_size"})," parameter."]})}),"\n",(0,i.jsx)(n.h2,{id:"why-the-model-fail",children:"Why the Model Fail?"}),"\n",(0,i.jsx)(n.p,{children:"This is a complex question, and we need to break it down step by step."}),"\n",(0,i.jsx)(n.p,{children:"Let's use an image from the MIDV-2020 dataset as an example. You can download this image and test it yourself:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"example",src:t(83368).A+"",width:"2160",height:"3840"})}),"\n",(0,i.jsx)(n.h3,{id:"document-size-in-the-image",children:"Document Size in the Image"}),"\n",(0,i.jsx)(n.p,{children:"The first consideration is the size of the document in the image. A document that is too large or too small may not be detected by the model."}),"\n",(0,i.jsx)(n.p,{children:"Based on our training data, the document scale ranges from approximately 1/2 to 1/8 of the image size, as shown below:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"scale",src:t(96953).A+"",width:"3002",height:"1856"})}),"\n",(0,i.jsx)(n.p,{children:"This means if your document size in the image is smaller than a single grid in the 1/8 scale, the model will likely ignore it, assuming it's part of the background."}),"\n",(0,i.jsx)(n.p,{children:"Detecting very small documents is often not meaningful for downstream tasks, so this characteristic was retained during training."}),"\n",(0,i.jsx)(n.h3,{id:"missing-document-corners",children:"Missing Document Corners"}),"\n",(0,i.jsx)(n.p,{children:"A document that is too large usually doesn't affect the model, but in such cases, document corners might be cropped at the image edges or extend beyond the image."}),"\n",(0,i.jsx)(n.p,{children:"Since the model primarily focuses on corner detection, missing corners can lead to unstable estimation results. If a missing corner is at the document's edge, the model may judge it as an invalid document and not output a polygon, as shown below:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"missing corner",src:t(76803).A+"",width:"3762",height:"1640"})}),"\n",(0,i.jsx)(n.h3,{id:"blurry-document-in-the-image",children:"Blurry Document in the Image"}),"\n",(0,i.jsx)(n.p,{children:"Another cause of detection failure is a blurry document, which can make it difficult for the model to identify the document's boundaries, resulting in detection failure, as shown below:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"blurry",src:t(20078).A+"",width:"2401",height:"1650"})}),"\n",(0,i.jsx)(n.h3,{id:"unknown-document-types",children:"Unknown Document Types"}),"\n",(0,i.jsx)(n.p,{children:"Our model is limited in scale, ranging from 5MB to 20MB. While the model has a certain degree of generalization ability, it may fail to detect certain special documents not present in the training dataset."}),"\n",(0,i.jsx)(n.p,{children:'For example, if we assume the blue calculator in the image below is a "special document":'}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"unknown",src:t(59057).A+"",width:"540",height:"360"})}),"\n",(0,i.jsx)(n.p,{children:'The model will output an empty polygon because it doesn\'t recognize the "calculator" as a document. The solution for this is to manually label and train the model with this "special document" to fine-tune the model.'}),"\n",(0,i.jsx)(n.h2,{id:"model-visualization",children:"Model Visualization"}),"\n",(0,i.jsx)(n.p,{children:"We haven't packaged this feature, as it's just an intermediate step, followed by other image post-processing steps."}),"\n",(0,i.jsx)(n.p,{children:"But if you really want to see it, here's some code to visualize the model's output. Assuming you're using a heatmap model, you can visualize the model's output as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nimport docsaidkit as D\nimport numpy as np\nfrom docaligner import DocAligner\nfrom docaligner.heatmap_reg.infer import preprocess\n\nmodel = DocAligner()\n\nimg = D.imread('midv2020_example.jpg')\n\nimg_infos = preprocess(\n    img=img,\n    img_size_infer=(256, 256)\n)\n\nheatmap = model.detector.model(**img_infos['input'])['heatmap'][0].sum(0)\nheatmap = np.uint8(heatmap * 255)\nheatmap = D.imresize(heatmap, size=img.shape[:2])\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\noutput = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\nD.imwrite(output)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"heatmap",src:t(83615).A+"",width:"2300",height:"1677"})}),"\n",(0,i.jsx)(n.p,{children:"With the above code, you can see the model's output as a heatmap, where darker colors represent regions more likely to be document corners. In case of detection failure, this visualization might help identify potential issues."}),"\n",(0,i.jsx)(n.h2,{id:"contact-us",children:"Contact Us"}),"\n",(0,i.jsx)(n.p,{children:"If the above solutions do not resolve your issue, you can email us the problematic images for further inspection."}),"\n",(0,i.jsxs)(n.p,{children:["Please contact us via email: ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"mailto:docsaidlab@gmail.com",children:"docsaidlab@gmail.com"})})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},20078:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/blur_corner-643dea1ec620aba6a9bdeba1b321ee3d.jpg"},36844:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/flat_result-d8d626f2c82ccace8b1dbefe9efee53b.jpg"},83615:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/heatmap_corner-ebd0a2a2c078504ad41dff39ae1bd65a.jpg"},83368:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/midv2020_example-de86f77781eb201f4fd33fb575648822.jpg"},76803:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/missing_corner-ec7a0a3366653774e2d2da9bf5cafb38.jpg"},53912:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/run_test_card-fb53e9375df9e395862eba27eea849e7.jpg"},96953:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/scale_corner-766b948def2434e7983bd193560edf3d.jpg"},59057:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/unknown_corner-b2a8fc1e98c2a15bdf50fb460810ab1a.jpg"},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(96540);const s={},a=i.createContext(s);function o(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);