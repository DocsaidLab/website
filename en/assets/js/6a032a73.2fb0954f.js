"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["93491"],{54249:function(e,n,o){o.r(n),o.d(n,{frontMatter:()=>s,toc:()=>c,default:()=>p,metadata:()=>t,assets:()=>a,contentTitle:()=>d});var t=JSON.parse('{"id":"mrzscanner/advance","title":"Advanced Settings","description":"When calling the MRZScanner model, you can pass parameters to configure advanced settings.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/mrzscanner/advance.md","sourceDirName":"mrzscanner","slug":"/mrzscanner/advance","permalink":"/en/docs/mrzscanner/advance","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1740280801000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Quick Start","permalink":"/en/docs/mrzscanner/quickstart"},"next":{"title":"Model Design","permalink":"/en/docs/mrzscanner/model_arch"}}'),r=o(74848),i=o(84429);let s={sidebar_position:4},d="Advanced Settings",a={},c=[{value:"Initialization",id:"initialization",level:2},{value:"1. Backend",id:"1-backend",level:3},{value:"2. ModelType",id:"2-modeltype",level:3},{value:"3. ModelCfg",id:"3-modelcfg",level:3},{value:"ModelType.spotting",id:"modeltypespotting",level:2},{value:"ModelType.two_stage",id:"modeltypetwo_stage",level:2},{value:"ModelType.detection",id:"modeltypedetection",level:2},{value:"ModelType.recognition",id:"modeltyperecognition",level:2}];function l(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"advanced-settings",children:"Advanced Settings"})}),"\n",(0,r.jsxs)(n.p,{children:["When calling the ",(0,r.jsx)(n.code,{children:"MRZScanner"})," model, you can pass parameters to configure advanced settings."]}),"\n",(0,r.jsx)(n.h2,{id:"initialization",children:"Initialization"}),"\n",(0,r.jsx)(n.p,{children:"The following are the advanced setting options during the initialization phase:"}),"\n",(0,r.jsx)(n.h3,{id:"1-backend",children:"1. Backend"}),"\n",(0,r.jsxs)(n.p,{children:["Backend is an enumeration type used to specify the computation backend of the ",(0,r.jsx)(n.code,{children:"MRZScanner"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"It includes the following options:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"cpu"}),": Use the CPU for computation."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"cuda"}),": Use the GPU for computation (requires appropriate hardware support)."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from capybara import Backend\n\nmodel = MRZScanner(backend=Backend.cuda) # Use CUDA backend\n#\n# Or\n#\nmodel = MRZScanner(backend=Backend.cpu) # Use CPU backend\n"})}),"\n",(0,r.jsx)(n.p,{children:"We use ONNXRuntime as the inference engine for the model. Although ONNXRuntime supports multiple backend engines (including CPU, CUDA, OpenCL, DirectX, TensorRT, etc.), we have made a slight encapsulation to simplify usage. Currently, we provide only the CPU and CUDA backend engines. Additionally, using CUDA computation requires both appropriate hardware support and the installation of the corresponding CUDA drivers and toolkit."}),"\n",(0,r.jsx)(n.p,{children:"If CUDA is not installed on your system, or the installed version is incorrect, you will not be able to use the CUDA computation backend."}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["If you have other needs, please refer to the ",(0,r.jsx)(n.a,{href:"https://onnxruntime.ai/docs/execution-providers/index.html",children:(0,r.jsx)(n.strong,{children:"ONNXRuntime official documentation"})})," for customization."]}),"\n",(0,r.jsxs)(n.li,{children:["For installation-related issues, please refer to the ",(0,r.jsx)(n.a,{href:"https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements",children:(0,r.jsx)(n.strong,{children:"ONNXRuntime Release Notes"})}),"."]}),"\n"]})}),"\n",(0,r.jsx)(n.h3,{id:"2-modeltype",children:"2. ModelType"}),"\n",(0,r.jsxs)(n.p,{children:["ModelType is an enumeration type used to specify the model type used by ",(0,r.jsx)(n.code,{children:"MRZScanner"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Currently, the following options are available:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"spotting"}),": Uses an end-to-end model architecture and loads a single model."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"two_stage"}),": Uses a two-stage model architecture and loads two models."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"detection"}),": Loads only the detection model for MRZ."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"recognition"}),": Loads only the recognition model for MRZ."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["You can specify the model to use through the ",(0,r.jsx)(n.code,{children:"model_type"})," parameter."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from mrzscanner import MRZScanner\n\nmodel = MRZScanner(model_type=MRZScanner.spotting)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-modelcfg",children:"3. ModelCfg"}),"\n",(0,r.jsxs)(n.p,{children:["You can view all available models using ",(0,r.jsx)(n.code,{children:"list_models"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from mrzscanner import MRZScanner\n\nprint(MRZScanner().list_models())\n# {\n#    'spotting': ['20240919'],\n#    'detection': ['20250222'],\n#    'recognition': ['20250221']\n# }\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Select the version you want and specify the model to use with parameters such as ",(0,r.jsx)(n.code,{children:"spotting_cfg"}),", ",(0,r.jsx)(n.code,{children:"detection_cfg"}),", ",(0,r.jsx)(n.code,{children:"recognition_cfg"}),", etc., along with ",(0,r.jsx)(n.code,{children:"ModelType"}),"."]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"spotting"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"model = MRZScanner(\n   model_type=ModelType.spotting,\n   spotting_cfg='20240919'\n)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"two_stage"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"model = MRZScanner(\n   model_type=ModelType.two_stage,\n   detection_cfg='20250222',\n   recognition_cfg='20250221'\n)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"detection"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"model = MRZScanner(\n   model_type=ModelType.detection,\n   detection_cfg='20250222'\n)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"recognition"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"model = MRZScanner(\n   model_type=ModelType.recognition,\n   recognition_cfg='20250221'\n)\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You may also choose not to specify, as we have already configured default versions for each model."}),"\n",(0,r.jsx)(n.h2,{id:"modeltypespotting",children:"ModelType.spotting"}),"\n",(0,r.jsx)(n.p,{children:"This model is an end-to-end model that directly detects the position of the MRZ and performs recognition. The downside is that its accuracy is relatively lower and it does not return the coordinates of the MRZ."}),"\n",(0,r.jsx)(n.p,{children:"Example usage:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import cv2\nfrom skimage import io\nfrom mrzscanner import MRZScanner, ModelType\n\n# Create the model\nmodel = MRZScanner(\n   model_type=ModelType.spotting,\n   spotting_cfg='20240919'\n)\n\n# Read image from URL\nimg = io.imread('https://github.com/DocsaidLab/MRZScanner/blob/main/docs/test_mrz.jpg?raw=true')\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n# Model inference\nresult = model(img, do_center_crop=True, do_postprocess=False)\n\n# Output results\nprint(result)\n# {\n#    'mrz_polygon': None,\n#    'mrz_texts': [\n#        'PCAZEQAOARIN<<FIDAN<<<<<<<<<<<<<<<<<<<<<<<<<',\n#        'C946302620AZE6707297F23031072W12IMJ<<<<<<<40'\n#    ],\n#    'msg': <ErrorCodes.NO_ERROR: 'No error.'>\n# }\n"})}),"\n",(0,r.jsx)(n.h2,{id:"modeltypetwo_stage",children:"ModelType.two_stage"}),"\n",(0,r.jsx)(n.p,{children:"This model is a two-stage model that first detects the position of the MRZ and then performs recognition. The advantage is higher accuracy, and it also returns the coordinates of the MRZ."}),"\n",(0,r.jsx)(n.p,{children:"Example usage, where we can also draw the MRZ's position:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import cv2\nfrom skimage import io\nfrom mrzscanner import MRZScanner, ModelType\n\n# Create the model\nmodel = MRZScanner(\n   model_type=ModelType.two_stage,\n   detection_cfg='20250222',\n   recognition_cfg='20250221'\n)\n\n# Read image from URL\nimg = io.imread('https://github.com/DocsaidLab/MRZScanner/blob/main/docs/test_mrz.jpg?raw=true')\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n# Model inference\nresult = model(img, do_center_crop=True, do_postprocess=False)\n\n# Output results\nprint(result)\n# {\n#     'mrz_polygon':\n#         array(\n#             [\n#                 [ 158.536 , 1916.3734],\n#                 [1682.7792, 1976.1683],\n#                 [1677.1018, 2120.8926],\n#                 [ 152.8586, 2061.0977]\n#             ],\n#             dtype=float32\n#         ),\n#     'mrz_texts': [\n#         'PCAZEQAQARIN<<FIDAN<<<<<<<<<<<<<<<<<<<<<<<<<',\n#         'C946302620AZE6707297F23031072W12IMJ<<<<<<<40'\n#     ],\n#     'msg': <ErrorCodes.NO_ERROR: 'No error.'>\n# }\n\n# Draw the MRZ's position\nfrom capybara import draw_polygon, imwrite, centercrop\n\npoly_img = draw_polygon(img, result['mrz_polygon'], color=(0, 0, 255), thickness=5)\nimwrite(centercrop(poly_img))\n"})}),"\n",(0,r.jsx)("div",{align:"center",children:(0,r.jsx)("figure",{style:{width:"70%"},children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"demo_two_stage",src:o(51630).A+"",width:"2160",height:"2160"})})})}),"\n",(0,r.jsx)(n.h2,{id:"modeltypedetection",children:"ModelType.detection"}),"\n",(0,r.jsx)(n.p,{children:"This model only detects the position of the MRZ and does not perform recognition."}),"\n",(0,r.jsx)(n.p,{children:"Example usage:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import cv2\nfrom skimage import io\nfrom mrzscanner import MRZScanner, ModelType\n\n# Create the model\nmodel = MRZScanner(\n   model_type=ModelType.detection,\n   detection_cfg='20250222',\n)\n\n# Read image from URL\nimg = io.imread('https://github.com/DocsaidLab/MRZScanner/blob/main/docs/test_mrz.jpg?raw=true')\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n# Model inference\nresult = model(img, do_center_crop=True)\n\n# Output results\nprint(result)\n# {\n#     'mrz_polygon':\n#         array(\n#             [\n#                 [ 158.536 , 1916.3734],\n#                 [1682.7792, 1976.1683],\n#                 [1677.1018, 2120.8926],\n#                 [ 152.8586, 2061.0977]\n#             ],\n#             dtype=float32\n#         ),\n#     'mrz_texts': None,\n#     'msg': <ErrorCodes.NO_ERROR: 'No error.'>\n# }\n"})}),"\n",(0,r.jsx)(n.p,{children:"The MRZ detection result is the same as before, so we will not draw it again here."}),"\n",(0,r.jsx)(n.h2,{id:"modeltyperecognition",children:"ModelType.recognition"}),"\n",(0,r.jsx)(n.p,{children:"This model only performs MRZ recognition and does not detect the position of the MRZ."}),"\n",(0,r.jsx)(n.p,{children:"To use this model, you need to prepare the cropped MRZ image and pass it to the model."}),"\n",(0,r.jsx)(n.p,{children:"First, let's prepare the cropped MRZ image using the previously detected coordinates:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom skimage import io\nfrom capybara import imwarp_quadrangle, imwrite\n\npolygon = np.array([\n    [ 158.536 , 1916.3734],\n    [1682.7792, 1976.1683],\n    [1677.1018, 2120.8926],\n    [ 152.8586, 2061.0977]\n], dtype=np.float32)\n\nimg = io.imread('https://github.com/DocsaidLab/MRZScanner/blob/main/docs/test_mrz.jpg?raw=true')\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\nmrz_img = imwarp_quadrangle(img, polygon)\nimwrite(mrz_img)\n"})}),"\n",(0,r.jsx)(n.p,{children:"After running the above code, we can extract the cropped MRZ image:"}),"\n",(0,r.jsx)("div",{align:"center",children:(0,r.jsx)("figure",{style:{width:"90%"},children:(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"demo_recognition_warp",src:o(63361).A+"",width:"1525",height:"144"})})})}),"\n",(0,r.jsx)(n.p,{children:"Once we have the image, we can run the recognition model separately:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from mrzscanner import MRZScanner, ModelType\n\n# Create the model\nmodel = MRZScanner(\n   model_type=ModelType.recognition,\n   recognition_cfg='20250221'\n)\n\n# Input the cropped MRZ image\nresult = model(mrz_img, do_center_crop=False)\n\n# Output results\nprint(result)\n# {\n#     'mrz_polygon': None,\n#     'mrz_texts': [\n#         'PCAZEQAQARIN<<FIDAN<<<<<<<<<<<<<<<<<<<<<<<<<',\n#         'C946302620AZE6707297F23031072W12IMJ<<<<<<<40'\n#     ],\n#     'msg': <ErrorCodes.NO_ERROR: 'No error.'>\n# }\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["Note that the parameter setting is ",(0,r.jsx)(n.code,{children:"do_center_crop=False"})," because we have already cropped the image."]})})]})}function p(e={}){let{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},63361:function(e,n,o){o.d(n,{A:()=>t});let t=o.p+"assets/images/demo_recognition_warp-75a5dd13f59298999be4b571d269c3fa.jpg"},51630:function(e,n,o){o.d(n,{A:()=>t});let t=o.p+"assets/images/demo_two_stage-b074a7b7946d3a0c11793a683d62a1d4.jpg"},84429:function(e,n,o){o.d(n,{R:()=>s,x:()=>d});var t=o(96540);let r={},i=t.createContext(r);function s(e){let n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);