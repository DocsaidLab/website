"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2473],{30405:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>r,metadata:()=>o,toc:()=>h});var i=n(74848),s=n(28453);const r={},a="[16.12] FPN",o={id:"feature-fusion/fpn/index",title:"[16.12] FPN",description:"The Pyramid Structure",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/feature-fusion/1612-fpn/index.md",sourceDirName:"feature-fusion/1612-fpn",slug:"/feature-fusion/fpn/",permalink:"/en/papers/feature-fusion/fpn/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:1726040814e3,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[16.03] Hourglass",permalink:"/en/papers/feature-fusion/hourglass/"},next:{title:"[18.03] PANet",permalink:"/en/papers/feature-fusion/panet/"}},l={},h=[{value:"The Pyramid Structure",id:"the-pyramid-structure",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"FPN Model Design",id:"fpn-model-design",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Is It Really Better?",id:"is-it-really-better",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const t={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"1612-fpn",children:"[16.12] FPN"})}),"\n",(0,i.jsx)(t.h2,{id:"the-pyramid-structure",children:"The Pyramid Structure"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/1612.03144",children:"Feature Pyramid Networks for Object Detection"})})}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.p,{children:"Imagine a scenario."}),"\n",(0,i.jsx)(t.p,{children:"Given a typical convolutional network model's operation flow: first, an input image, possibly of size 3 x 224 x 224, is passed through successive layers of downsampling, ultimately producing a high-dimensional semantic feature map, potentially 256 x 7 x 7. Conventionally, the final output of such a model is 1/32 the original size, meaning that for an input image of 224 x 224, the final feature map is 7 x 7."}),"\n",(0,i.jsx)(t.p,{children:"This feature map has various descriptions:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Low-resolution feature map (since it's only 1/32 the size of the original image)"}),"\n",(0,i.jsx)(t.li,{children:"High-dimensional semantic features (as it condenses features from the entire image with a large receptive field)"}),"\n",(0,i.jsx)(t.li,{children:"Top-level features (bottom | original image -> C1 -> C2 -> ... -> C5 | top)"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"This design poses no issue for classification tasks, where the goal is to reference the entire input image content to output one or several possible classification results."}),"\n",(0,i.jsx)(t.p,{children:'However, this approach doesn\'t work well for object detection. As mentioned, this feature map is 1/32 the scale. If an object we want to detect is smaller than 32 x 32 pixels, it might "disappear" during downsampling, making it hard for the model to recognize it on the final feature map.'}),"\n",(0,i.jsx)(t.p,{children:'Okay, saying it "disappears" might be an exaggeration. In reality, if the model uses convolution operations for downsampling, the object might still leave some trace in the nearby pixels.'}),"\n",(0,i.jsx)(t.admonition,{type:"tip",children:(0,i.jsx)(t.p,{children:"If you find this description too mystical, the point is that the model will struggle to detect it, requiring more capacity to remember this tiny feature, which can degrade overall performance."})}),"\n",(0,i.jsx)(t.p,{children:"This indicates that to improve object detection performance, we need to do something to preserve these features."}),"\n",(0,i.jsx)(t.p,{children:"Around the end of 2015, SSD was proposed:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.strong,{children:(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/1512.02325",children:"SSD: Single Shot MultiBox Detector (2015.12)"})})}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"The primary aim was to improve upon YOLO v1."}),"\n",(0,i.jsx)(t.p,{children:"Based on YOLO v1's architecture, it added prediction heads at different feature scales, combining high-dimensional features (P3~P5) for predictions. This was an early attempt at a pyramid feature structure and can be considered a precursor to feature pyramid networks. However, this design had drawbacks: high computational cost and inability of low-dimensional features to reference high-dimensional semantic information."}),"\n",(0,i.jsx)(t.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,i.jsx)(t.p,{children:"In this paper, the authors explicitly pointed out these main issues:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Limitations of Traditional Feature Pyramids"})}),"\n",(0,i.jsx)(t.p,{children:"Traditional feature pyramid strategies were the main tool for recognizing multi-scale objects in the era of handcrafted features. However, in the deep learning era, these methods no longer meet current needs. The main issue is their relatively weak ability to handle features at different scales. Particularly when dealing with large amounts of image data, their processing speed is far from sufficient for real-time applications. Additionally, these methods merely perform simple feature scaling without deeper feature fusion and optimization, limiting their recognition effectiveness."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Limitations of SSD"})}),"\n",(0,i.jsx)(t.p,{children:"To address the above issues, SSD, as an emerging strategy, attempted to utilize the pyramid feature hierarchy of deep convolutional networks. Its goal was to fully replace traditional feature pyramid strategies. However, SSD had some evident design flaws. To avoid using lower-level features, SSD deliberately did not reuse computed high-resolution layers, instead choosing to add new layers to build its pyramid. This approach increased computational complexity and overlooked the importance of high-resolution mappings in the feature hierarchy. These high-resolution mappings are crucial for detecting small objects, which SSD's strategy clearly ignored."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,i.jsx)(t.h3,{id:"fpn-model-design",children:"FPN Model Design"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fpn_3",src:n(95508).A+"",width:"1024",height:"887"})}),"\n",(0,i.jsx)(t.p,{children:"The main goal of FPN is to improve SSD's design. The authors proposed a structure, shown above, to create a feature pyramid with strong semantics at all scales while maintaining the pyramid shape of the convolutional network's feature hierarchy."}),"\n",(0,i.jsx)(t.p,{children:"To achieve this, the authors designed a structure that:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.strong,{children:"Combines low-resolution features with high-resolution features through top-down pathways and lateral connections."})}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"This sentence essentially concludes the paper, leaving the implementation and testing parts."}),"\n",(0,i.jsx)(t.p,{children:"However, let's look at some implementation details provided by the authors."}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Bottom-up Pathway"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fpn_2",src:n(92881).A+"",width:"1024",height:"396"})}),"\n",(0,i.jsx)(t.p,{children:"The first part is the data pathway from bottom features to top features. Since the paper doesn't provide an image reference, I drew a structure diagram with actual numbers and simplified information for explanation, hoping to give you a more intuitive sense of data flow in the model:"}),"\n",(0,i.jsx)(t.p,{children:"Taking ResNet18 as an example, an input image of 224 x 224 x 3. Based on PyTorch syntax, the channel count is at the front. After passing through ResNet18, we get five different resolution feature maps. Let's define these feature maps:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"P1: 1/2 size feature map, 64 x 112 x 112."}),"\n",(0,i.jsx)(t.li,{children:"P2: 1/4 size feature map, 64 x 56 x 56."}),"\n",(0,i.jsx)(t.li,{children:"P3: 1/8 size feature map, 128 x 28 x 28."}),"\n",(0,i.jsx)(t.li,{children:"P4: 1/16 size feature map, 256 x 14 x 14."}),"\n",(0,i.jsx)(t.li,{children:"P5: 1/32 size feature map, 512 x 7 x 7."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"This image passes through the model's downsampling process, termed as the bottom-up pathway in the paper."}),"\n",(0,i.jsx)(t.p,{children:"It's worth noting that most model architectures do not use P1 and P2 feature maps in the feature pyramid due to their large size, which consumes substantial computational resources."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Top-down Pathway and Lateral Connections"})}),"\n",(0,i.jsx)(t.p,{children:"Recall the previous steps, where input data flows from left to right."}),"\n",(0,i.jsx)(t.p,{children:"The second stage of feature pyramid design involves merging features from right to left."}),"\n",(0,i.jsx)(t.p,{children:"Let's zoom in on the P4 - P5 section:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fpn_3",src:n(31984).A+"",width:"1140",height:"828"})}),"\n",(0,i.jsx)(t.p,{children:"During this fusion process, we first deal with the high-dimensional feature map due to its smaller size."}),"\n",(0,i.jsx)(t.p,{children:"Here, we upsample the P5 feature map to ensure size consistency."}),"\n",(0,i.jsx)(t.p,{children:"Next, we aim to add these different dimension feature maps. However, this introduces a challenge: channel mismatch."}),"\n",(0,i.jsx)(t.p,{children:"As shown above, the feature map from Block5 has 512 channels, while the feature map from Block4 has only 256 channels. Due to the channel mismatch, direct addition is impossible. To resolve this, we use a 1\xd71 convolution to adjust the feature map's channel count. Note that there's no fixed rule for adjusting channel counts; you can predefine a suitable channel count based on practical needs, like setting it to 64."}),"\n",(0,i.jsx)(t.p,{children:"Each P1 - P5 feature map needs to pass through a 1\xd71 convolution layer to ensure consistent channel counts, avoiding alignment issues."}),"\n",(0,i.jsx)(t.p,{children:"After aligning the channels and sizes, we can directly add feature maps from different layers, completing one round of fusion."}),"\n",(0,i.jsx)(t.p,{children:"To further illustrate, let's look at the fusion from P4 to P3:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fpn_5",src:n(3650).A+"",width:"1024",height:"392"})}),"\n",(0,i.jsx)(t.p,{children:"Finally, here's a more practical architecture design, which might look like this:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fpn_6",src:n(32909).A+"",width:"1024",height:"346"})}),"\n",(0,i.jsx)(t.admonition,{type:"tip",children:(0,i.jsx)(t.p,{children:"Scaling each feature map before fusion is a common design in practical engineering implementations."})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(t.h3,{id:"is-it-really-better",children:"Is It Really Better?"}),"\n",(0,i.jsx)(t.p,{children:"To summarize the results: Yes."}),"\n",(0,i.jsx)(t.p,{children:"The authors conducted several experiments to illustrate this, and we'll highlight some key charts from the paper. First, let's look at the ablation study provided in the paper:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fpn_4",src:n(16319).A+"",width:"1024",height:"231"})}),"\n",(0,i.jsx)(t.p,{children:"This table discusses the effects of removing different components: (d) removes the top-down pathway, (e) removes lateral connections, and (f) removes the feature pyramid representation."}),"\n",(0,i.jsx)(t.p,{children:"We see that removing any component causes a significant performance drop, sometimes making the model barely better than the baseline."}),"\n",(0,i.jsx)(t.p,{children:"Next, let's look at another table:"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"fpn_1",src:n(35110).A+"",width:"1024",height:"218"})}),"\n",(0,i.jsx)(t.p,{children:"The authors compared their method with single-model results from COCO competition winners, including the 2016 winner G-RMI and the 2015 winner Faster R-CNN+++. Without using various fancy techniques, a single FPN model already surpasses these strong, meticulously designed competitors. On the test-dev set, FPN improves the current best result by 0.5 points in AP."}),"\n",(0,i.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(t.p,{children:"FPN introduces a simple framework that can be combined with various backbone networks to build powerful feature pyramids. This method shows significant improvements over multiple strong benchmark models and competition winners."}),"\n",(0,i.jsx)(t.p,{children:"FPN makes two points clear:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"First, when addressing problems with multi-scale characteristics, consider techniques for merging features from different scales."}),"\n",(0,i.jsx)(t.li,{children:'Second, feature fusion can be summarized in three steps: "Bottom-up, top-down, then add."'}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Following this paper, many discussions emerged, such as how to design better feature fusion strategies, improve fusion efficiency, or adjust fusion weights, among other scenarios."}),"\n",(0,i.jsx)(t.p,{children:"There are many subsequent papers, and we\u2019ll explore them together when we have time."})]})}function c(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},35110:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/fpn_1-806ea6b48d355fcaea10b6ebeea1065c.jpg"},92881:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/fpn_2-c3d635c32d5e688c8cd4718addfd29a7.jpg"},95508:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/fpn_3-8343b88168361c005df0ee97a372bbf8.jpg"},16319:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/fpn_4-11a1be8958d93fbeb388f60f6f00c1a4.jpg"},3650:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/fpn_5-9d164ae1834a36be83c64d1262a3b821.jpg"},32909:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/fpn_6-29914d6ce938ac71055641c746b66e9a.jpg"},31984:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/fpn_7-f85c22c3c6fe9e3c51d4d9fc8bc59b0c.jpg"},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>o});var i=n(96540);const s={},r=i.createContext(s);function a(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);