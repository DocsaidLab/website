"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([["8962"],{5734:function(n,e,t){t.r(e),t.d(e,{metadata:()=>i,contentTitle:()=>a,default:()=>u,assets:()=>c,toc:()=>d,frontMatter:()=>s});var i=JSON.parse('{"id":"capybara/funcs/onnxengine/onnxengine","title":"ONNXEngine","description":"ONNXEngine(modelpath int = 0, backend Dict[str, Any] = {}, provideroption: Dict[str, Any] = {})","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/capybara/funcs/onnxengine/onnxengine.md","sourceDirName":"capybara/funcs/onnxengine","slug":"/capybara/funcs/onnxengine/","permalink":"/en/docs/capybara/funcs/onnxengine/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1734942587000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"get_onnx_metadata","permalink":"/en/docs/capybara/funcs/onnxengine/get_onnx_metadata"},"next":{"title":"write_metadata_into_onnx","permalink":"/en/docs/capybara/funcs/onnxengine/write_metadata_into_onnx"}}'),o=t("85893"),r=t("50065");let s={},a="ONNXEngine",c={},d=[];function l(n){let e={a:"a",blockquote:"blockquote",code:"code",h1:"h1",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"onnxengine",children:"ONNXEngine"})}),"\n",(0,o.jsxs)(e.blockquote,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsxs)(e.a,{href:"https://github.com/DocsaidLab/Capybara/blob/975d62fba4f76db59e715c220f7a2af5ad8d050e/capybara/onnxengine/engine.py#L18",children:["ONNXEngine(model_path: Union[str, Path], gpu_id: int = 0, backend: Union[str, int, Backend] = Backend.cpu, session_option: Dict[str, Any] = ",", provider_option: Dict[str, Any] = ",")"]})}),"\n"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Description"}),": Initializes the ONNX model inference engine."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Parameters"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"model_path"})," (",(0,o.jsx)(e.code,{children:"Union[str, Path]"}),"): The file name or serialized ONNX or ORT format model byte string."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"gpu_id"})," (",(0,o.jsx)(e.code,{children:"int"}),"): The GPU ID. Default is 0."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"backend"})," (",(0,o.jsx)(e.code,{children:"Union[str, int, Backend]"}),"): The inference backend, which can be ",(0,o.jsx)(e.code,{children:"Backend.cpu"})," or ",(0,o.jsx)(e.code,{children:"Backend.cuda"}),". Default is ",(0,o.jsx)(e.code,{children:"Backend.cpu"}),"."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"session_option"})," (",(0,o.jsx)(e.code,{children:"Dict[str, Any]"}),"): Parameters for ",(0,o.jsx)(e.code,{children:"onnxruntime.SessionOptions"})," to set session options. Default is ",(0,o.jsx)(e.code,{children:"{}"}),". For detailed configuration, refer to: ",(0,o.jsx)(e.a,{href:"https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.SessionOptions",children:"SessionOptions"}),"."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"provider_option"})," (",(0,o.jsx)(e.code,{children:"Dict[str, Any]"}),"): Parameters for ",(0,o.jsx)(e.code,{children:"onnxruntime.provider_options"}),". Default is ",(0,o.jsx)(e.code,{children:"{}"}),". For detailed configuration, refer to: ",(0,o.jsx)(e.a,{href:"https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#configuration-options",children:"CUDAExecutionProvider"}),"."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Inference"})}),"\n",(0,o.jsx)(e.p,{children:"When loading the model, this function loads the information inside the ONNX file and provides a dictionary with input and output shapes and data types."}),"\n",(0,o.jsxs)(e.p,{children:["Therefore, when calling the ",(0,o.jsx)(e.code,{children:"ONNXEngine"})," instance, you can directly use this dictionary to obtain the output results."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Example"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import capybara as cb\n\nmodel_path = 'model.onnx'\nengine = cb.ONNXEngine(model_path)\nprint(engine)\n\n# Inferencing\n# Assuming the model has two inputs and two outputs and named:\n#   'input1', 'input2', 'output1', 'output2'.\ninput_data = {\n    'input1': np.random.randn(1, 3, 224, 224).astype(np.float32),\n    'input2': np.random.randn(1, 3, 224, 224).astype(np.float32),\n}\n\noutputs = engine(**input_data)\n\noutput_data1 = outputs['output1']\noutput_data2 = outputs['output2']\n"})}),"\n"]}),"\n"]})]})}function u(n={}){let{wrapper:e}={...(0,r.a)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(l,{...n})}):l(n)}},50065:function(n,e,t){t.d(e,{Z:function(){return a},a:function(){return s}});var i=t(67294);let o={},r=i.createContext(o);function s(n){let e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);