"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["37157"],{1787:function(e,i,n){n.r(i),n.d(i,{frontMatter:()=>r,default:()=>h,toc:()=>c,metadata:()=>t,assets:()=>l,contentTitle:()=>o});var t=JSON.parse('{"id":"retail-product/retail-product-recognition-review/index","title":"[20.11] RPR: Review","description":"Retail Product Recognition","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/retail-product/2011-retail-product-recognition-review/index.md","sourceDirName":"retail-product/2011-retail-product-recognition-review","slug":"/retail-product/retail-product-recognition-review/","permalink":"/en/papers/retail-product/retail-product-recognition-review/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1751293824000,"frontMatter":{"title":"[20.11] RPR: Review","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[20.11] DPSNet","permalink":"/en/papers/retail-product/dpsnet/"},"next":{"title":"[22.06] DeepACO","permalink":"/en/papers/retail-product/deepaco/"}}'),s=n(85893),a=n(50065);let r={title:"[20.11] RPR: Review",authors:"Z. Yuan"},o=void 0,l={},c=[{value:"Retail Product Recognition",id:"retail-product-recognition",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Traditional Methods",id:"traditional-methods",level:2},{value:"Deep Learning Methods",id:"deep-learning-methods",level:2},{value:"Challenges in Product Recognition",id:"challenges-in-product-recognition",level:2},{value:"Challenge 1: Large Number of Categories",id:"challenge-1-large-number-of-categories",level:3},{value:"Challenge 2: Domain Gap in Data Distribution",id:"challenge-2-domain-gap-in-data-distribution",level:3},{value:"Challenge 3: High Intraclass Variation",id:"challenge-3-high-intraclass-variation",level:3},{value:"Challenge 4: Insufficient System Flexibility",id:"challenge-4-insufficient-system-flexibility",level:3},{value:"Technical Overview",id:"technical-overview",level:2},{value:"CNN-Based Approaches",id:"cnn-based-approaches",level:3},{value:"Data Augmentation",id:"data-augmentation",level:3},{value:"Fine-Grained Classification",id:"fine-grained-classification",level:3},{value:"Few-Shot Learning",id:"few-shot-learning",level:3},{value:"Public Datasets",id:"public-datasets",level:2},{value:"On-Shelf Image Datasets",id:"on-shelf-image-datasets",level:3},{value:"Checkout Image Datasets",id:"checkout-image-datasets",level:3},{value:"Practical Recommendations",id:"practical-recommendations",level:3},{value:"Future Research Directions",id:"future-research-directions",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){let i={a:"a",admonition:"admonition",blockquote:"blockquote",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.h2,{id:"retail-product-recognition",children:"Retail Product Recognition"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://onlinelibrary.wiley.com/doi/pdf/10.1155/2020/8875910",children:(0,s.jsx)(i.strong,{children:"Deep Learning for Retail Product Recognition: Challenges and Techniques"})})}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.p,{children:"Recently, I revisited and organized references in the field of Automatic Check-Out (ACO)."}),"\n",(0,s.jsx)(i.p,{children:"We begin with several survey papers to quickly grasp the overall landscape of this domain."}),"\n",(0,s.jsx)(i.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,s.jsx)(i.p,{children:"The main goal of Retail Product Recognition technology is to assist retailers in effective product management and to enhance the customer shopping experience. Traditionally, the most widely used method has been barcode recognition, which automatically captures product information by scanning the barcode printed on product packaging."}),"\n",(0,s.jsx)(i.p,{children:"However, since barcode placement is not fixed, in practice it often requires manual rotation of the product to align with the scanner, causing delays in the process."}),"\n",(0,s.jsx)(i.p,{children:"According to a survey by Digimarc, approximately 45% of customers report that barcode scanning is inconvenient."}),"\n",(0,s.jsx)(i.admonition,{type:"tip",children:(0,s.jsx)(i.p,{children:"Digimarc Corporation is a publicly traded company headquartered in Beaverton, Oregon, USA, specializing in the development of digital watermarking and serialized QR code recognition technologies. These technologies are used to enhance product authentication, supply chain tracking, and recycling sorting applications."})}),"\n",(0,s.jsx)(i.p,{children:"In this context, RFID technology can be considered a possible alternative. It transmits data via radio waves and does not rely on line-of-sight scanning to complete recognition tasks, offering theoretical efficiency advantages. Each product is tagged individually, allowing remote reading without the need for precise alignment."}),"\n",(0,s.jsx)(i.p,{children:"The drawback is cost."}),"\n",(0,s.jsx)(i.p,{children:"In the retail industry, where margins are tight, affixing an RFID tag to every item accumulates into a significant expense over time. Additionally, when multiple items are identified simultaneously, RFID signals can suffer from occlusion or interference, leading to errors, which is unfavorable for cost control in high-volume product sales."}),"\n",(0,s.jsx)(i.p,{children:"With the rapid digitalization of the retail sector, companies are increasingly seeking to improve operational efficiency and customer experience through artificial intelligence technologies."}),"\n",(0,s.jsx)(i.p,{children:"According to a Juniper Research report, global retail spending on AI-related services is expected to grow from $3.6 billion in 2019 to $12 billion by 2023, demonstrating strong investment in such technologies. On the other hand, as supermarkets display an ever-growing number of products, labor management costs rise significantly, further driving retailers to pursue higher automation in recognition solutions."}),"\n",(0,s.jsx)(i.p,{children:"The widespread adoption of digital imaging devices has facilitated the generation of large-scale product image datasets, forming a crucial foundation for developing computer vision recognition systems."}),"\n",(0,s.jsx)(i.p,{children:"Product recognition tasks can be viewed as a combined problem of image classification and object detection, with the core objective of automatically identifying product categories and locations through images. This technology can be applied in multiple scenarios:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Shelf Management"}),": Automatically detecting out-of-stock products and alerting staff to restock. Research indicates that fully implementing shelf plans can increase sales by 7.8% and gross profit by 8.1%."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Self-Checkout Systems"}),": Shortening checkout time via product image recognition, enhancing customer satisfaction. SCO system deployment grew steadily from 2014 to 2019 and is widely adopted to reduce labor costs."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Assistance for the Visually Impaired"}),": Helping visually impaired customers recognize product information (such as price, brand, and expiration date), lowering shopping barriers and improving autonomy and social participation."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"From a technical perspective, compared to traditional handcrafted feature methods, deep learning can automatically learn features directly from images, offering higher recognition capability and generalization. Its multilayer structure can extract more detailed semantic information, suitable for complex and multi-category product scenarios."}),"\n",(0,s.jsx)(i.p,{children:"Currently, research teams have applied deep learning to the retail domain and achieved concrete results across multiple tasks. Industry applications such as Amazon Go and Walmart Intelligent Retail Lab have already emerged."}),"\n",(0,s.jsx)(i.p,{children:'Despite the increasing volume of related research in recent years, systematic surveys focused on "deep learning for product recognition tasks" remain limited. Previously, only two surveys on retail shelf product detection have been published; both excluded checkout scenarios and did not focus on deep learning approaches.'}),"\n",(0,s.jsx)(i.p,{children:"The authors of this paper review over one hundred publications from leading conferences and journals like CVPR, ICCV, and AAAI, attempting to integrate existing techniques, challenges, and resources. They hope this paper can serve as an introductory guide for researchers and engineers in the field, helping them quickly grasp core problems and existing achievements."}),"\n",(0,s.jsx)(i.admonition,{type:"tip",children:(0,s.jsx)(i.p,{children:"We especially appreciate authors who are so dedicated and helpful\u2014our sincere gratitude."})}),"\n",(0,s.jsx)(i.h2,{id:"traditional-methods",children:"Traditional Methods"}),"\n",(0,s.jsx)(i.p,{children:"The core of product image recognition lies in extracting representative features from packaging images to accomplish classification and identification tasks."}),"\n",(0,s.jsx)(i.p,{children:"Early computer vision research commonly adopted a modular processing pipeline, decomposing the recognition system into several key steps:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Image Capture"}),": Collecting product images via cameras or mobile devices."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Preprocessing"}),": Removing noise and simplifying information in the input images, including image segmentation, geometric transformations, and contrast enhancement."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Feature Extraction"}),": Analyzing image patches to identify stable features invariant to position or scale changes."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Feature Classification"}),": Mapping extracted features into vector space and applying specific classification algorithms for prediction."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Recognition Output"}),": Producing product category results from a pretrained classifier."]}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["The crucial step in this framework is ",(0,s.jsx)(i.strong,{children:"feature extraction"}),", as its accuracy directly affects the final recognition performance. Before deep learning became widespread, researchers heavily relied on handcrafted features to capture visual characteristics of images."]}),"\n",(0,s.jsx)(i.p,{children:"Two classic methods are:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"SIFT (Scale-Invariant Feature Transform)"}),": Proposed by David Lowe in 1999, it extracts local features at multiple scales using an image pyramid, offering invariance to rotation, translation, and scale, and has been widely used in object matching and classification."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"SURF (Speeded Up Robust Features)"}),": Developed in 2006 based on SIFT, it optimizes computational efficiency and suits applications with real-time requirements."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"However, these handcrafted features depend on developers\u2019 expertise and assumptions, limiting their ability to capture all potentially important information in images. Moreover, when product categories are large, packaging designs vary significantly, or shooting conditions change (e.g., angle, lighting), handcrafted features struggle to maintain recognition stability and scalability. This prompted the research community to gradually shift toward data-driven deep learning methods, which learn the most discriminative feature representations directly from images through end-to-end training."}),"\n",(0,s.jsx)(i.h2,{id:"deep-learning-methods",children:"Deep Learning Methods"}),"\n",(0,s.jsx)(i.p,{children:"Deep learning, a subfield of machine learning, aims to automatically learn multi-level representations from data to capture high-level semantic structures. This approach avoids the limitations of manual feature design and is particularly suitable for high-dimensional data such as images, speech, and text."}),"\n",(0,s.jsx)(i.p,{children:"In image recognition tasks, deep learning advantages have been amplified by improved GPU computing power, gradually replacing traditional methods to become mainstream solutions. In retail product recognition, deep learning mainly covers two tasks:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Image Classification"}),": Assigning input images to predefined categories. With sufficient training data, models achieve accuracy surpassing human-level performance."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Object Detection"}),": Beyond classification, models also predict object locations in images (represented by bounding boxes). This task demands higher model design and computational efficiency and is an indispensable module in product recognition."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The breakthrough in deep learning for images primarily stems from convolutional neural networks (CNNs). Inspired by physiological studies of the cat visual cortex, LeCun et al. first proposed using CNNs for image classification in 1988, successfully applying them to handwritten digit and check recognition."}),"\n",(0,s.jsx)(i.p,{children:"After 2010, the ImageNet challenge accelerated rapid evolution of CNN architectures, spawning various mainstream models:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"AlexNet (2012)"}),": Introduced ReLU and Dropout, breaking traditional image recognition bottlenecks and igniting the deep learning boom."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"GoogLeNet (2014)"}),": Employed Inception modules to reduce parameter count while increasing model depth."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"VGG (2014)"}),": Emphasized uniform 3x3 convolution kernels for easy architecture composition and reuse."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"ResNet (2015)"}),": Proposed residual connections to solve the degradation problem in very deep networks, enabling training of models over 100 layers."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Recent research has also extended CNN applications to 3D structure recognition, developing Multiview CNNs that input multi-angle images for more precise classification, suitable for advanced tasks like 3D product recognition."}),"\n",(0,s.jsxs)(i.p,{children:["In summary, deep learning\u2019s two main driving forces are ",(0,s.jsx)(i.strong,{children:"large-scale data and deeper network structures"}),". Their synergy continuously advances model capabilities in visual recognition tasks, laying the technical foundation for product recognition systems."]}),"\n",(0,s.jsx)(i.p,{children:"Returning to object detection tasks:"}),"\n",(0,s.jsx)(i.p,{children:"Within deep learning, the core goal of object detection is:"}),"\n",(0,s.jsxs)(i.blockquote,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Automatically identifying object categories and their locations (bounding boxes) within images."})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Before deep learning, object detection largely relied on sliding window strategies that scanned fixed-size windows over the entire image, classifying each patch to determine target presence. This method was extremely time-consuming and inefficient for large images or scenes with multiple objects."}),"\n",(0,s.jsx)(i.p,{children:"With deep learning, object detection algorithms are broadly categorized into:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Two-stage methods (region proposal first)"}),"\nRepresented by the R-CNN series, the process involves two stages: first, generating candidate object regions via algorithms like Selective Search; second, classifying and refining these regions with CNNs."]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"R-CNN: Processes each candidate region individually with CNN, improving accuracy but slow in speed."}),"\n",(0,s.jsx)(i.li,{children:"Fast R-CNN: Applies CNN to the entire image once, then uses ROI pooling on feature maps, greatly reducing redundant computation."}),"\n",(0,s.jsx)(i.li,{children:"Faster R-CNN: Introduces a Region Proposal Network (RPN) to learn proposals automatically within the network, sharing features with the classifier, becoming the current mainstream for high accuracy."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"One-stage methods (end-to-end regression)"}),"\nDirectly regress object locations and categories from the image, omitting the proposal stage, thus faster but initially less accurate."]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Representative models include YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector), which show clear advantages in real-time applications such as instant checkout and visual navigation."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Both methods have pros and cons: Two-stage models generally offer higher stability in complex backgrounds, while One-stage models suit deployment environments with low latency requirements. Practical applications choose based on the trade-off between accuracy and timeliness."}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"90%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"pipeline of image-based product recognition",src:n(86282).Z+"",width:"1508",height:"580"})})})}),"\n",(0,s.jsx)(i.p,{children:"Technically, product recognition tasks can be regarded as specialized applications of object detection, with a typical approach as illustrated above:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Product Detection"}),": Use an object detection model to generate multiple bounding boxes indicating product regions."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Region Cropping"}),": Crop each predicted region into a single product image."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Image Classification"}),": Input cropped images into classification models to infer product categories."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"In recent years, several companies have deployed deep learning technologies in retail environments:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Amazon Go (2018)"}),": Utilizes dozens of cameras to capture customer trajectories and combines CNN models to recognize shopping behavior and products. To compensate for pure image recognition limitations, the system integrates Bluetooth and weight sensors to improve overall accuracy."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Walmart IRL (2019)"}),": Focuses on real-time shelf monitoring, using cameras and deep learning models to automatically detect out-of-stock situations and alert restocking personnel."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Chinese Companies (DeepBlue Technology, Malong Technologies)"}),": Offer integrated automatic vending machines, smart weighing systems, and product recognition modules that combine product image analysis, real-time checkout, and classification recommendation. Malong\u2019s AI Fresh system specially targets fresh produce, addressing unstructured visual features such as variations in fruit and vegetable appearance."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Currently, although initial commercial deployments exist, deep learning-based product recognition technologies still face many challenges:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Trade-offs between accuracy and inference speed;"}),"\n",(0,s.jsx)(i.li,{children:"High visual similarity among different products leading to misclassification;"}),"\n",(0,s.jsx)(i.li,{children:"Imbalanced multi-class data and prominent long-tail distributions;"}),"\n",(0,s.jsx)(i.li,{children:"Deployment costs and multi-device stability require evaluation;"}),"\n",(0,s.jsx)(i.li,{children:"Handling non-ideal real-world factors such as occlusion, reflections, and hand interference remains difficult."}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Based on these observations, deep learning holds the most potential for product recognition tasks, but its practical application still needs further empirical research and large-scale field deployment to improve and optimize."}),"\n",(0,s.jsx)(i.h2,{id:"challenges-in-product-recognition",children:"Challenges in Product Recognition"}),"\n",(0,s.jsx)(i.p,{children:"Although product recognition can be viewed as a variant of object detection tasks, the practical requirements and environments differ significantly from general object detection, making direct transfer of existing models difficult."}),"\n",(0,s.jsx)(i.p,{children:"In this section, the authors summarize four major challenges faced by retail product recognition, as outlined below:"}),"\n",(0,s.jsx)(i.h3,{id:"challenge-1-large-number-of-categories",children:"Challenge 1: Large Number of Categories"}),"\n",(0,s.jsxs)(i.p,{children:["Compared to typical object detection tasks, the most distinctive feature of product recognition is that the ",(0,s.jsx)(i.strong,{children:"number of categories far exceeds standard datasets"}),"."]}),"\n",(0,s.jsx)(i.p,{children:"A medium-sized supermarket often stocks thousands of Stock Keeping Units (SKUs), far beyond common datasets. In practice, a single image frequently contains a dozen or more product categories, with subtle differences among categories (such as different specifications within the same brand), resulting in recognition difficulties much greater than usual detection tasks."}),"\n",(0,s.jsx)(i.p,{children:"Moreover, mainstream models like Faster R-CNN, YOLO, and SSD assume a fixed number of classes in their classification heads. When the number of categories expands to thousands, both accuracy and recall drop significantly."}),"\n",(0,s.jsx)(i.p,{children:"The paper\u2019s experiments, as shown below, illustrate this:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"60%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"large classes",src:n(78909).Z+"",width:"1176",height:"856"})})})}),"\n",(0,s.jsxs)(i.p,{children:["Regardless of model architecture, ",(0,s.jsx)(i.strong,{children:"accuracy noticeably decreases when classes increase from 20 to 80"}),"."]}),"\n",(0,s.jsx)(i.p,{children:"Thus, relying solely on traditional object detection architectures faces learning bottlenecks and inference instability due to the high dimensionality of classification in product recognition. This challenge extends beyond architecture design, involving data distribution, inter-class feature representation, and classification strategy design."}),"\n",(0,s.jsx)(i.h3,{id:"challenge-2-domain-gap-in-data-distribution",children:"Challenge 2: Domain Gap in Data Distribution"}),"\n",(0,s.jsx)(i.p,{children:"Deep learning models depend heavily on large annotated datasets for training. However, product recognition faces three main data acquisition constraints:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"High annotation cost:"}),"\nBounding box or segmentation annotation for product recognition usually requires manual labor. Creating tens of thousands of labeled training images demands significant time and human resources. Although tools like LabelImg and LabelMe aid annotation, overall production cost remains a barrier to large-scale expansion."]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"domain gap",src:n(85010).Z+"",width:"1584",height:"374"})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Domain gap between training data and real scenarios:"}),"\nExisting product datasets are often captured under ideal conditions\u2014fixed angles and simple backgrounds (e.g., rotating platforms). Test or deployment environments typically feature complex backgrounds, variable lighting, and frequent occlusions, causing a gap between training and real-world performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"domain gap",src:n(7251).Z+"",width:"1224",height:"464"})})})}),"\n",(0,s.jsxs)(i.ol,{start:"3",children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Imbalanced data and long-tail distribution:"}),"\nProduct datasets commonly exhibit \u201Cfew samples per many classes.\u201D Unlike VOC or COCO which have relatively balanced category distributions, retail datasets contain fewer images but many classes, making learning difficult."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"unbalance",src:n(8873).Z+"",width:"1196",height:"680"})})})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"In summary, data insufficiency limits model performance and hampers generalization and rapid transfer learning for new products. Without systematic solutions for data scarcity and domain shifts, even advanced architectures may fail to meet deployment requirements."}),"\n",(0,s.jsx)(i.h3,{id:"challenge-3-high-intraclass-variation",children:"Challenge 3: High Intraclass Variation"}),"\n",(0,s.jsxs)(i.p,{children:["A major difficulty in product recognition lies in accurately distinguishing products with ",(0,s.jsx)(i.strong,{children:"high intraclass heterogeneity"}),", also known as ",(0,s.jsx)(i.strong,{children:"sub-category recognition"})," or ",(0,s.jsx)(i.strong,{children:"fine-grained classification"}),"."]}),"\n",(0,s.jsx)(i.p,{children:"This challenge has the following characteristics:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Extremely subtle visual differences:"})," Cookies of different flavors or packaging sizes within the same brand may differ only in color saturation or text placement, sometimes indistinguishable by the human eye."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Diverse appearance variations:"})," The same product may look quite different from varying angles or scales; models must be invariant to scale and viewpoint."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Significant environmental interference:"})," Lighting, background, and occlusion significantly impact recognition, posing challenges to model decision boundaries."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Fine-grained classification is well-studied in other domains (e.g., bird species or car model recognition), often relying on extra annotations like keypoints or part alignment to help models learn subtle differences. However, in retail scenarios, challenges are even more pronounced:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Visual similarity between products extends beyond shape to packaging structure, colors, and fonts."}),"\n",(0,s.jsx)(i.li,{children:"Lack of dedicated fine-grained product datasets; most existing datasets only label overall categories without clear sub-category definitions."}),"\n",(0,s.jsx)(i.li,{children:"Without additional labeled data or expert knowledge, models struggle to learn effective discrimination, resulting in increased misclassification or confusion rates."}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"For example, (a) below shows two visually similar products with different flavors:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"intraclass products",src:n(65245).Z+"",width:"1212",height:"360"})})})}),"\n",(0,s.jsx)(i.p,{children:"Differences lie only in slight adjustments of packaging text color and placement; (b) shows the same brand\u2019s different capacity packages, where size difference is hard to judge from a single image. These examples highlight the need for fine-grained representation to address \u201Cextremely similar but not identical\u201D recognition requirements in practical retail environments."}),"\n",(0,s.jsx)(i.h3,{id:"challenge-4-insufficient-system-flexibility",children:"Challenge 4: Insufficient System Flexibility"}),"\n",(0,s.jsx)(i.p,{children:"Retail products are frequently updated, with new items introduced and packaging regularly redesigned. For product image recognition systems, retraining the entire model whenever a new product appears is time-consuming and impractical."}),"\n",(0,s.jsx)(i.p,{children:"An ideal system should have:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Rapid scalability:"})," Ability to incorporate new classes with very few samples (few-shot or zero-shot learning)."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Continual learning capability:"})," Learning new products without forgetting old classes (continual or lifelong learning)."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"However, CNN architectures commonly suffer from the \u201Ccatastrophic forgetting\u201D problem: fine-tuning on new categories causes significant degradation in recognition performance on previously learned classes."}),"\n",(0,s.jsx)(i.p,{children:"For example:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"new classes",src:n(71594).Z+"",width:"1224",height:"712"})})})}),"\n",(0,s.jsx)(i.p,{children:"The model originally recognizes \u201Corange\u201D but loses this ability after training only on the \u201Cbanana\u201D category."}),"\n",(0,s.jsx)(i.p,{children:"Currently, mainstream approaches still rely on retraining the entire model with complete data, which incurs high deployment costs and efficiency bottlenecks. Future recognition architectures supporting the following features would greatly benefit industrial applications:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Models with long-term memory capacity;"}),"\n",(0,s.jsx)(i.li,{children:"Support for incremental class training;"}),"\n",(0,s.jsx)(i.li,{children:"Integration of sample replay or regularization techniques to mitigate forgetting."}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["The ",(0,s.jsx)(i.strong,{children:"flexibility"})," of product recognition systems will determine their usability and lifecycle in rapidly changing markets."]}),"\n",(0,s.jsx)(i.h2,{id:"technical-overview",children:"Technical Overview"}),"\n",(0,s.jsx)(i.p,{children:"This chapter summarizes existing techniques proposed in the literature to address the four major challenges previously outlined. The focus is on recognition architectures centered on deep learning, supplemented by auxiliary methods that can be integrated with them."}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"60%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"techniques",src:n(56541).Z+"",width:"1164",height:"832"})})})}),"\n",(0,s.jsx)(i.p,{children:"Through this categorized summary, readers can more quickly grasp the solution landscape and research trends in product recognition tasks."}),"\n",(0,s.jsx)(i.admonition,{type:"info",children:(0,s.jsx)(i.p,{children:"In the following notes, references will be denoted by \u3010xx\u3011corresponding to citation numbers in the original papers; readers are encouraged to look up the original sources using these identifiers for detailed information."})}),"\n",(0,s.jsx)(i.h3,{id:"cnn-based-approaches",children:"CNN-Based Approaches"}),"\n",(0,s.jsx)(i.p,{children:"One of the core challenges in retail product classification is handling the massive number of categories. Within this context, CNN models are widely used for image feature extraction, generating recognizable embedding vectors as feature descriptors for classification or similarity retrieval."}),"\n",(0,s.jsx)(i.p,{children:"Early handcrafted features like SIFT and SURF, although invariant to rotation and scale, lack semantic-level representations and cannot support large-scale category recognition demands, thus gradually being replaced by CNNs."}),"\n",(0,s.jsx)(i.p,{children:"Below are several classic works; interested readers can refer to the original papers by their citation numbers."}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"80%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"cnn-based",src:n(53749).Z+"",width:"1208",height:"424"})})})}),"\n",(0,s.jsx)(i.p,{children:"While most methods support hundreds to a thousand product categories, medium-to-large supermarkets typically exceed this scale, leaving room for improvement. Two recent representative works further tackle classification tasks with over a thousand categories:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Tonioni et al.\u301020\u3011"}),": Used VGG backbone with MAC (Maximum Activations of Convolutions) features to build whole-image embeddings, handling 3,288 product categories with Precision = 57.07% and mAP = 36.02%."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Karlinsky et al.\u301021\u3011"}),": Employed fine-tuned VGG-F (fixing layers 2\u201315), recognizing 3,235 product categories and achieving mAP = 52.16%."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"These studies demonstrate that CNNs have potential to scale to thousands of categories, but practical recognition performance\u2014especially in recall and fine-grained discrimination\u2014still has significant room for improvement."}),"\n",(0,s.jsx)(i.p,{children:"Additionally, YOLO9000 proposes a detection framework recognizing up to 9,000 classes using an improved Darknet implementation. Its key innovations include multi-dataset joint training and semantic embeddings (e.g., WordTree). However, its training requires millions of labeled images, making it difficult to apply in retail scenarios where product data is costly and annotation is challenging."}),"\n",(0,s.jsx)(i.p,{children:'In summary, CNNs provide a viable foundation for large-scale product classification, but enhancing their scalability, data efficiency, and recognition accuracy under "category explosion" remains a key technical challenge.'}),"\n",(0,s.jsx)(i.h3,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,s.jsx)(i.p,{children:"Deep learning methods rely heavily on training data, but acquiring large annotated datasets in retail product recognition is time-consuming and expensive. Thus, data augmentation becomes a crucial strategy to mitigate data scarcity."}),"\n",(0,s.jsx)(i.p,{children:"Common data augmentation techniques fall into two main categories, as summarized below:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"cnn-based",src:n(67335).Z+"",width:"1212",height:"272"})})})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Traditional image transformation methods (common synthesis methods)"})}),"\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Generative models"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Traditional synthesis methods mainly expand original images through geometric and photometric transformations, as illustrated below:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"common synthesis methods",src:n(32712).Z+"",width:"1224",height:"808"})})})}),"\n",(0,s.jsxs)(i.p,{children:["Popular augmentation tools like ",(0,s.jsx)(i.a,{href:"https://github.com/albumentations-team/albumentations",children:(0,s.jsx)(i.strong,{children:"Albumentations"})})," provide comprehensive APIs supporting translation, rotation, scaling, flipping, random occlusion, noise addition, color enhancement, brightness, and contrast adjustment. These are widely applied in product detection tasks."]}),"\n",(0,s.jsx)(i.p,{children:"Although easy to implement, traditional augmentation struggles to simulate complex real-world conditions such as lighting changes, background clutter, and natural occlusions. Therefore, researchers have turned to generative models to improve data realism."}),"\n",(0,s.jsx)(i.p,{children:"Generative models offer more realistic image synthesis and mainly include two architectures:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"VAE (Variational Autoencoder)"}),"\nUtilizes an encoder-decoder framework to generate samples, suitable for feature learning and attribute control, though currently limited in image-to-image translation tasks."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"GAN (Generative Adversarial Networks)"}),"\nEmploys adversarial training between generator and discriminator to produce visually realistic samples, supporting style transfer and image synthesis."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Relevant literature is summarized below:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"gan-based",src:n(84230).Z+"",width:"1204",height:"640"})})})}),"\n",(0,s.jsx)(i.p,{children:"VAEs consist of encoder and decoder components that learn latent space distributions to generate samples resembling original data. Although no specific application of VAEs in product recognition exists yet, research in other domains shows promise."}),"\n",(0,s.jsx)(i.p,{children:"For example, in face and bird image generation tasks, VAEs can control attributes to produce samples achieving cosine similarity of 0.9057 and reasonable mean squared errors on Wild and CUB datasets\u3010101\u3011. Other studies use conditional VAEs for zero-shot learning, attaining strong performance on AwA, CUB, SUN, and ImageNet datasets\u3010112\u3011."}),"\n",(0,s.jsx)(i.p,{children:"These successes suggest VAEs have potential to enhance data diversity and attribute control, and may be extended to product recognition tasks in the future."}),"\n",(0,s.jsx)(i.p,{children:"In contrast, GANs have made more substantial breakthroughs in image generation recently."}),"\n",(0,s.jsx)(i.p,{children:"Since their introduction in 2014, adversarial training between generator and discriminator enables the synthesis of large amounts of realistic images. Researchers have applied GANs to various data augmentation tasks, including nighttime vehicle detection\u3010115\u3011, semi-supervised semantic hashing\u3010116\u3011, and license plate image generation\u3010122\u3011, all significantly improving model performance."}),"\n",(0,s.jsx)(i.p,{children:"Applications such as PixelCNN combined with one-hot class encoding for generating category-specific images\u3010119\u3011, or CycleGAN for style transfer and simulating real-world scenario variations, demonstrate strong generalization and transfer capabilities."}),"\n",(0,s.jsx)(i.p,{children:"Though few GAN applications exist specifically for product recognition, several studies have preliminarily validated their feasibility."}),"\n",(0,s.jsx)(i.p,{children:"For example, Wei et al.\u30107\u3011combine background synthesis with CycleGAN style transfer to generate product images suited for checkout counter settings (as shown below), training an FPN detector with 96.57% mAP."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"cyclegan",src:n(60061).Z+"",width:"1818",height:"494"})}),"\n",(0,s.jsx)(i.p,{children:"Subsequently, Li et al.\u301078\u3011propose DPNet to select reliable images from synthetic data, boosting checkout accuracy to 80.51%. Another study\u301071\u3011attempts to combine GANs with adversarial encoder training to generate visual samples usable for product recognition."}),"\n",(0,s.jsx)(i.p,{children:"A limitation is that current methods mostly generate flat backgrounds, not yet simulating complex real checkout or shelf scenarios involving background textures, product overlap, and occlusions. Hence, generating more realistic product images remains a promising direction."}),"\n",(0,s.jsx)(i.p,{children:"Future developments may involve enhancing semantic control, integrating 3D modeling and physical rendering engines, or combining domain adaptation and cross-domain augmentation strategies to further reduce the gap between synthetic data and real environments."}),"\n",(0,s.jsx)(i.h3,{id:"fine-grained-classification",children:"Fine-Grained Classification"}),"\n",(0,s.jsx)(i.p,{children:"Fine-grained classification is one of the representative challenging tasks in computer vision, aiming to distinguish subcategories under the same higher-level category, such as different flower species, car models, or animal breeds."}),"\n",(0,s.jsx)(i.p,{children:"When applied to product recognition, the challenge intensifies due to high visual similarity among products combined with multiple interference factors like motion blur, lighting variations, deformation, viewpoint, and placement."}),"\n",(0,s.jsx)(i.p,{children:"Based on existing literature, methods for fine-grained product recognition can be broadly categorized into two groups:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Fine Feature Representation"})}),"\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Context Awareness"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The core of fine-grained classification is to extract discriminative subtle features from visually similar objects. According to the strength of supervision signals, approaches are divided into \u201Cstrongly supervised\u201D and \u201Cweakly supervised\u201D methods:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Strongly Supervised Methods"})}),"\n",(0,s.jsx)(i.p,{children:"These methods require additional annotations such as bounding boxes and part information, enabling precise alignment of local regions and allowing the model to focus on key differences."}),"\n",(0,s.jsx)("div",{style:{whiteSpace:"nowrap",overflowX:"auto",fontSize:"1rem",lineHeight:"0.8",justifyContent:"center",display:"flex"},children:(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Study"}),(0,s.jsx)(i.th,{children:"Method"}),(0,s.jsx)(i.th,{children:"Highlight"}),(0,s.jsx)(i.th,{children:"Application"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Part-based R-CNN\u3010127\u3011"}),(0,s.jsx)(i.td,{children:"R-CNN based, extracts global and local features"}),(0,s.jsx)(i.td,{children:"Achieved SOTA on bird dataset"}),(0,s.jsx)(i.td,{children:"Inspired local feature fusion in product recognition"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Pose-normalized CNN\u3010137\u3011"}),(0,s.jsx)(i.td,{children:"Uses DPM for localization and part extraction, followed by SVM classification"}),(0,s.jsx)(i.td,{children:"75.7% accuracy"}),(0,s.jsx)(i.td,{children:"Suitable for products with significant pose variation"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"DiffNet\u3010139\u3011"}),(0,s.jsx)(i.td,{children:"Compares differences between two similar product images to auto-generate difference annotations"}),(0,s.jsx)(i.td,{children:"No annotation required for common products"}),(0,s.jsx)(i.td,{children:"Product recognition mAP = 95.56%"})]})]})]})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Weakly Supervised Methods"})}),"\n",(0,s.jsx)(i.p,{children:"Weakly supervised methods do not require additional annotations; models learn to automatically discover local regions, suitable for scenarios where annotation costs are high."}),"\n",(0,s.jsx)("div",{style:{whiteSpace:"nowrap",overflowX:"auto",fontSize:"1rem",lineHeight:"0.8",justifyContent:"center",display:"flex"},children:(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Study"}),(0,s.jsx)(i.th,{children:"Method"}),(0,s.jsx)(i.th,{children:"Concept"}),(0,s.jsx)(i.th,{children:"Performance"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Two-Level Attention\u3010126\u3011"}),(0,s.jsx)(i.td,{children:"Extracts both global and local attention features"}),(0,s.jsx)(i.td,{children:"Learns discriminative regions without part annotations"}),(0,s.jsx)(i.td,{children:"Effective for fine-grained classification"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Bilinear CNN\u3010141\u3011"}),(0,s.jsx)(i.td,{children:"Dual-branch CNN collaborating to extract regional features"}),(0,s.jsx)(i.td,{children:"One branch detects regions, the other classifies features"}),(0,s.jsx)(i.td,{children:"84.1% accuracy on Caltech-UCSD birds dataset"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Attention Map\u301074\u3011"}),(0,s.jsx)(i.td,{children:"Uses attention to guide model focus on details"}),(0,s.jsx)(i.td,{children:"Applied on CAPG-GP dataset"}),(0,s.jsx)(i.td,{children:"Significantly outperforms baseline"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Discriminative Patch + SVM\u3010143\u3011"}),(0,s.jsx)(i.td,{children:"Extracts key middle-layer patches from packaging for classification"}),(0,s.jsx)(i.td,{children:"Suitable for visually similar products"}),(0,s.jsx)(i.td,{children:"Good results on supermarket shelf classification"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Self-Attention Module\u3010144\u3011"}),(0,s.jsx)(i.td,{children:"Uses activation maps to identify key image locations"}),(0,s.jsx)(i.td,{children:"Improves cross-domain classification"}),(0,s.jsx)(i.td,{children:"Enhances model generalization"})]})]})]})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.p,{children:["Another approach is ",(0,s.jsx)(i.strong,{children:"Context Awareness"}),"."]}),"\n",(0,s.jsx)(i.p,{children:"When product appearance alone is insufficient for effective classification, \u201Ccontextual information\u201D can serve as an important auxiliary clue, especially since product placement on shelves often follows regular patterns, implying semantic relationships among neighboring items."}),"\n",(0,s.jsx)("div",{style:{whiteSpace:"nowrap",overflowX:"auto",fontSize:"1rem",lineHeight:"0.8",justifyContent:"center",display:"flex"},children:(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Study"}),(0,s.jsx)(i.th,{children:"Method"}),(0,s.jsx)(i.th,{children:"Content"}),(0,s.jsx)(i.th,{children:"Results"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"CRF + CNN\u301053\u3011"}),(0,s.jsx)(i.td,{children:"Combines CNN features with visual similarity of adjacent products"}),(0,s.jsx)(i.td,{children:"Incorporates neighboring context to learn product embeddings"}),(0,s.jsx)(i.td,{children:"91% accuracy, 87% recall"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"SIFT + Context\u301064\u3011"}),(0,s.jsx)(i.td,{children:"Traditional features combined with arrangement relations for hybrid classification"}),(0,s.jsx)(i.td,{children:"11.4% improvement over no-context methods"}),(0,s.jsx)(i.td,{children:"Practical but non-deep learning"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Graph-based Consistency Check\u3010148\u3011"}),(0,s.jsx)(i.td,{children:"Models product arrangement as subgraph isomorphism problem"}),(0,s.jsx)(i.td,{children:"Detects out-of-stock and misplaced products"}),(0,s.jsx)(i.td,{children:"Emphasizes spatial consistency reasoning"})]})]})]})}),"\n",(0,s.jsx)(i.p,{children:"Overall, context awareness techniques are still in early exploratory stages with limited application cases. Future integration with Transformer architectures, spatial modeling, or graph neural networks may further improve classification performance and adaptability in real scenarios."}),"\n",(0,s.jsx)(i.h3,{id:"few-shot-learning",children:"Few-Shot Learning"}),"\n",(0,s.jsx)(i.p,{children:"In practical applications, retail product varieties continuously change, with frequent new product launches and packaging updates. Retraining the entire model every time categories change incurs high time and labor costs."}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"One-Shot Learning (Few-Shot Learning)"})," is thus proposed as a solution, with the core goal:"]}),"\n",(0,s.jsxs)(i.blockquote,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"To recognize new categories using only a very small number of samples (even a single image) without retraining the entire classifier."})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"This technique originates from distance metric learning\u3010149\u3011, where deep models map images into a feature space and classification is performed by nearest neighbor search based on the distance between a query sample and class centers."}),"\n",(0,s.jsx)(i.p,{children:"As illustrated below:"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"metric learning",src:n(51443).Z+"",width:"1112",height:"676"})})})}),"\n",(0,s.jsx)(i.p,{children:"Input image X is classified to the category whose feature center (e.g., C1, C2, C3) is closest in feature space."}),"\n",(0,s.jsx)(i.p,{children:"The key characteristics and application potentials of this method are:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Supports dynamic category expansion:"})," New products can be added to the feature database without retraining the model;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Greatly reduces training data requirements:"})," Particularly suitable for long-tail categories and data-scarce scenarios;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Can integrate with CNN feature extraction modules"})," to maintain semantic embedding quality and classification stability."]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Representative applications in image classification and object detection include:"}),"\n",(0,s.jsx)("div",{style:{whiteSpace:"nowrap",overflowX:"auto",fontSize:"1rem",lineHeight:"0.8",justifyContent:"center",display:"flex"},children:(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Task"}),(0,s.jsx)(i.th,{children:"Method"}),(0,s.jsx)(i.th,{children:"Key Concept"}),(0,s.jsx)(i.th,{children:"Summary of Results"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Image Classification\u3010152\u3011"}),(0,s.jsx)(i.td,{children:"Combines CNN embeddings with color information in metric learning"}),(0,s.jsx)(i.td,{children:"Addresses embedding distortion due to lighting and color variance"}),(0,s.jsx)(i.td,{children:"Improves person re-identification performance"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Image Classification\u3010150\u3011"}),(0,s.jsx)(i.td,{children:"Matching Networks based on neural metric learning"}),(0,s.jsx)(i.td,{children:"Enables fast recognition of new classes on ImageNet"}),(0,s.jsx)(i.td,{children:"One-shot accuracy improved from 87.6% to 93.2%"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Object Detection\u3010155\u3011"}),(0,s.jsx)(i.td,{children:"Combined with R-CNN for animal detection"}),(0,s.jsx)(i.td,{children:"Few-shot animal recognition"}),(0,s.jsx)(i.td,{children:"Successfully applied in scenarios with very limited training samples"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Video Segmentation\u3010154\u3011"}),(0,s.jsx)(i.td,{children:"Requires only a single labeled frame to track specific objects"}),(0,s.jsx)(i.td,{children:"Fine-tunes CNN embeddings on target"}),(0,s.jsx)(i.td,{children:"Enhances one-shot recognition and cross-frame tracking"})]})]})]})}),"\n",(0,s.jsx)(i.p,{children:"Several works also apply these methods to product recognition:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Geng et al.\u301074\u3011"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Propose a coarse-to-fine framework combining feature matching with a one-shot classifier, allowing new product categories to be added without retraining."}),"\n",(0,s.jsx)(i.li,{children:"Evaluated on datasets GroZi-3.2k (mAP = 73.93%), GP-20 (65.55%), and GP181 (85.79%)."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Tonioni et al.\u301020\u3011"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Use similarity matching strategy comparing query images to product samples via CNN features."}),"\n",(0,s.jsx)(i.li,{children:"Classifies using just a single sample, seamlessly handling packaging changes and new category introduction."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"One-shot learning offers product recognition systems a more flexible and scalable design paradigm. Current mainstream approaches focus on combining CNN features with metric learning classification; future directions may integrate few-shot classification, meta-learning, and cross-domain adaptation to better handle real-world variations."}),"\n",(0,s.jsx)(i.h2,{id:"public-datasets",children:"Public Datasets"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"90%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"dataset",src:n(55322).Z+"",width:"1778",height:"646"})})})}),"\n",(0,s.jsx)(i.p,{children:"The performance of deep models depends heavily on data quality and scale, but manual annotation of product images is often costly."}),"\n",(0,s.jsx)(i.p,{children:"To facilitate method comparison and rapid prototyping, the research community has released several public datasets, which can be categorized by application scenarios into:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"On-Shelf Images"}),": Products statically arranged on shelves, simulating restocking, arrangement inspection, and shopping guidance scenarios."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Checkout Images"}),": Checkout viewpoints handling crowded occlusion, multiple mixed items, and product counting challenges."]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"on-shelf-image-datasets",children:"On-Shelf Image Datasets"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"GroZi\u2011120"})," is one of the earliest widely cited retail product recognition datasets, containing 120 product categories. The training set has 676 white-background single product images taken under ideal conditions, suitable for one-shot model design; the test set includes 4,973 real shelf images and 30 minutes of video clips with varied lighting and angles, specifically for evaluating domain adaptation capability."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"GroZi\u20113.2k"})," expands categories and samples, covering 80 major product categories with over 8,000 web-crawled training images; test images come from five physical stores, captured by smartphones, containing 680 images with manual annotations. This dataset is especially suitable for fine-grained classification tasks and domain shift evaluation."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"GroZi\u2011120",src:n(13130).Z+"",width:"932",height:"428"})})})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Freiburg Grocery"})," collected by a German research team contains 25 daily product categories. The training set consists of about 5,000 smartphone images resized to 256\xd7256; the test set includes 74 high-resolution images captured by Kinect v2, featuring occlusions and noise. This dataset effectively tests multi-scale robustness."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Freiburg Grocery",src:n(39774).Z+"",width:"1128",height:"424"})})})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Cigarette Dataset"})," focuses on cigarette recognition with 10 classes. The training set has 3,600 single product images; the test set consists of 354 shelf images from 40 retail stores, containing about 13,000 object annotations. The dataset\u2019s products are visually very similar and densely arranged, making it suitable for testing fine-grained recognition under occlusion."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Cigarette Dataset",src:n(72458).Z+"",width:"1200",height:"480"})})})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Grocery Store Dataset"})," contains 81 product categories with 5,125 images from 18 stores. It uniquely provides both \u201Ciconic\u201D images (from product web pages) and \u201Cnatural\u201D images (in-store photos), making it suitable for cross-domain learning and retrieval tasks."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Grocery Store Dataset",src:n(61958).Z+"",width:"1076",height:"616"})})})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"GP181"})," is a subset of GroZi\u20113.2k with only 183 training and 73 test images, each precisely annotated with bounding boxes. Its small scale and high quality make it ideal for rapid prototyping, few-shot learning, and combined dataset experiments."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"90%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"GP181",src:n(38497).Z+"",width:"1664",height:"362"})})})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"checkout-image-datasets",children:"Checkout Image Datasets"}),"\n",(0,s.jsx)(i.p,{children:"In self-checkout scenarios, image conditions are more challenging due to overlapping products, fixed viewpoints, and variable quantities."}),"\n",(0,s.jsx)(i.p,{children:"Two representative datasets supporting related research are:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"D2S (Dataset to Shop)"})," is the first checkout dataset providing instance-level masks, containing 21,000 high-resolution images covering 60 common retail products (e.g., bottled drinks, cereals, fruits, vegetables). Training images are all single product shots; test images contain mixed product arrangements with 1 to 15 objects, including some synthetic images. The dataset emphasizes diversity in lighting, background, and angles, ideal for testing model generalization and fine segmentation accuracy. Original papers show even strong models like Mask R-CNN or RetinaNet suffer accuracy drops at IoU = 0.75, reflecting scenario complexity."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"D2S",src:n(87603).Z+"",width:"1196",height:"568"})})})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"RPC (Retail Product Checkout)"})," is currently the largest and most practical checkout dataset, comprising 83,739 images covering 200 product categories, organized into 17 mid-level classes. Training data are multi-view single product images (captured by four cameras); testing involves top-down crowded checkout images. The dataset provides bounding boxes and category annotations and defines ",(0,s.jsx)(i.strong,{children:"Checkout Accuracy (cAcc)"}),": a sample is successful only if all products in a single image are correctly recognized and counted. Baseline results show original models (FPN) reach only 56.7% cAcc; with DPNet filtering reliable synthetic samples, cAcc improves to 80.5%. This highlights the critical role of data augmentation quality in checkout tasks."]}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"RPC",src:n(50887).Z+"",width:"1224",height:"548"})})})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"practical-recommendations",children:"Practical Recommendations"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["For ",(0,s.jsx)(i.strong,{children:"product retrieval, fine-grained recognition, or domain adaptation"})," tasks, GroZi\u20113.2k, Grocery Store, or GP181 are recommended."]}),"\n",(0,s.jsxs)(i.li,{children:["For ",(0,s.jsx)(i.strong,{children:"detection and segmentation under occlusion"}),", Cigarette and Freiburg Grocery datasets are preferred."]}),"\n",(0,s.jsxs)(i.li,{children:["For research focused on ",(0,s.jsx)(i.strong,{children:"checkout tasks and counting accuracy"}),", RPC is the best current benchmark; for testing occlusion generalization and segmentation, D2S is ideal."]}),"\n",(0,s.jsx)(i.li,{children:"Most datasets provide both ideal background and in-the-wild images, suitable for testing data augmentation, style transfer, and cross-domain learning strategies."}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Future research will require larger, more diverse, and longitudinal benchmarks. Researchers may also consider developing shared annotation tools and semi-automatic labeling methods to reduce data acquisition and maintenance costs."}),"\n",(0,s.jsx)(i.h2,{id:"future-research-directions",children:"Future Research Directions"}),"\n",(0,s.jsx)(i.p,{children:"The following six research directions are identified by the authors as key avenues for advancing the field:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Generating Product Images Using Deep Neural Networks"}),"\nThe largest publicly available dataset currently covers only 200 product categories, far below the thousands of SKUs in real supermarkets. Frequent packaging updates make comprehensive image collection impractical. Generative models such as DCGAN and CycleGAN have demonstrated the ability to synthesize realistic images. Developing generators that can simulate shelf viewpoints, occlusion structures, and lighting variations would greatly enhance training data diversity and adaptability."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Incorporating Graph Neural Networks for Shelf Arrangement Inspection"}),"\nProduct placement follows spatial structures and contextual patterns that traditional convolutional architectures struggle to capture due to their inability to model non-Euclidean relationships between objects. GNNs can model connections (adjacency, category similarity) between nodes (products) and have been used in recommendation systems and knowledge graph construction. In planogram tasks, GNNs can learn discrepancies between observed and ideal shelf graphs to aid out-of-stock and misplacement detection."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Leveraging Transfer Learning for Cross-Store Recognition"}),"\nMost existing object detection models assume consistent data distribution between training and testing. However, real stores differ greatly in lighting, background, and display styles, often requiring retraining. Transfer learning using pretrained models (e.g., ImageNet) can facilitate rapid adaptation to new environments. Strategies like unsupervised domain adaptation and few-shot fine-tuning can reduce data requirements."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Multimodal Feature Learning Combining Packaging Text and Images"}),"\nFine-grained products often look visually similar, but packaging text (e.g., flavor, volume) provides key distinguishing information. Humans frequently rely on reading packaging text when shopping. Joint learning of image features and OCR-extracted textual information can compensate for pure visual limitations. Future work may explore multimodal pretraining using vision-language models such as BLIP and CLIP."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Model Update Mechanisms Supporting Incremental Learning"}),"\nDeep models suffer from catastrophic forgetting: learning new classes degrades recognition of existing ones. Incremental learning enables models to add new product classes without full retraining. Some studies propose dual-network architectures where an old network preserves historical knowledge, and a new network trains on new classes, combined via feature realignment or distillation. This direction is highly valuable for practical deployment."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Improving Accuracy of Regression-Based Detection Methods"}),"\nRegression-based detectors like YOLO and SSD offer real-time performance suitable for edge devices and self-checkout systems but lag behind two-stage methods (e.g., Faster R-CNN) in accuracy. Future research should focus on improving localization and classification performance without sacrificing inference speed, through approaches such as anchor-free architectures and lightweight attention mechanisms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(i.p,{children:"This review focuses on recent deep learning developments in retail product recognition, systematically organizing and analyzing state-of-the-art strategies from task fundamentals and application challenges. It is structured around four main technical challenges:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Large-scale Classification"})}),"\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Data Limitation"})}),"\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Intraclass Variation"})}),"\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"System Flexibility and Rapid Update Capability"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"In addition to summarizing leading methods, the paper introduces representative datasets and experimental benchmarks to help new researchers quickly understand the technical landscape, lower entry barriers, and focus on promising research breakthroughs."}),"\n",(0,s.jsx)(i.p,{children:"Facing increasing product diversity and complex retail scenarios, the authors hope future researchers can deepen model design and system deployment strategies on this foundation, advancing intelligent retail perception technologies toward higher-level practical applications."})]})}function h(e={}){let{wrapper:i}={...(0,a.a)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},86282:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img1-71e1d4e810038c545f9d4251984afc04.jpg"},67335:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img10-756f3e37d3ed8f7b07a1d04405667da7.jpg"},32712:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img11-1656609ca0ec0bd52883afbbc48b5c84.jpg"},84230:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img12-de359f41f8084e0dea3e3fdebe1e41ec.jpg"},60061:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img13-5ebc0a09bcf2533ca4bbf6b29d8fa80b.jpg"},51443:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img14-68e0e3837efb303e28d84ac8f4a7ec9d.jpg"},55322:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img15-ce427782aa8efc15610e4001a3f2d247.jpg"},13130:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img16-a03a1357e781167a24e701deac4208c3.jpg"},39774:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img17-c1276f665f70b2e2e2713aee150854e4.jpg"},72458:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img18-dfafbe22a87c9204ee84494861ef745f.jpg"},61958:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img19-d5d2b0e1facc10b3bb6c253d508aa9eb.jpg"},78909:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img2-d22281f4b59a1045a398a579f7c303de.jpg"},38497:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img20-a44b4c0efda9a522013070bf02a603c0.jpg"},87603:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img21-e10d56942162b338e7a8046d4578686b.jpg"},50887:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img22-53b67b30d16f9b0afc42972e1ceb3c46.jpg"},85010:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img3-029aad43816b45a288c2e637a287d32b.jpg"},7251:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img4-1de51520d838b446530489d3331c1b25.jpg"},8873:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img5-e3d6023a6002f0e5286859cefe24c186.jpg"},65245:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img6-326ad5eddf6574487948d7db965ccf5b.jpg"},71594:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img7-80b439ddf15c1a3e526f133e23b72cae.jpg"},56541:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img8-dc52ef5fa133476b5ebd7a7362376b68.jpg"},53749:function(e,i,n){n.d(i,{Z:()=>t});let t=n.p+"assets/images/img9-343d8c002016ade55b458913b2b4b2fc.jpg"},50065:function(e,i,n){n.d(i,{Z:()=>o,a:()=>r});var t=n(67294);let s={},a=t.createContext(s);function r(e){let i=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);