"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["90685"],{74446:function(e,t,n){n.r(t),n.d(t,{default:()=>d,frontMatter:()=>r,metadata:()=>i,assets:()=>l,toc:()=>c,contentTitle:()=>o});var i=JSON.parse('{"id":"feature-fusion/unet/index","title":"[15.05] U-Net","description":"The Dawn of Integration","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/feature-fusion/1505-unet/index.md","sourceDirName":"feature-fusion/1505-unet","slug":"/feature-fusion/unet/","permalink":"/en/papers/feature-fusion/unet/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1733839479000,"frontMatter":{"title":"[15.05] U-Net","authors":"Zephyr"},"sidebar":"papersSidebar","previous":{"title":"Feature Fusion (7)","permalink":"/en/papers/category/feature-fusion-7"},"next":{"title":"[16.03] Hourglass","permalink":"/en/papers/feature-fusion/hourglass/"}}'),a=n("85893"),s=n("50065");let r={title:"[15.05] U-Net",authors:"Zephyr"},o=void 0,l={},c=[{value:"The Dawn of Integration",id:"the-dawn-of-integration",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Discussion",id:"discussion",level:2},{value:"ISBI Cell Tracking Challenge 2015",id:"isbi-cell-tracking-challenge-2015",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let t={a:"a",admonition:"admonition",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"the-dawn-of-integration",children:"The Dawn of Integration"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/1505.04597",children:(0,a.jsx)(t.strong,{children:"U-Net: Convolutional Networks for Biomedical Image Segmentation"})})}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:"At a time when the VGG network had just emerged, many unmet needs still lingered."}),"\n",(0,a.jsx)(t.p,{children:"Researchers discovered that traditional CNN architectures were insufficiently granular and failed to meet the challenges posed by biomedical image segmentation."}),"\n",(0,a.jsx)(t.p,{children:"This realization led to the development of a groundbreaking model, one that became a classic in the field of image segmentation."}),"\n",(0,a.jsx)(t.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,a.jsx)(t.p,{children:"In contrast to the thriving image classification domain, where ImageNet dominated and satisfied the research community, biomedical image segmentation researchers faced a different reality."}),"\n",(0,a.jsx)(t.p,{children:"In this field, the availability of training data was severely limited, falling short of the requirements for training deep learning models."}),"\n",(0,a.jsx)(t.p,{children:"The solution to this problem was not straightforward. A previous approach involved dividing training data into multiple smaller patches to generate more training samples. However, this method led to another issue\u2014the loss of contextual information, which in turn reduced segmentation accuracy."}),"\n",(0,a.jsx)(t.p,{children:"Around the same time, another research paper introduced the fully convolutional network (FCN) architecture, which provided some inspiration to the authors."}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://arxiv.org/abs/1411.4038",children:(0,a.jsx)(t.strong,{children:"[14.11] Fully Convolutional Networks for Semantic Segmentation"})})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"fcn arch",src:n(53836).Z+"",width:"1028",height:"516"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Perhaps this architecture could be applied to the problem of biomedical image segmentation to address the issue of contextual information loss."}),"\n",(0,a.jsx)(t.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,a.jsx)(t.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"U-Net arch",src:n(73419).Z+"",width:"1224",height:"824"})}),"\n",(0,a.jsx)(t.p,{children:"Using the entire image did solve the problem of losing contextual information, but the issue of insufficient data remained. The authors proposed the U-Net architecture, which improves segmentation accuracy by reusing high-resolution feature maps and simultaneously reduces the model's data dependency."}),"\n",(0,a.jsx)(t.p,{children:"The diagram above illustrates the U-Net architecture. You can ignore the numbers for now because the authors did not use padding in the convolutional layers, which results in a reduction in feature map size after each convolution. This can be distracting when first encountering the architecture."}),"\n",(0,a.jsx)(t.p,{children:"Let's break the diagram in half and focus on the left side:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"U-Net arch left",src:n(44632).Z+"",width:"2809",height:"1911"})}),"\n",(0,a.jsx)(t.p,{children:"This is the part we often refer to as the Backbone. This section can be swapped out for different architectures. If you prefer MobileNet, use MobileNet. If you prefer ResNet, use ResNet."}),"\n",(0,a.jsx)(t.p,{children:"A basic Backbone design includes five layers of downsampling, corresponding to the five output layers shown in the diagram."}),"\n",(0,a.jsx)(t.p,{children:"Now, let's look at the right side:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"U-Net arch right",src:n(64381).Z+"",width:"2809",height:"1911"})}),"\n",(0,a.jsx)(t.p,{children:"This section is commonly referred to as the Neck. The key characteristic here is the upsampling that begins from the lowest layer. The method can be simple interpolation or more complex transposed convolution. In this paper, the authors used transposed convolution."}),"\n",(0,a.jsx)(t.p,{children:"After upsampling, we obtain higher-resolution feature maps, which are then fused with the feature maps from the corresponding layers. The fusion can be done through simple concatenation or addition, and here the authors opted for concatenation."}),"\n",(0,a.jsx)(t.p,{children:"Following this process, we finally obtain a segmentation result that matches the original image's size. The number of channels can be adjusted based on the segmentation task: one channel for binary segmentation and multiple channels for multi-class segmentation."}),"\n",(0,a.jsxs)(t.admonition,{type:"tip",children:[(0,a.jsx)(t.p,{children:"If you choose addition instead of concatenation, you'll end up with another classic architecture: FPN."}),(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"/en/papers/feature-fusion/fpn/",children:(0,a.jsx)(t.strong,{children:"[16.12] FPN: The Pyramid Structure"})})}),"\n"]})]}),"\n",(0,a.jsx)(t.h2,{id:"discussion",children:"Discussion"}),"\n",(0,a.jsx)(t.h3,{id:"isbi-cell-tracking-challenge-2015",children:"ISBI Cell Tracking Challenge 2015"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"isbi",src:n(72343).Z+"",width:"1224",height:"376"})}),"\n",(0,a.jsx)(t.p,{children:"The authors applied U-Net to the ISBI 2014 and 2015 Cell Tracking Challenges:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"On the PhC-U373 dataset, U-Net achieved a 92% IoU, significantly outperforming the second-place model, which achieved 83%."}),"\n",(0,a.jsx)(t.li,{children:"On the DIC-HeLa dataset, U-Net reached a 77.5% IoU, again vastly exceeding the second-place model's 46%."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"These results demonstrate that U-Net excels across different types of microscopy image segmentation tasks and significantly outperforms other existing methods."}),"\n",(0,a.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(t.p,{children:"U-Net's design preserves high-resolution feature maps, enhancing segmentation accuracy through the fusion of contextual information while reducing the model's reliance on large datasets. The architecture is simple and easily extensible, making it suitable for various image segmentation tasks, including cell segmentation, organ segmentation, and lesion detection."}),"\n",(0,a.jsx)(t.p,{children:"Compared to FPN, the concatenation-based structure of U-Net results in a larger number of parameters and higher computational costs, which can be a limitation in resource-constrained environments. Each architecture has its strengths, and it is valuable to learn different designs and select the most appropriate architecture based on the specific requirements of the task at hand."})]})}function d(e={}){let{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},73419:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img1-f922eb2739ff565ac098d06463c3918c.jpg"},72343:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img2-cebbdc59f99024adc1816027a335cb64.jpg"},53836:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img3-bcda4d0d5aed45e91d91f8aff2fea782.jpg"},44632:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img4-9f953f1151f6c2839e0bfeec27afcb2c.jpg"},64381:function(e,t,n){n.d(t,{Z:function(){return i}});let i=n.p+"assets/images/img5-acd222eac735a66798d7c5df998668f1.jpg"},50065:function(e,t,n){n.d(t,{Z:function(){return o},a:function(){return r}});var i=n(67294);let a={},s=i.createContext(a);function r(e){let t=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);