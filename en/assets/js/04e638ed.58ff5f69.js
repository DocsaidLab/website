"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[16652],{94913:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});var r=t(85437),a=t(74848),i=t(28453);const l={slug:"file-crawler-python-implementation",title:"Python Implementation of a Web File Downloader",authors:"Zephyr",image:"/en/img/2024/0923.webp",tags:["Python","File Crawler"],description:"Implement a simple web file downloader."},o=void 0,s={authorsImageUrls:[void 0]},d=[{value:"Install Required Packages",id:"install-required-packages",level:2},{value:"The Code",id:"the-code",level:2},{value:"Running the Script",id:"running-the-script",level:2}];function u(e){const n={code:"code",h2:"h2",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"We came across a webpage containing hundreds of PDF file links."}),"\n",(0,a.jsx)(n.p,{children:"As engineers, if we were to download them manually, it would be highly inefficient, right?"}),"\n",(0,a.jsx)(n.p,{children:"So, what we need here is a small script that will help us download all the files."}),"\n",(0,a.jsx)(n.h2,{id:"install-required-packages",children:"Install Required Packages"}),"\n",(0,a.jsx)(n.p,{children:"First, you need to install the necessary packages. If you haven't installed them yet, you can do so using the following command:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install requests beautifulsoup4 urllib3\n"})}),"\n",(0,a.jsx)(n.h2,{id:"the-code",children:"The Code"}),"\n",(0,a.jsx)(n.p,{children:"Without further ado, since the script is already written, let's dive straight into the code!"}),"\n",(0,a.jsx)(n.p,{children:"The parts highlighted are the ones you\u2019ll need to modify yourself. Adjust the script according to your needs."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:'{13,16} title="file_crawler.py"',children:'import os\nfrom urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Simulating a browser\'s headers\nheaders = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"\n}\n\n# Web page URL\nurl = "put_your_url_here"\n\n# Target file format\ntarget_format = ".pdf"\n\n# Send an HTTP GET request with headers\nresponse = requests.get(url, headers=headers)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Use BeautifulSoup to parse the HTML\n    soup = BeautifulSoup(response.text, "html.parser")\n\n    # Find all <a> tags and filter those with href attributes matching the target format\n    target_links = []\n    for link in soup.find_all("a"):\n        href = link.get("href")\n        if href and href.endswith(target_format):  # Specify the file format you want to download\n            target_links.append(urljoin(url, href))\n\n    # Create a folder to save the files\n    os.makedirs("downloads", exist_ok=True)\n\n    # Download each file\n    for url in target_links:\n        file_name = url.split("/")[-1]  # Extract the filename from the URL\n        file_path = os.path.join("downloads", file_name)\n\n        # Send a request to download the file\n        response = requests.get(url, headers=headers)  # Add headers again\n        if response.status_code == 200:\n            with open(file_path, "wb") as f:\n                f.write(response.content)\n            print(f"Downloaded: {file_name}")\n        else:\n            print(f"Failed to download: {url}")\nelse:\n    print(f"Unable to access the webpage, status code: {response.status_code}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"running-the-script",children:"Running the Script"}),"\n",(0,a.jsx)(n.p,{children:"Once you\u2019re done, you can simply run the script to download all the files matching the target format."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python file_crawler.py\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>o});var r=t(96540);const a={},i=r.createContext(a);function l(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),r.createElement(i.Provider,{value:n},e.children)}},85437:e=>{e.exports=JSON.parse('{"permalink":"/en/blog/file-crawler-python-implementation","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024/09-23-file-crawler/index.md","title":"Python Implementation of a Web File Downloader","description":"Implement a simple web file downloader.","date":"2024-09-23T00:00:00.000Z","tags":[{"inline":true,"label":"Python","permalink":"/en/blog/tags/python"},{"inline":true,"label":"File Crawler","permalink":"/en/blog/tags/file-crawler"}],"readingTime":1.705,"hasTruncateMarker":true,"authors":[{"name":"Zephyr","title":"Engineer","url":"https://github.com/zephyr-sh","imageURL":"https://github.com/zephyr-sh.png","key":"Zephyr","page":null}],"frontMatter":{"slug":"file-crawler-python-implementation","title":"Python Implementation of a Web File Downloader","authors":"Zephyr","image":"/en/img/2024/0923.webp","tags":["Python","File Crawler"],"description":"Implement a simple web file downloader."},"unlisted":false,"prevItem":{"title":"Update Docusaurus to 3.6.0","permalink":"/en/blog/update-docusaurus-to-3-6-0"},"nextItem":{"title":"Automatically Count Articles in Docusaurus Sidebar","permalink":"/en/blog/customized-docusaurus-sidebars-auto-count"}}')}}]);