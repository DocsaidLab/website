"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[6116],{27740:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var s=n(74848),t=n(28453);const r={},a="[21.08] SimVLM",o={id:"transformers/multimodality/simvlm/index",title:"[21.08] SimVLM",description:"Simplifying Things",source:"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/transformers/multimodality/2108-simvlm/index.md",sourceDirName:"transformers/multimodality/2108-simvlm",slug:"/transformers/multimodality/simvlm/",permalink:"/en/papers/transformers/multimodality/simvlm/",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"zephyr-sh",lastUpdatedAt:172603751e4,frontMatter:{},sidebar:"papersSidebar",previous:{title:"[21.07] ALBEF",permalink:"/en/papers/transformers/multimodality/albef/"},next:{title:"[21.11] METER",permalink:"/en/papers/transformers/multimodality/meter/"}},l={},d=[{value:"Simplifying Things",id:"simplifying-things",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"SimVLM Model Design",id:"simvlm-model-design",level:3},{value:"Pretraining Strategy",id:"pretraining-strategy",level:3},{value:"Datasets",id:"datasets",level:3},{value:"Discussion",id:"discussion",level:2},{value:"How Does the Model Perform?",id:"how-does-the-model-perform",level:3},{value:"How Is the Zero-shot Performance?",id:"how-is-the-zero-shot-performance",level:3},{value:"What Contributes to the Model\u2019s Success?",id:"what-contributes-to-the-models-success",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const i={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"2108-simvlm",children:"[21.08] SimVLM"})}),"\n",(0,s.jsx)(i.h2,{id:"simplifying-things",children:"Simplifying Things"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://arxiv.org/abs/2108.10904",children:(0,s.jsx)(i.strong,{children:"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"})})}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.p,{children:"When things get too complicated, they can become daunting:"}),"\n",(0,s.jsx)(i.p,{children:"Do we really need to make it this complicated?"}),"\n",(0,s.jsx)(i.p,{children:"At this point, GPT-3 had already been released and achieved quite impressive results without complicating things too much."}),"\n",(0,s.jsx)(i.p,{children:"The authors of this paper, based on this idea, thought it might be time to move away from the traditional encoder architecture."}),"\n",(0,s.jsx)(i.p,{children:"Perhaps, we can simplify things?"}),"\n",(0,s.jsx)(i.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,s.jsx)(i.p,{children:"The authors identified several key issues:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Drawbacks of the Pretraining-Finetuning Paradigm"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"While pretraining models like BERT on large-scale unlabeled text corpora using Masked Language Modeling (MLM) followed by finetuning has become mainstream, recent autoregressive language models such as GPT-3 have shown strong performance with few-shot learning, without the need for finetuning."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Challenges in Multimodal Alignment"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Establishing a correspondence between vision and language is challenging. Early approaches relied on manually annotated datasets for object detection and MLM-based pretraining for fusion models."}),"\n",(0,s.jsx)(i.li,{children:"Due to the limited scale of human-annotated data, previous methods not only required complex pretraining schemes but also introduced task-specific auxiliary losses, complicating the entire Visual Language Pretraining (VLP) protocol."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Lack of Zero-Shot Capability"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Current methods based on pretraining and finetuning perform poorly in zero-shot settings, where the model\u2019s generalization ability to unseen tasks is limited."}),"\n",(0,s.jsx)(i.li,{children:"Some methods focus only on specific tasks, making them unsuitable as general-purpose pretrained representations. For instance, certain approaches focus solely on image classification or image-text retrieval tasks."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"solution",children:"Solution"}),"\n",(0,s.jsx)(i.h3,{id:"simvlm-model-design",children:"SimVLM Model Design"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"model_arch",src:n(41567).A+"",width:"1224",height:"704"})}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"PrefixLM"})}),"\n",(0,s.jsx)(i.p,{children:'Inspired by the zero-shot capabilities of autoregressive language models, the authors propose a new approach called "Prefix Language Modeling" or "PrefixLM". Unlike traditional autoregressive language models, PrefixLM has several key features:'}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Bidirectional Attention on the Prefix Sequence"}),":"]}),"\n",(0,s.jsx)(i.p,{children:"PrefixLM considers both the preceding and following context of the prefix sequence, meaning it learns from both past and future information."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Autoregressive Decomposition Only for the Prefix"}),":"]}),"\n",(0,s.jsx)(i.p,{children:"It performs autoregressive decomposition only after the prefix, attempting to predict the subsequent sequence."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Image as the Prefix of Text"}),":"]}),"\n",(0,s.jsx)(i.p,{children:"In vision-language tasks, PrefixLM treats images as prefixes to textual descriptions. This is based on the observation that images often precede textual descriptions."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.admonition,{type:"tip",children:[(0,s.jsx)(i.p,{children:"Suppose we have an image showing a dog playing with a ball in the park. The task of PrefixLM would be to generate a description of this image. The process might look like this:"}),(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Image as Prefix"}),": First, the image's feature representation (possibly through a visual model like ResNet or VGG) is used as a prefix input to the model."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Partial Text Description"}),': With this image as the prefix, a brief description might be added, such as "A dog...".']}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Bidirectional Context Consideration"}),': The model then starts generating the description from the decoder, considering both the context generated so far (e.g., "A dog") and the image\'s prefix information.']}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.strong,{children:"Continued Description Generation"}),': Considering the above context, the model might generate "playing with a ball in the park".']}),"\n"]}),"\n"]}),(0,s.jsx)(i.p,{children:'Combining the image prefix and the generated text, we get the full description: "A dog playing with a ball in the park."'})]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Architecture Design"})}),"\n",(0,s.jsx)(i.p,{children:"Using Transformer as the core architecture, they combine a sequence-to-sequence language model with both visual and textual modules:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Visual Module"})}),"\n",(0,s.jsx)(i.p,{children:"Here, instead of the Linear layer used in ViT to convert image patches to features, ResNet is used. The original ViT accepts only image inputs, but here a part of the text input is incorporated into the token sequence."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Text Module"})}),"\n",(0,s.jsx)(i.p,{children:"Following the SentencePiece tokenization method, specifically the Byte-Pair Encoding (BPE) method, which iteratively merges the most frequent character pairs into new units until a predefined vocabulary size is reached, then learns the encoding for this fixed vocabulary."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.admonition,{type:"tip",children:[(0,s.jsx)(i.p,{children:"Wait a minute! This architecture looks a lot like VL-T5!"}),(0,s.jsx)(i.p,{children:"Let's take a look at the VL-T5 architecture:"}),(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"vl-t5",src:n(17271).A+"",width:"1224",height:"296"})}),(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Both have an Encoder-Decoder structure."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Both incorporate image and text information into the Encoder."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"But they are indeed different!"}),"\n"]}),"\n"]}),(0,s.jsx)(i.p,{children:"At first glance, they seem similar, but they are quite different in practice."}),(0,s.jsxs)(i.p,{children:["In VL-T5, ",(0,s.jsx)(i.strong,{children:"the output of the object detector"})," is used, showing the model cropped image fragments. Additionally, it incorporates two text descriptions:"]}),(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"The first segment: the prefix that tells the model the type of task."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"The second segment: the actual question."}),"\n"]}),"\n"]}),(0,s.jsx)(i.p,{children:"This design leaves most of the difficulties to the Encoder, making the Decoder's role less significant. In fact, the Decoder could even be removed, adding a [CLS] token to the Encoder to answer questions, potentially achieving similar performance."}),(0,s.jsx)(i.p,{children:'In contrast, SimVLM does not leave all the tasks to the Encoder. It creates a "crime scene" scenario where the Decoder, acting as a detective, must infer the results from the clues left by the Encoder.'}),(0,s.jsx)(i.p,{children:'According to the authors, the "prefix" can even be a text description of the image rather than the image itself.'}),(0,s.jsx)(i.p,{children:"This design difference led to nearly a 10% performance improvement in downstream tasks."})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"pretraining-strategy",children:"Pretraining Strategy"}),"\n",(0,s.jsx)(i.p,{children:"There\u2019s no MLM, ITM, or any other techniques you might expect."}),"\n",(0,s.jsx)(i.p,{children:'The authors use only the "PrefixLM" strategy for pretraining, which involves playing a text completion game like GPT, with the addition of image clues.'}),"\n",(0,s.jsx)(i.p,{children:"Wow! This simplicity caught us off guard..."}),"\n",(0,s.jsx)(i.h3,{id:"datasets",children:"Datasets"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"ALIGN Training Dataset"}),': A large dataset of images with associated descriptions or annotations, often containing "a lot of noise". This image-text pair dataset supports multimodal learning tasks and is typically used for visual language pretraining.']}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Colossal Clean Crawled Corpus (C4)"}),": A large text dataset used for pretraining language models, compiled and cleaned from web texts, designed for efficient and large-scale pretraining."]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(i.h3,{id:"how-does-the-model-perform",children:"How Does the Model Perform?"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"result",src:n(46388).A+"",width:"1024",height:"407"})}),"\n",(0,s.jsx)(i.p,{children:"SimVLM can achieve excellent performance with its simplified pretraining and finetuning approach, seamlessly integrating into the pretraining-finetuning workflow:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Model Comparison"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"SimVLM is compared with several state-of-the-art Vision Language Pretraining (VLP) methods, including: LXMERT, VL-T5, UNITER, OSCAR, Villa, SOHO, UNIMO, and VinVL."}),"\n",(0,s.jsx)(i.li,{children:"SimVLM outperforms all compared models on multimodal tasks, setting new SOTA results."}),"\n",(0,s.jsx)(i.li,{children:"This indicates that SimVLM\u2019s generative pretraining method is competitive, and its simple framework with weak supervision can learn high-quality multimodal representations."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Performance on Specific Tasks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Discriminative Tasks"})}),"\n",(0,s.jsx)(i.p,{children:"SimVLMbase, even with a smaller capacity, outperforms all other methods. Notably, SimVLMhuge is the first model to achieve over 80% accuracy on the VQA task, nearly 4 points higher than the previous SOTA (VinVL)."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Complex Visual Language Reasoning"})}),"\n",(0,s.jsx)(i.p,{children:"SimVLM surpasses previous methods on both NLVR2 and SNLI-VE."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Generative Tasks"})}),"\n",(0,s.jsx)(i.p,{children:"SimVLM shows significant improvements, particularly on the CoCo Captions \u201cKarpathy\u201d 5k test split and the NoCaps benchmark, outperforming previous models using more complex CIDEr optimization reinforcement learning methods."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Image Translation"})}),"\n",(0,s.jsx)(i.p,{children:"SimVLM also demonstrates effectiveness in the Multi30k English-to-German image translation task."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"how-is-the-zero-shot-performance",children:"How Is the Zero-shot Performance?"}),"\n",(0,s.jsx)(i.p,{children:"The authors explored three main zero-shot application scenarios:"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Zero-shot/Few-shot Image Captioning"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"result1",src:n(67517).A+"",width:"1024",height:"492"})}),"\n",(0,s.jsx)(i.p,{children:"SimVLM\u2019s pretraining process can be seen as an interpretation of the image captioning target on the web. When used in zero-shot or few-shot settings, the model\u2019s performance is comparable to fully supervised models. Using certain prefix prompts, such as \u201cA picture of,\u201d improves caption quality. The model demonstrates strong generalization capabilities, recognizing real-world concepts and providing detailed descriptions of visual inputs."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Zero-shot Cross-modal Transfer"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"result2",src:n(74722).A+"",width:"1024",height:"506"})}),"\n",(0,s.jsx)(i.p,{children:"SimVLM, a VLP model, is used in this study. Since text training data typically costs less than visual data, the model is finetuned on pure text data and then evaluated on joint vision-language tasks. This method is validated on the SNLI-VE and Multi30k datasets."}),"\n",(0,s.jsx)(i.p,{children:"Particularly in the SNLI-VE application, SimVLM achieves satisfactory zero-shot transfer results by finetuning on text NLI datasets and then using image data as input, comparable to fully supervised methods."}),"\n",(0,s.jsx)(i.p,{children:"Notably, when image features are masked and predictions are made using only hypotheses, the model's performance is similar to random guessing, confirming SimVLM\u2019s effectiveness in cross-modal transfer. Additionally, the model successfully transfers from one modality and language to another, demonstrating its cross-domain and cross-language capabilities."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Open-domain Visual Question Answering (VQA)"})}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"result3",src:n(38379).A+"",width:"1024",height:"487"})}),"\n",(0,s.jsx)(i.p,{children:"In VQA tasks, traditional methods typically frame the question as multi-label classification over a set of 3,129 predefined candidate answers. However, this approach is limited in real-world applications, as a fixed answer set cannot cover all possible scenarios, making open-domain VQA challenging."}),"\n",(0,s.jsx)(i.p,{children:"Experiments show that SimVLM performs exceptionally well in open-domain VQA, outperforming other baseline models, even on questions where the answers are not in the predefined candidate set. Notably, the model can generate meaningful answers for unseen questions even with only a subset of predefined answers used for training. However, without finetuning, the model may struggle to generate meaningful answers for some questions."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"what-contributes-to-the-models-success",children:"What Contributes to the Model\u2019s Success?"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"ablation",src:n(72976).A+"",width:"1002",height:"824"})}),"\n",(0,s.jsx)(i.p,{children:"Firstly, when the model only has a decoder without a bidirectional encoder, its performance on VQA significantly drops. This result suggests that combining bidirectional encoding with unidirectional decoding positively impacts model performance."}),"\n",(0,s.jsx)(i.p,{children:"Next, the study found that among pretraining objectives, PrefixLM outperforms other strategies. This not only demonstrates the effectiveness of PrefixLM but also indicates the importance of having a unified and consistent objective formulation when handling both visual and textual data."}),"\n",(0,s.jsx)(i.p,{children:"Additionally, during training, while weakly aligned image-text data helps the model understand the relationship between vision and text, pure text corpora are also indispensable. This is because pure text corpora provide rich language information, aiding the model in achieving a deeper understanding of language."}),"\n",(0,s.jsx)(i.p,{children:"Lastly, the study emphasizes the importance of convolution stages in Vision-Language (VL) tasks. Specifically, when using three convolution (conv) blocks, the model\u2019s performance is most prominent. This finding reveals the different granularities and characteristics in representing image and text data, suggesting that considering these differences in the model architecture is beneficial."}),"\n",(0,s.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(i.p,{children:"Achieving such results with such a simple architecture is indeed impressive."}),"\n",(0,s.jsx)(i.p,{children:"However, this simplicity does not mean it\u2019s easy to discover or implement. SimVLM\u2019s success exemplifies this principle. This paper not only showcases an effective technical strategy but also emphasizes that in today\u2019s seemingly complex technical world, it is still possible to find simple and straightforward solutions."}),"\n",(0,s.jsx)(i.p,{children:'Often, people tend to believe that complex problems require complex solutions. SimVLM, with its straightforward "PrefixLM" strategy, breaks this stereotype and provides a clear direction for future research.'}),"\n",(0,s.jsx)(i.p,{children:"We can envision that, based on SimVLM\u2019s excellent performance, future researchers might try optimizing from the perspective of ViT, given its numerous evolutionary forms; or deepening the strategy from GPT\u2019s perspective, both of which could further advance the field of visual language pretraining."})]})}function c(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},41567:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/simvlm_1-974d9e80d7b4943767c8686ce2d485a3.jpg"},46388:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/simvlm_2-080a074d01c13a7a04e90dd07b296b06.png"},67517:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/simvlm_3-bc7b69f887e5d30b149a0bf76edd3999.png"},74722:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/simvlm_4-3f82f33f16f582275279c32de216898e.png"},38379:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/simvlm_5-3c215f6bdd6cd9611fa201a4bc9f45c3.png"},72976:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/simvlm_6-19b0d20d66a698c693749537deb31989.png"},17271:(e,i,n)=>{n.d(i,{A:()=>s});const s=n.p+"assets/images/vlt5_2-d4ad3c84bf0acfb645076c8d79e4be97.jpg"},28453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>o});var s=n(96540);const t={},r=s.createContext(t);function a(e){const i=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:i},e.children)}}}]);