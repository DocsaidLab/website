"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["65381"],{49732:function(e,s,n){n.r(s),n.d(s,{frontMatter:()=>l,default:()=>h,contentTitle:()=>r,assets:()=>c,toc:()=>o,metadata:()=>i});var i=JSON.parse('{"id":"face-antispoofing/fas-challenge/index","title":"[24.04] FAS-Challenge","description":"Arsenal","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/face-antispoofing/2404-fas-challenge/index.md","sourceDirName":"face-antispoofing/2404-fas-challenge","slug":"/face-antispoofing/fas-challenge/","permalink":"/en/papers/face-antispoofing/fas-challenge/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1748877226000,"frontMatter":{"title":"[24.04] FAS-Challenge","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[24.03] CFPL-FAS","permalink":"/en/papers/face-antispoofing/cfpl-fas/"},"next":{"title":"[24.04] PD-FAS","permalink":"/en/papers/face-antispoofing/pd-fas/"}}'),a=n(85893),t=n(50065);let l={title:"[24.04] FAS-Challenge",authors:"Z. Yuan"},r=void 0,c={},o=[{value:"Arsenal",id:"arsenal",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Unified Dataset: UniAttackData",id:"unified-dataset-uniattackdata",level:2},{value:"Evaluation Protocols and Generalization Design",id:"evaluation-protocols-and-generalization-design",level:3},{value:"Competition Process and Rules",id:"competition-process-and-rules",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"Arsenal",id:"arsenal-1",level:2},{value:"1. MTFace",id:"1-mtface",level:3},{value:"2. SeaRecluse",id:"2-searecluse",level:3},{value:"3. duileduile",id:"3-duileduile",level:3},{value:"4. BSP-Idiap",id:"4-bsp-idiap",level:3},{value:"5. VAI-Face",id:"5-vai-face",level:3},{value:"6. L&amp;L&amp;W",id:"6-llw",level:3},{value:"7. SARM",id:"7-sarm",level:3},{value:"8. M2-Purdue",id:"8-m2-purdue",level:3},{value:"9. Cloud Recesses",id:"9-cloud-recesses",level:3},{value:"10. Image Lab",id:"10-image-lab",level:3},{value:"11. BOVIFOCR-UFPR",id:"11-bovifocr-ufpr",level:3},{value:"12. Inria-CENATAV-Tec",id:"12-inria-cenatav-tec",level:3},{value:"13. Vicognit",id:"13-vicognit",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){let s={a:"a",annotation:"annotation",blockquote:"blockquote",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mover:"mover",mrow:"mrow",msub:"msub",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.h2,{id:"arsenal",children:"Arsenal"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.a,{href:"https://arxiv.org/abs/2404.06211",children:(0,a.jsx)(s.strong,{children:"Unified Physical-Digital Attack Detection Challenge"})})}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.p,{children:"This is the FAS competition held at CVPR2024, officially titled:"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.a,{href:"https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024",children:(0,a.jsx)(s.strong,{children:"5th Chalearn Face Anti-spoofing Workshop and Challenge@CVPR2024"})})}),"\n",(0,a.jsx)(s.p,{children:"It\u2019s a lively event that everyone working in the FAS field can check out."}),"\n",(0,a.jsx)(s.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,a.jsx)(s.p,{children:'The essence of the Face Anti-Spoofing (FAS) task is to mine the "syntax" of liveness from images.'}),"\n",(0,a.jsx)(s.p,{children:"However, in reality, attack methods have evolved into two parallel technical branches:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Physical Attacks (PA):"})," The impersonator presents the face through physical media such as paper, screens, or silicone masks. These interferences mostly occur at the sensor level and have direct interaction with the real world."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Digital Attacks (DA):"})," Such as Deepfake, face swapping, or adversarial examples, which manipulate from the data generation source or feature level. Although visually realistic, their essence is synthetic."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Traditional methods mostly model one of these categories, thus model performance is limited to their training distribution and cannot generalize broadly."}),"\n",(0,a.jsx)(s.p,{children:"Although both physical and digital attacks belong to the \u201Cfake\u201D class in final classification, their image statistical features and variation directions are highly heterogeneous. This causes intra-class feature distances far larger than expected, leading to a generalization bottleneck."}),"\n",(0,a.jsx)(s.p,{children:'Currently, the core reasons why it is difficult to build a "unified model" are twofold:'}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Lack of large-scale unified datasets:"})," Past works typically splice separately collected PA and DA datasets, without covering complete attack types for the same ID."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Absence of public evaluation benchmarks:"})," Physical and digital attacks use different metrics and protocols, making consistent comparison across domains impossible."]}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["This is the background for initiating the ",(0,a.jsx)(s.strong,{children:"Unified Physical-Digital Attack Detection Challenge"}),". Through new datasets, standard protocols, and open competitions, it attempts to define a new problem setting:"]}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Can a single model handle two heterogeneous spoofing types simultaneously while maintaining discriminative power in unseen domains?"})}),"\n"]}),"\n",(0,a.jsx)(s.h2,{id:"unified-dataset-uniattackdata",children:"Unified Dataset: UniAttackData"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"70%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"uniattackdata",src:n(29683).Z+"",width:"996",height:"532"})})})}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"UniAttackData"})," is currently the largest and most comprehensively designed unified attack dataset, covering 1,800 subjects and totaling 28,706 facial video clips, consisting of the following three sample types:"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Live:"})," 1,800 genuine video clips"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Physical Attacks (PA):"})," 5,400 clips, including print, screen, 3D masks, etc."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Digital Attacks (DA):"})," 21,506 clips, including Deepfake, Face Swap, adversarial examples, etc."]}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["The dataset\u2019s key feature is that ",(0,a.jsx)(s.strong,{children:"each ID has complete corresponding attack samples"}),", ensuring the model\u2019s learning process does not skew due to unbalanced attack type distribution. This design avoids models over-relying on irrelevant secondary features such as identity, ethnicity, or lighting, refocusing training on detecting the \u201Cspoof\u201D itself."]}),"\n",(0,a.jsx)(s.p,{children:"To prevent models from prematurely exploiting dataset artifacts, the research team performed meticulous preprocessing: face regions were cropped and name steganography applied, ensuring no extraneous clues remain at the pixel level in any image."}),"\n",(0,a.jsx)(s.h3,{id:"evaluation-protocols-and-generalization-design",children:"Evaluation Protocols and Generalization Design"}),"\n",(0,a.jsx)(s.p,{children:"The challenge includes two main protocols simulating different real-world deployment scenarios:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Protocol 1: Unified Attack Detection"}),"\nSimulates scenarios where the model must simultaneously recognize both PA and DA, testing the model\u2019s integration and classification capability for mixed attack types."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Protocol 2: Generalization to Unseen Attacks"}),"\nZero-shot testing for \u201Cunseen attack types,\u201D further divided into:"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Protocol 2.1: Unseen Digital Attacks"})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Protocol 2.2: Unseen Physical Attacks"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This protocol adopts a leave-one-type-out strategy, meaning the model completely excludes one attack type during training, forcing it to learn more semantically generalizable discriminative logic."}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h3,{id:"competition-process-and-rules",children:"Competition Process and Rules"}),"\n",(0,a.jsx)(s.p,{children:"The overall competition runs in two phases on the CodaLab platform:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Development Phase (2/1\u20132/22)"}),"\nProvides labeled training data and unlabeled development set; participants can repeatedly submit predictions to the leaderboard for iterative model tuning."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Final Phase (2/23\u20133/3)"}),"\nReveals development set labels and releases an unlabeled test set. Teams must submit final predictions without access to true test labels. The last submission counts as the official score, and teams must publicly release their code and fact sheets to qualify for awards."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,a.jsx)(s.p,{children:"The challenge uses the ISO/IEC 30107-3 international standard metrics to quantify model spoof detection ability, including:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"APCER"})," (Attack Presentation Classification Error Rate)"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"BPCER"})," (Bona Fide Presentation Classification Error Rate)"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ACER"})," (Average Classification Error Rate)"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"AUC"})," (Area Under the ROC Curve)"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"The main ranking metric is ACER, with AUC as a secondary indicator. To maintain evaluation consistency, the final ACER is computed using a threshold calibrated by the Equal Error Rate (EER) on the development set."}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"arsenal-1",children:"Arsenal"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"70%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"participants",src:n(47889).Z+"",width:"984",height:"960"})})})}),"\n",(0,a.jsx)(s.p,{children:"Next is a technical overview of the top 13 participating teams. Let\u2019s take a closer look at what they actually did."}),"\n",(0,a.jsx)(s.h3,{id:"1-mtface",children:"1. MTFace"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"mtface",src:n(3793).Z+"",width:"1224",height:"664"})})})}),"\n",(0,a.jsxs)(s.p,{children:["In this challenge spanning both digital and physical attacks, the ",(0,a.jsx)(s.strong,{children:"MTFace"})," team proposed the ultimate winning solution."]}),"\n",(0,a.jsx)(s.p,{children:"The MTFace architecture can be briefly described as:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Optimized Data Augmentation for Comprehensive Face Attack Detection Across Physical and Digital Domains"})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:'The name sounds long but captures the core issue: how to expose the model during training to "sufficiently diverse and realistic" spoof appearances.'}),"\n",(0,a.jsx)(s.p,{children:"The core of MTFace lies in the joint design of data augmentation and loss balancing."}),"\n",(0,a.jsx)(s.p,{children:"Its data preprocessing steps are as follows:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Face detection and cropping:"})," Detect faces in all images, with an additional 20-pixel margin to retain peripheral features."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Face mask extraction:"})," Pre-extract masks of live data faces for subsequent augmentation use."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:'Then, for the generalization requirements of different protocols, two "semantically aligned augmentation strategies" were designed:'}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Moire simulation augmentation"})," (for Protocols 1 and 2.1)\nScreen replay often causes moire patterns, which become important visual cues for attack recognition.\nMTFace simulates this interference by injecting moire patterns into original images, enabling the model to anticipate this physical phenomenon\u2019s potential impact."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Self-fusion augmentation"}),' (for Protocols 1 and 2.2)\nInspired by previous literature, MTFace uses live data as a base and injects surface features of digital attacks.\nThrough color distortion, spatial deformation, and mask folding, it creates "hybrid samples of digital spoofs," improving the model\u2019s discrimination against Deepfake-type attacks.']}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"The ratio between genuine and attack samples varies greatly across protocols."}),"\n",(0,a.jsx)(s.p,{children:"Without adjustment, the model tends to bias toward the more numerous class, losing the ability to recognize rare types."}),"\n",(0,a.jsx)(s.p,{children:"To address this, MTFace adjusts cross-entropy weight configurations per protocol:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Protocol 1: live : fake = 1 : 1"}),"\nMaintain balanced loss to ensure unbiased learning."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Protocol 2.1 (Unseen DA): live : fake = 5 : 1"}),"\nStrengthen learning of live features to boost defense against digital attacks."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Protocol 2.2 (Unseen PA): live : fake = 2 : 1"}),"\nControl the physical attack sample proportion to avoid over-reliance on moire patterns and mask contours."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Such meticulous tuning allows the model to precisely focus on key features under different protocols while maintaining generalization resilience."}),"\n",(0,a.jsxs)(s.p,{children:["MTFace ultimately adopts ",(0,a.jsx)(s.strong,{children:"ResNet-50 as backbone"}),", loading ",(0,a.jsx)(s.strong,{children:"ImageNet pretrained weights"}),"."]}),"\n",(0,a.jsx)(s.p,{children:"No ViT, no custom large model\u2014just strategy and data, clinching first place in this competition."}),"\n",(0,a.jsx)(s.h3,{id:"2-searecluse",children:"2. SeaRecluse"}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"SeaRecluse"})," chose to build defenses through data boundaries and proportion configurations."]}),"\n",(0,a.jsx)(s.p,{children:"Their solution is named:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Cross-domain Face Anti-spoofing in Unified Physical-Digital Attack Dataset"})}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["This architecture does not emphasize extreme transformations or style transfer. Instead, it focuses on ",(0,a.jsx)(s.strong,{children:"data usage ratios and cropping strategies"})," to optimize model stability and generalization in cross-domain tasks, closer to real deployment conditions."]}),"\n",(0,a.jsxs)(s.p,{children:["SeaRecluse uses ",(0,a.jsx)(s.strong,{children:"SCRFD"})," to detect and crop faces from uncropped images in the training set."]}),"\n",(0,a.jsx)(s.p,{children:"Unlike other teams, they distinguish and supplement between loose and tight crops as one form of data augmentation."}),"\n",(0,a.jsxs)(s.p,{children:["Additionally, for different task protocols, ",(0,a.jsx)(s.strong,{children:"data split ratios and augmentation strategies differ completely"}),":"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Protocol 1: No additional augmentation"}),"\nSimulate baseline recognition performance, training on 80% training data mixed with validation set."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Protocol 2.1 (Unseen DA):"}),"\nDownsample and pad live data edges, expanding real face data to ",(0,a.jsx)(s.strong,{children:"3\xd7"})," the original to balance real and fake samples."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Protocol 2.2 (Unseen PA):"}),"\nTo enhance model perception of fake samples, downsample fake faces by ",(0,a.jsx)(s.strong,{children:"4\xd7 and 8\xd7"}),", totaling ",(0,a.jsx)(s.strong,{children:"7\xd7"})," the original amount."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"The team also corrected some images with abnormal aspect ratios to restore reasonable proportions, preventing the model from learning erroneous biases due to visual distortion."}),"\n",(0,a.jsxs)(s.p,{children:["For image augmentation, all tasks applied standard operations (flipping, random cropping, etc.), with Protocol 2.1 additionally using ",(0,a.jsx)(s.strong,{children:"Gaussian Blur"})," to simulate photographic blur and distant-view defocus."]}),"\n",(0,a.jsxs)(s.p,{children:["The backbone network is ",(0,a.jsx)(s.strong,{children:"ConvNeXt V2"}),"; chosen as a trade-off between performance and computational cost considering resource efficiency and challenge demands."]}),"\n",(0,a.jsx)(s.p,{children:"To further strengthen generalization, the team applied two training techniques:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Image CutMix:"})," Mixes two images and labels to improve adaptation to visual boundaries and spatial variations."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Label Smoothing:"})," Converts hard labels to soft labels to reduce overfitting risk."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"These techniques help the model focus on semantic-level features and mitigate overfitting caused by data imbalance."}),"\n",(0,a.jsx)(s.p,{children:"SeaRecluse\u2019s approach is not drastic but rather wall-building\u2014starting from task-specific data ratios and cropping scales to confine spoofing within the model\u2019s field of vision."}),"\n",(0,a.jsx)(s.p,{children:"It is a patient solution and a deployment-oriented mindset."}),"\n",(0,a.jsx)(s.h3,{id:"3-duileduile",children:"3. duileduile"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"duileduile",src:n(15045).Z+"",width:"1064",height:"688"})})})}),"\n",(0,a.jsxs)(s.p,{children:["The ",(0,a.jsx)(s.strong,{children:"duileduile"})," team designed a modular two-stage learning pipeline that combats the fluid boundary between real and spoof through abstract visual representations. The core of their architecture is:"]}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Swin Transformer + Masked Image Modeling + Physical and Digital Attack Corresponding Augmentation"})}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["The backbone model is ",(0,a.jsx)(s.strong,{children:"Swin-Large Transformer"}),", extracting a 1536-dimensional feature vector, with strong regional awareness and hierarchical abstraction capabilities."]}),"\n",(0,a.jsxs)(s.p,{children:["In the ",(0,a.jsx)(s.strong,{children:"pretraining stage"}),", duileduile employs the ",(0,a.jsx)(s.strong,{children:"simMIM (Simple Masked Image Modeling)"})," strategy,\nwhich partitions the image into non-overlapping patches and randomly masks some parts, forcing the model to learn \u201Creconstructing the whole from incomplete inputs.\u201D"]}),"\n",(0,a.jsx)(s.p,{children:"This self-supervised method effectively enhances the model\u2019s robustness in scenarios with feature loss or occlusion attacks,\nespecially benefiting zero-shot tests on unseen attack types in Protocol 2."}),"\n",(0,a.jsxs)(s.p,{children:["After training the visual syntax, the process moves to the ",(0,a.jsx)(s.strong,{children:"fine-tuning stage"}),". At this point, the strategy shifts from large-scale data stacking to precise pattern fitting per attack type, with corresponding augmentation procedures designed as follows:"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Gaussian Noise:"})," Simulates pixel-level noise and compression artifacts caused by digital attacks"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ColorJitter + Moire Pattern + Gamma Correction:"})," Reconstructs light and shadow variations and display biases of physical attacks"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"These augmentations are not uniformly applied to all data but probabilistically applied according to different training samples and protocol tasks, exposing the model to varied disturbances and spoofs each time it learns."}),"\n",(0,a.jsx)(s.p,{children:"Compared to the first two teams focusing on data ratios and semantic augmentation, duileduile\u2019s method is closer to a platform-like anti-spoof strategy. It applies consistent settings across protocols, possessing high transfer potential and structural consistency."}),"\n",(0,a.jsx)(s.h3,{id:"4-bsp-idiap",children:"4. BSP-Idiap"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"bsp-idiap",src:n(68073).Z+"",width:"1224",height:"492"})})})}),"\n",(0,a.jsxs)(s.p,{children:["The ",(0,a.jsx)(s.strong,{children:"BSP-Idiap"})," team took a different path: ",(0,a.jsx)(s.strong,{children:"returning to the signal\u2019s intrinsic texture rhythm"})," by reconstructing attack signals in the frequency domain."]}),"\n",(0,a.jsxs)(s.p,{children:["Their method is called: ",(0,a.jsx)(s.strong,{children:"DBPixBiS (Dual-Branch Pixel-wise Binary Supervision)"})]}),"\n",(0,a.jsx)(s.p,{children:"Building upon their prior design philosophy, they expanded it into a dual-branch architecture."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"DBPixBiS"})," employs a dual-branch neural network structure:"]}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"RGB branch:"})," Uses ",(0,a.jsx)(s.strong,{children:"Central Difference Convolution (CDC)"})," instead of traditional convolution, emphasizing local texture changes and enhancing the model\u2019s perception of abnormal edges and subtle variation areas."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Fourier branch:"})," Applies Fourier transform to input images and feeds them into a separate feature path, capturing spoofing textures in the frequency domain, such as repetitiveness, interference, and compression residues."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This design enables the model to simultaneously perceive \u201Cvisual anomalies\u201D in the image and \u201Csignal-level spoofing\u201D within the data."}),"\n",(0,a.jsx)(s.p,{children:"To mitigate overfitting and the subtlety of adversarial examples, BSP-Idiap adopts highly targeted training designs:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Pixel-wise Binary Supervision:"})," Supervises binary classification at each pixel on the feature map instead of the entire image, improving recognition of localized spoofing."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Attentional Angular Margin Loss:"})," Adds an angular margin penalty during training, guiding the model to distinctly separate live and spoof feature vectors, reinforcing stable class boundaries."]}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["During testing, the model performs global average pooling on the feature map and uses the ",(0,a.jsx)(s.strong,{children:"mean activation"})," as the final spoof probability score."]}),"\n",(0,a.jsx)(s.h3,{id:"5-vai-face",children:"5. VAI-Face"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"vaiface",src:n(75306).Z+"",width:"1064",height:"520"})})})}),"\n",(0,a.jsxs)(s.p,{children:["In this unified recognition battle, some teams build mechanisms, some restore syntax, and the ",(0,a.jsx)(s.strong,{children:"VAI-Face"})," team chose a third way: ",(0,a.jsx)(s.strong,{children:"create anomalies to strengthen recognition."})]}),"\n",(0,a.jsxs)(s.p,{children:["Their solution centers on the ",(0,a.jsx)(s.strong,{children:"Dinov2 Vision Transformer (ViT-Large)"})," architecture, aiming to ",(0,a.jsx)(s.strong,{children:"expose flaws in distorted faces through visual attention."})]}),"\n",(0,a.jsxs)(s.p,{children:["Their key data augmentation strategy is the ",(0,a.jsx)(s.strong,{children:"deliberate asymmetric treatment of live and fake images."})]}),"\n",(0,a.jsx)(s.p,{children:"They treat the two as semantically heterogeneous sources and thus apply drastically different augmentation pipelines:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Live images:"})," Only undergo ",(0,a.jsx)(s.strong,{children:"RandomResizedCrop"})," and ",(0,a.jsx)(s.strong,{children:"HorizontalFlip"}),", preserving natural distribution and geometric stability."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Fake images:"})," Subject to heavy asymmetric perturbations such as blur, distortion, and custom cutout occlusions, simulating various unnatural traces and structural breakages common in spoof images."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This strategy trains the ViT model to identify abnormal geometry and texture."}),"\n",(0,a.jsx)(s.p,{children:"Beyond data strategies, VAI-Face exhibits high engineering sensitivity in training configuration:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"OneCycleLR:"})," Precisely controls learning rate ramp-up and decay to improve convergence efficiency and generalization."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Label Smoothing:"})," Prevents overconfidence on specific patterns, reducing overfitting."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Mixup Augmentation:"})," Combines two images and labels to improve robustness at sample space boundaries."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Optimizer:"})," Uses ADAN, a novel optimizer combining adaptive gradients with momentum, providing more stable gradient dynamics in this challenge."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"ViT-Large inherently possesses strong regional relational modeling ability. When processing spoof images, its global attention mechanism captures subtle inconsistencies, amplifying flaws hard to conceal. They do not add extra branches or generative modules but maximize the backbone\u2019s recognition potential through finely tuned data perturbations and learning schedules, squeezing out generalization from a simple architecture."}),"\n",(0,a.jsx)(s.h3,{id:"6-llw",children:"6. L&L&W"}),"\n",(0,a.jsxs)(s.p,{children:["While most models choose holistic input and full-image classification, the ",(0,a.jsx)(s.strong,{children:"L&L&W"})," team took the opposite approach, starting from local details: slicing a face into countless fragments and piecing together subtle clues of authenticity or spoof."]}),"\n",(0,a.jsx)(s.p,{children:"Their core strategy revolves around:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Patch-based Feature Learning + Frequency-guided Sampling + Local Attention Guidance"})}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["The process begins with image patch extraction, splitting each input image into multiple small regions for independent feature learning. On this foundation, they introduce the ",(0,a.jsx)(s.strong,{children:"Centre Difference Attention (CDA)"})," mechanism."]}),"\n",(0,a.jsx)(s.p,{children:'CDA is an attention method enhancing subtle texture differences, focusing on regions that "should be consistent but show minor variations," such as edge halos, misaligned reassembly, and low-frequency fusion failures.'}),"\n",(0,a.jsxs)(s.p,{children:["Beyond spatial details, L&L&W also leverage hidden frequency-domain clues. They designed the ",(0,a.jsx)(s.strong,{children:"High-Frequency Wavelet Sampler (HFWS)"})," module to concentrate on high-frequency bands in images, aiming to detect compression artifacts, fusion distortions, and unnatural texture discontinuities left during forgery."]}),"\n",(0,a.jsx)(s.p,{children:'This dual-domain fused feature strategy incorporates spatial attention and frequency intensity into the recognition process, allowing the model not only to see "where it looks odd" but also to hear "at which frequencies the anomaly lies."'}),"\n",(0,a.jsxs)(s.p,{children:["To improve prediction stability and multi-angle coverage, the team generates ",(0,a.jsx)(s.strong,{children:"36 different cropped versions"})," per image during testing. Each patch is independently evaluated by the model, and results are averaged to form the final score."]}),"\n",(0,a.jsx)(s.h3,{id:"7-sarm",children:"7. SARM"}),"\n",(0,a.jsxs)(s.p,{children:["The ",(0,a.jsx)(s.strong,{children:"SARM"})," team\u2019s design focuses not on complex structures or data augmentation but on a repeated \u201Ctraining the trainer\u201D process."]}),"\n",(0,a.jsx)(s.p,{children:"Their method is called:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Multi-Attention Training (MAT) + Label Flip Augmentation"})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This is a staged visual refinement approach: first, train a model to understand the task, then use this model to finely tune and adapt to different attack types."}),"\n",(0,a.jsx)(s.p,{children:"SARM splits the process into two phases:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Phase One:"})," Train dedicated pre-detectors for each protocol (P1, P2.1, P2.2) using ",(0,a.jsx)(s.strong,{children:"Supervised Contrastive Learning"})," to maximize semantic distance between live and fake representations."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Phase Two:"})," Fine-tune the actual anti-spoofing model starting from phase one\u2019s representations as initial weights."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This design strengthens \u201Cexpected object understanding\u201D per task, reducing misleading effects common in direct end-to-end training, especially effective for cross-domain generalization tasks (P2.1 and P2.2)."}),"\n",(0,a.jsxs)(s.p,{children:["In data augmentation, SARM innovates by ",(0,a.jsx)(s.strong,{children:"transforming live faces into fake ones and labeling them as fake."})]}),"\n",(0,a.jsxs)(s.p,{children:["This is not ordinary synthetic spoofing but a weak spoof simulation using ",(0,a.jsx)(s.strong,{children:"OpenCV style transformations"}),"."]}),"\n",(0,a.jsx)(s.p,{children:"For training data in P2.1 and P2.2, they apply style variations including hue shifts, lighting changes, gamma adjustments, and camouflage filters."}),"\n",(0,a.jsxs)(s.p,{children:["These processed live images are labeled as ",(0,a.jsx)(s.strong,{children:"spoof"}),", creating a ",(0,a.jsx)(s.strong,{children:"label-flip augmentation"})," set to generate more diverse, distribution-closer \u201Cweak attack samples,\u201D thereby narrowing the domain gap."]}),"\n",(0,a.jsxs)(s.p,{children:["The optimizer is ",(0,a.jsx)(s.strong,{children:"Adam"}),", combined with cross-entropy and contrastive losses for stable training. Protocol P1 uses standard strategies for stable convergence, while P2.1/P2.2 introduces enhanced augmentation."]}),"\n",(0,a.jsx)(s.p,{children:'This method mainly relies on "prior task intention understanding" to drive model convergence.'}),"\n",(0,a.jsx)(s.h3,{id:"8-m2-purdue",children:"8. M2-Purdue"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"m2-purdue",src:n(66453).Z+"",width:"1064",height:"368"})})})}),"\n",(0,a.jsxs)(s.p,{children:["The ",(0,a.jsx)(s.strong,{children:"M2-Purdue"})," team offers a different strategy."]}),"\n",(0,a.jsx)(s.p,{children:"They abandoned complex architecture design and heavy data augmentation, opting to use CLIP\u2019s semantic representations combined with an \u201Cextreme risk\u2013oriented\u201D loss, named:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Robust Face Attack Detection with CLIP + MLP + CVAR\u2013AUC Loss Fusion"})}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["The pipeline starts with standard image preprocessing, resizing all inputs to ",(0,a.jsx)(s.strong,{children:"224\xd7224"})," to ensure scale consistency."]}),"\n",(0,a.jsxs)(s.p,{children:["Next, they extract semantic features via ",(0,a.jsx)(s.strong,{children:"CLIP\u2019s image encoder"}),", converting visual information into deep embedding representations."]}),"\n",(0,a.jsxs)(s.p,{children:["The key here is not creating new features but \u201Cleveraging a model pretrained on massive data,\u201D allowing CLIP to output generalized semantic features. They then attach a three-layer ",(0,a.jsx)(s.strong,{children:"MLP classifier"})," as the task-specific decision module \u2014 the only part requiring fine-tuning \u2014 minimalist yet precise, fitting modern lightweight deployment needs."]}),"\n",(0,a.jsx)(s.p,{children:"The most distinctive aspect is the dual loss design, integrating two supervisory signals:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"CVAR (Conditional Value at Risk) Loss:"})," Common in financial risk control, focusing on tail risk\u2014the hardest-to-classify, highest-risk sample regions."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"AUC (Area Under Curve) Loss:"})," Emphasizes overall discriminative ability, optimizing the model\u2019s perception of correct ranking."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"The overall loss is defined as:"}),"\n",(0,a.jsx)(s.span,{className:"katex-display",children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{mathvariant:"script",children:"L"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mi,{children:"\u03BB"}),(0,a.jsx)(s.mo,{children:"\u22C5"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{mathvariant:"script",children:"L"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{mathvariant:"normal",children:"C"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"V"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"A"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"R"})]})]}),(0,a.jsx)(s.mo,{children:"+"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"\u03BB"}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"}),(0,a.jsx)(s.mo,{children:"\u22C5"}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{mathvariant:"script",children:"L"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{mathvariant:"normal",children:"A"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"U"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"C"})]})]})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\mathcal{L} = \\lambda \\cdot \\mathcal{L}_{\\mathrm{CVAR}} + (1 - \\lambda) \\cdot \\mathcal{L}_{\\mathrm{AUC}}"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathcal",children:"L"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"\u03BB"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22C5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathcal",children:"L"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3283em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord mathrm mtight",children:"CVAR"})})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"+"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord",children:"1"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"\u03BB"}),(0,a.jsx)(s.span,{className:"mclose",children:")"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22C5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathcal",children:"L"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3283em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:(0,a.jsx)(s.span,{className:"mord mathrm mtight",children:"AUC"})})})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]})]})]})]})}),"\n",(0,a.jsxs)(s.p,{children:["where parameter ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"\u03BB"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\lambda"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"\u03BB"})]})})]})," balances \u201Cextreme risk sensitivity\u201D and \u201Coverall generalization.\u201D"]}),"\n",(0,a.jsx)(s.p,{children:"The underlying logic is: not striving for all-round correctness, but minimizing errors in the \u201Cmost error-prone zones.\u201D"}),"\n",(0,a.jsxs)(s.p,{children:["Training uses ",(0,a.jsx)(s.strong,{children:"Adam"})," for parameter updates without excessive hyperparameter tuning. Built on CLIP features and risk-aware objectives, this simple architecture achieves stable convergence and decent cross-domain recognition."]}),"\n",(0,a.jsx)(s.h3,{id:"9-cloud-recesses",children:"9. Cloud Recesses"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"cloud-recesses",src:n(22750).Z+"",width:"1224",height:"202"})})})}),"\n",(0,a.jsxs)(s.p,{children:["If the essence of anti-spoofing is finding flaws on the face, then the ",(0,a.jsx)(s.strong,{children:"Cloud Recesses"})," team\u2019s strategy provides a creative twist: ",(0,a.jsx)(s.strong,{children:"masking the most recognizable parts, forcing the model to understand authenticity from subtler signals."})]}),"\n",(0,a.jsx)(s.p,{children:"Their method is called:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Random Masking for Face Anti-Spoofing Detection"})}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["This is a data-level adversarial training approach that directly obscures key facial regions, ",(0,a.jsx)(s.strong,{children:"forcing the model to learn liveness recognition even without clear views of eyes and mouth."})]}),"\n",(0,a.jsx)(s.p,{children:"The overall process breaks down into three steps:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Face Detection:"})," Use ",(0,a.jsx)(s.strong,{children:"RetinaFace"})," to crop faces from original images, standardizing them to 256\xd7256 size."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Keypoint Annotation:"})," Use ",(0,a.jsx)(s.strong,{children:"dlib"})," to detect 68 facial landmarks, precisely outlining eyes, nose tip, lips, etc."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Random Masking:"})," For each training sample, randomly mask three to five key regions, depriving the model of shortcut cues relying on facial features."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This design intentionally disrupts features the model might overly depend on (such as eye sclera texture and mouth contours), forcing it to learn more abstract, stable liveness cues like skin granularity, facial contour continuity, and local motion blur."}),"\n",(0,a.jsxs)(s.p,{children:["The backbone is ",(0,a.jsx)(s.strong,{children:"EfficientNet"}),", balancing accuracy and efficiency, suitable for lightweight recognition on masked images."]}),"\n",(0,a.jsx)(s.p,{children:"Training is straightforward without complex hyperparameter tuning, treating the masking strategy as a core data-level regularizer that imposes \u201Cvisual stress,\u201D compelling the model to reason with incomplete information."}),"\n",(0,a.jsx)(s.p,{children:"Cloud Recesses\u2019 solution involves no extra modules, no dual branches, no feature fusion \u2014 just removing key visual cues from the face to test if the model can recognize truth from darkness."}),"\n",(0,a.jsx)(s.h3,{id:"10-image-lab",children:"10. Image Lab"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"image_lab",src:n(55434).Z+"",width:"1224",height:"592"})})})}),"\n",(0,a.jsxs)(s.p,{children:['In anti-spoofing detection tasks, we usually want the model to "see clearly," but the ',(0,a.jsx)(s.strong,{children:"Image Lab"}),' team chose to have the model "see many times." Their architecture is called:']}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Multiattention-Net: A Deep Visual Recognition Network Composed of Multiple Attention Layers"})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This design systematically integrates multi-stage spatial information and multiple attentions, extracting possible anomaly cues from different dimensions of the image."}),"\n",(0,a.jsxs)(s.p,{children:["The network starts with a ",(0,a.jsx)(s.strong,{children:"7\xd77 convolutional layer"})," capturing local texture details, followed by ten layers of ",(0,a.jsx)(s.strong,{children:"modified squeezed residual blocks"}),", each paired with ",(0,a.jsx)(s.strong,{children:"max pooling"}),", progressively abstracting input information into increasingly global spatial semantics."]}),"\n",(0,a.jsxs)(s.p,{children:["Throughout this process, spatial information is continuously extracted at each layer, finally followed by a ",(0,a.jsx)(s.strong,{children:"Dual Attention Block"})," that strengthens the recognition weights of key regions. This enables the model to discover spoofing traces both in fine details and structure."]}),"\n",(0,a.jsxs)(s.p,{children:["Finally, ",(0,a.jsx)(s.strong,{children:"Global Average Pooling (GAP)"})," reduces dimensions, and all layer outputs are concatenated before entering a fully connected layer for classification."]}),"\n",(0,a.jsxs)(s.p,{children:["During training, Image Lab uses ",(0,a.jsx)(s.strong,{children:"Binary Focal Cross Entropy Loss"}),", a loss function designed to penalize ",(0,a.jsx)(s.strong,{children:"minority classes"})," and ",(0,a.jsx)(s.strong,{children:"high-confidence errors"})," more strongly, with the formula:"]}),"\n",(0,a.jsx)(s.span,{className:"katex-display",children:(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{mathvariant:"script",children:"L"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{separator:"true",children:","}),(0,a.jsxs)(s.mover,{accent:"true",children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{children:"^"})]}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"\u03B1"}),(0,a.jsx)(s.mo,{children:"\u22C5"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsxs)(s.mover,{accent:"true",children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{children:"^"})]}),(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mo,{stretchy:"false",children:")"}),(0,a.jsx)(s.mi,{children:"\u03B3"})]}),(0,a.jsx)(s.mo,{children:"\u22C5"}),(0,a.jsx)(s.mi,{children:"log"}),(0,a.jsx)(s.mo,{children:"\u2061"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsxs)(s.mover,{accent:"true",children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{children:"^"})]}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsx)(s.mi,{children:"\u03B1"}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"}),(0,a.jsx)(s.mo,{children:"\u22C5"}),(0,a.jsxs)(s.msup,{children:[(0,a.jsxs)(s.mover,{accent:"true",children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{children:"^"})]}),(0,a.jsx)(s.mi,{children:"\u03B3"})]}),(0,a.jsx)(s.mo,{children:"\u22C5"}),(0,a.jsx)(s.mi,{children:"log"}),(0,a.jsx)(s.mo,{children:"\u2061"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mo,{children:"\u2212"}),(0,a.jsxs)(s.mover,{accent:"true",children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{children:"^"})]}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\mathcal{L}(y, \\hat{y}) = -\\alpha \\cdot (1 - \\hat{y})^\\gamma \\cdot \\log(\\hat{y}) - (1 - \\alpha) \\cdot \\hat{y}^\\gamma \\cdot \\log(1 - \\hat{y})"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord mathcal",children:"L"}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,a.jsx)(s.span,{className:"mpunct",children:","}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsx)(s.span,{className:"mord accent",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6944em"},children:[(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"accent-body",style:{left:"-0.1944em"},children:(0,a.jsx)(s.span,{className:"mord",children:"^"})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1944em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"mclose",children:")"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6667em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(s.span,{className:"mord",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03B1"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22C5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord",children:"1"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord accent",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6944em"},children:[(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"accent-body",style:{left:"-0.1944em"},children:(0,a.jsx)(s.span,{className:"mord",children:"^"})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1944em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsxs)(s.span,{className:"mclose",children:[(0,a.jsx)(s.span,{className:"mclose",children:")"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.7144em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05556em"},children:"\u03B3"})})]})})})})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22C5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsxs)(s.span,{className:"mop",children:["lo",(0,a.jsx)(s.span,{style:{marginRight:"0.01389em"},children:"g"})]}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord accent",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6944em"},children:[(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"accent-body",style:{left:"-0.1944em"},children:(0,a.jsx)(s.span,{className:"mord",children:"^"})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1944em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"mclose",children:")"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord",children:"1"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03B1"}),(0,a.jsx)(s.span,{className:"mclose",children:")"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22C5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.9088em",verticalAlign:"-0.1944em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord accent",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6944em"},children:[(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"accent-body",style:{left:"-0.1944em"},children:(0,a.jsx)(s.span,{className:"mord",children:"^"})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1944em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.7144em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05556em"},children:"\u03B3"})})]})})})})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u22C5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsxs)(s.span,{className:"mop",children:["lo",(0,a.jsx)(s.span,{style:{marginRight:"0.01389em"},children:"g"})]}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord",children:"1"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord accent",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6944em"},children:[(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"accent-body",style:{left:"-0.1944em"},children:(0,a.jsx)(s.span,{className:"mord",children:"^"})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1944em"},children:(0,a.jsx)(s.span,{})})})]})}),(0,a.jsx)(s.span,{className:"mclose",children:")"})]})]})]})}),"\n",(0,a.jsx)(s.p,{children:"where:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsxs)(s.mover,{accent:"true",children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mo,{children:"^"})]})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\hat{y}"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.8889em",verticalAlign:"-0.1944em"}}),(0,a.jsx)(s.span,{className:"mord accent",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6944em"},children:[(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"accent-body",style:{left:"-0.1944em"},children:(0,a.jsx)(s.span,{className:"mord",children:"^"})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.1944em"},children:(0,a.jsx)(s.span,{})})})]})})]})})]})," is the predicted probability, ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"y"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"y"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]})})]})," is the ground truth;"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"\u03B1"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mn,{children:"0.25"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\alpha = 0.25"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03B1"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"0.25"})]})]})]})," (to handle class imbalance);"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"\u03B3"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mn,{children:"3"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\gamma = 3"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03B3"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"3"})]})]})]})," (to emphasize hard samples)."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Though Multiattention-Net is deeper than many other participants\u2019 models, its modular design and residual stability ensure smooth training, and combined with meticulous loss weighting, it demonstrates strong generalization and convergence efficiency."}),"\n",(0,a.jsx)(s.h3,{id:"11-bovifocr-ufpr",children:"11. BOVIFOCR-UFPR"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"bovifocr-ufpr",src:n(15624).Z+"",width:"1224",height:"548"})})})}),"\n",(0,a.jsxs)(s.p,{children:['In this battle focused on "recognizing spoofs," ',(0,a.jsx)(s.strong,{children:"BOVIFOCR-UFPR"})," is among the few teams extending analysis beyond the 2D plane. Rather than only analyzing image colors and textures, they attempt to ",(0,a.jsx)(s.strong,{children:"reconstruct the 3D authenticity of the entire face"}),", thereby detecting spatial inconsistencies that spoofs cannot reproduce."]}),"\n",(0,a.jsx)(s.p,{children:"Their core solution is:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"3D Reconstruction + ArcFace + Chamfer Loss"})}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["Inspired by ",(0,a.jsx)(s.strong,{children:"3DPC-Net"}),", the framework centers on an ",(0,a.jsx)(s.strong,{children:"Encoder-Decoder architecture"}),":"]}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Preprocessing:"})," Uses high-quality alignment and cropping to ensure consistent image scale and unified facial regions."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Encoder:"})," Employs ",(0,a.jsx)(s.strong,{children:"ResNet-50"})," backbone to extract high-level features."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Decoder:"})," Transforms features into corresponding ",(0,a.jsx)(s.strong,{children:"3D point cloud representations"}),", simulating the face's geometric shape in space as a basis for subsequent discrimination."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:'With this structure, the model learns not only "how similar it looks" but "how plausible it is," verifying spatial consistency.'}),"\n",(0,a.jsx)(s.p,{children:"During training, the team combines two feature-guided losses:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ArcFace Loss:"})," Enhances class separation, ensuring feature vectors from different identities are angularly separable."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Chamfer Distance Loss:"})," Measures spatial distance between predicted and ground-truth 3D point clouds to constrain geometric accuracy."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"The model must classify live/fake and simultaneously output a 3D geometry consistent with reality."}),"\n",(0,a.jsx)(s.p,{children:'This method is currently the only team using "3D point cloud reconstruction," demonstrating the potential of geometry-driven anti-spoofing.'}),"\n",(0,a.jsx)(s.p,{children:"It is a cross-dimensional recognition approach, relying on geometric plausibility rather than pixel details to build defenses."}),"\n",(0,a.jsx)(s.h3,{id:"12-inria-cenatav-tec",children:"12. Inria-CENATAV-Tec"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"inria-cenatav-tec",src:n(49154).Z+"",width:"1224",height:"356"})})})}),"\n",(0,a.jsxs)(s.p,{children:["The ",(0,a.jsx)(s.strong,{children:"Inria-CENATAV-Tec"})," team returned to a classic question:"]}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Can stable anti-spoofing be achieved under limited computational resources?"})}),"\n"]}),"\n",(0,a.jsxs)(s.p,{children:["Their answer is: ",(0,a.jsx)(s.strong,{children:"MobileNetV3-spoof with hyperparameter tuning"})]}),"\n",(0,a.jsx)(s.p,{children:"This is a dynamic balance experiment between model complexity and recognition accuracy."}),"\n",(0,a.jsx)(s.p,{children:"The pipeline reflects their systematic and conservative strategy:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Facial landmark detection:"})," Uses ",(0,a.jsx)(s.strong,{children:"ResNet-50"})," for landmark detection."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Alignment and fallback:"})," If landmarks are detected, use ",(0,a.jsx)(s.strong,{children:"InsightFace template"})," for precise alignment; otherwise, keep original image resized."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This approach balances rigor (precise alignment when landmarks are available) and fault tolerance (does not discard data without landmarks), showing practical understanding of real-world scenarios."}),"\n",(0,a.jsxs)(s.p,{children:["Choosing ",(0,a.jsx)(s.strong,{children:"MobileNetV3-large1.25"})," as backbone is key."]}),"\n",(0,a.jsxs)(s.p,{children:["This architecture is optimized for edge AI and low-power devices, introducing ",(0,a.jsx)(s.strong,{children:"SE attention blocks"})," and ",(0,a.jsx)(s.strong,{children:"h-swish nonlinear activation"})," to balance parameters and recognition power."]}),"\n",(0,a.jsxs)(s.p,{children:["With ",(0,a.jsx)(s.strong,{children:"SGD optimizer + Multi-step Learning Rate scheduler"}),", the model converges steadily in stages rather than through a one-shot global descent."]}),"\n",(0,a.jsx)(s.p,{children:"Besides basic augmentations (cropping, flipping), the team normalizes mean and standard deviation per protocol dataset to prevent feature shifts caused by domain heterogeneity."}),"\n",(0,a.jsx)(s.p,{children:"This per-protocol preprocessing design, though engineering-level, significantly impacts generalization stability and is a major reason this lightweight model sustains recognition ability across multitask scenarios."}),"\n",(0,a.jsx)(s.h3,{id:"13-vicognit",children:"13. Vicognit"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"vicognit",src:n(83067).Z+"",width:"1224",height:"472"})})})}),"\n",(0,a.jsxs)(s.p,{children:["Among all participating teams, ",(0,a.jsx)(s.strong,{children:"Vicognit"})," is one of the few that explicitly focused on Vision Transformers (ViT)."]}),"\n",(0,a.jsx)(s.p,{children:"Their solution is named:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"FASTormer: Leveraging Vision Transformers for Face Anti-Spoofing"})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This is a design with pure intent and clear strategy, avoiding data manipulation, additional modules, or multi-branch paths. They rely fully on the Transformer architecture\u2019s ability to encode relationships and capture sequential structure, allowing the model to autonomously establish facial liveness syntax from spatial information."}),"\n",(0,a.jsx)(s.p,{children:"The core of Vicognit\u2019s approach is feeding the ViT model with original-resolution inputs without unnecessary downsampling or compression, preserving spatial detail and enabling self-attention to naturally leverage global relationships."}),"\n",(0,a.jsx)(s.p,{children:"Though not novel per se, this strategy is challenging in face anti-spoofing tasks."}),"\n",(0,a.jsx)(s.p,{children:"Since differences between real and fake faces often lie not in geometry but in texture, material, and subtle perturbations, ViT\u2019s global relational modeling fits well to learn these fine and irregular semantic gaps."}),"\n",(0,a.jsx)(s.p,{children:"Their training strategy includes:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Carefully tuning learning rate and weight decay for stable, precise convergence;"}),"\n",(0,a.jsx)(s.li,{children:"Using adaptive training methods to avoid early overfitting common in Transformer architectures;"}),"\n",(0,a.jsx)(s.li,{children:"Omitting extra augmentation pipelines, keeping the architecture simple and focusing learning pressure on sequence pattern construction itself."}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This approach allows FASTormer to effectively capture semantic key points without extra guidance, maintaining good generalization and flexible recognition of unseen patterns."}),"\n",(0,a.jsx)(s.p,{children:"Vicognit\u2019s contribution lies in empirically demonstrating the feasibility of pure Transformer architectures for FAS."}),"\n",(0,a.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"90%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"result",src:n(26263).Z+"",width:"1224",height:"636"})})})}),"\n",(0,a.jsx)(s.p,{children:"According to official final statistics, the top five teams exhibit three key characteristics:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"The top three teams clearly outperform others on ACER"}),", demonstrating high generalization stability."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"The first-place team leads in ACER, AUC, and BPCER,"})," but the best APCER was claimed by the fifth-place team, showing different models have selective advantages on different error types."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"All top five teams come from industry,"})," reflecting the practical design strategies\u2019 substantial impact on UAD effectiveness."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"There is a large ACER variance across teams,"})," indicating UAD is still in early technical exploration without stable consensus or definitive architectures."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"This challenge ultimately reveals not only who won but which model design philosophies hold up."}),"\n",(0,a.jsx)(s.p,{children:"From an overall perspective, the distilled generalization design principles in current UAD tasks can be summarized into three paths:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Path One: Build global awareness using large models (e.g., ViT and CLIP)."})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Path Two: Build robustness through data perturbations (e.g., masking, style transfer, self-fusion augmentations)."})}),"\n",(0,a.jsx)(s.li,{children:(0,a.jsx)(s.strong,{children:"Path Three: Use semantic alignment as feature regularization (e.g., supervised contrastive learning, ArcFace, dual-branch alignment)."})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Almost all effective solutions integrate subsets of these logics in some form."}),"\n",(0,a.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(s.p,{children:"Based on observations and organizers\u2019 summaries of this competition, future UAD development still requires breakthroughs in three major directions:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"More comprehensive dataset construction:"}),"\nAlthough UniAttackData has laid the foundation for unified cross-attack datasets, improvements remain needed in attack type diversity, subject variability, and image quality. Especially with emerging adversarial attacks and style-transferred deepfakes, current sample sizes are insufficient for systematic generalization validation."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Visual Language Model (VLM)-guided generalization strategies:"}),"\nIntroduction of VLMs like CLIP and DINOv2 offers a semantic-level generalization pressure mechanism. More effective leveraging of these multimodal pretrained knowledge for UAD may reduce dependence on labeled spoof data."]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Task protocol and standard reconstruction:"}),"\nExisting protocols, though representative, struggle to cover mixed attacks, multimodal scenarios, and mobile deployment. Developing higher-level task definitions and layered evaluation mechanisms (e.g., distinguishing high-risk from tolerable errors) is necessary to enhance real-world reliability."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Currently, we still walk within the mist of recognizing spoofs."}),"\n",(0,a.jsx)(s.p,{children:"Different teams have illuminated various algorithmic paths\u2014some piercing anomalies via attention, some building robustness through masking, some disentangling spatial truth via 3D reconstruction."}),"\n",(0,a.jsx)(s.p,{children:"But the ultimate question remains unchanged:"}),"\n",(0,a.jsxs)(s.blockquote,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"Do we really understand how a fake face is composed?"})}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"Perhaps the answer is yet to form, but after this open technical contest, we have stepped one step closer."})]})}function h(e={}){let{wrapper:s}={...(0,t.a)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},29683:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img1-8a5cd34f0ce53328e21f3125ac8ac38b.jpg"},15624:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img10-b475a1dfa8c62536b27fe66be5d8eb40.jpg"},49154:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img11-f9acab71992cd26ce0f352cec1c5f5ca.jpg"},83067:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img12-050b94ce8aab2e913e31313c4e054670.jpg"},26263:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img13-0f39a57e34a5766d4b9a7a521f62d8f9.jpg"},47889:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img2-1523068332cc59f1541c54bc01e76275.jpg"},3793:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img3-c251938668e977ee1c78a4a9b56cdd56.jpg"},15045:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img4-e8bd4c8d043ae908fb86df3aeeb387ab.jpg"},68073:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img5-3934b23d1af1d92f25595e9655148b29.jpg"},75306:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img6-9908645fe3b714d593151a33d5ebacad.jpg"},66453:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img7-b8999c9986b76d02b625d8ce0c0bd160.jpg"},22750:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img8-a95385ae759abf761b195b11da68c59b.jpg"},55434:function(e,s,n){n.d(s,{Z:()=>i});let i=n.p+"assets/images/img9-45f8e28df018874a2b203fa2a595e56d.jpg"},50065:function(e,s,n){n.d(s,{Z:()=>r,a:()=>l});var i=n(67294);let a={},t=i.createContext(a);function l(e){let s=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);