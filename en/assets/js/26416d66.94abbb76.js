"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["84499"],{86410:function(e,n,i){i.r(n),i.d(n,{frontMatter:()=>l,default:()=>d,toc:()=>c,metadata:()=>s,assets:()=>o,contentTitle:()=>a});var s=JSON.parse('{"id":"lightweight/mobilenet-v4/index","title":"[24.04] MobileNet-V4","description":"Five Years of Evolution","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/lightweight/2404-mobilenet-v4/index.md","sourceDirName":"lightweight/2404-mobilenet-v4","slug":"/lightweight/mobilenet-v4/","permalink":"/en/papers/lightweight/mobilenet-v4/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739242156000,"frontMatter":{"title":"[24.04] MobileNet-V4","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[21.10] MobileViT","permalink":"/en/papers/lightweight/mobilevit/"},"next":{"title":"Mamba (4)","permalink":"/en/papers/category/mamba"}}'),t=i(85893),r=i(50065);let l={title:"[24.04] MobileNet-V4",authors:"Z. Yuan"},a=void 0,o={},c=[{value:"Five Years of Evolution",id:"five-years-of-evolution",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Hybrid Attention",id:"hybrid-attention",level:3},{value:"Efficient Attention",id:"efficient-attention",level:3},{value:"Various Convolutional Network Architectures",id:"various-convolutional-network-architectures",level:3},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Network Architecture",id:"network-architecture",level:3},{value:"Mobile MQA",id:"mobile-mqa",level:3},{value:"Cross-Device Optimization",id:"cross-device-optimization",level:3},{value:"Discussion",id:"discussion",level:2},{value:"ImageNet Experimental Results",id:"imagenet-experimental-results",level:3},{value:"Effectiveness of MQA",id:"effectiveness-of-mqa",level:3},{value:"MNv4-Conv-S",id:"mnv4-conv-s",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let n={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"five-years-of-evolution",children:"Five Years of Evolution"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2404.10518",children:(0,t.jsx)(n.strong,{children:"MobileNetV4 - Universal Models for the Mobile Ecosystem"})})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:"Five years have passed since the release of MobileNetV3."}),"\n",(0,t.jsx)(n.p,{children:"During this time, the academic community has made significant progress with new network architectures and the application of multi-head attention mechanisms."}),"\n",(0,t.jsx)(n.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(n.p,{children:"With these new advancements, it's tempting to integrate them into MobileNet for improved performance."}),"\n",(0,t.jsx)(n.p,{children:"Firstly, recent trends in attention mechanisms have led to two primary approaches:"}),"\n",(0,t.jsx)(n.h3,{id:"hybrid-attention",children:"Hybrid Attention"}),"\n",(0,t.jsx)(n.p,{children:"This approach focuses on combining the strengths of CNNs and Transformers, making attention mechanisms more suitable for lightweight networks."}),"\n",(0,t.jsx)(n.p,{children:"Notable works include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2108.05895",children:(0,t.jsx)(n.strong,{children:"[21.08] Mobile-Former: Bridging MobileNet and Transformer"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2110.02178",children:(0,t.jsx)(n.strong,{children:"[21.10] MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2303.14189",children:(0,t.jsx)(n.strong,{children:"[23.03] FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization"})})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"efficient-attention",children:"Efficient Attention"}),"\n",(0,t.jsxs)(n.p,{children:["This approach aims to improve the efficiency of MHSA, reducing the complexity from ",(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"O"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsxs)(n.msup,{children:[(0,t.jsx)(n.mi,{children:"n"}),(0,t.jsx)(n.mn,{children:"2"})]}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"O(n^2)"})]})})}),(0,t.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"1.0641em",verticalAlign:"-0.25em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"O"}),(0,t.jsx)(n.span,{className:"mopen",children:"("}),(0,t.jsxs)(n.span,{className:"mord",children:[(0,t.jsx)(n.span,{className:"mord mathnormal",children:"n"}),(0,t.jsx)(n.span,{className:"msupsub",children:(0,t.jsx)(n.span,{className:"vlist-t",children:(0,t.jsx)(n.span,{className:"vlist-r",children:(0,t.jsx)(n.span,{className:"vlist",style:{height:"0.8141em"},children:(0,t.jsxs)(n.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(n.span,{className:"mord mtight",children:"2"})})]})})})})})]}),(0,t.jsx)(n.span,{className:"mclose",children:")"})]})})]})," to ",(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"O"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsx)(n.mi,{children:"n"}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"O(n)"})]})})}),(0,t.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"O"}),(0,t.jsx)(n.span,{className:"mopen",children:"("}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"n"}),(0,t.jsx)(n.span,{className:"mclose",children:")"})]})})]}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Key papers include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2107.06263",children:(0,t.jsx)(n.strong,{children:"[21.07] CMT: Convolutional Neural Networks Meet Vision Transformers"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2205.14756",children:(0,t.jsx)(n.strong,{children:"[22.05] EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2206.02680",children:(0,t.jsx)(n.strong,{children:"[22.06] Separable Self-attention for Mobile Vision Transformers"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2207.05501",children:(0,t.jsx)(n.strong,{children:"[22.07] Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2212.08059",children:(0,t.jsx)(n.strong,{children:"[22.12] Rethinking Vision Transformers for MobileNet Size and Speed"})})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"various-convolutional-network-architectures",children:"Various Convolutional Network Architectures"}),"\n",(0,t.jsx)(n.p,{children:"Beyond attention mechanisms, there are other architectural overviews worth considering:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/en/papers/classic-cnns/convnext/",children:(0,t.jsx)(n.strong,{children:"[22.01] ConvNeXt: Making Convolutions Great Again"})})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These serve as valuable references."}),"\n",(0,t.jsx)(n.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,t.jsx)(n.h3,{id:"network-architecture",children:"Network Architecture"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"uib block",src:i(87659).Z+"",width:"1324",height:"504"})}),"\n",(0,t.jsx)(n.p,{children:"The authors first extended the inverted bottleneck (IB) from MobileNet-V2."}),"\n",(0,t.jsxs)(n.p,{children:["In the diagram, the ",(0,t.jsx)(n.code,{children:"Inverted Bottleneck"})," is the basic structure of MobileNet-V2, which you are likely familiar with."]}),"\n",(0,t.jsxs)(n.p,{children:["The authors added an additional ",(0,t.jsx)(n.code,{children:"Depthwise Convolution"}),' before this structure and allowed the front and middle parts to "combine freely."']}),"\n",(0,t.jsx)(n.p,{children:"This enables four different combinations:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Both selected: Extra DW"}),"\n",(0,t.jsx)(n.li,{children:"First DW selected: Conv Next structure"}),"\n",(0,t.jsx)(n.li,{children:"Middle DW selected: Original IB"}),"\n",(0,t.jsx)(n.li,{children:"Neither selected: Simple FFN"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This flexible module is termed ",(0,t.jsx)(n.code,{children:"UIB"})," (Universal Inverted Bottleneck) in the paper."]}),"\n",(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:["Besides ",(0,t.jsx)(n.code,{children:"UIB"}),", the paper introduces another fixed component for subsequent NAS searches called ",(0,t.jsx)(n.code,{children:"Alternative Fused IB"}),"."]}),(0,t.jsx)(n.p,{children:"One search space rule is:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Fixed initial layer: Start with ",(0,t.jsx)(n.code,{children:"Conv2D"})," (3x3 kernel, stride=2) in the first stage, followed by ",(0,t.jsx)(n.code,{children:"FusedIB"})," in the second stage to balance efficiency and accuracy."]}),"\n"]})]}),"\n",(0,t.jsx)(n.h3,{id:"mobile-mqa",children:"Mobile MQA"}),"\n",(0,t.jsx)(n.p,{children:"After designing the convolutional network architecture, the authors introduced a new attention module: Multi-Query Attention (MQA)."}),"\n",(0,t.jsx)(n.p,{children:"This structure originates from:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/en/papers/transformers/mqa/",children:(0,t.jsx)(n.strong,{children:"[19.11] MQA: Sharing Key-Value"})})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"If you recall the basic structure of attention mechanisms, they consist of Query, Key, and Value."}),"\n",(0,t.jsx)(n.p,{children:"In multi-head attention, each head has its own Query, Key, and Value."}),"\n",(0,t.jsx)(n.p,{children:"The main idea here is that each head has a different Query but shares the same Key and Value."}),"\n",(0,t.jsx)(n.p,{children:"Previous experiments have shown that this design doesn't reduce model performance, especially when the batch size is relatively small compared to the feature dimensions. This significantly reduces computation."}),"\n",(0,t.jsx)(n.p,{children:"This is particularly relevant for mobile devices, where inference often uses a batch size of 1."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"The authors claim to be the first to apply MQA in lightweight networks."})}),"\n",(0,t.jsx)(n.p,{children:"Unlike text-based attention mechanisms, this one is applied to images."}),"\n",(0,t.jsx)(n.p,{children:"Thus, the authors also referenced another paper:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/en/papers/vision-transformers/pvt/",children:(0,t.jsx)(n.strong,{children:"[21.02] PVT: Spatial-Reduction Attention"})})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This paper introduced the Spatial-Reduction Attention (SRA) module, which reduces the spatial dimensions of feature maps before applying attention, thus reducing computation."}),"\n",(0,t.jsx)(n.p,{children:"Combining the two concepts, replacing SRA's attention with MQA and replacing pooling with convolution results in the Mobile MQA proposed in this paper."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"The paper lists almost ten worthwhile papers to read, which we'll compile later... so busy."})}),"\n",(0,t.jsx)(n.h3,{id:"cross-device-optimization",children:"Cross-Device Optimization"}),"\n",(0,t.jsx)(n.p,{children:"The core goal of this paper is cross-device optimization."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"In previous MobileNet-V3 research, optimization was only done for the Pixel series."})}),"\n",(0,t.jsx)(n.p,{children:"During the experiments, the authors gathered several key insights:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-path efficiency issues"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Group convolutions and similar multi-path designs, despite having lower FLOPs counts, have lower efficiency due to memory access complexity."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware support is crucial"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Advanced modules like Squeeze and Excite (SE), GELU, and LayerNorm lack good support on mobile devices."}),"\n",(0,t.jsx)(n.li,{children:"LayerNorm lags behind BatchNorm."}),"\n",(0,t.jsx)(n.li,{children:"SE is slow on mobile accelerators."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"The power of simplicity"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Simple modules like ReLU, Conv2D, DW-Conv2D, and BatchNorm are well-supported on mobile devices."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Based on these insights, the NAS search system prioritizes widely supported standard modules, combining them with the aforementioned ",(0,t.jsx)(n.code,{children:"UIB"})," and ",(0,t.jsx)(n.code,{children:"MQA"})," modules during the search."]}),"\n",(0,t.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)(n.h3,{id:"imagenet-experimental-results",children:"ImageNet Experimental Results"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"imagenet-result-1",src:i(95184).Z+"",width:"1224",height:"708"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"imagenet-result-2",src:i(49739).Z+"",width:"1224",height:"908"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"In the table, the MobileNet series (V1, V2, V3) have been retrained with modern training schedules, resulting in higher performance than reported in the original papers."})}),"\n",(0,t.jsx)(n.p,{children:"The MNv4 models excel in efficiency and demonstrate compatibility across different hardware platforms, enabling seamless deployment in the mobile ecosystem without platform-specific adjustments."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance on CPU"}),": The MNv4 models are twice as fast as MobileNetV3 and significantly outperform other models with similar accuracy."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance on EdgeTPU"}),": MNv4 is twice as fast as MobileNet V3."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance on GPU and Apple ANE"}),": MNv4 shows excellent front-end performance on the S23 GPU and CoreML on the iPhone 13."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Comparison with competing models"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"FastViT"}),": Ranks second on the Apple Neural Engine but has over five times the latency on GPUs compared to MNv4."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"EfficientViT"}),": Performs well on GPUs but has higher latency on the Apple Neural Engine."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MobileNet V3"}),": Performs well on CPUs but poorly on other hardware platforms."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The authors note that MNv4's success is due to the use of UIB blocks, enhanced NAS recipes, and carefully designed search spaces."}),"\n",(0,t.jsx)(n.p,{children:"This model's universality sets a new standard for mobile models achieving optimal performance across various hardware platforms."}),"\n",(0,t.jsx)(n.h3,{id:"effectiveness-of-mqa",children:"Effectiveness of MQA"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"mqa",src:i(76796).Z+"",width:"1224",height:"336"})}),"\n",(0,t.jsx)(n.p,{children:"Experiments confirmed the advantages of MQA in hybrid models."}),"\n",(0,t.jsx)(n.p,{children:"As shown in the table, MQA achieves over 39% acceleration on EdgeTPU and Samsung S23 GPU compared to MHSA, with negligible quality loss (-0.03%)."}),"\n",(0,t.jsx)(n.p,{children:"MQA also reduces MAC and model parameters by over 25%."}),"\n",(0,t.jsx)(n.h3,{id:"mnv4-conv-s",children:"MNv4-Conv-S"}),"\n",(0,t.jsx)(n.p,{children:"The paper proposes four models, with MNv4-Conv-S being the smallest."}),"\n",(0,t.jsx)(n.p,{children:"For other model architectures, please refer to the paper directly."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"MNv4-Conv-S",src:i(95053).Z+"",width:"1224",height:"996"})}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"This is a universal and efficient model designed for optimal operation across the entire mobile ecosystem."}),"\n",(0,t.jsx)(n.p,{children:"MobileNetV4, through a series of innovations and technical optimizations, achieves Pareto-optimal performance on mobile CPUs, GPUs, DSPs, and dedicated accelerators, which is rare in lightweight model research."}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Pareto Optimality is a concept from economics and engineering that describes an optimal state where no party's situation can improve without at least one party's situation worsening. This concept originates from Italian sociologist Vilfredo Pareto."})}),"\n",(0,t.jsx)(n.p,{children:"\uFF0A"}),"\n",(0,t.jsx)(n.p,{children:"In past development experiences, every new lightweight model required extensive testing on various phones..."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Not to see which is best, but to see which phones have issues."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For example, the ",(0,t.jsx)(n.code,{children:"FastViT"})," architecture, which gained significant attention in 2023, showed confusingly reduced performance outside of iPhones."]}),"\n",(0,t.jsx)(n.p,{children:"We appreciate Google testing this round for us, allowing us to use it directly."})]})}function d(e={}){let{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},87659:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/img1-3d70aa43813018cb370af597d75d7001.jpg"},76796:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/img2-f262bd37b07c5eadc795a2bfb79234b1.jpg"},49739:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/img3-43b1f9ea88a03db7c54a037f11d49731.jpg"},95053:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/img4-98882a72392a6f5387725c6adb45133d.jpg"},95184:function(e,n,i){i.d(n,{Z:()=>s});let s=i.p+"assets/images/img5-9722932525e217f3431ea92073bbd641.jpg"},50065:function(e,n,i){i.d(n,{Z:()=>a,a:()=>l});var s=i(67294);let t={},r=s.createContext(t);function l(e){let n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);