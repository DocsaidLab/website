"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["84211"],{62313:function(e,n,i){i.r(n),i.d(n,{default:()=>d,frontMatter:()=>r,metadata:()=>t,assets:()=>c,toc:()=>l,contentTitle:()=>o});var t=JSON.parse('{"id":"contrastive-learning/moco-v2/index","title":"[20.03] MoCo v2","description":"A Comeback","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/contrastive-learning/2003-moco-v2/index.md","sourceDirName":"contrastive-learning/2003-moco-v2","slug":"/contrastive-learning/moco-v2/","permalink":"/en/papers/contrastive-learning/moco-v2/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1739242156000,"frontMatter":{"title":"[20.03] MoCo v2","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[20.02] SimCLR v1","permalink":"/en/papers/contrastive-learning/simclr-v1/"},"next":{"title":"[20.06] SimCLR v2","permalink":"/en/papers/contrastive-learning/simclr-v2/"}}'),s=i("85893"),a=i("50065");let r={title:"[20.03] MoCo v2",authors:"Z. Yuan"},o=void 0,c={},l=[{value:"A Comeback",id:"a-comeback",level:2},{value:"Problem Definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"Enhanced MLP Head",id:"enhanced-mlp-head",level:3},{value:"Stronger Data Augmentation",id:"stronger-data-augmentation",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Comparison with SimCLR",id:"comparison-with-simclr",level:3},{value:"Computational Load and Resource Consumption",id:"computational-load-and-resource-consumption",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let n={a:"a",admonition:"admonition",annotation:"annotation",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"a-comeback",children:"A Comeback"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2003.04297",children:(0,s.jsx)(n.strong,{children:"Improved Baselines with Momentum Contrastive Learning"})})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"FAIR introduced MoCo v1, but before they could even enjoy their success, Google presented SimCLR, which showed a significant performance boost!"}),"\n",(0,s.jsx)(n.p,{children:"This was a major blow to FAIR, and they couldn't just sit back and relax! So, one month after the release of SimCLR, FAIR incorporated some of its strengths and launched MoCo v2."}),"\n",(0,s.jsx)(n.p,{children:"The message was clear: MoCo's framework is still a better choice!"}),"\n",(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"The main comparison in this paper is with SimCLR, and interested readers can refer to our previous article:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/en/papers/contrastive-learning/simclr-v1/",children:(0,s.jsx)(n.strong,{children:"[20.02] SimCLR v1: Winning with Batch Size"})})}),"\n"]})]}),"\n",(0,s.jsx)(n.h2,{id:"problem-definition",children:"Problem Definition"}),"\n",(0,s.jsx)(n.p,{children:"SimCLR made three major improvements in its implementation:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Very large batch sizes"})," (4k or 8k), ensuring a sufficient number of negative samples."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MLP head"}),": The output features of the encoder are passed through a 2-layer MLP (with ReLU) before applying contrastive loss. After training, the encoder\u2019s output (without the MLP output) is usually used for downstream tasks."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stronger data augmentation"}),": This includes stronger color jittering, blurring, etc."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"However, despite the excellent performance, the batch size in SimCLR is really too large, and most people cannot run it."}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"Google, with its ample resources, can use TPUs to handle 4k or 8k batch sizes."})}),"\n",(0,s.jsx)(n.p,{children:"Thus, the authors in this paper attempt to integrate SimCLR\u2019s two major improvements into MoCo:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"MLP projection head"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Stronger data augmentation"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"They found that these improvements are also effective within the MoCo framework and can achieve results surpassing SimCLR even in a typical 8-GPU environment (without requiring ultra-large batch sizes)."}),"\n",(0,s.jsx)(n.p,{children:"This is clearly more user-friendly for general users and makes contrastive learning research easier to reproduce."}),"\n",(0,s.jsx)(n.h2,{id:"solution",children:"Solution"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"model_arch",src:i(47228).Z+"",width:"1200",height:"952"})})})}),"\n",(0,s.jsx)(n.p,{children:"The diagram above shows the architectural differences between SimCLR and MoCo."}),"\n",(0,s.jsx)(n.p,{children:"SimCLR is an end-to-end mechanism, requiring a batch with enough negative samples, typically requiring a batch size of 4k to 8k."}),"\n",(0,s.jsx)(n.p,{children:'In contrast, MoCo uses a "momentum encoder" to maintain representations of negative samples. The negative sample vectors are stored in a "queue" that is shared across batches. Therefore, even with a small batch size, MoCo can use a large queue, providing plenty of negative samples.'}),"\n",(0,s.jsx)(n.p,{children:"Thus, MoCo does not need large batches to retain many negative samples, requiring less hardware resources."}),"\n",(0,s.jsx)(n.h3,{id:"enhanced-mlp-head",children:"Enhanced MLP Head"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"mlp_head",src:i(52284).Z+"",width:"2112",height:"852"})})})}),"\n",(0,s.jsxs)(n.p,{children:["The authors replaced the original fully connected (fc) projection head in MoCo with a 2-layer MLP (2048 units in the intermediate layer, with ReLU), testing different temperature parameters ",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsx)(n.mi,{children:"\u03C4"})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\tau"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,s.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.1132em"},children:"\u03C4"})]})})]}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The results, shown in the table, indicate that with just the fc (no MLP), the ImageNet linear classification accuracy is around 60%\u201361%. However, after replacing it with an MLP, the accuracy increases to about 66% (at the optimal ",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsxs)(n.mrow,{children:[(0,s.jsx)(n.mi,{children:"\u03C4"}),(0,s.jsx)(n.mo,{children:"="}),(0,s.jsx)(n.mn,{children:"0.2"})]}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\tau = 0.2"})]})})}),(0,s.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,s.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.1132em"},children:"\u03C4"}),(0,s.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.jsx)(n.span,{className:"mrel",children:"="}),(0,s.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.6444em"}}),(0,s.jsx)(n.span,{className:"mord",children:"0.2"})]})]})]}),"), which is a significant improvement."]}),"\n",(0,s.jsx)(n.p,{children:"However, in transfer learning for object detection, the improvement from the MLP head is smaller. This raises a common question:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:'"Is a higher linear classification score always better? Does it directly correlate with actual downstream tasks?"'})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Sometimes, this is not entirely true. Linear classification scores are a reference indicator, but they don\u2019t guarantee the same level of improvement in all downstream tasks."}),"\n",(0,s.jsx)(n.h3,{id:"stronger-data-augmentation",children:"Stronger Data Augmentation"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"more_aug",src:i(83287).Z+"",width:"2112",height:"852"})})})}),"\n",(0,s.jsx)(n.p,{children:'Next, the authors added SimCLR\u2019s "blur augmentation" and slightly stronger color distortion to MoCo\u2019s original augmentation strategy. They found that even without the MLP head, just strengthening data augmentation increased ImageNet linear classification accuracy from 60.6% to 63.4%. The detection results also showed similar or better improvements than when using the MLP head.'}),"\n",(0,s.jsx)(n.p,{children:"Finally, when both the MLP head and strong augmentation were used, ImageNet linear classification reached 67.3%, with significant improvements in detection as well."}),"\n",(0,s.jsx)(n.h2,{id:"discussion",children:"Discussion"}),"\n",(0,s.jsx)(n.h3,{id:"comparison-with-simclr",children:"Comparison with SimCLR"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"compare",src:i(81025).Z+"",width:"980",height:"372"})})})}),"\n",(0,s.jsx)(n.p,{children:"SimCLR used a batch size of 4k or 8k in its paper, training for 1000 epochs, achieving 69.3% linear classification accuracy on ImageNet."}),"\n",(0,s.jsx)(n.p,{children:'The authors integrated the two improvements (MLP + augmentation) into MoCo under a more feasible hardware setup (8-GPU, batch size = 256), creating "MoCo v2."'}),"\n",(0,s.jsx)(n.p,{children:"The results show:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"With 200 epochs of pretraining, MoCo v2 achieves 67.5%, which is 5.6% higher than SimCLR\u2019s 61.9% with the same setup, and better than the large-batch SimCLR\u2019s 66.6%."}),"\n",(0,s.jsx)(n.li,{children:"If pretraining is extended to 800 epochs, MoCo v2 reaches 71.1%, surpassing SimCLR\u2019s 69.3%."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This means MoCo, without the need for ultra-large batches, can handle a large number of negative samples via the queue mechanism and, by also applying SimCLR\u2019s MLP head and stronger augmentation, outperforms SimCLR."}),"\n",(0,s.jsx)(n.h3,{id:"computational-load-and-resource-consumption",children:"Computational Load and Resource Consumption"}),"\n",(0,s.jsx)("div",{align:"center",children:(0,s.jsx)("figure",{style:{width:"70%"},children:(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"train_cost",src:i(83334).Z+"",width:"792",height:"176"})})})}),"\n",(0,s.jsx)(n.p,{children:"The authors also compared the memory and time costs of the end-to-end mechanism and the MoCo mechanism on 8-GPUs."}),"\n",(0,s.jsx)(n.p,{children:"In the end-to-end mechanism, using a batch size of 4k is almost impossible on 8-GPUs. Even with a batch size of 256, the end-to-end method requires more memory and time than MoCo, because it needs to backpropagate through both the query and key encoders."}),"\n",(0,s.jsx)(n.p,{children:"In contrast, MoCo only updates the query encoder, making it more efficient overall."}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"In summary, the concepts of SimCLR can be directly applied to MoCo, and due to MoCo's memory bank mechanism, training does not require massive GPU resources. It is, in any case, an economical and highly effective choice."}),"\n",(0,s.jsx)(n.p,{children:"In this round, FAIR successfully made a comeback against Google."})]})}function d(e={}){let{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},47228:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img1-06eee6082b5833731afe383db9b162e8.jpg"},83287:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img2-1-b09ba36b812469ba4c3787dacbe92e2d.jpg"},52284:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img2-3faf908d8d35c6dbe565af70946e35d7.jpg"},81025:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img3-26543d6c5d62f02b931a861c5c6aa9f0.jpg"},83334:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/img4-8913a3eafe574cda2b24b4f0648011b5.jpg"},50065:function(e,n,i){i.d(n,{Z:function(){return o},a:function(){return r}});var t=i(67294);let s={},a=t.createContext(s);function r(e){let n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);