"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["75496"],{30482:function(e,s,n){n.r(s),n.d(s,{default:()=>o,frontMatter:()=>r,metadata:()=>i,assets:()=>h,toc:()=>c,contentTitle:()=>l});var i=JSON.parse('{"id":"model-tuning/blip2/index","title":"[23.01] BLIP-2","description":"Q-Former debuts","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/model-tuning/2301-blip2/index.md","sourceDirName":"model-tuning/2301-blip2","slug":"/model-tuning/blip2/","permalink":"/en/papers/model-tuning/blip2/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1747981730000,"frontMatter":{"title":"[23.01] BLIP-2","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[22.03] CoCoOp","permalink":"/en/papers/model-tuning/cocoop/"},"next":{"title":"Multimodality (24)","permalink":"/en/papers/category/multimodality-24"}}'),t=n("85893"),a=n("50065");let r={title:"[23.01] BLIP-2",authors:"Z. Yuan"},l=void 0,h={},c=[{value:"Q-Former debuts",id:"q-former-debuts",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Phase 1 Pre-Training",id:"phase-1-pre-training",level:3},{value:"Phase 2 Pre-Training",id:"phase-2-pre-training",level:3},{value:"Pre-Training Setup",id:"pre-training-setup",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Zero-shot VQA",id:"zero-shot-vqa",level:3},{value:"Finetuned VQA",id:"finetuned-vqa",level:3},{value:"Image Captioning",id:"image-captioning",level:3},{value:"Image-Text Retrieval",id:"image-text-retrieval",level:3},{value:"Text-to-Image Generation Showcase",id:"text-to-image-generation-showcase",level:3},{value:"Failure Cases",id:"failure-cases",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){let s={a:"a",admonition:"admonition",annotation:"annotation",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.h2,{id:"q-former-debuts",children:"Q-Former debuts"}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.a,{href:"https://arxiv.org/abs/2301.12597",children:(0,t.jsx)(s.strong,{children:"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"})})}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.p,{children:"This is the second-generation architecture named BLIP."}),"\n",(0,t.jsx)(s.p,{children:"Unlike the first-generation BLIP, which was a multimodal project, the second-generation BLIP focuses on using a small number of parameters to guide large pre-trained models toward desired directions."}),"\n",(0,t.jsxs)(s.admonition,{type:"tip",children:[(0,t.jsx)(s.p,{children:"For readers unfamiliar with BLIP, you can refer to our previous article:"}),(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/en/papers/multimodality/blip/",children:(0,t.jsx)(s.strong,{children:"[22.01] BLIP: Synthetic Text Technology"})})}),"\n"]})]}),"\n",(0,t.jsx)(s.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,t.jsx)(s.p,{children:"We continue to focus on the field of Vision-Language Models (VLM)."}),"\n",(0,t.jsx)(s.p,{children:"Current pre-training architectures often rely on large-scale training, which not only requires significant resources but also does not transfer well to every downstream task due to their diverse nature."}),"\n",(0,t.jsx)(s.p,{children:"If we could directly leverage pre-trained unimodal LLMs and align multimodal features under frozen parameters, wouldn\u2019t it save us the effort of retraining a multimodal model?"}),"\n",(0,t.jsx)(s.p,{children:"With this idea in mind, the authors designed a lightweight query transformer, called Q-Former, as a bridge to extract key features from image encoders and convert them into outputs that the language model can understand."}),"\n",(0,t.jsx)(s.p,{children:"Is this really possible? Let\u2019s dive in and learn!"}),"\n",(0,t.jsx)(s.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,t.jsx)(s.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"q-former",src:n(61540).Z+"",width:"1358",height:"502"})})})}),"\n",(0,t.jsx)(s.p,{children:"This architecture diagram might look a bit complex, so let\u2019s break it down step by step:"}),"\n",(0,t.jsx)(s.p,{children:"Starting from the far left, an image is input and processed by a pre-trained image encoder. The parameters of this encoder are frozen, meaning it only extracts features without participating in training."}),"\n",(0,t.jsx)(s.p,{children:'Next, on the right, the entire block consists of the "Q-Former" module, which is a dual-structured design. This architecture takes two inputs:'}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Learned Query Embeddings"}),": These parameters are randomly initialized, with a length set to 32 tokens as described in the paper."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Text Input Describing the Image"}),": This is similar to common language model inputs, consisting of natural language descriptions."]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:["In the diagram, the orange portion, which represents ",(0,t.jsx)(s.strong,{children:"Self-Attention"}),', directly uses pre-trained language model weights as initialization parameters. After undergoing the Self-Attention computation, the Q-Former queries the features output by the image encoder. Essentially, the "Learned Query" here is primarily responsible for aligning image features with textual descriptions.']}),"\n",(0,t.jsxs)(s.admonition,{type:"tip",children:[(0,t.jsx)(s.p,{children:'In other words, we randomly generate a set of tokens, query them against text features first, and then query them against image features. This produces a "hybrid" feature.'}),(0,t.jsx)(s.p,{children:'Finally, we align this "hybrid" feature with the textual features, and the process is complete for this round.'})]}),"\n",(0,t.jsx)(s.h3,{id:"phase-1-pre-training",children:"Phase 1 Pre-Training"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"objectives",src:n(24185).Z+"",width:"760",height:"412"})})})}),"\n",(0,t.jsxs)(s.p,{children:["In the diagram above, ",(0,t.jsx)(s.strong,{children:"Q"})," represents the Query token, and ",(0,t.jsx)(s.strong,{children:"T"})," represents the Text token. Masked sections are indicated with darker shading."]}),"\n",(0,t.jsx)(s.p,{children:"During Phase 1, Q-Former is combined with a frozen image encoder for image-text pair pre-training, optimizing the following three objective functions:"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Image-Text Matching (ITM)"})}),"\n",(0,t.jsxs)(s.p,{children:["This objective function is used to train the ",(0,t.jsx)(s.strong,{children:"Learned Query"}),", represented by the ",(0,t.jsx)(s.strong,{children:"Q"})," portion in the diagram."]}),"\n",(0,t.jsxs)(s.p,{children:["The attention mask design, as shown above, allows queries and text tokens to interact. A linear classifier performs binary classification, and the matching score is calculated as the average classification result across all query embeddings. A ",(0,t.jsx)(s.strong,{children:"hard negative mining strategy"})," is applied during training to generate more challenging negative samples."]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Image-Grounded Text Generation (ITG)"})}),"\n",(0,t.jsxs)(s.p,{children:["This objective function trains the ",(0,t.jsx)(s.strong,{children:"Text tokens"}),", represented by the ",(0,t.jsx)(s.strong,{children:"T"})," portion in the diagram."]}),"\n",(0,t.jsxs)(s.p,{children:["A multimodal causal self-attention mask is used here. Query tokens (",(0,t.jsx)(s.strong,{children:"Q"}),") can interact with each other, while text tokens (",(0,t.jsx)(s.strong,{children:"T"}),") can only attend to the query tokens and the preceding text tokens in the sequence."]}),"\n",(0,t.jsx)(s.admonition,{type:"tip",children:(0,t.jsxs)(s.p,{children:["Instead of using a ",(0,t.jsx)(s.code,{children:"[CLS]"})," token, a ",(0,t.jsx)(s.code,{children:"[DEC]"})," token is introduced as the start token for the decoding task."]})}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Image-Text Contrastive Learning (ITC)"})}),"\n",(0,t.jsxs)(s.p,{children:["The two objectives above train the ",(0,t.jsx)(s.strong,{children:"Q"})," and ",(0,t.jsx)(s.strong,{children:"T"})," parts separately. However, since both describe the same instance, it is necessary to align these two features, which is the purpose of this objective function."]}),"\n",(0,t.jsxs)(s.p,{children:["This objective aligns the similarity between positive image-text pairs and distinguishes them from negative pairs by adjusting the alignment between the query output ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"Z"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"Z"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"Z"})]})})]})," and the textual feature ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"t"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"t"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6151em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"t"})]})})]}),". Here, ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"t"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"t"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6151em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"t"})]})})]})," represents the output embedding of the ",(0,t.jsx)(s.code,{children:"[CLS]"})," token."]}),"\n",(0,t.jsx)(s.p,{children:"To prevent information leakage, a unimodal self-attention mask is used, ensuring that queries and text tokens cannot directly observe each other."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.h3,{id:"phase-2-pre-training",children:"Phase 2 Pre-Training"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"pretraining",src:n(69308).Z+"",width:"1606",height:"482"})})})}),"\n",(0,t.jsx)(s.admonition,{type:"tip",children:(0,t.jsxs)(s.p,{children:["The first phase focuses on aligning image features with textual descriptions, while the second phase involves pre-training for ",(0,t.jsx)(s.strong,{children:"text generation"}),"."]})}),"\n",(0,t.jsx)(s.p,{children:"As shown in the diagram, Q-Former is connected to a frozen large language model (LLM) to leverage its language generation capabilities. During this stage, the following two architectures can be employed for generation:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Decoder-Only Architecture"}),": Pre-trained with a language modeling loss, requiring the LLM to generate text conditioned on visual features."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Encoder-Decoder Architecture"}),": Pre-trained with a prefix language modeling loss. The input text is split into a prefix and a suffix. The prefix text, along with visual features, is sent to the LLM encoder, while the suffix text serves as the generation target for the LLM decoder."]}),"\n"]}),"\n",(0,t.jsx)(s.admonition,{type:"tip",children:(0,t.jsxs)(s.p,{children:["It is important to note that the ",(0,t.jsx)(s.strong,{children:"Image Encoder"})," and ",(0,t.jsx)(s.strong,{children:"LLM"})," are frozen in this phase. The goal is to use the visual cues extracted by Q-Former to guide the LLM in generating text."]})}),"\n",(0,t.jsxs)(s.p,{children:["In this design, the query embeddings ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"Z"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"Z"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"Z"})]})})]})," output by Q-Former are linearly projected to the same dimension as the LLM\u2019s text embeddings. These projected query embeddings are prepended to the input text embeddings, serving as ",(0,t.jsx)(s.strong,{children:"soft visual prompts"}),"."]}),"\n",(0,t.jsx)(s.p,{children:"Since Q-Former has been pre-trained to extract visual features relevant to language, it acts as an information bottleneck, effectively filtering out irrelevant visual details and reducing the burden on the LLM for vision-language alignment."}),"\n",(0,t.jsxs)(s.admonition,{type:"tip",children:[(0,t.jsx)(s.p,{children:'For readers interested in the concept of "Soft Prompts," you can refer to our earlier article:'}),(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.a,{href:"/en/papers/model-tuning/soft-prompts/",children:(0,t.jsx)(s.strong,{children:"[21.04] Soft Prompts: Subtle Whispers of Guidance"})})}),"\n"]})]}),"\n",(0,t.jsx)(s.h3,{id:"pre-training-setup",children:"Pre-Training Setup"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Pre-Training Steps"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Feature Learning Stage"}),": 250,000 steps."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Generative Learning Stage"}),": 80,000 steps."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Batch Sizes"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Feature Learning"}),": 2320 (ViT-L) or 1680 (ViT-g)."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Generative Learning"}),": 1920 (OPT) or 1520 (FlanT5)."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Compute Efficiency"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["Training on a single 16 \xd7 A100 (40G) machine:","\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Feature Learning Stage (Largest Model)"}),": Less than 6 days."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Generative Learning Stage (Largest Model)"}),": Less than 3 days."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Optimizer and Learning Rate"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Optimizer"}),": AdamW (",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsxs)(s.msub,{children:[(0,t.jsx)(s.mi,{children:"\u03B2"}),(0,t.jsx)(s.mn,{children:"1"})]}),(0,t.jsx)(s.mo,{children:"="}),(0,t.jsx)(s.mn,{children:"0.9"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\beta_1 = 0.9"})]})})}),(0,t.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.8889em",verticalAlign:"-0.1944em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05278em"},children:"\u03B2"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(s.span,{className:"vlist-r",children:[(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,t.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0528em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"1"})})]})}),(0,t.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(s.span,{})})})]})})]}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(s.span,{className:"mrel",children:"="}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,t.jsx)(s.span,{className:"mord",children:"0.9"})]})]})]}),", ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsxs)(s.msub,{children:[(0,t.jsx)(s.mi,{children:"\u03B2"}),(0,t.jsx)(s.mn,{children:"2"})]}),(0,t.jsx)(s.mo,{children:"="}),(0,t.jsx)(s.mn,{children:"0.98"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\beta_2 = 0.98"})]})})}),(0,t.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.8889em",verticalAlign:"-0.1944em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05278em"},children:"\u03B2"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(s.span,{className:"vlist-r",children:[(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,t.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0528em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"2"})})]})}),(0,t.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(s.span,{})})})]})})]}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(s.span,{className:"mrel",children:"="}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,t.jsx)(s.span,{className:"mord",children:"0.98"})]})]})]}),", weight decay ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mn,{children:"0.05"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"0.05"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,t.jsx)(s.span,{className:"mord",children:"0.05"})]})})]}),")."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Learning Rate Schedule"}),": Cosine decay with a peak learning rate of ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mn,{children:"1"}),(0,t.jsx)(s.mo,{children:"\xd7"}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mn,{children:"10"}),(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mo,{children:"\u2212"}),(0,t.jsx)(s.mn,{children:"4"})]})]})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"1 \\times 10^{-4}"})]})})}),(0,t.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,t.jsx)(s.span,{className:"mord",children:"1"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.8141em"}}),(0,t.jsx)(s.span,{className:"mord",children:"1"}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord",children:"0"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.8141em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(s.span,{className:"mord mtight",children:[(0,t.jsx)(s.span,{className:"mord mtight",children:"\u2212"}),(0,t.jsx)(s.span,{className:"mord mtight",children:"4"})]})})]})})})})})]})]})]})]})," and linear warmup for 2000 steps; the minimum learning rate in the second stage is ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mn,{children:"5"}),(0,t.jsx)(s.mo,{children:"\xd7"}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mn,{children:"10"}),(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mo,{children:"\u2212"}),(0,t.jsx)(s.mn,{children:"5"})]})]})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"5 \\times 10^{-5}"})]})})}),(0,t.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,t.jsx)(s.span,{className:"mord",children:"5"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.8141em"}}),(0,t.jsx)(s.span,{className:"mord",children:"1"}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord",children:"0"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.8141em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(s.span,{className:"mord mtight",children:[(0,t.jsx)(s.span,{className:"mord mtight",children:"\u2212"}),(0,t.jsx)(s.span,{className:"mord mtight",children:"5"})]})})]})})})})})]})]})]})]}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(s.li,{children:["\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Image Processing"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Image Size"}),": ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mn,{children:"224"}),(0,t.jsx)(s.mo,{children:"\xd7"}),(0,t.jsx)(s.mn,{children:"224"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"224 \\times 224"})]})})}),(0,t.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,t.jsx)(s.span,{className:"mord",children:"224"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,t.jsx)(s.span,{className:"mord",children:"224"})]})]})]}),"."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Augmentation"}),": Random cropping and horizontal flipping."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"table1",src:n(51973).Z+"",width:"1728",height:"376"})})})}),"\n",(0,t.jsx)(s.p,{children:"Without saying much, let\u2019s first look at the results."}),"\n",(0,t.jsx)(s.p,{children:"Compared to previous state-of-the-art models, BLIP-2 achieves higher performance while significantly reducing the number of trainable parameters required during VLM pre-training."}),"\n",(0,t.jsx)(s.h3,{id:"zero-shot-vqa",children:"Zero-shot VQA"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"table2",src:n(74281).Z+"",width:"1224",height:"584"})})})}),"\n",(0,t.jsxs)(s.p,{children:["The authors first tested BLIP-2\u2019s zero-shot capability. The table above shows BLIP-2\u2019s performance on various VQA tasks, including ",(0,t.jsx)(s.strong,{children:"VQAv2"}),", ",(0,t.jsx)(s.strong,{children:"GQA"}),", and ",(0,t.jsx)(s.strong,{children:"OK-VQA"}),"."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Evaluation Setup"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:['The prompt for the OPT model was "',(0,t.jsx)(s.code,{children:"Question: {}"}),' Answer:".']}),"\n",(0,t.jsxs)(s.li,{children:['The prompt for the FlanT5 model was "',(0,t.jsx)(s.code,{children:"Question: {}"}),' Short answer:".']}),"\n",(0,t.jsxs)(s.li,{children:["Beam search was used to generate answers, with a beam width of 5, and a length penalty of ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mo,{children:"\u2212"}),(0,t.jsx)(s.mn,{children:"1"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"-1"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,t.jsx)(s.span,{className:"mord",children:"\u2212"}),(0,t.jsx)(s.span,{className:"mord",children:"1"})]})})]})," to encourage concise answers."]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:["On ",(0,t.jsx)(s.strong,{children:"VQAv2"})," and ",(0,t.jsx)(s.strong,{children:"GQA"}),", BLIP-2 outperformed Flamingo80B, especially on ",(0,t.jsx)(s.strong,{children:"VQAv2"}),", where it achieved an ",(0,t.jsx)(s.strong,{children:"8.7% improvement"}),", despite having only ",(0,t.jsx)(s.strong,{children:"1/54"})," the number of trainable parameters compared to Flamingo80B."]}),"\n",(0,t.jsxs)(s.p,{children:["On ",(0,t.jsx)(s.strong,{children:"OK-VQA"}),", BLIP-2 performed slightly worse than Flamingo80B, likely because OK-VQA emphasizes open-world knowledge. Flamingo80B leverages the Chinchilla language model (70B), which has more extensive knowledge than BLIP-2\u2019s FlanT5XXL (11B)."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Zero-shot Capability Observations"}),":"]}),"\n",(0,t.jsx)(s.p,{children:"The first phase of multimodal pre-training is crucial for the model\u2019s zero-shot capability."}),"\n",(0,t.jsx)(s.p,{children:"Q-Former learns text-related visual features during this phase, reducing the burden on the LLM for vision-language alignment."}),"\n",(0,t.jsxs)(s.p,{children:["Without this phase, the model suffers from catastrophic forgetting, with performance significantly declining as training progresses. (This was observed as a ",(0,t.jsx)(s.strong,{children:"15% performance drop"})," in the OPT model.)"]}),"\n",(0,t.jsx)(s.h3,{id:"finetuned-vqa",children:"Finetuned VQA"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"finetune",src:n(47416).Z+"",width:"1024",height:"720"})})})}),"\n",(0,t.jsx)(s.p,{children:"The authors fine-tuned BLIP-2 on annotated VQA data, freezing the LLM\u2019s parameters and fine-tuning only the Q-Former and image encoder."}),"\n",(0,t.jsx)(s.p,{children:"In this task, BLIP-2 reached state-of-the-art performance among open-ended generation models for the visual question answering task, demonstrating its ability to focus on extracting visual features relevant to the question."}),"\n",(0,t.jsx)(s.h3,{id:"image-captioning",children:"Image Captioning"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"table2",src:n(24835).Z+"",width:"1614",height:"504"})})})}),"\n",(0,t.jsx)(s.p,{children:"The authors evaluated BLIP-2's performance on the image captioning task and compared it with other methods."}),"\n",(0,t.jsxs)(s.p,{children:['For the image captioning task, the initial prompt was "',(0,t.jsx)(s.code,{children:"a photo of"}),",\" and the model was trained using language modeling loss. During fine-tuning, the LLM's parameters were frozen, and only the Q-Former and image encoder parameters were updated."]}),"\n",(0,t.jsx)(s.p,{children:"On the COCO dataset, BLIP-2 achieved state-of-the-art performance and demonstrated strong generalization ability for zero-shot transfer on the NoCaps validation set."}),"\n",(0,t.jsx)(s.h3,{id:"image-text-retrieval",children:"Image-Text Retrieval"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"retrieval",src:n(68720).Z+"",width:"1556",height:"634"})})})}),"\n",(0,t.jsx)(s.p,{children:"The authors evaluated BLIP-2 on the image-text retrieval task using the COCO and Flickr30K datasets and analyzed the impact of different loss functions on performance."}),"\n",(0,t.jsx)(s.p,{children:"BLIP-2 achieved state-of-the-art performance in zero-shot image-text retrieval, significantly outperforming existing methods."}),"\n",(0,t.jsx)(s.p,{children:"The authors examined the contribution of each loss function to the results, as shown in the table below:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"ITC (Image-Text Contrastive) Loss"})," and ",(0,t.jsx)(s.strong,{children:"ITM (Image-Text Matching) Loss"}),": Directly learn the similarity between images and text."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"ITG (Image-Grounded Text Generation) Loss"}),": Enhances the query\u2019s ability to extract visual features relevant to the text, further improving vision-language alignment."]}),"\n"]}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"80%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"retrieval",src:n(53331).Z+"",width:"912",height:"264"})})})}),"\n",(0,t.jsx)(s.h3,{id:"text-to-image-generation-showcase",children:"Text-to-Image Generation Showcase"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"demo",src:n(29810).Z+"",width:"1224",height:"788"})})})}),"\n",(0,t.jsx)(s.p,{children:"The authors showcased examples of various zero-shot image-to-text capabilities, including visual knowledge reasoning, visual commonsense reasoning, visual conversation, and personalized image-to-text generation."}),"\n",(0,t.jsx)(s.admonition,{type:"tip",children:(0,t.jsx)(s.p,{children:"The images in the paper are quite large. For space considerations, we have not included the entire image here. Interested readers can refer to the original paper."})}),"\n",(0,t.jsx)(s.h3,{id:"failure-cases",children:"Failure Cases"}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("figure",{style:{width:"90%"},children:(0,t.jsx)(s.p,{children:(0,t.jsx)(s.img,{alt:"fail demo",src:n(77859).Z+"",width:"1588",height:"526"})})})}),"\n",(0,t.jsx)(s.p,{children:"Similar to previous LLMs, the model may produce unusual results for unseen objects or concepts. Additionally, it is prone to associating incorrect entities or concepts, highlighting areas for improvement in the future."}),"\n",(0,t.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(s.p,{children:"The concept of Q-Former demonstrates that with a minimal number of parameters and just a few tokens, large language models can be guided to perform multimodal tasks effectively."}),"\n",(0,t.jsx)(s.p,{children:"If we want to address problems in specific domains, this approach allows us to transfer the capabilities of large pre-trained models toward our desired objectives."}),"\n",(0,t.jsx)(s.admonition,{type:"tip",children:(0,t.jsx)(s.p,{children:"In recent years, Q-Former has become a common sight in the field of Face Anti-Spoofing (FAS) research!"})})]})}function o(e={}){let{wrapper:s}={...(0,a.a)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},61540:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img1-791ac547f2c8f03927aaf2f7ff4dbc8c.jpg"},53331:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img10-80f7afc69444ebd24d8bc43c950cb31f.jpg"},77859:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img11-510651f6df184913be9ea17904716283.jpg"},24185:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img2-9b927fe9752cdc3aad2a2d235c4838ab.jpg"},69308:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img3-c0d545fc49a52f7a1d8c2393de68e58b.jpg"},51973:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img4-6068c788ca50dfdad61a9d797981d985.jpg"},74281:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img5-691e5f0dcd924f2dd83c8083a6f92fc6.jpg"},29810:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img6-5ee13454ac94d648b16587f80dc6b5d9.jpg"},24835:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img7-683d2f3ffb9173763f5f13a2dcd7e72d.jpg"},47416:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img8-c05c9de74e09daf454442bad8083ea6c.jpg"},68720:function(e,s,n){n.d(s,{Z:function(){return i}});let i=n.p+"assets/images/img9-a38965b0cbfb5c4e5923469623276fe3.jpg"},50065:function(e,s,n){n.d(s,{Z:function(){return l},a:function(){return r}});var i=n(67294);let t={},a=i.createContext(t);function r(e){let s=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:s},e.children)}}}]);