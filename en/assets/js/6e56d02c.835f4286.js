"use strict";(self.webpackChunkdocsaid_website=self.webpackChunkdocsaid_website||[]).push([["46937"],{31572:function(e,s,n){n.r(s),n.d(s,{frontMatter:()=>r,toc:()=>o,default:()=>d,metadata:()=>a,assets:()=>c,contentTitle:()=>l});var a=JSON.parse('{"id":"face-antispoofing/wmca/index","title":"[19.09] WMCA","description":"The Invisible Face","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/face-antispoofing/1909-wmca/index.md","sourceDirName":"face-antispoofing/1909-wmca","slug":"/face-antispoofing/wmca/","permalink":"/en/papers/face-antispoofing/wmca/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1743909228000,"frontMatter":{"title":"[19.09] WMCA","authors":"Z. Yuan"},"sidebar":"papersSidebar","previous":{"title":"[19.05] VA-FAS","permalink":"/en/papers/face-antispoofing/vafas/"},"next":{"title":"[20.03] CDCN","permalink":"/en/papers/face-antispoofing/cdcn/"}}'),i=n(74848),t=n(84429);let r={title:"[19.09] WMCA",authors:"Z. Yuan"},l,c={},o=[{value:"The Invisible Face",id:"the-invisible-face",level:2},{value:"Defining the Problem",id:"defining-the-problem",level:2},{value:"Solving the Problem",id:"solving-the-problem",level:2},{value:"Model Architecture",id:"model-architecture",level:3},{value:"WMCA Dataset",id:"wmca-dataset",level:3},{value:"Seven Types of Attacks",id:"seven-types-of-attacks",level:3},{value:"Evaluation Protocol and Experimental Setup",id:"evaluation-protocol-and-experimental-setup",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"Discussion",id:"discussion",level:2},{value:"Baseline System Performance in Grandtest",id:"baseline-system-performance-in-grandtest",level:3},{value:"MC-CNN vs Best Baseline (Grandtest)",id:"mc-cnn-vs-best-baseline-grandtest",level:3},{value:"Recognition Performance in Unseen Attack Scenarios",id:"recognition-performance-in-unseen-attack-scenarios",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){let s={a:"a",admonition:"admonition",annotation:"annotation",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",mtext:"mtext",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.h2,{id:"the-invisible-face",children:"The Invisible Face"}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1909.08848",children:(0,i.jsx)(s.strong,{children:"Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network"})})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.p,{children:"In the development of a field, new terms are often defined. These terms not only describe the technology but also influence the direction of future research."}),"\n",(0,i.jsx)(s.p,{children:"In this paper, spoofing attacks are referred to as Presentation Attacks (PA). The definition of PA is:"}),"\n",(0,i.jsxs)(s.blockquote,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Disruptive requests initiated by a user to the biometric recognition subsystem that may serve the following purposes:"})}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"Impersonation Attack"})}),"\n",(0,i.jsx)(s.li,{children:(0,i.jsx)(s.strong,{children:"Obfuscation Attack"})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Both situations are classified as Presentation Attack (PA)."})}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"defining-the-problem",children:"Defining the Problem"}),"\n",(0,i.jsx)(s.p,{children:"When Convolutional Neural Networks (CNNs) entered the Face Anti-Spoofing (FAS) field, traditional methods relying on handcrafted features were quickly replaced. However, even with CNNs, many methods still only use RGB images for judgment, that is, facial photos taken under normal visible light conditions."}),"\n",(0,i.jsx)(s.p,{children:'The issue is that spoofing techniques have long since evolved beyond the "Print" level. Nowadays, silicone masks, 3D models, and other sophisticated techniques have become more common. Relying solely on RGB information is no longer sufficient.'}),"\n",(0,i.jsx)(s.p,{children:"As a result, some research started to introduce depth information to help, such as using Time-of-Flight (TOF), structured light, or dual-lens systems to create 3D vision, detecting facial distance and contour changes."}),"\n",(0,i.jsx)(s.p,{children:"Since we're already introducing more dimensions to solve the problem, why not go a step further?"}),"\n",(0,i.jsxs)(s.blockquote,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Besides RGB images, we can find additional information such as depth, heat, and more."})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:'This paper starts from this perspective. The authors not only used color and depth data but also incorporated Near-Infrared (NIR) and thermal imaging (Thermal) to include more "invisible facial cues" into the model.'}),"\n",(0,i.jsx)(s.p,{children:"However, this also encounters a very practical issue: datasets are not keeping up. Most open datasets only include RGB data, and those with depth data are limited in quantity, which hinders the training of stable models, let alone performing generalization tests."}),"\n",(0,i.jsx)(s.p,{children:"Thus, another key contribution of this paper is the introduction of a new dataset:"}),"\n",(0,i.jsxs)(s.blockquote,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"WMCA (Wide Multi-Channel Presentation Attack)."})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"It not only includes various 2D and 3D spoofing techniques (such as paper masks, silicone masks, rigid materials, transparent shields, etc.), but also provides synchronized data from four channels: RGB, depth, NIR, and thermal images."}),"\n",(0,i.jsx)(s.p,{children:"By allowing the model to see more dimensions of information, it becomes harder for attackers to deceive the system simultaneously."}),"\n",(0,i.jsx)(s.h2,{id:"solving-the-problem",children:"Solving the Problem"}),"\n",(0,i.jsx)(s.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"90%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"model_arch",src:n(44754).A+"",width:"1546",height:"520"})})})}),"\n",(0,i.jsx)(s.p,{children:"In terms of model design, the authors propose a multi-channel convolutional neural network architecture (Multi-Channel CNN, MC-CNN), aimed at integrating information from different sensor channels to improve the model's ability to detect spoofing attacks."}),"\n",(0,i.jsx)(s.p,{children:"In the FAS field, the lack of datasets remains a significant issue. As a result, most research does not train a CNN from scratch but instead opts to use pre-trained networks for transfer learning."}),"\n",(0,i.jsxs)(s.p,{children:["This paper follows the same approach by using a pre-trained face recognition model as the backbone. The chosen model is the lightweight ",(0,i.jsx)(s.a,{href:"https://arxiv.org/abs/1511.02683",children:(0,i.jsx)(s.strong,{children:"LightCNN"})}),", which is used to extract facial features from each channel."]}),"\n",(0,i.jsx)(s.p,{children:"Before entering the network, images from each channel undergo alignment and normalization processing. Face detection is done using MTCNN, and feature point detection is done using SDM. The eyes and mouth are aligned to a common coordinate, and then adjusted to 128\xd7128 grayscale images."}),"\n",(0,i.jsx)(s.p,{children:"The RGB channel serves as the reference, and other channels, such as depth, near-infrared, and thermal images, are synchronized based on the RGB feature points to ensure consistency across both space and time. Non-RGB images are also standardized via MAD and converted into 8-bit format for easier input into the model."}),"\n",(0,i.jsx)(s.p,{children:"Each channel passes through a shared LightCNN architecture to extract the corresponding facial features. These feature vectors, each with a dimension of 256, are then concatenated, resulting in a unified representation, which is passed through two fully connected layers for classification:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"The first fully connected layer has 10 nodes."}),"\n",(0,i.jsx)(s.li,{children:"The second layer is a sigmoid output (1 node) indicating whether it is a spoofing attack."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The overall training uses binary cross-entropy (BCE) as the loss function:"}),"\n",(0,i.jsx)(s.span,{className:"katex-display",children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"L"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mo,{children:"\u2212"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"y"}),(0,i.jsx)(s.mi,{children:"log"}),(0,i.jsx)(s.mo,{children:"\u2061"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"p"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\u2212"}),(0,i.jsx)(s.mi,{children:"y"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mi,{children:"log"}),(0,i.jsx)(s.mo,{children:"\u2061"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{children:"\u2212"}),(0,i.jsx)(s.mi,{children:"p"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"L = \u2212(y \\log(p) + (1 \u2212 y) \\log(1 \u2212 p))"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"L"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord",children:"\u2212"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mop",children:["lo",(0,i.jsx)(s.span,{style:{marginRight:"0.01389em"},children:"g"})]}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"+"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mop",children:["lo",(0,i.jsx)(s.span,{style:{marginRight:"0.01389em"},children:"g"})]}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"p"}),(0,i.jsx)(s.span,{className:"mclose",children:"))"})]})]})]})}),"\n",(0,i.jsxs)(s.p,{children:["Where ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"y"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"y"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"})]})})]})," is the true label (1 for ",(0,i.jsx)(s.code,{children:"REAL"}),", 0 for ",(0,i.jsx)(s.code,{children:"SPOOF"}),"), and ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"p"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"p"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"p"})]})})]})," is the model's predicted probability."]}),"\n",(0,i.jsx)(s.h3,{id:"wmca-dataset",children:"WMCA Dataset"}),"\n",(0,i.jsxs)(s.admonition,{type:"tip",children:[(0,i.jsx)(s.p,{children:"If you're thinking about creating your own dataset, the design details in this section might offer you plenty of inspiration."}),(0,i.jsxs)(s.p,{children:["For more information, check out: ",(0,i.jsx)(s.a,{href:"https://www.idiap.ch/en/scientific-research/data/wmca",children:(0,i.jsx)(s.strong,{children:"IDIAP/data"})})]})]}),"\n",(0,i.jsx)(s.p,{children:"To validate the performance of multi-channel models in anti-spoofing tasks, the authors additionally created a new dataset called:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"WMCA (Wide Multi-Channel Presentation Attack)"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"This dataset is carefully designed not only to fill the gap in the number of channels present in existing PAD datasets but also to cover a rich and challenging variety of attack methods."}),"\n",(0,i.jsxs)(s.p,{children:["The dataset includes video data from ",(0,i.jsx)(s.strong,{children:"72 subjects"}),", with each person recorded for both bonafide (real samples) and multiple presentation attack scenarios, with each video lasting approximately 10 seconds. The recording process uses multiple sensors synchronized to capture the data, resulting in a total of about ",(0,i.jsx)(s.strong,{children:"5.1 TB"})," of uncompressed data, making it one of the few high-quality FAS datasets available."]}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"60%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"data collection",src:n(38301).A+"",width:"860",height:"588"})})})}),"\n",(0,i.jsx)(s.p,{children:"WMCA uses two sets of sensors for synchronized recording, covering various spectra and depth information. The devices used are shown in the image above, where:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"(b) Seek Thermal Compact PRO"}),": Provides thermal imaging (Thermal)"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"(c) Intel RealSense SR300"}),": Provides RGB, depth, and near-infrared (NIR) channels"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["These sensors are modularly installed on the same optical frame to maintain consistent shooting angles and distances. The RGB channel records at 30fps, while thermal imaging records at 15fps. Each video stores approximately ",(0,i.jsx)(s.strong,{children:"300 frames"})," per channel (thermal images store 150 frames), with all data being uncompressed for high-quality analysis. The resulting image samples are shown below:"]}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"80%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"data example",src:n(70003).A+"",width:"1000",height:"1072"})})})}),"\n",(0,i.jsx)(s.p,{children:"To ensure spatial and temporal consistency of the multi-channel data, the authors developed a custom calibration board with thermal contrast characteristics and used high-power lighting for synchronized shooting. They also used custom software to automatically capture corresponding points for calibration."}),"\n",(0,i.jsx)(s.p,{children:"The data collection lasted for five months and spanned seven different recording stages. Each stage involved changes in lighting, backgrounds, and device settings, simulating a variety of real-world scenarios, including:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Lighting"}),": Sunlight, LED side lighting, office lighting, etc."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Background"}),": Uniform backgrounds vs. complex backgrounds"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Wearing Glasses"}),': For bonafide samples, both "with glasses" and "without glasses" versions were recorded']}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Masks and Fake Heads"}),": Hot air was blown across the face before recording to enhance the camouflage realism in the thermal channel"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"All participants faced the camera with a neutral expression and maintained a distance of approximately 40 cm. The entire shooting process was controlled by an operator to ensure consistency and comparability across all data."}),"\n",(0,i.jsx)(s.h3,{id:"seven-types-of-attacks",children:"Seven Types of Attacks"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"50%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"spoofing types",src:n(96417).A+"",width:"524",height:"448"})})})}),"\n",(0,i.jsxs)(s.p,{children:["The detailed number of attack samples in WMCA is shown in the table above. It contains ",(0,i.jsx)(s.strong,{children:"1332 spoof attack samples"}),", categorized into seven main types, each with different levels of challenge:"]}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Glasses"}),": Funny fake glasses, paper glasses, etc. (Partial occlusion attack)"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Fake Head"}),": Various model heads, some heated to simulate body temperature"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Print Attack"}),": Using high-resolution inkjet printers or office laser printers to output facial photos"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Replay Attack"}),": Using devices like an iPad or smartphone to play pre-recorded facial videos"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Rigid Mask"}),": High-fidelity rigid masks, decorative plastic masks"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Flexible Mask"}),": Homemade silicone masks that mimic real facial features"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Paper Mask"}),": Paper masks designed to resemble a real identity"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"These attack types range from the simplest 2D attacks to highly realistic 3D attacks. Coupled with multi-spectral data, this significantly enhances the dataset\u2019s utility for training and testing models\u2019 generalization abilities."}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"80%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"spoofing types",src:n(6838).A+"",width:"992",height:"736"})})})}),"\n",(0,i.jsx)(s.h3,{id:"evaluation-protocol-and-experimental-setup",children:"Evaluation Protocol and Experimental Setup"}),"\n",(0,i.jsx)(s.p,{children:'WMCA supports two experimental protocols corresponding to the "known attacks" and "unknown attacks" scenarios:'}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Grandtest Protocol"}),": The simulated attack types are known. The data is divided into train / dev / eval sets, with each subject appearing only once. All attack types are distributed across the three sets."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Unseen Attack Protocol (LOO)"}),': A stricter test designed for generalization ability. Seven leave-one-out (LOO) protocols are defined, where each time one attack type is excluded as an "unknown attack" and only appears in the test set. For example, in the "LOO fakehead" protocol, fake head attacks do not appear in the training or validation sets and are only evaluated in the testing phase.']}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Each video is sampled for 50 frames (average sampling), and each corresponding set of four-channel images from the same time point forms a training sample, treated as an independent sample for training and inference."}),"\n",(0,i.jsx)(s.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,i.jsx)(s.p,{children:"Performance evaluation uses the three standard metrics defined by [ISO/IEC 30107-3]:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"APCER"})," (Attack Presentation Classification Error Rate) - The rate of attack samples being misclassified as real."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"BPCER"})," (Bonafide Presentation Classification Error Rate) - The rate of real samples being misclassified as attacks."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"ACER"})," (Average Classification Error Rate) - The average of the above two errors."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Additionally, ROC curves are plotted to assist in comparing the overall classification abilities of various methods."}),"\n",(0,i.jsxs)(s.admonition,{type:"tip",children:[(0,i.jsx)(s.p,{children:"In previous papers, you may have encountered similar metrics, such as:"}),(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"False Rejection Rate (FRR)"}),": The proportion of legitimate access requests incorrectly rejected (legitimate users misclassified as attacks)."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"False Acceptance Rate (FAR)"}),": The proportion of attack samples incorrectly accepted (attackers misclassified as legitimate users)."]}),"\n"]}),(0,i.jsxs)(s.p,{children:["These two error rates combine into the ",(0,i.jsx)(s.strong,{children:"Half Total Error Rate (HTER)"}),", calculated as:"]}),(0,i.jsx)(s.span,{className:"katex-display",children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mtext,{children:"HTER"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsxs)(s.mfrac,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mtext,{children:"FAR"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mtext,{children:"FRR"})]}),(0,i.jsx)(s.mn,{children:"2"})]})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\text{HTER} = \\frac{\\text{FAR} + \\text{FRR}}{2}"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord text",children:(0,i.jsx)(s.span,{className:"mord",children:"HTER"})}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"2.0463em",verticalAlign:"-0.686em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mopen nulldelimiter"}),(0,i.jsx)(s.span,{className:"mfrac",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsxs)(s.span,{className:"vlist",style:{height:"1.3603em"},children:[(0,i.jsxs)(s.span,{style:{top:"-2.314em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(s.span,{className:"mord",children:(0,i.jsx)(s.span,{className:"mord",children:"2"})})]}),(0,i.jsxs)(s.span,{style:{top:"-3.23em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(s.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,i.jsxs)(s.span,{style:{top:"-3.677em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord text",children:(0,i.jsx)(s.span,{className:"mord",children:"FAR"})}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"+"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mord text",children:(0,i.jsx)(s.span,{className:"mord",children:"FRR"})})]})]})]}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200B"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.686em"},children:(0,i.jsx)(s.span,{})})})]})}),(0,i.jsx)(s.span,{className:"mclose nulldelimiter"})]})]})]})]})}),(0,i.jsx)(s.hr,{}),(0,i.jsx)(s.p,{children:"Conceptually, they are the same, but over time, researchers have provided more refined definitions and standardized names to avoid misunderstandings in discussions."})]}),"\n",(0,i.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,i.jsx)(s.h3,{id:"baseline-system-performance-in-grandtest",children:"Baseline System Performance in Grandtest"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"90%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"baseline",src:n(18372).A+"",width:"1224",height:"488"})})})}),"\n",(0,i.jsx)(s.p,{children:"The above chart shows the performance of the baseline system under the grandtest protocol, with thresholds based on a BPCER = 1% setting from the dev set. We can observe several key points from the table:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["In the single-channel tests, the ",(0,i.jsx)(s.strong,{children:"Infrared channel combined with RDWT-Haralick features"})," performed the best, indicating that IR information provides strong discriminatory power for liveness detection."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"FASNet, although using only the RGB channel, outperformed the IQM-LBP series"})," due to its structural design and ImageNet pre-trained weights, showing that deep learning architectures can optimize feature learning even with a single channel."]}),"\n",(0,i.jsxs)(s.li,{children:["While ",(0,i.jsx)(s.strong,{children:"score-level fusion of multiple channels can further improve recognition performance"}),", the improvement is limited (ACER still around a few percent), indicating that traditional methods struggle to effectively integrate cross-channel features, which is still insufficient for high-security applications."]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"mc-cnn-vs-best-baseline-grandtest",children:"MC-CNN vs Best Baseline (Grandtest)"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"70%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"mc-cnn",src:n(16087).A+"",width:"1076",height:"368"})})})}),"\n",(0,i.jsx)(s.p,{children:"The above figure compares the final performance of MC-CNN with the best baseline system on the dev and test sets:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"MC-CNN achieved ACER = 0.3% on the test set, significantly outperforming the baseline (typically above 2%)"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"By learning joint representations across multiple channels, the model effectively integrates complementary information from RGB, Depth, IR, and Thermal channels. This result highlights that score fusion alone is limited, and the key is designing multi-channel architectures with joint embedding capabilities."}),"\n",(0,i.jsx)(s.h3,{id:"recognition-performance-in-unseen-attack-scenarios",children:"Recognition Performance in Unseen Attack Scenarios"}),"\n",(0,i.jsx)("div",{align:"center",children:(0,i.jsx)("figure",{style:{width:"90%"},children:(0,i.jsx)(s.p,{children:(0,i.jsx)(s.img,{alt:"unseen",src:n(21626).A+"",width:"1224",height:"320"})})})}),"\n",(0,i.jsx)(s.p,{children:"To evaluate the model's generalization ability to unseen attacks, the authors designed seven LOO protocols in WMCA, where one attack type is excluded from the training and validation sets and appears only in the test set:"}),"\n",(0,i.jsx)(s.p,{children:"In most unseen attack scenarios, MC-CNN outperforms the traditional baseline, demonstrating strong generalization across attack types. For example, after excluding silicone or replay attacks, the system still effectively detects unseen attacks (ACER is noticeably lower than the baseline)."}),"\n",(0,i.jsxs)(s.p,{children:["However, the ",(0,i.jsx)(s.strong,{children:"glasses attack"})," performed poorly in both the baseline and MC-CNN systems. The primary reason for this is the high similarity of the visual features between bonafide individuals wearing glasses and the glasses attack, particularly in the depth and thermal channels, which resulted in the model misclassifying it as a real sample."]}),"\n",(0,i.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(s.p,{children:"The shortcomings of this paper are quite apparent. In real-world applications, we often only have access to RGB images. The hardware costs and synchronization requirements of multi-channel sensor devices make this approach difficult to implement directly in practice."}),"\n",(0,i.jsx)(s.p,{children:'However, from the perspective of the dataset, this paper defines the "unseen attack" evaluation setting, which differs from traditional single-classification criteria. This is still valuable in the context of today\'s open-set and anomaly detection discussions. Additionally, the challenges identified by the authors (such as detecting partial attacks and the fuzzy boundaries between attack types) remain significant issues that researchers still face.'}),"\n",(0,i.jsx)(s.p,{children:'Lastly, the concept of "multi-channel" could be further expanded into "multi-modal":'}),"\n",(0,i.jsxs)(s.blockquote,{children:["\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Not only different spectral or depth dimensions of image inputs but also combining voice, behavior, contextual information, and other heterogeneous perception sources to develop a truly cross-perceptual-domain defense system."})}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"From this perspective, while this paper may not be a pioneer in multi-modal AI, it provides an early prototype of sensor fusion for joint perception, which remains insightful for future developments."})]})}function d(e={}){let{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},44754:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img1-f98ed369d24bd01755499ae8105bd209.jpg"},38301:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img2-6f7b79382cef64977a08536100520554.jpg"},70003:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img4-a21e4ec1b6d16b2a48c5ea6866df9cad.jpg"},6838:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img5-ca9c9b95b9e98e00acdb7a656d12653d.jpg"},96417:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img6-83a7a22a412e68001228ea887a86e982.jpg"},18372:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img7-ca165f55d3e3cb3daadd65a793fd988f.jpg"},16087:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img8-ad3fa476beaa7da87c0108d773f02abd.jpg"},21626:function(e,s,n){n.d(s,{A:()=>a});let a=n.p+"assets/images/img9-09712c84c6902275ca9daeba1057e6f8.jpg"},84429:function(e,s,n){n.d(s,{R:()=>r,x:()=>l});var a=n(96540);let i={},t=a.createContext(i);function r(e){let s=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:s},e.children)}}}]);