"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[31131],{88334:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"text-recognition/wwwstr/index","title":"[19.04] WWWSTR","description":"Data and Model Analysis","source":"@site/i18n/en/docusaurus-plugin-content-docs-papers/current/text-recognition/1904-wwwstr/index.md","sourceDirName":"text-recognition/1904-wwwstr","slug":"/text-recognition/wwwstr/","permalink":"/en/papers/text-recognition/wwwstr/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"zephyr-sh","lastUpdatedAt":1730706876000,"frontMatter":{},"sidebar":"papersSidebar","previous":{"title":"[18.11] SAR","permalink":"/en/papers/text-recognition/sar/"},"next":{"title":"[19.10] SATRN","permalink":"/en/papers/text-recognition/satrn/"}}');var a=n(74848),t=n(28453);const r={},l="[19.04] WWWSTR",c={},o=[{value:"Data and Model Analysis",id:"data-and-model-analysis",level:2},{value:"Dataset Analysis",id:"dataset-analysis",level:2},{value:"Training Datasets",id:"training-datasets",level:3},{value:"Testing Datasets",id:"testing-datasets",level:3},{value:"Version Differences",id:"version-differences",level:3},{value:"Architecture Analysis",id:"architecture-analysis",level:2},{value:"Transformation Stage",id:"transformation-stage",level:3},{value:"Feature Extraction Stage",id:"feature-extraction-stage",level:3},{value:"Sequence Modeling Stage",id:"sequence-modeling-stage",level:3},{value:"Prediction Stage",id:"prediction-stage",level:3},{value:"Experimental Setup",id:"experimental-setup",level:2},{value:"Discussion",id:"discussion",level:2},{value:"Accuracy-Time Trade-off Analysis",id:"accuracy-time-trade-off-analysis",level:3},{value:"Accuracy-Memory Trade-off Analysis",id:"accuracy-memory-trade-off-analysis",level:3},{value:"Module Analysis",id:"module-analysis",level:3},{value:"Failure Case Analysis",id:"failure-case-analysis",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const s={a:"a",admonition:"admonition",annotation:"annotation",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.header,{children:(0,a.jsx)(s.h1,{id:"1904-wwwstr",children:"[19.04] WWWSTR"})}),"\n",(0,a.jsx)(s.h2,{id:"data-and-model-analysis",children:"Data and Model Analysis"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.a,{href:"https://arxiv.org/abs/1904.01906",children:(0,a.jsx)(s.strong,{children:"What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis"})})}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.p,{children:"This paper, published by Clova AI, does not propose new techniques but instead presents a comprehensive analysis of existing algorithms and datasets, as well as an exploration of the best combinations of different modules to find the optimal solution."}),"\n",(0,a.jsx)(s.h2,{id:"dataset-analysis",children:"Dataset Analysis"}),"\n",(0,a.jsx)(s.p,{children:"Different studies have used various dataset combinations for training, which may result in unclear performance improvements. It becomes difficult to determine if the improvement is due to the proposed new model or the usage of a better or larger dataset."}),"\n",(0,a.jsx)(s.p,{children:"This inconsistency in datasets affects the fairness of results."}),"\n",(0,a.jsx)(s.h3,{id:"training-datasets",children:"Training Datasets"}),"\n",(0,a.jsx)(s.p,{children:"Due to the high cost of annotating scene text images, it's challenging to acquire sufficient labeled data, and most STR (Scene Text Recognition) models opt for synthetic datasets for training."}),"\n",(0,a.jsx)(s.p,{children:"Below are two widely used synthetic datasets in STR research:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"MJSynth (MJ)"})}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.a,{href:"https://www.robots.ox.ac.uk/~vgg/data/text/",children:(0,a.jsx)(s.strong,{children:"Synthetic Word Dataset"})})}),"\n",(0,a.jsx)("div",{align:"left",children:(0,a.jsx)("figure",{style:{width:"60%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"MJSynth",src:n(50501).A+"",width:"628",height:"496"})})})}),"\n",(0,a.jsx)(s.p,{children:"MJSynth is a synthetic dataset specifically designed for STR, containing 8.9 million word-box images. The generation process involves font rendering, border and shadow rendering, background coloring, merging fonts and backgrounds, applying perspective distortions, mixing with real images, and adding noise."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.strong,{children:"SynthText (ST)"})}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.a,{href:"https://www.robots.ox.ac.uk/~vgg/data/scenetext/",children:(0,a.jsx)(s.strong,{children:"SynthText in the Wild Dataset"})})}),"\n",(0,a.jsx)("div",{align:"left",children:(0,a.jsx)("figure",{style:{width:"60%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"SynthText\uff08ST\uff09",src:n(23336).A+"",width:"592",height:"508"})})})}),"\n",(0,a.jsx)(s.p,{children:"SynthText was originally created for scene text detection but has also been used in STR research. Researchers cropped word boxes from this dataset for STR training. After cropping, the SynthText dataset contains about 5.5 million training samples."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"testing-datasets",children:"Testing Datasets"}),"\n",(0,a.jsx)(s.p,{children:"Different versions of benchmark datasets have certain differences."}),"\n",(0,a.jsx)(s.p,{children:"For example, in IC03, a difference of 7 samples can lead to a 0.8% performance gap, which is a significant difference when comparing prior research results."}),"\n",(0,a.jsx)(s.p,{children:"The sample number differences in IC13 and IC15 are even larger than those in IC03."}),"\n",(0,a.jsx)(s.p,{children:'Seven real-scene STR datasets are widely used for post-training model evaluation. These datasets can be categorized into "regular datasets" and "irregular datasets" based on the difficulty of the text and its geometric layout:'}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Regular Datasets"}),": These contain text images where characters are evenly spaced and arranged horizontally, making them relatively easier to process:"]}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"60%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"regular",src:n(53339).A+"",width:"652",height:"344"})})})}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"IIIT5K-Words (IIIT)"}),": A dataset from Google image search containing 2,000 training images and 3,000 evaluation images."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Street View Text (SVT)"}),": Collected from Google Street View, this dataset contains outdoor street scenes, with some images affected by noise, blur, or low resolution. It includes 257 training images and 647 evaluation images."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ICDAR2003 (IC03)"}),": Used in the ICDAR 2003 Robust Reading competition, it contains 1,156 training images and 1,110 evaluation images. There are two versions (860 and 867 images) due to the exclusion of words that are too short or contain non-alphanumeric characters."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ICDAR2013 (IC13)"}),": This dataset inherits most of the images from IC03 and was created for the ICDAR 2013 Robust Reading competition. It contains 848 training images and 1,095 evaluation images. Two versions are available for evaluation, one with 857 images and the other with 1,015."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Irregular Datasets"}),": These contain more challenging scene text, such as curved, rotated, or distorted text:"]}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"60%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"irregular",src:n(5502).A+"",width:"496",height:"344"})})})}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ICDAR2015 (IC15)"}),": This dataset, used in the ICDAR 2015 Robust Reading competition, contains 4,468 training images and 2,077 evaluation images. The images are captured from a natural movement perspective using Google Glass, resulting in many images with noise, blur, and rotation. Evaluation versions include 1,811 and 2,077 images."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"SVT Perspective (SP)"}),": Collected from Google Street View, it includes 645 evaluation images, many with perspective distortions due to non-frontal views."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"CUTE80 (CT)"}),": A dataset collected from natural scenes, containing 288 cropped evaluation images, many of which feature curved text."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"version-differences",children:"Version Differences"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"different-versions",src:n(27898).A+"",width:"1468",height:"746"})}),"\n",(0,a.jsx)(s.p,{children:"Based on the table above, different studies have used different versions of benchmark datasets for model evaluation, particularly in the IC03, IC13, and IC15 datasets."}),"\n",(0,a.jsx)(s.p,{children:"For example, in IC03, a difference of 7 samples can lead to a 0.8% performance gap, which is significant when comparing results. The differences in sample sizes between IC13 and IC15 are even larger. These dataset version differences can lead to significant errors when evaluating model performance, which must be carefully considered when comparing different models."}),"\n",(0,a.jsx)(s.admonition,{type:"tip",children:(0,a.jsx)(s.p,{children:"Version errors can lead to major issues\u2014be cautious!"})}),"\n",(0,a.jsx)(s.h2,{id:"architecture-analysis",children:"Architecture Analysis"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"architecture",src:n(48585).A+"",width:"1980",height:"220"})}),"\n",(0,a.jsx)(s.p,{children:'STR (Scene Text Recognition) is similar to "object detection" tasks and "sequence prediction" tasks, benefiting from both Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).'}),"\n",(0,a.jsx)(s.p,{children:"The earliest model to combine CNN and RNN for STR was CRNN, which used CNNs for feature extraction and RNNs for sequence prediction. Many variants have since been proposed to improve performance, introducing different modules to handle complex features like font styles and backgrounds."}),"\n",(0,a.jsx)(s.p,{children:"Some methods even omitted the RNN stage to reduce inference time. Subsequent research has introduced attention-based decoders to improve character sequence prediction."}),"\n",(0,a.jsx)(s.p,{children:"The STR architecture typically follows these four stages:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Transformation"}),": Spatial transformation networks (STN) are used to normalize input text images, reducing the burden on subsequent stages."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Feature Extraction"}),": This stage converts input images into representations focused on character recognition while suppressing features irrelevant to fonts, colors, sizes, and backgrounds."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Sequence Modeling"}),": This stage captures contextual information in the character sequence, improving the accuracy of each character's prediction."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Prediction"}),": The final character sequence is predicted based on the extracted features."]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"transformation-stage",children:"Transformation Stage"}),"\n",(0,a.jsx)(s.p,{children:"This stage transforms input images X into normalized images X'."}),"\n",(0,a.jsx)(s.p,{children:"Since text images in natural scenes vary in shape (such as curved or slanted text), if passed directly to later stages, the feature extraction stage would need to learn representations invariant to these geometric distortions."}),"\n",(0,a.jsx)(s.p,{children:"To alleviate this burden, some research has employed Thin-Plate Spline (TPS) transformation, a variant of spatial transformation networks (STN). TPS normalizes text regions by performing smooth spline interpolation between a set of control points."}),"\n",(0,a.jsx)(s.p,{children:"TPS is flexible in handling varying scales of text images and standardizes text regions into predetermined rectangles."}),"\n",(0,a.jsx)(s.h3,{id:"feature-extraction-stage",children:"Feature Extraction Stage"}),"\n",(0,a.jsx)(s.p,{children:"In this stage, CNNs abstract the input image (X or X') into visual feature maps."}),"\n",(0,a.jsx)(s.p,{children:"Each column of the feature map corresponds to a distinguishable receptive field in the horizontal direction of the input image, and these features are used to predict the character within each receptive field."}),"\n",(0,a.jsx)(s.p,{children:"Three common architectures are studied: VGG, RCNN, and ResNet:"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"VGG"}),": A simple architecture composed of multiple convolutional layers and a few fully connected layers."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"RCNN"}),": A variant of CNN that recursively adjusts the receptive fields, adapting to character shapes."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"ResNet"}),": Incorporates residual connections to alleviate the difficulties of training deep CNNs."]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"sequence-modeling-stage",children:"Sequence Modeling Stage"}),"\n",(0,a.jsxs)(s.p,{children:["The output from the feature extraction stage is reshaped into a feature sequence V, where each column ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"v"}),(0,a.jsx)(s.mi,{children:"i"})]}),(0,a.jsx)(s.mo,{children:"\u2208"}),(0,a.jsx)(s.mi,{children:"V"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"v_i \\in V"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6891em",verticalAlign:"-0.15em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"v"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3117em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"i"})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"\u2208"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"})]})]})]})," is treated as one frame in the sequence."]}),"\n",(0,a.jsxs)(s.p,{children:["However, this sequence may lack contextual information, so previous studies have used Bidirectional Long Short-Term Memory (BiLSTM) networks to enhance the sequence ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"H"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mi,{children:"S"}),(0,a.jsx)(s.mi,{children:"e"}),(0,a.jsx)(s.mi,{children:"q"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"."}),(0,a.jsx)(s.mo,{stretchy:"false",children:"("}),(0,a.jsx)(s.mi,{children:"V"}),(0,a.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"H = Seq.(V)"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.08125em"},children:"H"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"S"}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"e"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"q"}),(0,a.jsx)(s.span,{className:"mord",children:"."}),(0,a.jsx)(s.span,{className:"mopen",children:"("}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"}),(0,a.jsx)(s.span,{className:"mclose",children:")"})]})]})]}),", capturing contextual relationships in the character sequence."]}),"\n",(0,a.jsx)(s.h3,{id:"prediction-stage",children:"Prediction Stage"}),"\n",(0,a.jsxs)(s.p,{children:["In this stage, the character sequence ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"Y"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mn,{children:"1"})]}),(0,a.jsx)(s.mo,{separator:"true",children:","}),(0,a.jsxs)(s.msub,{children:[(0,a.jsx)(s.mi,{children:"y"}),(0,a.jsx)(s.mn,{children:"2"})]}),(0,a.jsx)(s.mo,{separator:"true",children:","}),(0,a.jsx)(s.mo,{children:"\u2026"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"Y = y_1, y_2, \u2026"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"Y"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"1"})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mpunct",children:","}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.3011em"},children:(0,a.jsxs)(s.span,{style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"2"})})]})}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,a.jsx)(s.span,{})})})]})})]}),(0,a.jsx)(s.span,{className:"mpunct",children:","}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,a.jsx)(s.span,{className:"minner",children:"\u2026"})]})]})]})," is predicted from sequence ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"H"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"H"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.08125em"},children:"H"})]})})]}),"."]}),"\n",(0,a.jsx)(s.p,{children:"Based on previous research, the authors provide two prediction options:"}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Connectionist Temporal Classification (CTC)"}),": This method predicts sequences of variable length with a fixed number of features. By removing repeated characters and blanks, the final character sequence is produced."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Attention-based Sequence Prediction (Attn)"}),": This method automatically captures the information flow in the input sequence, learning a character-level language model that represents the dependencies between output classes."]}),"\n"]}),"\n",(0,a.jsx)(s.p,{children:"These modules can be selected or adjusted as needed to suit different STR applications."}),"\n",(0,a.jsx)(s.h2,{id:"experimental-setup",children:"Experimental Setup"}),"\n",(0,a.jsx)(s.p,{children:"As previously mentioned, the dataset significantly affects STR model performance. To ensure a fair comparison, the authors standardized the training, validation, and evaluation datasets used."}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Training Dataset"}),": A combined dataset of MJSynth (8.9 million samples) and SynthText (5.5 million samples), totaling 14.4 million samples."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Validation Dataset"}),": A combination of the training sets from IC13, IC15, IIIT, and SVT, used as the validation dataset."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Training Parameters"}),": AdaDelta optimizer, decay rate \u03c1=0.95, batch size 192, total iterations 300,000, gradient clipping at 5, He initialization method, and model validation every 2,000 steps."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Duplicate Data Handling"}),": Part of the IC03 training data was excluded due to 34 scene images (containing 215 word boxes) overlapping with the IC13 evaluation dataset."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Evaluation Metrics"}),":","\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Accuracy"}),": Success rate of word prediction for the nine evaluation datasets."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Speed"}),": Average processing time per image (in milliseconds)."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Memory"}),": The number of trainable floating-point parameters in the entire STR model."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Experimental Environment"}),": Intel Xeon(R) E5-2630 v4 2.20GHz CPU, NVIDIA TESLA P40 GPU, 252GB RAM."]}),"\n"]}),"\n",(0,a.jsx)(s.h2,{id:"discussion",children:"Discussion"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"discussion",src:n(10540).A+"",width:"1678",height:"990"})}),"\n",(0,a.jsx)(s.p,{children:"The authors analyzed the accuracy-speed and accuracy-memory trade-offs for different module combinations."}),"\n",(0,a.jsx)(s.p,{children:"The graph below shows the trade-off curves for all module combinations, including six previously proposed STR models (marked with asterisks)."}),"\n",(0,a.jsx)(s.h3,{id:"accuracy-time-trade-off-analysis",children:"Accuracy-Time Trade-off Analysis"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"80%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"accuracy-time",src:n(42255).A+"",width:"1224",height:"472"})})})}),"\n",(0,a.jsx)(s.p,{children:"According to chart (a), T1 is the fastest model as it does not include any transformation or sequence modules."}),"\n",(0,a.jsxs)(s.p,{children:["From T1 to T5, the modules are progressively added (in ",(0,a.jsx)(s.strong,{children:"bold"}),"): ",(0,a.jsx)(s.strong,{children:"ResNet"}),", ",(0,a.jsx)(s.strong,{children:"BiLSTM"}),", ",(0,a.jsx)(s.strong,{children:"TPS"})," (Thin-Plate Spline), and ",(0,a.jsx)(s.strong,{children:"Attn"})," (Attention mechanism)."]}),"\n",(0,a.jsx)(s.p,{children:"Through these changes from T1 to T5, only one module is altered each time, providing a smooth transition between performance and computational efficiency, allowing minimal trade-offs between performance and efficiency depending on the application scenario."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"ResNet, BiLSTM, and TPS"})," significantly improve accuracy (69.5%\u219282.9%) with a moderate speed drop overall (1.3 ms \u2192 10.9 ms)."]}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Attn"})," further increases accuracy by 1.1%, but efficiency is drastically reduced (27.6 ms), showing a high efficiency cost."]}),"\n",(0,a.jsx)(s.h3,{id:"accuracy-memory-trade-off-analysis",children:"Accuracy-Memory Trade-off Analysis"}),"\n",(0,a.jsx)("div",{align:"center",children:(0,a.jsx)("figure",{style:{width:"80%"},children:(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"accuracy-memory",src:n(69874).A+"",width:"1224",height:"456"})})})}),"\n",(0,a.jsx)(s.p,{children:"According to chart (b), P1 is the model with the least memory consumption. As modules are progressively added from P1 to P5, accuracy improves while memory increases."}),"\n",(0,a.jsxs)(s.p,{children:["In terms of accuracy-memory trade-offs, one module is also altered at a time, and accuracy gradually improves with modules added in this order: ",(0,a.jsx)(s.strong,{children:"Attn"}),", ",(0,a.jsx)(s.strong,{children:"TPS"}),", ",(0,a.jsx)(s.strong,{children:"BiLSTM"}),", and ",(0,a.jsx)(s.strong,{children:"ResNet"}),"."]}),"\n",(0,a.jsxs)(s.p,{children:["Compared to VGG used in T1, ",(0,a.jsx)(s.strong,{children:"RCNN"})," is more lightweight in P1-P4, offering good accuracy-memory trade-offs. RCNN uses fewer standalone CNN layers, which are recursively applied, providing lightweight and efficient performance."]}),"\n",(0,a.jsx)(s.p,{children:"Transformation, sequence, and prediction modules have minimal impact on memory consumption (1.9M\u21927.2M parameters) but can significantly boost accuracy (75.4%\u219282.3%)."}),"\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"ResNet"})," is introduced in the final stage, improving accuracy by 1.7% but substantially increasing memory consumption to 49.6M floating-point parameters. Therefore, memory-sensitive applications can freely choose transformation, sequence, and prediction modules but should avoid high-load feature extractors like ResNet."]}),"\n",(0,a.jsx)(s.h3,{id:"module-analysis",children:"Module Analysis"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"module-analysis",src:n(10997).A+"",width:"1400",height:"664"})}),"\n",(0,a.jsx)(s.p,{children:"The authors analyzed the performance of each module in terms of accuracy, speed, and memory requirements."}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Accuracy Improvement"}),": Compared to regular benchmark datasets, performance improvements on irregular datasets are approximately double. When comparing accuracy improvement and time usage, the optimal module upgrade order is: ",(0,a.jsx)(s.strong,{children:"ResNet, BiLSTM, TPS, Attn"}),". This is the most effective order for upgrading from the baseline combination (None-VGG-None-CTC) and aligns with the accuracy-time trade-off upgrade sequence (T1\u2192T5)."]}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Accuracy-Memory Perspective"}),": From a memory consumption standpoint, the most effective module upgrade order is: ",(0,a.jsx)(s.strong,{children:"RCNN, Attn, TPS, BiLSTM, ResNet"}),". This matches the accuracy-memory trade-off upgrade sequence (P1\u2192P5). However, the most efficient module upgrade orders for time and memory are exactly opposite."]}),"\n"]}),"\n",(0,a.jsx)(s.h3,{id:"failure-case-analysis",children:"Failure Case Analysis"}),"\n",(0,a.jsx)(s.p,{children:(0,a.jsx)(s.img,{alt:"failure-case",src:n(86858).A+"",width:"1952",height:"346"})}),"\n",(0,a.jsx)(s.p,{children:"The authors also analyzed all failure cases."}),"\n",(0,a.jsx)(s.p,{children:"The chart shows six common types of failures. Of the 8,539 examples in the benchmark datasets, 644 images (7.5%) were not correctly recognized by any model."}),"\n",(0,a.jsxs)(s.ol,{children:["\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Calligraphy Fonts"}),":"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:'Special fonts for brands (e.g., "Coca Cola") or street store names (e.g., "Cafe") remain a challenge. These diverse font styles require a new feature extractor to provide more adaptable visual features.'}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Future Research"}),": Regularization may help prevent the model from overfitting to the font styles in the training dataset."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Vertical Text"}),":"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Most current STR models assume that the text images are oriented horizontally, making it difficult to handle vertical text. Some models have attempted to utilize vertical information, but handling vertical text remains underdeveloped."}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Future Research"}),": Further research may need to focus on recognizing vertical text more effectively."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Special Characters"}),":"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Since current benchmark tests do not evaluate special characters, existing research excludes them during training, causing models to misclassify these characters as alphabetic or numeric."}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Future Research"}),": Including special characters in training could improve the accuracy of the IIIT dataset from 87.9% to 90.3%."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Severe Occlusion"}),":"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Current methods do not fully utilize contextual information to overcome occlusion issues."}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Future Research"}),": Using stronger language models to maximize the use of contextual information could help."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Low Resolution"}),":"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Existing models do not explicitly address low-resolution cases."}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Future Research"}),": Using image pyramids or super-resolution modules may improve performance."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(s.li,{children:["\n",(0,a.jsxs)(s.p,{children:[(0,a.jsx)(s.strong,{children:"Label Noise"}),":"]}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Some failure cases are due to mislabeled data in the datasets. Upon examination, all benchmark datasets contain noisy labels."}),"\n",(0,a.jsxs)(s.li,{children:[(0,a.jsx)(s.strong,{children:"Label Noise Statistics"}),":","\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"The proportion of incorrect labels without considering special characters is 1.3%."}),"\n",(0,a.jsx)(s.li,{children:"The proportion of incorrect labels considering special characters is 6.1%."}),"\n",(0,a.jsx)(s.li,{children:"The proportion of incorrect labels considering case sensitivity is 24.1%."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(s.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(s.p,{children:"This paper provides an in-depth and comprehensive analysis of the scene text recognition field, offering a unified framework for fairly comparing the performance of different models."}),"\n",(0,a.jsx)(s.p,{children:"If you don\u2019t have time to read other papers, at least read this one!"}),"\n",(0,a.jsx)(s.p,{children:"It will help you quickly understand the state of the text recognition field as of 2019 and provide clear directions for your research, avoiding many potential pitfalls."})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},27898:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img1-b6fee3f37b9d379ef249184dc7ac9da9.jpg"},10997:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img11-72b5ebee7f9f3dbf028858bd37b02093.jpg"},86858:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img12-6664cd07ad0357a4b4f63cfd0e92f8fb.jpg"},50501:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img2-0f35900d2381dfb775886a6c83dafd2c.jpg"},23336:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img3-7752dfa9ee5593b40ad9ab6a9ef3f2f2.jpg"},53339:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img4-8e9b6e395d7cf54cb195821c5bc05745.jpg"},5502:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img5-f5bb08005f432285a4c4cabbb0e0d444.jpg"},48585:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img6-153ec9b98a6d0dad378fdb020d7d330a.jpg"},10540:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img7-7eb0f4a4b882cc8ecddcedc5f74911f9.jpg"},42255:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img8-896efaa39d3072851748eaa82af58ee1.jpg"},69874:(e,s,n)=>{n.d(s,{A:()=>i});const i=n.p+"assets/images/img9-3da481f6010ccad62790170a14d92e82.jpg"},28453:(e,s,n)=>{n.d(s,{R:()=>r,x:()=>l});var i=n(96540);const a={},t=i.createContext(a);function r(e){const s=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);