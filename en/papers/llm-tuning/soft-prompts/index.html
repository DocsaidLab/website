<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-llm-tuning/soft-prompts/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">[21.04] Soft Prompts | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/llm-tuning/soft-prompts/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[21.04] Soft Prompts | DOCSAID"><meta data-rh="true" name="description" content="Strings Whispering Like Secrets"><meta data-rh="true" property="og:description" content="Strings Whispering Like Secrets"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/llm-tuning/soft-prompts/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/llm-tuning/soft-prompts/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/llm-tuning/soft-prompts/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/llm-tuning/soft-prompts/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.51ef4fe1.css">
<script src="/en/assets/js/runtime~main.799ea81b.js" defer="defer"></script>
<script src="/en/assets/js/main.b2fd8eaf.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a><a class="navbar__item navbar__link" href="/en/playground/intro">Playground</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/llm-tuning/soft-prompts/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/llm-tuning/soft-prompts/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="Expand sidebar category &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="Expand sidebar category &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="Expand sidebar category &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="Expand sidebar category &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/llm-tuning-5">LLM Tuning (5)</a><button aria-label="Collapse sidebar category &#x27;LLM Tuning (5)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/llm-tuning/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/llm-tuning/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/llm-tuning/prefix-tuning/">[21.01] Prefix-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/llm-tuning/soft-prompts/">[21.04] Soft Prompts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/llm-tuning/lora/">[21.06] LoRA</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/multimodality-20">Multimodality (20)</a><button aria-label="Expand sidebar category &#x27;Multimodality (20)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization-1">Normalization (1)</a><button aria-label="Expand sidebar category &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection-8">Object Detection (8)</a><button aria-label="Expand sidebar category &#x27;Object Detection (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="Expand sidebar category &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="Expand sidebar category &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="Expand sidebar category &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-recognition-16">Text Recognition (16)</a><button aria-label="Expand sidebar category &#x27;Text Recognition (16)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-spotting-3">Text Spotting (3)</a><button aria-label="Expand sidebar category &#x27;Text Spotting (3)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/transformers-15">Transformers (15)</a><button aria-label="Expand sidebar category &#x27;Transformers (15)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="Expand sidebar category &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">All Notes: 130 entries</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/llm-tuning-5"><span itemprop="name">LLM Tuning (5)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[21.04] Soft Prompts</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[21.04] Soft Prompts</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="strings-whispering-like-secrets">Strings Whispering Like Secrets<a href="#strings-whispering-like-secrets" class="hash-link" aria-label="Direct link to Strings Whispering Like Secrets" title="Direct link to Strings Whispering Like Secrets">​</a></h2>
<p><a href="https://arxiv.org/abs/2104.08691" target="_blank" rel="noopener noreferrer"><strong>The Power of Scale for Parameter-Efficient Prompt Tuning</strong></a></p>
<hr>
<p>We just recently explored <strong>Prefix-Tuning</strong>, and now it&#x27;s time to look at another new method: <strong>Prompt Tuning</strong>.</p>
<p>If you haven’t read about Prefix-Tuning yet, you may want to check out our previous article:</p>
<ul>
<li><a href="/en/papers/llm-tuning/prefix-tuning/"><strong>[21.01] Prefix-Tuning: Is it the Same or Different?</strong></a></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-definition">Problem Definition<a href="#problem-definition" class="hash-link" aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>In the design of Prefix-Tuning, a sequence of tokens called a <strong>prefix</strong> is added at the beginning of the model’s input to guide it in generating the desired output. This prefix needs to interact with every layer of the model to ensure that the guidance influences the model at all levels.</p>
<p>But can we simplify this process? What if we only provide guidance at the input layer? Would that be sufficient?</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>The concept of &quot;guidance&quot; here differs from the more commonly known <strong>Prompt Engineering</strong>. In Prompt Engineering, we use natural language prompts to steer the model&#x27;s output. This approach does not modify the input features, adjust parameters, or change the model architecture.</p><p><strong>Prompt Tuning</strong>, however, involves adding a special <strong>trainable token</strong> at the input layer. During training, the model learns how to utilize this token to discover the optimal way to guide itself.</p></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-solution">Problem Solution<a href="#problem-solution" class="hash-link" aria-label="Direct link to Problem Solution" title="Direct link to Problem Solution">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="prompt-tuning">Prompt Tuning<a href="#prompt-tuning" class="hash-link" aria-label="Direct link to Prompt Tuning" title="Direct link to Prompt Tuning">​</a></h3>
<div align="center"><figure style="width:80%"><p><img decoding="async" loading="lazy" alt="model arch" src="/en/assets/images/img1-03e4bbd9625e2ff48bffb2de21f56a1f.jpg" width="1184" height="640" class="img_ev3q"></p></figure></div>
<p>To better understand the concept of <strong>Prompt Tuning</strong>, the authors build on the <strong>text-to-text</strong> framework introduced by T5 (Raffel et al., 2020). T5 treats all tasks as text generation problems—whether it’s translation, summarization, or classification, each task is represented as generating text output from text input.</p>
<p>Traditional classification models rely on the probability function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Pr(y | X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span></span></span></span>, mapping an input <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> to an output class <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span>. This means that the model predicts which class the input belongs to.</p>
<p>However, the T5 framework focuses on <strong>conditional generation</strong> tasks, where the objective is to compute:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Pr_\theta(Y | X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span></span></span></span></span>
<p>where:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span></span></span></span> is the <strong>text sequence</strong> representing a class (e.g., &quot;positive&quot; or &quot;negative&quot;).</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span> represents the parameters of the Transformer model.</li>
</ul>
<p>The advantage of this approach is that it enables the model to generate richer text outputs, instead of merely assigning a class label.</p>
<p>A <strong>prompt</strong> is a piece of text added to the beginning of the input <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span> to guide the model toward generating the correct output <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span></span></span></span>. It provides context or instructions to help the model understand how to handle the input.</p>
<p>For instance, in a sentiment analysis task, a prompt might be:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&quot;Please determine the sentiment of the following sentence:&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This prompt is concatenated with the user’s input sentence to form the final input to the model.</p>
<p>In models like GPT-3, the prompt <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>p</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>p</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">P = \{p_1, p_2, ..., p_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> is part of the model&#x27;s embedding table, represented by frozen parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>. However, crafting prompts manually is labor-intensive, inconsistent, and non-differentiable.</p>
<p>To address these challenges, the authors propose <strong>Prompt Tuning</strong>.</p>
<p>Prompt Tuning introduces <strong>trainable prompt embeddings</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>P</mi></msub></mrow><annotation encoding="application/x-tex">\theta_P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> that are not limited to the original embedding table. These parameters can be automatically learned from training data.</p>
<p>The new conditional generation formula becomes:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>r</mi><mrow><mi>θ</mi><mo separator="true">;</mo><msub><mi>θ</mi><mi>P</mi></msub></mrow></msub><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">[</mo><mi>P</mi><mo separator="true">;</mo><mi>X</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Pr_{\theta; \theta_P}(Y | [P; X])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span><span class="mpunct mtight">;</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mord">∣</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">])</span></span></span></span></span>
<p>where:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>P</mi><mo separator="true">;</mo><mi>X</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[P; X]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">]</span></span></span></span> denotes the concatenation of the prompt <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span> and the input <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span>.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span> refers to the frozen model parameters.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>P</mi></msub></mrow><annotation encoding="application/x-tex">\theta_P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> represents the trainable prompt parameters.</li>
</ul>
<p>During training, only the prompt parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>P</mi></msub></mrow><annotation encoding="application/x-tex">\theta_P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> are updated using backpropagation, while the main model parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span> remain unchanged.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="implementation-steps">Implementation Steps<a href="#implementation-steps" class="hash-link" aria-label="Direct link to Implementation Steps" title="Direct link to Implementation Steps">​</a></h3>
<ol>
<li><strong>Input Embedding</strong>: Embed the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span> tokens of the input into a matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>e</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>e</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X_e \in \mathbb{R}^{n \times e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">e</span></span></span></span></span></span></span></span></span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">e</span></span></span></span> is the embedding dimension.</li>
<li><strong>Prompt Embedding</strong>: The prompt embeddings form a matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>e</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>p</mi><mo>×</mo><mi>e</mi></mrow></msup></mrow><annotation encoding="application/x-tex">P_e \in \mathbb{R}^{p \times e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">e</span></span></span></span></span></span></span></span></span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span> is the prompt length.</li>
<li><strong>Concatenation</strong>: The prompt and input embeddings are concatenated as:<!-- -->
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>P</mi><mi>e</mi></msub><mo separator="true">;</mo><msub><mi>X</mi><mi>e</mi></msub><mo stretchy="false">]</mo><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi>n</mi><mo stretchy="false">)</mo><mo>×</mo><mi>e</mi></mrow></msup></mrow><annotation encoding="application/x-tex">[P_e; X_e] \in \mathbb{R}^{(p + n) \times e}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.938em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">n</span><span class="mclose mtight">)</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">e</span></span></span></span></span></span></span></span></span></span></span></span></span>
</li>
<li><strong>Model Computation</strong>: The concatenated embeddings are fed into the encoder-decoder model for processing.</li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Let’s consider an example where the task is to determine the sentiment of the sentence: &quot;I love this movie!&quot;</p><p><strong>Traditional Approach</strong>:</p><ul>
<li><strong>Manual Prompt</strong>: &quot;Please determine the sentiment of the following sentence:&quot;</li>
<li><strong>Model Input</strong>: &quot;Please determine the sentiment of the following sentence: I love this movie!&quot;</li>
</ul><p><strong>Prompt Tuning Approach</strong>:</p><ol>
<li>
<p><strong>Initialize the Prompt</strong>:</p>
<ul>
<li>Set the prompt length to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">p=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">5</span></span></span></span>, meaning five trainable prompt vectors.</li>
<li>These vectors can be initialized randomly or selected from the embedding table.</li>
</ul>
</li>
<li>
<p><strong>Model Input</strong>: The prompt vectors are concatenated with the input sentence&#x27;s embeddings to form the model’s input.</p>
</li>
<li>
<p><strong>Training Process</strong>:</p>
<ul>
<li>Use a large amount of labeled sentiment analysis data.</li>
<li>Update only the prompt parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>P</mi></msub></mrow><annotation encoding="application/x-tex">\theta_P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> through backpropagation.</li>
<li>The model learns to generate the correct sentiment label when it sees the learned prompt.</li>
</ul>
</li>
<li>
<p><strong>Model Output</strong>: For the input &quot;I love this movie!&quot;, the model might generate &quot;positive&quot;.</p>
</li>
</ol></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p><img decoding="async" loading="lazy" alt="ablation" src="/en/assets/images/img2-1cc24cd284d57491ab8144c1d1e311d9.jpg" width="1292" height="984" class="img_ev3q"></p>
<p>The authors conducted a series of ablation studies to investigate several key questions about <strong>Prompt Tuning</strong>:</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-long-should-the-prompt-be">How Long Should the Prompt Be?<a href="#how-long-should-the-prompt-be" class="hash-link" aria-label="Direct link to How Long Should the Prompt Be?" title="Direct link to How Long Should the Prompt Be?">​</a></h3>
<p>As shown in Figure (a), the authors tested different prompt lengths <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>5</mn><mo separator="true">,</mo><mn>20</mn><mo separator="true">,</mo><mn>100</mn><mo separator="true">,</mo><mn>150</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{1, 5, 20, 100, 150\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">20</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">100</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">150</span><span class="mclose">}</span></span></span></span> across various model sizes (Small, Base, Large, XL, XXL).</p>
<p>The results indicate that for most models, using more than one token significantly improves performance. However, for the <strong>T5-XXL model</strong>, even a single-token prompt achieves decent results, suggesting that larger models require less prompting to perform well.</p>
<p>Performance gains plateau after <strong>20 tokens</strong>, with only minor improvements beyond that length.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-the-initialization-strategy-affect-performance">How Does the Initialization Strategy Affect Performance?<a href="#how-does-the-initialization-strategy-affect-performance" class="hash-link" aria-label="Direct link to How Does the Initialization Strategy Affect Performance?" title="Direct link to How Does the Initialization Strategy Affect Performance?">​</a></h3>
<p>In Figure (b), the authors compared three initialization strategies:</p>
<ol>
<li><strong>Random Initialization</strong>: Tokens are sampled uniformly from the range <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>0.5</mn><mo separator="true">,</mo><mn>0.5</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-0.5, 0.5]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">0.5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">0.5</span><span class="mclose">]</span></span></span></span>.</li>
<li><strong>Vocabulary Embedding Initialization</strong>: Tokens are initialized from embeddings of the <strong>5,000 most frequent words</strong> in T5’s vocabulary.</li>
<li><strong>Label Embedding Initialization</strong>: For classification tasks, labels are converted into embeddings. If a label consists of multiple tokens, the embeddings are averaged. If the prompt length exceeds the number of labels, the remaining tokens are filled using embeddings from the vocabulary.</li>
</ol>
<p>The results show that <strong>label embedding initialization</strong> yields the best performance across all model sizes, especially for <strong>smaller models</strong>, where the choice of initialization has a significant impact. On the other hand, <strong>T5-XXL</strong> is less sensitive to the initialization strategy, maintaining stable performance across different methods.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-the-pre-training-objective-affect-performance">How Does the Pre-training Objective Affect Performance?<a href="#how-does-the-pre-training-objective-affect-performance" class="hash-link" aria-label="Direct link to How Does the Pre-training Objective Affect Performance?" title="Direct link to How Does the Pre-training Objective Affect Performance?">​</a></h3>
<p>Figure (c) explores the impact of different pre-training objectives on Prompt Tuning:</p>
<ol>
<li><strong>Span Corruption</strong>: Uses the standard T5 span corruption objective during pre-training.</li>
<li><strong>Span Corruption + Sentinel</strong>: Adds sentinel tokens in the target output during fine-tuning to simulate the span corruption format used in pre-training.</li>
<li><strong>LM Adaptation</strong>: Fine-tunes the T5 model for an additional 100,000 steps using a <strong>language modeling (LM) objective</strong> instead of span corruption.</li>
</ol>
<p>The results show that models pre-trained with <strong>span corruption</strong> are not well-suited for Prompt Tuning with frozen parameters. This is because the model is accustomed to both reading and producing outputs with sentinel tokens. Even using the <strong>Span Corruption + Sentinel</strong> strategy to simulate the original pre-training format yields limited improvements.</p>
<p>In contrast, <strong>LM Adaptation</strong> significantly boosts performance across all model sizes, suggesting that switching to a pure language modeling objective makes the model more compatible with Prompt Tuning.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-the-duration-of-lm-adaptation-affect-performance">How Does the Duration of LM Adaptation Affect Performance?<a href="#how-does-the-duration-of-lm-adaptation-affect-performance" class="hash-link" aria-label="Direct link to How Does the Duration of LM Adaptation Affect Performance?" title="Direct link to How Does the Duration of LM Adaptation Affect Performance?">​</a></h3>
<p>Figure (d) analyzes the effect of the LM adaptation duration on Prompt Tuning performance.</p>
<p>The results show that extending the adaptation beyond <strong>100,000 steps</strong> provides diminishing returns, with the optimal results appearing around that mark. Transitioning from span corruption to an LM objective is not a straightforward process—it requires substantial training resources (equivalent to <strong>10% of the original T5 pre-training steps</strong>).</p>
<p>Despite the challenges, <strong>T5-XXL</strong> demonstrates high resilience across various non-optimal configurations. In contrast, smaller models sometimes outperform larger ones (Base, Large, and XL) under the span corruption setting. This inconsistency is not due to random noise, as the pattern was observed across <strong>three repeated experiments</strong> with low variance.</p>
<p>Compared to models pre-trained with span corruption, <strong>LM-adapted models</strong> exhibit much greater stability across all sizes, significantly reducing the risk of erratic performance. This highlights the importance of using LM adaptation for robust performance in Prompt Tuning.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="comparison-with-other-methods">Comparison with Other Methods<a href="#comparison-with-other-methods" class="hash-link" aria-label="Direct link to Comparison with Other Methods" title="Direct link to Comparison with Other Methods">​</a></h3>
<div align="center"><figure style="width:80%"><p><img decoding="async" loading="lazy" alt="comparison" src="/en/assets/images/img3-6f570e3deede22c97b6c84c904cda64e.jpg" width="792" height="716" class="img_ev3q"></p></figure></div>
<ul>
<li>
<p><strong>Prefix Tuning</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2101.00190" target="_blank" rel="noopener noreferrer"><strong>[21.01] Prefix-Tuning: Optimizing Continuous Prompts for Generation</strong></a></li>
</ul>
<p>Prefix Tuning involves adding <strong>trainable prefixes</strong> to every layer of the Transformer model, essentially acting as fixed activations for each layer. This method is suitable for models like <strong>GPT-2</strong> and <strong>BART</strong>, while Prompt Tuning is designed for <strong>T5</strong>.</p>
<p>In BART, Prefix Tuning requires adding prefixes to both the <strong>encoder and decoder</strong>, whereas Prompt Tuning only involves adding a prompt to the <strong>encoder</strong>.</p>
<p>Prompt Tuning requires fewer parameters since it only introduces a single prompt token at the input layer, rather than prefixes across all layers. Moreover, Prompt Tuning enables the Transformer to update its task representation dynamically based on the input, whereas Prefix Tuning requires re-parameterization to stabilize the training process.</p>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>WARP (Word-level Adversarial ReProgramming)</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2101.00121" target="_blank" rel="noopener noreferrer"><strong>[21.01] WARP: Word-level Adversarial ReProgramming</strong></a></li>
</ul>
<p>WARP adds prompt parameters to the input layer and uses the <strong>[MASK] token</strong> along with a learnable output layer to map masked segments to class predictions. However, WARP is limited to generating a <strong>single output</strong>, making it more suitable for <strong>classification tasks</strong> only.</p>
<p>In contrast, Prompt Tuning does not require any specialized input design or task-specific output layers, making it more versatile for a broader range of tasks. Additionally, Prompt Tuning achieves performance closer to full model fine-tuning without complex input transformations.</p>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>P-Tuning</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2103.10385" target="_blank" rel="noopener noreferrer"><strong>[21.03] GPT Understands, Too</strong></a></li>
</ul>
<p>P-Tuning introduces <strong>learnable continuous prompts</strong> embedded between the input tokens and relies on human-designed patterns for arrangement. To achieve high performance on tasks like <strong>SuperGLUE</strong>, P-Tuning requires a combination of <strong>prompt tuning and model fine-tuning</strong>, meaning both the prompt and model parameters need to be updated.</p>
<p>In contrast, <strong>Prompt Tuning</strong> only updates the prompt parameters while keeping the <strong>language model frozen</strong>, avoiding the overhead of full model fine-tuning and reducing computational cost.</p>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>Soft Words</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2104.06599" target="_blank" rel="noopener noreferrer"><strong>[21.04] Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</strong></a></li>
</ul>
<p>Soft Words rely on <strong>manually designed prompt templates</strong> and introduce learnable parameters (\Delta_i) at every layer of the Transformer model. As the model depth increases, the number of parameters grows, leading to larger memory requirements.</p>
<p>In contrast, Prompt Tuning maintains efficiency by <strong>not introducing additional parameters for each layer</strong>, keeping the parameter scale smaller and more manageable.</p>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>Adapters</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1902.00751" target="_blank" rel="noopener noreferrer"><strong>[19.02] Parameter-Efficient Transfer Learning for NLP</strong></a></li>
</ul>
<p>Adapters are small <strong>bottleneck layers</strong> inserted between the frozen layers of the main model. This method reduces the number of task-specific parameters. For example, fine-tuning the <strong>Adapter layers</strong> in BERT-Large only increases the parameter count by <strong>2–4%</strong> while maintaining near full fine-tuning performance.</p>
<p>While <strong>Adapters</strong> modify the model’s behavior by rewriting the activations in intermediate layers, <strong>Prompt Tuning</strong> adjusts the <strong>input representation</strong> without changing the internal computations, preserving the internal structure of the frozen model.</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-does-prompt-tuning-actually-encode">What Does Prompt Tuning Actually Encode?<a href="#what-does-prompt-tuning-actually-encode" class="hash-link" aria-label="Direct link to What Does Prompt Tuning Actually Encode?" title="Direct link to What Does Prompt Tuning Actually Encode?">​</a></h3>
<p>Prompt Tuning guides a language model to generate the desired output by <strong>adjusting prompt vectors in continuous embedding space</strong>. However, since these prompts operate in the <strong>continuous space</strong> rather than the discrete word space, it is challenging to interpret how they influence the model&#x27;s behavior. To better understand how Prompt Tuning works, the authors employed the following methods to analyze the effect of these prompts.</p>
<p>The authors calculated the <strong>cosine similarity</strong> between each prompt token and every token in the model’s vocabulary to identify the <strong>nearest-neighbor tokens</strong>. This helps reveal the semantic meaning encoded in each prompt token.</p>
<p><strong>Key Findings:</strong></p>
<ol>
<li>
<p><strong>Semantically Coherent Clusters</strong>:
The nearest-neighbor words for each prompt token tend to form <strong>clusters of semantically related words</strong>, indicating that Prompt Tuning effectively learns meaningful representations similar to those of related words in the vocabulary.</p>
</li>
<li>
<p><strong>Comparison with Random Vectors</strong>:
When <strong>random vectors</strong> were used in place of trained prompt tokens, they failed to form coherent semantic clusters. This confirms that the effect of Prompt Tuning is not random—it captures and encodes the underlying <strong>semantic structure</strong> of the language model.</p>
</li>
</ol>
<p>For longer prompt sequences, the authors observed that <strong>multiple prompt tokens often shared the same nearest-neighbor words</strong>, which raises two potential concerns:</p>
<ol>
<li>
<p><strong>Redundant Capacity</strong>:
Some of the information within the prompt tokens may be <strong>repetitive or unnecessary</strong>, limiting further performance improvements.</p>
</li>
<li>
<p><strong>Lack of Sequential Structure</strong>:
The learned prompt representations do not effectively capture <strong>positional information</strong> within the sequence, making it difficult for the model to accurately locate and interpret key information.</p>
</li>
</ol>
<p>An interesting observation is that many of the nearest-neighbor words for the prompt tokens often included <strong>class labels</strong> from the downstream tasks. This suggests that the <strong>expected output labels are implicitly stored</strong> within the prompt tokens, helping the model generate correct outputs.</p>
<p>Although the learned prompt sequences are difficult to interpret in natural language, they form <strong>semantically meaningful representations</strong> and enable effective internal adjustments within the model. These findings demonstrate that Prompt Tuning not only guides the model toward specific outputs but also has the potential to <strong>dynamically adjust the context</strong> within the model.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Prompt Tuning achieves <strong>performance comparable to traditional fine-tuning</strong> in various experiments, and the performance gap narrows further as the model size increases.</p>
<p>In <strong>zero-shot transfer tasks</strong>, Prompt Tuning exhibits superior generalization, suggesting that <strong>freezing the core language model parameters</strong> while limiting learning to lightweight prompt vectors can help avoid <strong>overfitting to specific domains</strong>.</p>
<p>Beyond its performance advantages, Prompt Tuning also offers benefits in terms of <strong>storage and serving costs</strong>. By keeping the pre-trained model frozen, it supports <strong>multi-task processing</strong> more efficiently and promotes the seamless integration of different prompts.</p>
<p>Looking ahead, a promising research direction could involve <strong>separating task-specific parameters from general language modeling parameters</strong>. This could lead to new ways of efficiently guiding models without the need for full fine-tuning.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-10-30T13:41:56.000Z" itemprop="dateModified">Oct 30, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/llm-tuning/prefix-tuning/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[21.01] Prefix-Tuning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/llm-tuning/lora/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[21.06] LoRA</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#strings-whispering-like-secrets" class="table-of-contents__link toc-highlight">Strings Whispering Like Secrets</a></li><li><a href="#problem-definition" class="table-of-contents__link toc-highlight">Problem Definition</a></li><li><a href="#problem-solution" class="table-of-contents__link toc-highlight">Problem Solution</a><ul><li><a href="#prompt-tuning" class="table-of-contents__link toc-highlight">Prompt Tuning</a></li><li><a href="#implementation-steps" class="table-of-contents__link toc-highlight">Implementation Steps</a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#how-long-should-the-prompt-be" class="table-of-contents__link toc-highlight">How Long Should the Prompt Be?</a></li><li><a href="#how-does-the-initialization-strategy-affect-performance" class="table-of-contents__link toc-highlight">How Does the Initialization Strategy Affect Performance?</a></li><li><a href="#how-does-the-pre-training-objective-affect-performance" class="table-of-contents__link toc-highlight">How Does the Pre-training Objective Affect Performance?</a></li><li><a href="#how-does-the-duration-of-lm-adaptation-affect-performance" class="table-of-contents__link toc-highlight">How Does the Duration of LM Adaptation Affect Performance?</a></li><li><a href="#comparison-with-other-methods" class="table-of-contents__link toc-highlight">Comparison with Other Methods</a></li><li><a href="#what-does-prompt-tuning-actually-encode" class="table-of-contents__link toc-highlight">What Does Prompt Tuning Actually Encode?</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Projects</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/terms-of-service">TermsOfUse</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/privacy-policy">Privacy Policy</a><span class="footer__link-separator">·</span><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="footer__link-item">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>