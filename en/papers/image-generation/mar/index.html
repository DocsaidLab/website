<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-image-generation/mar/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>[24.06] MAR | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/image-generation/mar/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[24.06] MAR | DOCSAID"><meta data-rh=true name=description content="Reforging the Order of Generation"><meta data-rh=true property=og:description content="Reforging the Order of Generation"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/image-generation/mar/><link data-rh=true rel=alternate href=https://docsaid.org/papers/image-generation/mar/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/image-generation/mar/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/image-generation/mar/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/image-generation/mar/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.9fbf773a.css><script src=/en/assets/js/runtime~main.ed192c14.js defer></script><script src=/en/assets/js/main.a7ee3834.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/image-generation/mar/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/image-generation/mar/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/image-generation/mar/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-1m2bkf9 ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="Expand sidebar category 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-35>Face Anti-Spoofing (35)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (35)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Collapse sidebar category 'Image Generation (1)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/image-generation/mar/>[24.06] MAR</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="Expand sidebar category 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="Expand sidebar category 'Vision Transformers (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 210 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/en/papers/category/image-generation-1><span itemprop=name>Image Generation (1)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[24.06] MAR</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[24.06] MAR</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/nbswords.png alt=nbswords class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/nbswords target=_blank rel="noopener noreferrer">nbswords</a></div><div class=docAuthorTitle_Yp5_>Research Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/nbswords target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/nbswords/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a><a href=https://x.com/nbswordsYu target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 512 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=reforging-the-order-of-generation>Reforging the Order of Generation<a href=#reforging-the-order-of-generation class=hash-link aria-label="Direct link to Reforging the Order of Generation" title="Direct link to Reforging the Order of Generation">​</a></h2>
<p><a href=https://arxiv.org/abs/2406.11838 target=_blank rel="noopener noreferrer"><strong>Autoregressive Image Generation without Vector Quantization</strong></a></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p>This article is simultaneously published on <a href=https://medium.com/@nbswords/autoregressive-image-generation-without-vector-quantization-516b68b5acfa target=_blank rel="noopener noreferrer"><strong>nbswords' Medium</strong></a></div></div>
<p>Current autoregressive image generation models often use vector quantization (VQ) to discretize images into tokens, mimicking the success of autoregressive models in the NLP domain. However, the authors argue that such a discrete space is not necessary for autoregressive image generation. Therefore, they propose an autoregressive image generation model based on continuous space, which achieves higher accuracy and faster inference time.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=background-knowledge>Background Knowledge<a href=#background-knowledge class=hash-link aria-label="Direct link to Background Knowledge" title="Direct link to Background Knowledge">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=vector-quantization-vq>Vector Quantization (VQ)<a href=#vector-quantization-vq class=hash-link aria-label="Direct link to Vector Quantization (VQ)" title="Direct link to Vector Quantization (VQ)">​</a></h3>
<p>This is a long-established technique to accelerate vector search. The method segments a feature space vector (embedding vector) into different groups, each represented by a centroid vector serving as an index. A codebook containing all centroid vector indices is then used to access these groups of vectors.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=VQ src=/en/assets/images/img1-f74853576533d516a078f6ba6abac69b.jpg width=875 height=656 class=img_ev3q></figure></div>
<p>For details, please refer to <a href=https://medium.com/@nbswords/survey-of-vector-space-search-26555890ca5e target=_blank rel="noopener noreferrer">Survey Of Vector Space Search</a> or <a href=https://en.wikipedia.org/wiki/Vector_quantization target=_blank rel="noopener noreferrer">Vector quantization wiki</a>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=auto-regressive-image-generation>Auto-regressive Image Generation<a href=#auto-regressive-image-generation class=hash-link aria-label="Direct link to Auto-regressive Image Generation" title="Direct link to Auto-regressive Image Generation">​</a></h3>
<p>Early Visual Autoregressive Models (VAR) treated the image generation task as GPT-like autoregressive text generation, viewing each pixel as a category. The model’s task was to perform multi-class prediction using categorical cross-entropy. Examples include Google's <a href=https://arxiv.org/abs/1802.05751 target=_blank rel="noopener noreferrer">Image Transformer, 2018</a> and OpenAI’s <a href=https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf target=_blank rel="noopener noreferrer">ImageGPT, 2020</a>.</p>
<p>To accelerate image generation speed, current autoregressive image generation models commonly introduce VQ for two-stage training: the first stage learns a codebook in the latent space for image reconstruction, and the second stage autoregressively generates images based on the learned codebook.</p>
<ul>
<li>
<p>Take <a href=https://arxiv.org/abs/1711.00937 target=_blank rel="noopener noreferrer">VQ-VAE, 2017</a> as an example:</p>
<ul>
<li>In the encode stage, a CNN extracts image features, then vector quantization is applied on the feature map <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>z</mi><mi>e</mi></msub></mrow><annotation encoding=application/x-tex>z_e</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> to obtain centroid vectors (purple vectors, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>e</mi><mn>1</mn></msub><mo>∼</mo><msub><mi>e</mi><mi>K</mi></msub></mrow><annotation encoding=application/x-tex>e_1 \sim e_K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">e</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>∼</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">e</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.07153em>K</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>). Next, distances between each feature point in <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>z</mi><mi>e</mi></msub></mrow><annotation encoding=application/x-tex>z_e</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> and the centroid vectors are computed, and the nearest centroid vector index replaces each feature point, producing the discrete representation <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>q</mi><mo stretchy=false>(</mo><mi>z</mi><mi mathvariant=normal>∣</mi><mi>x</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>q(z|x)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.03588em>q</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.04398em>z</span><span class=mord>∣</span><span class="mord mathnormal">x</span><span class=mclose>)</span></span></span></span>.</li>
<li>In the decode stage, the image is generated using the mapped back <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>Z</mi><mi>q</mi></msub></mrow><annotation encoding=application/x-tex>Z_q</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>Z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:-0.0715em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>q</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span> from <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>q</mi></mrow><annotation encoding=application/x-tex>q</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em></span><span class="mord mathnormal" style=margin-right:0.03588em>q</span></span></span></span>.</li>
</ul>
</li>
</ul>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=VQ-VAE src=/en/assets/images/img2-02517e55f6b6fd15a280d398132135be.jpg width=1304 height=537 class=img_ev3q></figure></div>
<p>Autoregressive models speeding up image generation with VQ sounds great, but are there no drawbacks?</p>
<p>Certainly, there are: 1. VQ encoders are hard to train; 2. VQ degrades the quality of the reconstructed images.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=method>Method<a href=#method class=hash-link aria-label="Direct link to Method" title="Direct link to Method">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=abandoning-vq-embracing-diffusion>Abandoning VQ, Embracing Diffusion<a href=#abandoning-vq-embracing-diffusion class=hash-link aria-label="Direct link to Abandoning VQ, Embracing Diffusion" title="Direct link to Abandoning VQ, Embracing Diffusion">​</a></h3>
<p>Since diffusion models can represent the joint probability distribution of all pixels or tokens in an image, why not use them to represent the probability distribution of each token?</p>
<ul>
<li>Diffusion models can generate images from noise conditioned on an input prompt/image.</li>
</ul>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=diffusion+text_prompt src=/en/assets/images/img3-a20e0d0d126d7c3c5ac6876c6c1ff3b1.jpg width=954 height=482 class=img_ev3q></figure></div>
<ul>
<li>The current approach conditions on the output of a transformer to generate images from noise (details of the transformer input will be explained later).</li>
</ul>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=diffusion+transformer src=/en/assets/images/img4-087d1806c9bd61c8eca3f09fab06eed7.jpg width=972 height=554 class=img_ev3q></figure></div>
<p>Their method autoregressively predicts the conditional latent variable <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>z</mi></mrow><annotation encoding=application/x-tex>z</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.04398em>z</span></span></span></span> for each token, then uses a diffusion model (a small MLP) to perform denoising and obtain the output <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>x</mi></mrow><annotation encoding=application/x-tex>x</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">x</span></span></span></span>'s probability distribution <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>p</mi><mo stretchy=false>(</mo><mi>x</mi><mi mathvariant=normal>∣</mi><mi>z</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>p(x|z)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal">p</span><span class=mopen>(</span><span class="mord mathnormal">x</span><span class=mord>∣</span><span class="mord mathnormal" style=margin-right:0.04398em>z</span><span class=mclose>)</span></span></span></span>.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=autoregressive_diffusion src=/en/assets/images/img5-c79c367e42bc3ed8656a502d29a6797f.jpg width=508 height=355 class=img_ev3q></figure></div>
<p>They propose a Diffusion Loss to replace the original categorical cross-entropy.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=diffusion_cal src=/en/assets/images/img6-f52aa8173a4413b25ec0f5a9f5e49a3a.jpg width=543 height=80 class=img_ev3q></figure></div>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=diffusion_cal_2 src="data:image/jpeg;base64,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" width=324 height=69 class=img_ev3q></figure></div>
<ul>
<li>
<p><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>ε</mi></mrow><annotation encoding=application/x-tex>\varepsilon</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">ε</span></span></span></span> is a noise sample drawn from a standard normal distribution <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi mathvariant=script>N</mi><mo stretchy=false>(</mo><mn>0</mn><mo separator=true>,</mo><mn>1</mn><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\mathcal{N}(0,1)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathcal" style=margin-right:0.14736em>N</span><span class=mopen>(</span><span class=mord>0</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord>1</span><span class=mclose>)</span></span></span></span>.</p>
</li>
<li>
<p><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>ε</mi><mi>θ</mi></msub></mrow><annotation encoding=application/x-tex>\varepsilon_{\theta}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">ε</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> is a small MLP, where <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>ε</mi><mi>θ</mi></msub><mo stretchy=false>(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant=normal>∣</mi><mi>t</mi><mo separator=true>,</mo><mi>z</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\varepsilon_{\theta}(x_t | t, z)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal">ε</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mord>∣</span><span class="mord mathnormal">t</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.04398em>z</span><span class=mclose>)</span></span></span></span> means the model takes timestep <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>t</mi></mrow><annotation encoding=application/x-tex>t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6151em></span><span class="mord mathnormal">t</span></span></span></span> and condition <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>z</mi></mrow><annotation encoding=application/x-tex>z</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.04398em>z</span></span></span></span> as inputs and predicts the noise vector <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>ε</mi><mi>θ</mi></msub></mrow><annotation encoding=application/x-tex>\varepsilon_{\theta}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">ε</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> given <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding=application/x-tex>x_t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>.</p>
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>t</mi></mrow><annotation encoding=application/x-tex>t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6151em></span><span class="mord mathnormal">t</span></span></span></span> is the timestamp in the noise schedule, and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>z</mi></mrow><annotation encoding=application/x-tex>z</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.04398em>z</span></span></span></span> is the condition vector generated by the Transformer based on context.</li>
</ul>
</li>
</ul>
<p>Finally, similar to DDPM, during inference the model uses the reverse diffusion process to generate images (from <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding=application/x-tex>x_t</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> to <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding=application/x-tex>x_0</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>).</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=reverse_diff src=/en/assets/images/img8-2ddbeaa437c343e08c991bd632d4a991.jpg width=741 height=72 class=img_ev3q></figure></div>
<p>Additionally, readers familiar with diffusion models for image generation will recognize the hyperparameter called temperature, which controls sampling randomness—higher temperature means more randomness, lower means less. In this work, temperature is controlled by <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>τ</mi><mo>∗</mo><msub><mi>σ</mi><mi>t</mi></msub><mi>δ</mi></mrow><annotation encoding=application/x-tex>\tau * \sigma_t \delta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4653em></span><span class="mord mathnormal" style=margin-right:0.1132em>τ</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>∗</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8444em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>σ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:0.03785em>δ</span></span></span></span>, following the method introduced in <a href=https://arxiv.org/abs/2105.05233 target=_blank rel="noopener noreferrer">Diffusion Models Beat GANs on Image Synthesis</a>.</p>
<p>Reflecting on this approach reveals a clever combination of VAE’s conditional generation and DDPM’s denoising: compressing image features with a VAE and generating images with DDPM.</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=VAE_and_Diffusion src=/en/assets/images/img9-2c8c638a704853f10387abcb1e484c74.jpg width=1603 height=431 class=img_ev3q></figure></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=integrating-concepts-from-autoregressive-and-masked-generative-models>Integrating Concepts from Autoregressive and Masked Generative Models<a href=#integrating-concepts-from-autoregressive-and-masked-generative-models class=hash-link aria-label="Direct link to Integrating Concepts from Autoregressive and Masked Generative Models" title="Direct link to Integrating Concepts from Autoregressive and Masked Generative Models">​</a></h3>
<p>As the author of <a href=https://arxiv.org/abs/2111.06377 target=_blank rel="noopener noreferrer">MAE (Masked Autoencoders Are Scalable Vision Learners), 2021</a>, Kai Ming naturally considered incorporating MAE’s ideas into the model.</p>
<ul>
<li>
<p>The main pipeline refers to follow-up works on MAE: <a href=https://arxiv.org/abs/2202.04200 target=_blank rel="noopener noreferrer">MaskGIT, 2022</a> and <a href=https://arxiv.org/abs/2211.09117 target=_blank rel="noopener noreferrer">MAGE, 2022</a>, both using Transformers for Masked Autoregressive (MAR) modeling. MaskGIT’s contribution is using a bidirectional transformer decoder to predict multiple tokens simultaneously, while MAGE unifies image representation learning and image synthesis.</p>
</li>
<li>
<p>This work adopts MAE’s bidirectional attention mechanism, placing mask tokens [m] in the middle layers so that all tokens can see each other, rather than only previous tokens. Loss is computed only on unmasked tokens.</p>
<ul>
<li>
<p>Note: This is not the conventional causal vs. bidirectional attention. For deeper understanding, refer to the original MAE paper.</p>
</li>
<li>
<p>The advantage is improved image generation quality; the disadvantage is that training and inference cannot leverage kv cache acceleration. However, since multiple tokens are predicted simultaneously, the method is still reasonably fast.</p>
</li>
</ul>
</li>
</ul>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=bidirect src=/en/assets/images/img10-aba7fcdbac546aa2e68355a286a723d1.jpg width=696 height=668 class=img_ev3q></figure></div>
<ul>
<li>The figure below compares standard sequential AR, random-order AR (random masking of one token), and their approach (random masking of multiple tokens with simultaneous prediction)—this reveals that the transformer input mentioned earlier is a masked image.</li>
</ul>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=AR_and_MAR src=/en/assets/images/img11-d2104821c1e5aa8f79ea0b971d3cedcc.jpg width=816 height=570 class=img_ev3q></figure></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=implementation>Implementation<a href=#implementation class=hash-link aria-label="Direct link to Implementation" title="Direct link to Implementation">​</a></h2>
<ul>
<li>Diffusion Loss: uses a cosine-shaped noise schedule; training uses 1000 DDPM steps while inference uses only 100 steps.</li>
<li>Denoising MLP (small MLP): consists of 3 blocks with 1024 channels each; each block contains LayerNorm, linear layer, SiLU activation, and residual connections. AdaLN is used to inject the transformer output <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>z</mi></mrow><annotation encoding=application/x-tex>z</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.04398em>z</span></span></span></span> into the LayerNorm layers.</li>
<li>Tokenizer: uses publicly available tokenizers from LDM, including VQ-16 and KL-16. VQ-16 is based on VQ-GAN with GAN loss and perceptual loss; KL-16 uses KL divergence regularization and does not rely on VQ.</li>
<li>Transformer: a ViT receives the token sequence from the tokenizer, adds positional encoding and a class token [CLS], then passes through 32 layers of transformer blocks each with 1024 channels.</li>
<li>Masked autoregressive models: training uses a masking ratio between 0.7 and 1.0 (e.g., 70% tokens randomly masked). To avoid sequences becoming too short, 64 [CLS] tokens are always padded. During inference, the masking ratio is gradually reduced from 1.0 to 0 using a cosine schedule over 64 steps by default.</li>
<li>Baseline autoregressive model: a GPT model with causal attention, input appended with a [CLS] token, supporting kv cache and temperature parameters.</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=experiments>Experiments<a href=#experiments class=hash-link aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h2>
<p>The model experiments were conducted using AR/MAR-L (~400M parameters), trained for 400 epochs on ImageNet 256×256 images.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=diffusion-loss-vs-cross-entropy-loss>Diffusion Loss vs. Cross-Entropy Loss<a href=#diffusion-loss-vs-cross-entropy-loss class=hash-link aria-label="Direct link to Diffusion Loss vs. Cross-Entropy Loss" title="Direct link to Diffusion Loss vs. Cross-Entropy Loss">​</a></h3>
<p>Among all AR/MAR variants, models trained with Diffusion Loss consistently outperformed those trained with cross-entropy loss. The AR model saw the smallest improvement, with increasing gains moving towards MAR+bidirectional+more than one predictions (preds), demonstrating the critical importance of Diffusion Loss for MAR models.</p>
<ul>
<li>Diffusion Loss also incorporates Classifier-Free Guidance (CFG), commonly used in diffusion models, to enhance generation quality.</li>
<li>Fréchet Inception Distance (FID) is better when lower, while Inception Score (IS) is better when higher; both metrics assess generated image quality.</li>
</ul>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="Diffusion Loss" src=/en/assets/images/img12-77434de701c031ee57b0b07c3ce8e694.jpg width=1498 height=433 class=img_ev3q></figure></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=tokenizers>Tokenizers<a href=#tokenizers class=hash-link aria-label="Direct link to Tokenizers" title="Direct link to Tokenizers">​</a></h3>
<p>Experiments tested different tokenizers paired with Diffusion Loss. Moving away from discrete space to continuous space allows Diffusion Loss to work on both continuous and discrete tokenizers.</p>
<ul>
<li>VQ-16 refers to taking the continuous latent before vector quantization in VQ-VAE as tokens. Both VQ-16 and KL-16 tokenizers come from LDM but trained on ImageNet instead of OpenImages.</li>
<li>Consistency Decoder is a non-VQ tokenizer originating from <a href=https://github.com/openai/consistencydecoder target=_blank rel="noopener noreferrer">DALL·E 3</a>.</li>
<li>Reconstruction FID (rFID) is better when lower, used to evaluate tokenizer quality.</li>
</ul>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=Tokenizers src=/en/assets/images/img13-8250cad95bfb70573f1c8f3f468b4419.jpg width=1430 height=321 class=img_ev3q></figure></div>
<p>Results show continuous tokenizers like KL-16 outperform discrete ones like VQ-16. The model also works compatibly with different tokenizers such as Consistency Decoder.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=denoising-mlp>Denoising MLP<a href=#denoising-mlp class=hash-link aria-label="Direct link to Denoising MLP" title="Direct link to Denoising MLP">​</a></h3>
<p>Performance comparison among MLPs of different sizes shows width=1024 yields the best results.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt="Denosing MLP" src=/en/assets/images/img14-83a8c46a5314c97327752c6bb9ba6cd9.jpg width=901 height=285 class=img_ev3q></figure></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=sampling-steps-of-diffusion-loss>Sampling Steps of Diffusion Loss<a href=#sampling-steps-of-diffusion-loss class=hash-link aria-label="Direct link to Sampling Steps of Diffusion Loss" title="Direct link to Sampling Steps of Diffusion Loss">​</a></h3>
<p>Different diffusion steps impact generation quality. Using 100 steps already achieves good performance.</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="Sampling steps of Diffusion Loss" src=/en/assets/images/img15-42662a0c0fc5e8a350f9a7bdd57dba6c.jpg width=950 height=303 class=img_ev3q></figure></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=temperature-of-diffusion-loss>Temperature of Diffusion Loss<a href=#temperature-of-diffusion-loss class=hash-link aria-label="Direct link to Temperature of Diffusion Loss" title="Direct link to Temperature of Diffusion Loss">​</a></h3>
<p>Temperature is also an important factor for Diffusion Loss.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=Temperature src=/en/assets/images/img16-12abcebd945dac79cac80483fc029a04.jpg width=892 height=423 class=img_ev3q></figure></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=speedaccuracy-trade-off>Speed/Accuracy Trade-off<a href=#speedaccuracy-trade-off class=hash-link aria-label="Direct link to Speed/Accuracy Trade-off" title="Direct link to Speed/Accuracy Trade-off">​</a></h3>
<p>Since kv cache cannot be used, this evaluation is critical. Testing was done on an A100 GPU with batch size=256.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=Speed src=/en/assets/images/img17-903f63b92d80a5694d99a2f6e25bcffb.jpg width=791 height=582 class=img_ev3q></figure></div>
<ul>
<li>MAR: each point represents different autoregressive steps (8 to 128).</li>
<li>DiT: each point represents different diffusion steps (50, 75, 150, 250), using DiT-XL here.</li>
<li>AR: uses kv cache.</li>
</ul>
<p>Despite no kv cache, this model maintains decent inference speed, although default settings (step=64) are noticeably slower.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=system-level-comparison>System-Level Comparison<a href=#system-level-comparison class=hash-link aria-label="Direct link to System-Level Comparison" title="Direct link to System-Level Comparison">​</a></h3>
<p>Compared with other models, only the largest model MAR-H achieves the best performance, but MAR-L already performs well.</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="System-level comparison" src=/en/assets/images/img18-5b9dbf15d3b7495d204403fb09706b63.jpg width=1543 height=846 class=img_ev3q></figure></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In summary, this work breaks away from traditional autoregressive image generation methods by combining Diffusion and MAE concepts to open a new direction. It demonstrates promising results using the simplest DDPM; better diffusion models should yield even stronger outcomes. Looking forward to more derivative works.</p>
<p>Below are some example images generated by the model.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=result src=/en/assets/images/img19-2dc1297b93dc97d40a264467615c3783.jpg width=942 height=935 class=img_ev3q></figure></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=reference>Reference<a href=#reference class=hash-link aria-label="Direct link to Reference" title="Direct link to Reference">​</a></h2>
<p><a href=https://zhouyifan.net/2024/07/27/20240717-ar-wo-vq/ target=_blank rel="noopener noreferrer">https://zhouyifan.net/2024/07/27/20240717-ar-wo-vq/</a></header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-05-19T05:33:17.000Z itemprop=dateModified>May 19, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-1m2bkf9" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-1m2bkf9"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-1m2bkf9" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-1m2bkf9">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/category/image-generation-1><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>Image Generation (1)</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/category/lightweight-10><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Lightweight (10)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#reforging-the-order-of-generation class="table-of-contents__link toc-highlight">Reforging the Order of Generation</a><li><a href=#background-knowledge class="table-of-contents__link toc-highlight">Background Knowledge</a><ul><li><a href=#vector-quantization-vq class="table-of-contents__link toc-highlight">Vector Quantization (VQ)</a><li><a href=#auto-regressive-image-generation class="table-of-contents__link toc-highlight">Auto-regressive Image Generation</a></ul><li><a href=#method class="table-of-contents__link toc-highlight">Method</a><ul><li><a href=#abandoning-vq-embracing-diffusion class="table-of-contents__link toc-highlight">Abandoning VQ, Embracing Diffusion</a><li><a href=#integrating-concepts-from-autoregressive-and-masked-generative-models class="table-of-contents__link toc-highlight">Integrating Concepts from Autoregressive and Masked Generative Models</a></ul><li><a href=#implementation class="table-of-contents__link toc-highlight">Implementation</a><li><a href=#experiments class="table-of-contents__link toc-highlight">Experiments</a><ul><li><a href=#diffusion-loss-vs-cross-entropy-loss class="table-of-contents__link toc-highlight">Diffusion Loss vs. Cross-Entropy Loss</a><li><a href=#tokenizers class="table-of-contents__link toc-highlight">Tokenizers</a><li><a href=#denoising-mlp class="table-of-contents__link toc-highlight">Denoising MLP</a><li><a href=#sampling-steps-of-diffusion-loss class="table-of-contents__link toc-highlight">Sampling Steps of Diffusion Loss</a><li><a href=#temperature-of-diffusion-loss class="table-of-contents__link toc-highlight">Temperature of Diffusion Loss</a><li><a href=#speedaccuracy-trade-off class="table-of-contents__link toc-highlight">Speed/Accuracy Trade-off</a><li><a href=#system-level-comparison class="table-of-contents__link toc-highlight">System-Level Comparison</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a><li><a href=#reference class="table-of-contents__link toc-highlight">Reference</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>