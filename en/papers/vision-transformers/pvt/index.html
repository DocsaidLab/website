<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-vision-transformers/pvt/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[21.02] PVT | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/vision-transformers/pvt/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[21.02] PVT | DOCSAID"><meta data-rh=true name=description content="Spatial Reduction Attention Mechanism"><meta data-rh=true property=og:description content="Spatial Reduction Attention Mechanism"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/vision-transformers/pvt/><link data-rh=true rel=alternate href=https://docsaid.org/papers/vision-transformers/pvt/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/vision-transformers/pvt/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/vision-transformers/pvt/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/vision-transformers/pvt/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/vision-transformers-13","name":"Vision Transformers (13)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/vision-transformers/pvt/","name":"[21.02] PVT","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.c544f161.js defer></script><script src=/en/assets/js/main.c5d293ad.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/vision-transformers/pvt/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/vision-transformers/pvt/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/vision-transformers/pvt/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="Expand sidebar category 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-42>Face Anti-Spoofing (42)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (42)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-13>Object Detection (13)</a><button aria-label="Expand sidebar category 'Object Detection (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/retail-product-1>Retail Product (1)</a><button aria-label="Expand sidebar category 'Retail Product (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/vision-transformers-13>Vision Transformers (13)</a><button aria-label="Collapse sidebar category 'Vision Transformers (13)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/vit/>[20.10] ViT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/deit/>[20.12] DeiT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/cpvt/>[21.02] CPVT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/vision-transformers/pvt/>[21.02] PVT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/swin-transformer/>[21.03] Swin Transformer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/mlp-mixer/>[21.05] MLP-Mixer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/beit/>[21.06] BEiT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/mae/>[21.11] MAE</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/poolformer/>[21.11] PoolFormer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/convmixer/>[22.01] ConvMixer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/beit-v2/>[22.08] BEiT v2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/caformer/>[22.10] CAFormer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/flexivit/>[22.12] FlexiViT</a></ul><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 224 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/vision-transformers-13><span>Vision Transformers (13)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[21.02] PVT</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[21.02] PVT</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=spatial-reduction-attention-mechanism>Spatial Reduction Attention Mechanism<a href=#spatial-reduction-attention-mechanism class=hash-link aria-label="Direct link to Spatial Reduction Attention Mechanism" title="Direct link to Spatial Reduction Attention Mechanism">​</a></h2>
<p><a href=https://arxiv.org/abs/2102.12122 target=_blank rel="noopener noreferrer"><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</strong></a></p>
<hr>
<p>ViT has achieved remarkable results in image classification, officially marking the entry of the Transformer architecture into the realm of computer vision.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=defining-the-problem>Defining the Problem<a href=#defining-the-problem class=hash-link aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem">​</a></h2>
<p>The ViT architecture begins by using 16 x 16 large kernel convolutions for patchifying images. For a 224 x 224 image, this process results in a 14 x 14 image. This resolution is sufficient for image classification tasks that rely on highly abstract global features. However, for dense prediction tasks like image segmentation or object detection, this patchifying method loses local details.</p>
<ul>
<li><strong>Because the details are lost in the 16 x 16 feature compression process.</strong></li>
</ul>
<p>Obviously, we need more refined features for dense prediction tasks. So, what if we change the 16 x 16 convolution to a 2 x 2 convolution?</p>
<ul>
<li><strong>Of course not!</strong></li>
</ul>
<p>Let's take a 224 x 224 image as an example:</p>
<ul>
<li>Using a 16 x 16 patch size, we get 14 x 14, totaling 196 input tokens.</li>
<li>Using a 2 x 2 patch size, we get 112 x 112, totaling 12,544 input tokens.</li>
</ul>
<p><strong>Imagine calculating a 12,544 x 12,544 self-attention matrix...</strong></p>
<p>Clearly, no one would consider this a good idea.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=solving-the-problem>Solving the Problem<a href=#solving-the-problem class=hash-link aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-architecture>Model Architecture<a href=#model-architecture class=hash-link aria-label="Direct link to Model Architecture" title="Direct link to Model Architecture">​</a></h3>
<p><img decoding=async loading=lazy alt="model architecture" src=/en/assets/images/img1-fc2045bdeaa56a12a3e69d5e8d908c08.jpg width=1368 height=676 class=img_ev3q></p>
<p>The above image shows the design of the PVT architecture. While it may look complex, it essentially follows the same principles as convolutional neural network (CNN) architectures.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=hierarchical-structure>Hierarchical Structure<a href=#hierarchical-structure class=hash-link aria-label="Direct link to Hierarchical Structure" title="Direct link to Hierarchical Structure">​</a></h3>
<p>First, we notice the hierarchical structure, similar to the common 1/2 downsampling in ConvNets, split into five stages. Therefore, there are 1/2 size feature maps, 1/4 size feature maps, 1/8 size feature maps, and so on. In the PVT architecture, it starts from a 1/4 size feature map and goes down to a 1/32 size feature map.</p>
<p>The downsampling process is implemented using convolutions with specified strides. For example, an input image of 3 x 224 x 224 with a stride of 4 convolution, producing 64 output channels, will result in a feature map of 64 x 56 x 56.</p>
<p>So, in the first stage, the input to the Transformer encoder is:</p>
<ul>
<li><strong>Sequence Length</strong>: 3136 (all image patches, i.e., 56 x 56)</li>
<li><strong>Feature Number</strong>: 64 (features per patch, i.e., the number of channels)</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=spatial-reduction-attention-sra>Spatial Reduction Attention (SRA)<a href=#spatial-reduction-attention-sra class=hash-link aria-label="Direct link to Spatial Reduction Attention (SRA)" title="Direct link to Spatial Reduction Attention (SRA)">​</a></h3>
<p><img decoding=async loading=lazy alt="spatial reduction attention" src=/en/assets/images/img2-3d0b1b4353ec53868f89a464ff17d3e5.jpg width=882 height=432 class=img_ev3q></p>
<p>After obtaining the feature maps, we realize that a (56 x 56) x (56 x 56) self-attention matrix is still enormous and needs serious handling.</p>
<p>The authors propose the concept of "Spatial Reduction Attention" (SRA), which retains the size of the queries (Q) but reduces the size of the keys (K) and values (V). For example, if the original self-attention map size is (56 x 56) x (56 x 56), reducing the key and value sizes to (7 x 7) will change the attention map size to 3136 x 49.</p>
<p>What seemed like an overly large self-attention matrix is now manageable.</p>
<p>Here's a closer look at the core concept of the paper. The key part is highlighted, and the <code>sr_ratio</code> setting in this paper is <strong>[8, 4, 2, 1]</strong>, indicating the spatial reduction ratio for each stage.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token comment" style=color:#999988;font-style:italic># Reference:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token comment" style=color:#999988;font-style:italic>#   - https://github.com/whai362/PVT/blob/v2/classification/pvt.py</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token keyword" style=color:#00009f>import</span><span class="token plain"> torch</span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token keyword" style=color:#00009f>import</span><span class="token plain"> torch</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">nn </span><span class="token keyword" style=color:#00009f>as</span><span class="token plain"> nn</span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token keyword" style=color:#00009f>class</span><span class="token plain"> </span><span class="token class-name">SpatialReductionAttention</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Module</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token keyword" style=color:#00009f>def</span><span class="token plain"> </span><span class="token function" style=color:#d73a49>__init__</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        num_heads</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>8</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        qkv_bias</span><span class="token operator" style=color:#393A34>=</span><span class="token boolean" style=color:#36acaa>False</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        qk_scale</span><span class="token operator" style=color:#393A34>=</span><span class="token boolean" style=color:#36acaa>None</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        attn_drop</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>0.</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        proj_drop</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>0.</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        sr_ratio</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>1</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style=color:#393A34>(</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">__init__</span><span class="token punctuation" style=color:#393A34>(</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">dim </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> dim</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">num_heads </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> num_heads</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        head_dim </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> dim </span><span class="token operator" style=color:#393A34>//</span><span class="token plain"> num_heads</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">scale </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> qk_scale </span><span class="token keyword" style=color:#00009f>or</span><span class="token plain"> head_dim </span><span class="token operator" style=color:#393A34>**</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>-</span><span class="token number" style=color:#36acaa>0.5</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">q </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Linear</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> bias</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">qkv_bias</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">kv </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Linear</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim </span><span class="token operator" style=color:#393A34>*</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> bias</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">qkv_bias</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">attn_drop </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Dropout</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">attn_drop</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">proj </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Linear</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">proj_drop </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Dropout</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">proj_drop</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">sr_ratio </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> sr_ratio</span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">        </span><span class="token keyword" style=color:#00009f>if</span><span class="token plain"> sr_ratio </span><span class="token operator" style=color:#393A34>></span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">            self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">sr </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Conv2d</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> kernel_size</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">sr_ratio</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> stride</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">sr_ratio</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">            self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">norm </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">LayerNorm</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token keyword" style=color:#00009f>def</span><span class="token plain"> </span><span class="token function" style=color:#d73a49>forward</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">self</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> H</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> W</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> N</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">shape</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        q </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">q</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">reshape</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> N</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">num_heads</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C </span><span class="token operator" style=color:#393A34>//</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">num_heads</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">permute</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>3</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">        </span><span class="token keyword" style=color:#00009f>if</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">sr_ratio </span><span class="token operator" style=color:#393A34>></span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">            x_ </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">permute</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">reshape</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> H</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> W</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">            x_ </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">sr</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x_</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">reshape</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>-</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">permute</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">            x_ </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">norm</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x_</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class="token-line theme-code-block-highlighted-line" style=color:#393A34><span class="token plain">            kv </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">kv</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x_</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">reshape</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>-</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">num_heads</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C </span><span class="token operator" style=color:#393A34>//</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">num_heads</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">permute</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>3</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>4</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token keyword" style=color:#00009f>else</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">            kv </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">kv</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">reshape</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>-</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">num_heads</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C </span><span class="token operator" style=color:#393A34>//</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">num_heads</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">permute</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>3</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>4</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        k</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> v </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> kv</span><span class="token punctuation" style=color:#393A34>[</span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>]</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> kv</span><span class="token punctuation" style=color:#393A34>[</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        attn </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">q @ k</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">transpose</span><span class="token punctuation" style=color:#393A34>(</span><span class="token operator" style=color:#393A34>-</span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>-</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>*</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">scale</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        attn </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> attn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">softmax</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token operator" style=color:#393A34>=</span><span class="token operator" style=color:#393A34>-</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        attn </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">attn_drop</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">attn</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> </span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">attn @ v</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">transpose</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">reshape</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> N</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">proj</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">proj_drop</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token keyword" style=color:#00009f>return</span><span class="token plain"> x</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-configuration>Model Configuration<a href=#model-configuration class=hash-link aria-label="Direct link to Model Configuration" title="Direct link to Model Configuration">​</a></h3>
<p>Finally, let's look at the PVT model configuration.</p>
<ol>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>P_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.13889em>P</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:-0.1389em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>: Patch size at stage <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6595em></span><span class="mord mathnormal">i</span></span></span></span></li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>C</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>C_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:-0.0715em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>: Number of output channels at stage <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6595em></span><span class="mord mathnormal">i</span></span></span></span></li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>L_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">L</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>: Number of encoder layers at stage <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6595em></span><span class="mord mathnormal">i</span></span></span></span></li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>R</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>R_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.00773em>R</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:-0.0077em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>: Reduction ratio of SRA at stage <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6595em></span><span class="mord mathnormal">i</span></span></span></span></li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>N_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>: Number of heads in SRA at stage <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6595em></span><span class="mord mathnormal">i</span></span></span></span></li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>E</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>E_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.05764em>E</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:-0.0576em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>: Expansion ratio in the feed-forward layer at stage <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6595em></span><span class="mord mathnormal">i</span></span></span></span></li>
</ol>
<p>The model design follows the design rules of ResNet:</p>
<ol>
<li>Use smaller output channels in the shallow stages.</li>
<li>Concentrate most computational resources in the middle stages.</li>
</ol>
<p>For discussion purposes, the table below shows a series of PVT models of different scales: PVT-Tiny, PVT-Small, PVT-Medium, and PVT-Large. Their parameter counts are comparable to ResNet18, ResNet50, ResNet101, and ResNet152, respectively.</p>
<p><img decoding=async loading=lazy alt="model configuration" src=/en/assets/images/img3-02585ce188d13dc5a82ad837d2604d7b.jpg width=1626 height=728 class=img_ev3q></p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=imagenet-performance>ImageNet Performance<a href=#imagenet-performance class=hash-link aria-label="Direct link to ImageNet Performance" title="Direct link to ImageNet Performance">​</a></h3>
<p><img decoding=async loading=lazy alt="imagenet performance" src=/en/assets/images/img4-15fa3d4bff76fa6641ca93058364be37.jpg width=1098 height=1196 class=img_ev3q></p>
<p>The authors compare PVT with two of the most representative CNN backbones, ResNet and ResNeXt, which are widely used in many downstream task benchmarks.</p>
<p>In the table above, PVT models outperform traditional CNN backbones with similar parameter counts and computational budgets.</p>
<p>For instance, with a similar GFLOP, PVT-Small achieves a top-1 error rate of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>20.2</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>20.2\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>20.2%</span></span></span></span>, outperforming ResNet50's <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>21.5</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>21.5\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>21.5%</span></span></span></span> by <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>1.3</mn></mrow><annotation encoding=application/x-tex>1.3</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6444em></span><span class=mord>1.3</span></span></span></span> percentage points (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>20.2</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>20.2\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>20.2%</span></span></span></span> vs. <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>21.5</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>21.5\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>21.5%</span></span></span></span>).</p>
<p>With similar or lower complexity, PVT models achieve comparable performance to recently proposed Transformer-based models like ViT and DeiT.</p>
<p>PVT-Large achieves a top-1 error rate of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>18.3</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>18.3\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>18.3%</span></span></span></span>, on par with ViT (DeiT)-Base/16's <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>18.3</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>18.3\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>18.3%</span></span></span></span>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=object-detection-performance>Object Detection Performance<a href=#object-detection-performance class=hash-link aria-label="Direct link to Object Detection Performance" title="Direct link to Object Detection Performance">​</a></h3>
<p><img decoding=async loading=lazy alt="dense prediction performance-retinanet" src=/en/assets/images/img5-ac354d02e9d4777d893ad9cdddcfa0af.jpg width=1428 height=516 class=img_ev3q></p>
<p><img decoding=async loading=lazy alt="dense prediction performance-maskrcnn" src=/en/assets/images/img6-a19bf9f68e70c920f53b1505cfb4fcd0.jpg width=1416 height=476 class=img_ev3q></p>
<p>Since this architecture targets dense prediction tasks from the outset, let's focus on PVT's performance in dense prediction tasks.</p>
<ol>
<li>
<p><strong>Datasets</strong>:</p>
<ul>
<li>Using the COCO benchmark</li>
<li>Training set: COCO train2017 (118k images)</li>
<li>Validation set: COCO val2017 (5k images)</li>
</ul>
</li>
<li>
<p><strong>Models and Initialization</strong>:</p>
<ul>
<li>Standard detectors: RetinaNet and Mask R-CNN</li>
<li>Backbone initialized with ImageNet pretrained weights</li>
<li>New layers initialized with Xavier initialization</li>
</ul>
</li>
<li>
<p><strong>Training Settings</strong>:</p>
<ul>
<li>Batch size: 16</li>
<li>Hardware: 8 V100 GPUs</li>
<li>Optimizer: AdamW</li>
<li>Initial learning rate: <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>1</mn><mo>×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=application/x-tex>1 \times 10^{-4}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7278em;vertical-align:-0.0833em></span><span class=mord>1</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8141em></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
</ol>
<p>With comparable parameter counts, PVT models significantly outperform traditional models. With RetinaNet, PVT-Tiny's AP is 4.9 points higher than ResNet18 (36.7 vs. 31.8).</p>
<p>With Mask R-CNN, PVT-Tiny's mask AP (APm) is 35.1, 3.9 points higher than ResNet18 (35.1 vs. 31.2), and even 0.7 points higher than ResNet50 (35.1 vs. 34.4).</p>
<p>These results suggest that PVT can serve as a good alternative to CNN backbones for object detection and instance segmentation tasks.</p>
<p>The image below shows PVT's results on the COCO validation set.</p>
<p><img decoding=async loading=lazy alt="coco validation results" src=/en/assets/images/img7-77b4feb4dea9efc0417f5e7a32867b85.jpg width=1024 height=784 class=img_ev3q></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=contribution-of-the-pyramid-structure>Contribution of the Pyramid Structure<a href=#contribution-of-the-pyramid-structure class=hash-link aria-label="Direct link to Contribution of the Pyramid Structure" title="Direct link to Contribution of the Pyramid Structure">​</a></h3>
<p>The authors conducted several ablation studies to verify the impact of different parts of PVT on performance.</p>
<p>First, the contribution analysis of the pyramid structure is shown in the table below:</p>
<p><img decoding=async loading=lazy alt="pyramid structure contribution" src=/en/assets/images/img8-01237fed10029bfad5ceee0d9a91776a.jpg width=1080 height=260 class=img_ev3q></p>
<p>Compared to the original ViT structure, PVT's pyramid structure improved the AP score by 8.7 percentage points.</p>
<p>This indicates that the pyramid structure design helps improve performance in dense prediction tasks.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=depth-vs-width-trade-off>Depth vs. Width Trade-off<a href=#depth-vs-width-trade-off class=hash-link aria-label="Direct link to Depth vs. Width Trade-off" title="Direct link to Depth vs. Width Trade-off">​</a></h3>
<p><img decoding=async loading=lazy alt="depth vs width" src=/en/assets/images/img9-5b41322f17249faebaf42c76113c6fe4.jpg width=994 height=196 class=img_ev3q></p>
<p>The authors further explore whether PVT should be deeper or wider, as well as the impact of feature map sizes at different stages on performance.</p>
<p>By multiplying PVT-Small's hidden dimension by 1.4, making it comparable to PVT-Medium in parameter count, the experiments show that deeper models perform better under similar parameter counts.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=addressing-but-not-solving-the-core-issue>Addressing but Not Solving the Core Issue<a href=#addressing-but-not-solving-the-core-issue class=hash-link aria-label="Direct link to Addressing but Not Solving the Core Issue" title="Direct link to Addressing but Not Solving the Core Issue">​</a></h3>
<p><img decoding=async loading=lazy alt=limitations src=/en/assets/images/img10-32e9701f3cb06f1821a22d3907b33e66.jpg width=1218 height=840 class=img_ev3q></p>
<p>Lastly, the authors discussed performance limitations.</p>
<p>As the input size increases, the GFLOPs of PVT grow faster than ResNet but slower than ViT. When the input size does not exceed 640×640 pixels, PVT-Small and ResNet50 have similar GFLOPs.</p>
<p>Additionally, with a fixed input image size of 800 pixels on the shorter side, the inference speed of RetinaNet based on PVT-Small is slower.</p>
<p>In practical scenarios, convolutional network architectures are better suited for large input sizes, a key direction for performance improvement.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>The root cause is that the SRA mechanism merely reduces the size of the self-attention matrix without addressing the computational complexity issue of the self-attention matrix at its core.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Although in 2021, comparing against ResNet and ResNeXt might seem conservative, this study introduces the pyramid structure into Transformers to provide a pure Transformer backbone for dense prediction tasks, rather than focusing on task-specific heads or image classification models, offering an important direction for future research.</p>
<p>At this point, Transformer-based models in computer vision are still in their early stages, and many potential technologies and applications remain to be explored.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-02-11T02:49:16.000Z itemprop=dateModified>Feb 11, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/vision-transformers/cpvt/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[21.02] CPVT</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/vision-transformers/swin-transformer/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[21.03] Swin Transformer</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#spatial-reduction-attention-mechanism class="table-of-contents__link toc-highlight">Spatial Reduction Attention Mechanism</a><li><a href=#defining-the-problem class="table-of-contents__link toc-highlight">Defining the Problem</a><li><a href=#solving-the-problem class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href=#model-architecture class="table-of-contents__link toc-highlight">Model Architecture</a><li><a href=#hierarchical-structure class="table-of-contents__link toc-highlight">Hierarchical Structure</a><li><a href=#spatial-reduction-attention-sra class="table-of-contents__link toc-highlight">Spatial Reduction Attention (SRA)</a><li><a href=#model-configuration class="table-of-contents__link toc-highlight">Model Configuration</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#imagenet-performance class="table-of-contents__link toc-highlight">ImageNet Performance</a><li><a href=#object-detection-performance class="table-of-contents__link toc-highlight">Object Detection Performance</a><li><a href=#contribution-of-the-pyramid-structure class="table-of-contents__link toc-highlight">Contribution of the Pyramid Structure</a><li><a href=#depth-vs-width-trade-off class="table-of-contents__link toc-highlight">Depth vs. Width Trade-off</a><li><a href=#addressing-but-not-solving-the-core-issue class="table-of-contents__link toc-highlight">Addressing but Not Solving the Core Issue</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>