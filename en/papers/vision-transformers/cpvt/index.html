<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-vision-transformers/cpvt/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.0"><title data-rh=true>[21.02] CPVT | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/vision-transformers/cpvt/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[21.02] CPVT | DOCSAID"><meta data-rh=true name=description content="Conditional Positional Encodings"><meta data-rh=true property=og:description content="Conditional Positional Encodings"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/vision-transformers/cpvt/><link data-rh=true rel=alternate href=https://docsaid.org/papers/vision-transformers/cpvt/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/vision-transformers/cpvt/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/vision-transformers/cpvt/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/vision-transformers/cpvt/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/vision-transformers-12","name":"Vision Transformers (12)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/vision-transformers/cpvt/","name":"[21.02] CPVT","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.4363dbfa.js defer></script><script src=/en/assets/js/main.33be072f.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/vision-transformers/cpvt/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/vision-transformers/cpvt/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/vision-transformers/cpvt/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-5uvb3z ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="Expand sidebar category 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-38>Face Anti-Spoofing (38)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (38)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="Expand sidebar category 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="Collapse sidebar category 'Vision Transformers (12)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/vit/>[20.10] ViT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/deit/>[20.12] DeiT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/vision-transformers/cpvt/>[21.02] CPVT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/pvt/>[21.02] PVT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/swin-transformer/>[21.03] Swin Transformer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/mlp-mixer/>[21.05] MLP-Mixer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/beit/>[21.06] BEiT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/mae/>[21.11] MAE</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/poolformer/>[21.11] PoolFormer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/convmixer/>[22.01] ConvMixer</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/beit-v2/>[22.08] BEiT v2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/vision-transformers/caformer/>[22.10] CAFormer</a></ul><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 213 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/vision-transformers-12><span>Vision Transformers (12)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[21.02] CPVT</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[21.02] CPVT</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conditional-positional-encodings>Conditional Positional Encodings<a href=#conditional-positional-encodings class=hash-link aria-label="Direct link to Conditional Positional Encodings" title="Direct link to Conditional Positional Encodings">​</a></h2>
<p><a href=https://arxiv.org/abs/2102.10882 target=_blank rel="noopener noreferrer"><strong>Conditional Positional Encodings for Vision Transformers</strong></a></p>
<hr>
<p>Shortly after the introduction of ViT (Vision Transformer), a wave of research emerged to improve its architecture.</p>
<p>If you've worked with ViT-based models, you might share the same frustration: the "learnable" positional encodings are challenging to use!</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=defining-the-problem>Defining the Problem<a href=#defining-the-problem class=hash-link aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem">​</a></h2>
<p>In ViT, the input image is divided into fixed-size patches, and each patch is combined with a corresponding learnable absolute positional encoding. However, this positional encoding approach comes with two significant issues:</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=sequence-expansion>Sequence Expansion<a href=#sequence-expansion class=hash-link aria-label="Direct link to Sequence Expansion" title="Direct link to Sequence Expansion">​</a></h3>
<p>The length of the absolute positional encoding is fixed, which limits the model’s ability to handle longer sequences than what it was trained on.</p>
<p>You might wonder: <strong>Can we just discard the positional encoding altogether?</strong></p>
<ul>
<li><strong>The answer is no!</strong></li>
</ul>
<p>Removing positional encoding dramatically reduces model performance, as shown in the DeiT (Data-efficient Image Transformer) paper. In the DeiT-tiny model trained on ImageNet, accuracy drops from 72.2% to 68.2% without positional encodings—a significant and catastrophic drop.</p>
<p>This makes sense because the order of the input sequence carries crucial information. Without positional encodings, the model would lose the ability to capture this order.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=translation-invariance>Translation Invariance<a href=#translation-invariance class=hash-link aria-label="Direct link to Translation Invariance" title="Direct link to Translation Invariance">​</a></h3>
<p>Translation invariance means the model can adjust its response when an object moves within the image.</p>
<p>In the ViT architecture, each image patch is added to a unique positional encoding, which causes the model to lose translation invariance. This negatively impacts performance, especially in classification tasks.</p>
<p>Research on DeiT has demonstrated that positional encodings can be extended to longer sequences using interpolation. However, this method requires multiple rounds of fine-tuning. If the model is deployed without fine-tuning, performance drastically declines.</p>
<p><strong>What about using relative positional encodings (RPE) instead?</strong></p>
<ul>
<li><strong>The answer: It addresses both of these issues.</strong></li>
</ul>
<p>But, because it lacks absolute position information, experiments have shown that models with relative positional encodings underperform compared to those with absolute encodings.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p><img decoding=async loading=lazy alt=DeiT-tiny src=/en/assets/images/img2-b656aafaf8ff6da6d5f7bae476d95a36.jpg width=1862 height=324 class=img_ev3q>
In fact, there's approximately a 1.5% accuracy difference between relative positional encodings and absolute positional encodings on the ImageNet dataset.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=solving-the-problem>Solving the Problem<a href=#solving-the-problem class=hash-link aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conditional-positional-encodings-1>Conditional Positional Encodings<a href=#conditional-positional-encodings-1 class=hash-link aria-label="Direct link to Conditional Positional Encodings" title="Direct link to Conditional Positional Encodings">​</a></h3>
<p><img decoding=async loading=lazy alt=CPVT src=/en/assets/images/img1-b788178c01ca25caf062b3dd7e8c5f05.jpg width=1224 height=656 class=img_ev3q></p>
<p>To address these issues, the authors propose a new type of positional encoding: <strong>Conditional Positional Encodings (CPE)</strong>.</p>
<p>As shown in the figure above, the leftmost image represents the original ViT architecture, where the image patches are directly added to a "learnable" absolute positional encoding.</p>
<p>In the center image, the proposed CPE architecture introduces a key modification: <strong>the positional encoding calculation occurs within the Transformer encoder itself</strong>.</p>
<p>Specifically, it’s applied <strong>after the first encoder layer but before the second one</strong>.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>What’s this about?<p>We’ll come back to that later—let’s first look at the structure of the positional encoding itself.</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=positional-encoding-generator>Positional Encoding Generator<a href=#positional-encoding-generator class=hash-link aria-label="Direct link to Positional Encoding Generator" title="Direct link to Positional Encoding Generator">​</a></h3>
<p><img decoding=async loading=lazy alt=PEG src=/en/assets/images/img3-4e5b46105bd1145ca924151d607061e0.jpg width=1378 height=514 class=img_ev3q></p>
<p>The Positional Encoding Generator (PEG) is key to the proposed CPE. Let’s break down its structure:</p>
<ul>
<li><strong>It’s simply a 2D convolution layer!</strong></li>
</ul>
<p>After the image is split into patches, these patches pass through a convolution layer to generate the corresponding positional encodings.</p>
<p>This convolution layer is designed with <strong>zero padding</strong>, which allows the model to infer specific positional information. Through convolution, the model can dynamically generate the appropriate positional encodings based on the input image, which is what gives this method the name <strong>Conditional Positional Encoding</strong>.</p>
<p>The paper provides implementation details, and here’s a simplified version:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#393A34;--prism-background-color:#f6f8fa><div class=codeBlockTitle_OeMC>Algorithm 1 PyTorch snippet of PEG by the authors</div><div class=codeBlockContent_QJqH><pre tabindex=0 class="prism-code language-python codeBlock_bY9V thin-scrollbar" style=color:#393A34;background-color:#f6f8fa><code class=codeBlockLines_e6Vv><span class=token-line style=color:#393A34><span class="token keyword" style=color:#00009f>import</span><span class="token plain"> torch</span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token keyword" style=color:#00009f>import</span><span class="token plain"> torch</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">nn </span><span class="token keyword" style=color:#00009f>as</span><span class="token plain"> nn</span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token keyword" style=color:#00009f>class</span><span class="token plain"> </span><span class="token class-name">VisionTransformer</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token keyword" style=color:#00009f>def</span><span class="token plain"> </span><span class="token function" style=color:#d73a49>__init__</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">self</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> layers</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>12</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>192</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> nhead</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>3</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> img_size</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>224</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> patch_size</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>16</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">pos_block </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> PEG</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">blocks </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">ModuleList</span><span class="token punctuation" style=color:#393A34>(</span><span class="token punctuation" style=color:#393A34>[</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">            TransformerEncoderLayer</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> nhead</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token operator" style=color:#393A34>*</span><span class="token number" style=color:#36acaa>4</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">            </span><span class="token keyword" style=color:#00009f>for</span><span class="token plain"> _ </span><span class="token keyword" style=color:#00009f>in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">layers</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token punctuation" style=color:#393A34>]</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">patch_embed </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> PatchEmbed</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">img_size</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> patch_size</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token operator" style=color:#393A34>*</span><span class="token number" style=color:#36acaa>4</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token keyword" style=color:#00009f>def</span><span class="token plain"> </span><span class="token function" style=color:#d73a49>forward_features</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">self</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> H</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> W </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">shape</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> patch_size </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">patch_embed</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        _H</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> _W </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> H </span><span class="token operator" style=color:#393A34>//</span><span class="token plain"> patch_size</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> W </span><span class="token operator" style=color:#393A34>//</span><span class="token plain"> patch_size</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> torch</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">cat</span><span class="token punctuation" style=color:#393A34>(</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">cls_tokens</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token keyword" style=color:#00009f>for</span><span class="token plain"> i</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> blk </span><span class="token keyword" style=color:#00009f>in</span><span class="token plain"> </span><span class="token builtin">enumerate</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">blocks</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">            x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> blk</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">            </span><span class="token keyword" style=color:#00009f>if</span><span class="token plain"> i </span><span class="token operator" style=color:#393A34>==</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">                x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">pos_block</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">x</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> _H</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> _W</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token keyword" style=color:#00009f>return</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>[</span><span class="token punctuation" style=color:#393A34>:</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain"></span><span class="token keyword" style=color:#00009f>class</span><span class="token plain"> </span><span class="token class-name">PEG</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Module</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token keyword" style=color:#00009f>def</span><span class="token plain"> </span><span class="token function" style=color:#d73a49>__init__</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">self</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>256</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> k</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>3</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token comment" style=color:#999988;font-style:italic># Only for demo use, more complicated functions are effective too.</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">pos </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> nn</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">Conv2d</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> k</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> k</span><span class="token operator" style=color:#393A34>//</span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> groups</span><span class="token operator" style=color:#393A34>=</span><span class="token plain">dim</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#393A34><span class="token plain">    </span><span class="token keyword" style=color:#00009f>def</span><span class="token plain"> </span><span class="token function" style=color:#d73a49>forward</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">self</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> H</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> W</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> N</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">shape</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        cls_token</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> feat_tokens </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>[</span><span class="token punctuation" style=color:#393A34>:</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>0</span><span class="token punctuation" style=color:#393A34>]</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>[</span><span class="token punctuation" style=color:#393A34>:</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>:</span><span class="token punctuation" style=color:#393A34>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        feat_tokens </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> feat_tokens</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">transpose</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">view</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">B</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> C</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> H</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> W</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> self</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">pos</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">feat_tokens</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"> </span><span class="token operator" style=color:#393A34>+</span><span class="token plain"> feat_tokens</span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">flatten</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">transpose</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> </span><span class="token number" style=color:#36acaa>2</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        x </span><span class="token operator" style=color:#393A34>=</span><span class="token plain"> torch</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">cat</span><span class="token punctuation" style=color:#393A34>(</span><span class="token punctuation" style=color:#393A34>(</span><span class="token plain">cls_token</span><span class="token punctuation" style=color:#393A34>.</span><span class="token plain">unsqueeze</span><span class="token punctuation" style=color:#393A34>(</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> x</span><span class="token punctuation" style=color:#393A34>)</span><span class="token punctuation" style=color:#393A34>,</span><span class="token plain"> dim</span><span class="token operator" style=color:#393A34>=</span><span class="token number" style=color:#36acaa>1</span><span class="token punctuation" style=color:#393A34>)</span><span class="token plain"></span><br></span><span class=token-line style=color:#393A34><span class="token plain">        </span><span class="token keyword" style=color:#00009f>return</span><span class="token plain"> x</span><br></span></code></pre></div></div>
<p>As you can see, the implementation of <code>PEG</code> really is just a single Conv2d layer without anything extra.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-training-strategy>Model Training Strategy<a href=#model-training-strategy class=hash-link aria-label="Direct link to Model Training Strategy" title="Direct link to Model Training Strategy">​</a></h3>
<p>To validate this new positional encoding approach, the authors followed the DeiT training strategy rigorously. The experimental setup is as follows:</p>
<ol>
<li>
<p><strong>Dataset</strong></p>
<ul>
<li>The study uses the ImageNet dataset, which includes 1,000 classes and 1.3 million images for training.</li>
<li>Evaluation is conducted on the 50,000-image validation set.</li>
<li>The larger JFT-300M dataset was not used.</li>
</ul>
</li>
<li>
<p><strong>Model Variants</strong></p>
<ul>
<li>Three model variants of different sizes were designed for various computational needs.</li>
<li>All experiments were run on Tesla V100 machines.</li>
<li>Training the tiny model for 300 epochs took approximately 1.3 days (on a single node with 8 V100 GPUs), while CPVT-S and CPVT-B took 1.6 days and 2.5 days, respectively.</li>
</ul>
</li>
<li>
<p><strong>Training Details</strong></p>
<ul>
<li>Except for CPVT-B, the other models were trained for 300 epochs using Tesla V100 machines with a batch size of 2048.</li>
<li>The AdamW optimizer was used for training.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=peg-placement>PEG Placement<a href=#peg-placement class=hash-link aria-label="Direct link to PEG Placement" title="Direct link to PEG Placement">​</a></h3>
<p><img decoding=async loading=lazy alt="PEG pos" src=/en/assets/images/img9-63f202131a0d76b3e7fc4ba6995e0b75.jpg width=1738 height=412 class=img_ev3q></p>
<p>Why is PEG placed after the first encoder layer and before the second?</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><ul>
<li><strong>Position 0</strong>: After the first encoder layer’s output.</li>
<li><strong>Position -1</strong>: Before the first encoder.</li>
</ul></div></div>
<p>The authors experimented with placing PEG in different positions in the model. According to the results (as shown in the table above), placing PEG at position 0 or position 3 led to the best Top-1 accuracy, around 72.4%. Placing PEG at position 0 performed better than at position -1, indicating that position 0 outperforms position -1.</p>
<p>The main difference between these positions is their receptive fields. Position 0 has a global receptive field, while position -1 can only see local regions. By increasing the convolution kernel size, the two positions should yield similar results.</p>
<p>To verify this hypothesis, the authors used a larger convolution kernel (27×27, with padding size 13) at position -1. The results showed similar performance (72.5%) to position 0, confirming this hypothesis.</p>
<p>However, increasing the convolution kernel size also increases computational complexity. Therefore, the authors chose to place PEG at position 0 to achieve better performance without added complexity.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=where-does-pegs-improvement-come-from>Where Does PEG’s Improvement Come From?<a href=#where-does-pegs-improvement-come-from class=hash-link aria-label="Direct link to Where Does PEG’s Improvement Come From?" title="Direct link to Where Does PEG’s Improvement Come From?">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="PEG improve" src=/en/assets/images/img8-c2468d5aa6c05dfe404f8795dc269a0c.jpg width=836 height=352 class=img_ev3q></figure></div>
<p>One might suspect that the improvement from PEG comes from the extra learnable parameters introduced by the convolution layer rather than the local relationships it retains. To test this, the authors applied PEG only to the calculation of Q and K in the attention layer to ensure that only positional information is transmitted.</p>
<p>As shown in the table above, under this condition, DeiT-tiny’s Top-1 accuracy reached 71.3%, significantly better than DeiT-tiny without positional encoding (68.2%) and close to the result when using PEG on Q, K, and V (72.4%). This demonstrates that PEG mainly serves as a positional encoding scheme rather than relying on extra learnable parameters from the convolution layer.</p>
<p>To further rule out that PEG’s performance improvement was due to additional parameters, the authors conducted another experiment: they randomly initialized a 3×3 PEG and fixed its weights. The result was a Top-1 accuracy of 71.3%, 3.1% higher than DeiT-tiny with no positional encoding. Since PEG’s weights were fixed, this performance improvement could only be attributed to the positional information provided by PEG.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>This means that even a "non-trainable" convolution kernel can enhance model performance.</div></div>
<p>Conversely, when using 12 convolution layers (with kernel size 1, i.e., no local relationships) to replace PEG, the number of learnable parameters far exceeded that of PEG, but performance only improved by 0.4%, reaching 68.6%. This further confirms that PEG’s effectiveness comes mainly from positional information, not the extra parameters.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=generalization-to-higher-resolutions>Generalization to Higher Resolutions<a href=#generalization-to-higher-resolutions class=hash-link aria-label="Direct link to Generalization to Higher Resolutions" title="Direct link to Generalization to Higher Resolutions">​</a></h3>
<p><img decoding=async loading=lazy alt="PEG res" src=/en/assets/images/img4-a33d3e891417ba9a132416b0eef85961.jpg width=1464 height=528 class=img_ev3q></p>
<p>As shown in the table above, PEG can directly generalize to larger image sizes without requiring fine-tuning. This was tested by evaluating models trained on 224 × 224 images with larger resolutions (384 × 384, 448 × 448, 512 × 512).</p>
<p>For the DeiT-tiny model, when using 384 × 384 input images, performance dropped from 72.2% to 71.2% with learnable positional encodings. Using sinusoidal encodings dropped it further to 70.8%.</p>
<p>In contrast, the CPVT model equipped with PEG can directly handle larger input images, and CPVT-Tiny (CPVT-Ti) saw performance increase from 73.4% to 74.2% at the 384 × 384 resolution. CPVT-Ti outperformed DeiT-tiny by 3.0%, with this gap continuing to widen as input resolution increases.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=reducing-complexity>Reducing Complexity<a href=#reducing-complexity class=hash-link aria-label="Direct link to Reducing Complexity" title="Direct link to Reducing Complexity">​</a></h3>
<p>In PEG, if depth-wise convolutions are used, the additional number of parameters is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>d</mi><mo>×</mo><mi>l</mi><mo>×</mo><msup><mi>k</mi><mn>2</mn></msup></mrow><annotation encoding=application/x-tex>d \times l \times k^2</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7778em;vertical-align:-0.0833em></span><span class="mord mathnormal">d</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.7778em;vertical-align:-0.0833em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>×</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8141em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03148em>k</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> (where <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>d</mi></mrow><annotation encoding=application/x-tex>d</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">d</span></span></span></span> is the model dimension, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>l</mi></mrow><annotation encoding=application/x-tex>l</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span></span></span></span> is the depth of the convolution, and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>k</mi></mrow><annotation encoding=application/x-tex>k</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal" style=margin-right:0.03148em>k</span></span></span></span> is the kernel size). For separable convolutions, the number of parameters is <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>l</mi><mo stretchy=false>(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><msup><mi>k</mi><mn>2</mn></msup><mi>d</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>l(d^2 + k^2d)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1.0641em;vertical-align:-0.25em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03148em>k</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class=mclose>)</span></span></span></span>.</p>
<p>For example, in CPVT-Tiny, when <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=application/x-tex>k = 3</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal" style=margin-right:0.03148em>k</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>3</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=application/x-tex>l = 1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>1</span></span></span></span> (i.e., the model dimension <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>d</mi><mo>=</mo><mn>192</mn></mrow><annotation encoding=application/x-tex>d = 192</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">d</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>192</span></span></span></span>), PEG introduces 1,728 parameters. In contrast, the DeiT-tiny model uses learnable positional encodings with 192 × 14 × 14 = 37,632 parameters.</p>
<p>Thus, CPVT-Tiny uses 35,904 fewer parameters than DeiT-tiny. Even using four layers of separable convolutions, the number of additional parameters in CPVT-Tiny is only 960 (38,952 − 37,632), which is almost negligible compared to DeiT-tiny’s total 5.7M parameters.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=performance-on-imagenet>Performance on ImageNet<a href=#performance-on-imagenet class=hash-link aria-label="Direct link to Performance on ImageNet" title="Direct link to Performance on ImageNet">​</a></h3>
<p><img decoding=async loading=lazy alt=ImageNet src=/en/assets/images/img7-5d5227263fc39991f9c9d0e5e47312a2.jpg width=1180 height=1080 class=img_ev3q></p>
<p>Compared to DeiT, CPVT models achieve higher Top-1 accuracy with similar inference speeds. CPVT models can improve performance with increasing input image resolution without requiring fine-tuning, while DeiT's performance drops as the resolution increases.</p>
<p>CPVT models equipped with global average pooling (GAP) have set a new performance record for ViT-based models, achieving state-of-the-art results.</p>
<p>When comparing the CPVT-Tiny and DeiT-tiny models after additional hard distillation training, using the RegNetY-160 model as a teacher, the CPVT model achieved a Top-1 accuracy of 75.9%, surpassing DeiT-tiny by 1.4%.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=comparison-with-other-positional-encodings>Comparison with Other Positional Encodings<a href=#comparison-with-other-positional-encodings class=hash-link aria-label="Direct link to Comparison with Other Positional Encodings" title="Direct link to Comparison with Other Positional Encodings">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="pe compare" src=/en/assets/images/img11-43d5b188b45c0e564923c2f83362c7c7.jpg width=1030 height=480 class=img_ev3q></figure></div>
<p>Finally, the authors compared PEG with other common positional encoding methods and analyzed the combination of PEG with absolute positional encoding (APE) and relative positional encoding (RPE).</p>
<ol>
<li>
<p><strong>Comparison of PEG and Other Positional Encodings</strong></p>
<ul>
<li>The DeiT-tiny model using learnable absolute positional encoding (LE) achieved a Top-1 accuracy of 72.2%.</li>
<li>Using 2D sinusoidal encoding resulted in similar performance.</li>
<li>When using relative positional encoding (RPE, with hyperparameter K set to 8), the Top-1 accuracy dropped to 70.5%, lower than LE. One reason for this is that RPE does not encode absolute position information, leading to reduced performance.</li>
</ul>
</li>
<li>
<p><strong>Combining PEG with Other Encodings</strong></p>
<p>Combining learnable absolute positional encoding with a single-layer PEG improved the baseline CPVT-Ti model's performance by 0.5%. When using four layers of PEG, the model achieved a Top-1 accuracy of 72.9%. Adding PEG to the first five Transformer blocks further improved accuracy to 73.4%, outperforming the effect of stacking PEG in a single block.</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>The authors demonstrate that PEG significantly improves model performance in handling long input sequences and maintaining translation invariance. In classification tasks, PEG outperforms both absolute positional encodings (APE) and relative positional encodings (RPE).</p>
<p>Furthermore, PEG is simple to implement, introduces minimal additional computational costs, and requires only a small number of extra parameters, making it an effective and practical solution.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>In research, we often encounter strong opinions about which architectures to use—whether you should exclusively use Transformer-based models without CNNs or CNNs without Transformers.<p>But from a practical perspective, a simple and effective method that boosts performance is exactly what we need.<p>Let's give it a try sometime and share the results later!</div></div></header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-02-11T02:49:16.000Z itemprop=dateModified>Feb 11, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-5uvb3z" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-5uvb3z"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-5uvb3z" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-5uvb3z">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/vision-transformers/deit/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[20.12] DeiT</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/vision-transformers/pvt/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[21.02] PVT</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#conditional-positional-encodings class="table-of-contents__link toc-highlight">Conditional Positional Encodings</a><li><a href=#defining-the-problem class="table-of-contents__link toc-highlight">Defining the Problem</a><ul><li><a href=#sequence-expansion class="table-of-contents__link toc-highlight">Sequence Expansion</a><li><a href=#translation-invariance class="table-of-contents__link toc-highlight">Translation Invariance</a></ul><li><a href=#solving-the-problem class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href=#conditional-positional-encodings-1 class="table-of-contents__link toc-highlight">Conditional Positional Encodings</a><li><a href=#positional-encoding-generator class="table-of-contents__link toc-highlight">Positional Encoding Generator</a><li><a href=#model-training-strategy class="table-of-contents__link toc-highlight">Model Training Strategy</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#peg-placement class="table-of-contents__link toc-highlight">PEG Placement</a><li><a href=#where-does-pegs-improvement-come-from class="table-of-contents__link toc-highlight">Where Does PEG’s Improvement Come From?</a><li><a href=#generalization-to-higher-resolutions class="table-of-contents__link toc-highlight">Generalization to Higher Resolutions</a><li><a href=#reducing-complexity class="table-of-contents__link toc-highlight">Reducing Complexity</a><li><a href=#performance-on-imagenet class="table-of-contents__link toc-highlight">Performance on ImageNet</a><li><a href=#comparison-with-other-positional-encodings class="table-of-contents__link toc-highlight">Comparison with Other Positional Encodings</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>