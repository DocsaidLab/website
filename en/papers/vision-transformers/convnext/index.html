<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-vision-transformers/convnext/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[22.01] ConvNeXt | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/vision-transformers/convnext/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[22.01] ConvNeXt | DOCSAID"><meta data-rh="true" name="description" content="Making Convolutions Great Again"><meta data-rh="true" property="og:description" content="Making Convolutions Great Again"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/vision-transformers/convnext/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/vision-transformers/convnext/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/vision-transformers/convnext/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/vision-transformers/convnext/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.64d77125.css">
<script src="/en/assets/js/runtime~main.c59fdd39.js" defer="defer"></script>
<script src="/en/assets/js/main.8684979a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/vision-transformers/convnext/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/vision-transformers/convnext/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns-9">Classic CNNs (9)</a><button aria-label="Expand sidebar category &#x27;Classic CNNs (9)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="Expand sidebar category &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="Expand sidebar category &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="Expand sidebar category &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/multimodality-18">Multimodality (18)</a><button aria-label="Expand sidebar category &#x27;Multimodality (18)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization-1">Normalization (1)</a><button aria-label="Expand sidebar category &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection-7">Object Detection (7)</a><button aria-label="Expand sidebar category &#x27;Object Detection (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="Expand sidebar category &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="Expand sidebar category &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="Expand sidebar category &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/transformers-13">Transformers (13)</a><button aria-label="Expand sidebar category &#x27;Transformers (13)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="Collapse sidebar category &#x27;Vision Transformers (11)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/vit/">[20.10] ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/deit/">[20.12] DeiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/pvt/">[21.02] PVT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/swin-transformer/">[21.03] Swin Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/mlp-mixer/">[21.05] MLP-Mixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/beit/">[21.06] BEiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/mae/">[21.11] MAE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/poolformer/">[21.11] PoolFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/convmixer/">[22.01] ConvMixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/vision-transformers/convnext/">[22.01] ConvNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/vision-transformers/caformer/">[22.10] CAFormer</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/vision-transformers-11"><span itemprop="name">Vision Transformers (11)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[22.01] ConvNeXt</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[22.01] ConvNeXt</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="making-convolutions-great-again">Making Convolutions Great Again<a href="#making-convolutions-great-again" class="hash-link" aria-label="Direct link to Making Convolutions Great Again" title="Direct link to Making Convolutions Great Again">​</a></h2>
<p><a href="https://arxiv.org/abs/2201.03545" target="_blank" rel="noopener noreferrer"><strong>A ConvNet for the 2020s</strong></a></p>
<hr>
<p>Looking back at the 2010s, deep learning made significant strides, profoundly impacting multiple fields.</p>
<p>At the heart of this progress was the revival of neural networks, particularly Convolutional Neural Networks (ConvNet).</p>
<p>Over the past decade, the field of visual recognition has successfully transitioned from handcrafted features to the design of ConvNet architectures.</p>
<p>Although the concept of ConvNet dates back to the 1980s, it wasn&#x27;t until 2012 that we truly witnessed its potential in visual feature learning, marked by the emergence of AlexNet, which heralded the &quot;ImageNet moment&quot; in computer vision.</p>
<p>Since then, the field has advanced rapidly with landmark works such as:</p>
<ul>
<li><a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener noreferrer"><strong>[14.09] VGG</strong></a></li>
<li><a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener noreferrer"><strong>[14.09] GoogLeNet</strong></a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener noreferrer"><strong>[15.12] ResNet</strong></a></li>
<li><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener noreferrer"><strong>[16.08] DenseNet</strong></a></li>
<li><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener noreferrer"><strong>[17.04] MobileNet</strong></a></li>
<li><a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener noreferrer"><strong>[19.05] EfficientNet</strong></a></li>
<li><a href="https://arxiv.org/abs/2003.13678" target="_blank" rel="noopener noreferrer"><strong>[20.03] RegNet</strong></a></li>
</ul>
<p>These seminal works emphasized efficiency and scalability, popularizing many practical design principles.</p>
<p>However, everything changed with the advent of ViT.</p>
<ul>
<li><a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer"><strong>[20.10] ViT</strong></a></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="defining-the-problem">Defining the Problem<a href="#defining-the-problem" class="hash-link" aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem">​</a></h2>
<p>The introduction of Vision Transformers (ViT) fundamentally disrupted network architecture design. Besides the initial &quot;slicing&quot; layer that splits the image into multiple patches, ViT did not introduce any image-specific inductive bias, making minimal modifications to the original NLP Transformer.</p>
<p>One significant advantage of ViT is its scalability: as the model and dataset sizes grow, its performance significantly surpasses standard ResNet.</p>
<p>To close the performance gap, hierarchical Transformers adopted hybrid strategies. For example, the &quot;sliding window&quot; strategy reintroduced local attention mechanisms, making Transformers behave more like ConvNet.</p>
<p>Swin Transformer is a milestone in this regard, proving that Transformers can serve as a general-purpose vision backbone, achieving state-of-the-art performance across various computer vision tasks beyond image classification.</p>
<p>The success and rapid adoption of Swin Transformer also indicate that the essence of convolutions remains irreplaceable.</p>
<ul>
<li><a href="https://arxiv.org/abs/2103.14030" target="_blank" rel="noopener noreferrer"><strong>[21.03] Swin Transformer</strong></a></li>
</ul>
<p>From these advancements, it becomes apparent that many of the improvements in Vision Transformers are essentially restoring the strengths of convolutions.</p>
<p>However, these attempts come at a cost: implementing sliding window self-attention can be expensive, and advanced methods like cyclic shifting can enhance efficiency but make system design more complex.</p>
<p>Ironically, ConvNet already possesses many of the required attributes, albeit in a simple, straightforward manner.</p>
<p>The only reason ConvNet lost momentum is that Transformers outperformed them in many vision tasks.</p>
<p>The authors argue: it shouldn&#x27;t be this way.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-problem">Solving the Problem<a href="#solving-the-problem" class="hash-link" aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="redesigning-network-architecture">Redesigning Network Architecture<a href="#redesigning-network-architecture" class="hash-link" aria-label="Direct link to Redesigning Network Architecture" title="Direct link to Redesigning Network Architecture">​</a></h3>
<p><img decoding="async" loading="lazy" alt="roadmap" src="/en/assets/images/img1-b31f804197da634ebeee43c8ace55419.jpg" width="3199" height="2475" class="img_ev3q"></p>
<p>For simplicity, the authors started with ResNet-50 and used the similarly sized Swin Transformer as a reference for comparison.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introducing-modern-training-techniques">Introducing Modern Training Techniques<a href="#introducing-modern-training-techniques" class="hash-link" aria-label="Direct link to Introducing Modern Training Techniques" title="Direct link to Introducing Modern Training Techniques">​</a></h3>
<p>ViT not only brought new network architectures but also introduced many modern training techniques. Therefore, without changing anything else, the authors first applied ViT&#x27;s training techniques to ConvNet.</p>
<ul>
<li>
<p>Increased training time from 90 epochs to 300 epochs.</p>
</li>
<li>
<p>Used the AdamW optimizer.</p>
</li>
<li>
<p>Employed image augmentation techniques:</p>
<ul>
<li><a href="https://arxiv.org/abs/1710.09412" target="_blank" rel="noopener noreferrer"><strong>Mixup</strong></a></li>
<li><a href="https://arxiv.org/abs/1905.04899" target="_blank" rel="noopener noreferrer"><strong>CutMix</strong></a></li>
<li><a href="https://arxiv.org/abs/1909.13719" target="_blank" rel="noopener noreferrer"><strong>RandAugment</strong></a></li>
<li><a href="https://arxiv.org/abs/1708.04896" target="_blank" rel="noopener noreferrer"><strong>Random Erasing</strong></a></li>
</ul>
</li>
<li>
<p>Applied regularization methods:</p>
<ul>
<li><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener noreferrer"><strong>Stochastic Depth</strong></a></li>
<li><a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener noreferrer"><strong>Label Smoothing</strong></a></li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>With the above training techniques, the performance of ResNet-50 improved by 2.7%, from 76.1% to 78.8%.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="macro-design">Macro Design<a href="#macro-design" class="hash-link" aria-label="Direct link to Macro Design" title="Direct link to Macro Design">​</a></h3>
<p>This part considers two design factors:</p>
<ol>
<li>
<p><strong>Stage Compute Ratio</strong></p>
<p>In ResNet, the cross-stage compute distribution design is mainly empirical.</p>
<p>For instance, ResNet&#x27;s stage-4 is designed to be compatible with downstream tasks (like object detection) where detector heads operate on a 14×14 feature plane.</p>
<p>Similarly, Swin-T follows the same principle but with a slightly different stage compute ratio of 1:1:3:1. For larger Swin Transformers, the ratio is 1:1:9:1.</p>
<p>To maintain similar FLOPs to Swin-T, the authors adjusted ResNet-50&#x27;s block numbers per stage from (3, 4, 6, 3) to (3, 3, 9, 3).</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Research related to stage compute ratio:</p><ul>
<li><a href="https://arxiv.org/abs/1905.13214" target="_blank" rel="noopener noreferrer"><strong>[19.05] On network design spaces for visual recognition</strong></a></li>
<li><a href="https://arxiv.org/abs/2003.13678" target="_blank" rel="noopener noreferrer"><strong>[20.03] Designing network design spaces</strong></a></li>
</ul></div></div>
<p>From this point onward, the model will use this stage compute ratio.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This adjustment increased the model&#x27;s accuracy from 78.8% to 79.4%.</p></div></div>
</li>
<li>
<p><strong>Stem Structure Design</strong></p>
<p>In ConvNet, the stem refers to the initial layer of input.</p>
<p>Typically, it is used for downsampling to quickly reduce the spatial size of the input.</p>
<p>In ViT, the stem is a patchify layer that splits the image into a series of patches using a large kernel convolution of 16×16.</p>
<p>In Swin-T, the stem is a 4x4 large convolution with a stride of 4.</p>
<p>Here, the authors adopted the same design as Swin-T and adjusted the ResNet-50&#x27;s stem to a 4×4 non-overlapping convolution.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This design increased the model&#x27;s accuracy from 79.4% to 79.5%.</p></div></div>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="resnext-ify">ResNeXt-ify<a href="#resnext-ify" class="hash-link" aria-label="Direct link to ResNeXt-ify" title="Direct link to ResNeXt-ify">​</a></h3>
<p>In this section, the authors adopted the ResNeXt design philosophy, which offers a better FLOPs/accuracy trade-off compared to a standard ResNet.</p>
<ul>
<li><a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="noopener noreferrer"><strong>Aggregated Residual Transformations for Deep Neural Networks</strong></a></li>
</ul>
<p>The core design of ResNeXt is grouped convolution, where convolutional filters are divided into different groups.</p>
<p>The guiding principle of ResNeXt is &quot;use more groups, expand width,&quot; applying grouped convolution in the 3×3 convolution layer.</p>
<p>In this paper, the authors used depthwise convolution, a special form of grouped convolution where the number of groups equals the number of channels.</p>
<p>Depthwise convolution is akin to the weighted sum operation in self-attention, operating on each channel individually, mixing information only in the spatial dimension.</p>
<p>Using depthwise convolution effectively reduces the network&#x27;s FLOPs but is expected to lower accuracy.</p>
<p>Following ResNeXt&#x27;s strategy, the authors increased the network width to match the same channel numbers as Swin-T (from 64 to 96).</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This improved the network performance to 80.5% while increasing FLOPs (5.3G).</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="inverted-bottleneck">Inverted Bottleneck<a href="#inverted-bottleneck" class="hash-link" aria-label="Direct link to Inverted Bottleneck" title="Direct link to Inverted Bottleneck">​</a></h3>
<p><img decoding="async" loading="lazy" alt="inverted-bottleneck" src="/en/assets/images/img2-a194d27930094ae1e36ef1476ca55abc.jpg" width="1224" height="432" class="img_ev3q"></p>
<p>Each block in Transformers has an inverted bottleneck structure, typically with an expansion ratio of 4.</p>
<p>Later, MobileNetV2 also promoted this concept, with the difference being the addition of a 3x3 depthwise convolution after expansion.</p>
<p>In the figure above, (a) is the basic structure of ResNeXt, while (b) is the basic structure of MobileNetV2. (c) is another option, moving the 3x3 depthwise convolution to the front, preparing for the next chapter on exploring large kernel convolutions.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Using the design in (b), the model&#x27;s accuracy increased to 80.6%.</p><p>In larger systems like ResNet-200 / Swin-B, more gains were achieved, increasing from 81.9% to 82.6%.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="large-kernel-convolution">Large Kernel Convolution<a href="#large-kernel-convolution" class="hash-link" aria-label="Direct link to Large Kernel Convolution" title="Direct link to Large Kernel Convolution">​</a></h3>
<p>One important characteristic of ViT is its non-local self-attention, granting the network global receptive fields in each layer.</p>
<p>This contrasts with traditional ConvNet, which uses smaller kernels (like 3×3, popularized by VGGNet).</p>
<p>To explore the application of large kernels, the article proposed moving the depthwise convolution layer upward, akin to placing the MHSA block before the MLP layer in Transformers.</p>
<p>Such structural adjustments can effectively reduce FLOPs and optimize performance.</p>
<p>After these adjustments, the study tested different kernel sizes (from 3×3 to 11×11):</p>
<ul>
<li>3x3: 79.9%</li>
<li>7x7: 80.6%</li>
</ul>
<p>The authors found that increasing the kernel size to 7×7 improved network performance from 79.9% to 80.6%, with FLOPs remaining unchanged.</p>
<p>Further increasing kernel size (beyond 7×7) showed no additional performance gains in the ResNet-200 model, indicating performance saturation at the 7×7 size.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>These steps did not enhance performance, but changing from (b) to (c) mimicked the style of Transformer self-attention.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="micro-design">Micro Design<a href="#micro-design" class="hash-link" aria-label="Direct link to Micro Design" title="Direct link to Micro Design">​</a></h3>
<ol>
<li>
<p><strong>Replacing ReLU with GELU</strong></p>
<p>Rectified Linear Unit (ReLU) has been the preferred activation function in ConvNet due to its simplicity and computational efficiency.</p>
<p>Gaussian Error Linear Unit (GELU), a smoother variant of ReLU, has gradually gained favor in state-of-the-art Transformer models, such as Google&#x27;s BERT and OpenAI&#x27;s GPT-2.</p>
<p>The authors replaced ReLU with GELU in ConvNet, and the model&#x27;s accuracy remained unchanged.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Although performance did not improve, the authors still preferred GELU.</p></div></div>
</li>
<li>
<p><strong>Reducing Activation Functions</strong></p>
<p><img decoding="async" loading="lazy" alt="activation" src="/en/assets/images/img3-85ad8b25b47e67d8c67d3ebcd89ba0b7.jpg" width="1096" height="1080" class="img_ev3q"></p>
<p>In Transformer architectures, particularly in the MLP block, typically only one activation function is used.</p>
<p>In contrast, the practice in ResNet modules is to use activation functions after each convolution layer, even after small 1×1 convolution layers.</p>
<p>The authors removed all GELU activation layers in the residual network block except for one between the two 1×1 convolution layers, mimicking the Transformer block&#x27;s style.</p>
<p>This adjustment improved performance, increasing accuracy by 0.7%, reaching 81.3%, comparable to Swin-T&#x27;s performance.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This design increased the model&#x27;s accuracy from 80.6% to 81.3%.</p></div></div>
</li>
<li>
<p><strong>Reducing Normalization Layers</strong></p>
<p>Transformers generally have fewer normalization layers, whereas in ConvNet, Batch Normalization appears after each convolution layer.</p>
<p>Therefore, the authors removed two additional Batch Normalization layers, retaining them only before the 1x1 convolutions.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This design increased the model&#x27;s accuracy from 81.3% to 81.4%.</p></div></div>
</li>
<li>
<p><strong>Replacing BatchNorm with LayerNorm</strong></p>
<p>Transformers use simpler Layer Normalization, achieving good performance across different application scenarios.</p>
<p>Directly replacing BN with LN in the original ResNet results in suboptimal performance.</p>
<ul>
<li><a href="https://arxiv.org/abs/2105.07576" target="_blank" rel="noopener noreferrer"><strong>[21.05] Rethinking &quot;batch&quot; in batchnorm</strong></a></li>
</ul>
<p>With all the modifications in network architecture and training techniques, the authors revisited the impact of replacing BN with LN, finding no difficulty in training the ConvNet model using LN.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This design increased the model&#x27;s accuracy from 81.4% to 81.5%.</p></div></div>
</li>
<li>
<p><strong>Downsampling Layer</strong></p>
<p>In the ResNet architecture, spatial downsampling is performed at the beginning of each stage using 3×3 convolutions with a stride of 2 and 1×1 convolutions with a stride of 2 in the shortcut connections.</p>
<p>Compared to ResNet, Swin Transformers add separate downsampling layers between stages.</p>
<p>Further investigation showed that adding normalization layers where spatial resolution changes can help stabilize training.</p>
<p>This includes multiple Layer Normalization (LN) layers used in Swin Transformers: one before each downsampling layer, one after the stem, and one after the final global average pooling.</p>
<p>Based on these results, the authors decided to use separate downsampling layers in their final model, naming it ConvNeXt.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This design increased the model&#x27;s accuracy from 81.5% to 82%.</p></div></div>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary-of-model-configurations">Summary of Model Configurations<a href="#summary-of-model-configurations" class="hash-link" aria-label="Direct link to Summary of Model Configurations" title="Direct link to Summary of Model Configurations">​</a></h3>
<p>These models represent a &quot;modernized&quot; upgrade to the ResNet architecture, with different variants primarily differing in the number of channels (C) and the number of blocks per stage (B).</p>
<p>The specific configurations for each variant are as follows:</p>
<ol>
<li>
<p><strong>ConvNeXt-T</strong> (Tiny):</p>
<ul>
<li>Channels C = (96, 192, 384, 768)</li>
<li>Blocks per stage B = (3, 3, 9, 3)</li>
</ul>
</li>
<li>
<p><strong>ConvNeXt-S</strong> (Small):</p>
<ul>
<li>Channels C = (96, 192, 384, 768)</li>
<li>Blocks per stage B = (3, 3, 27, 3)</li>
</ul>
</li>
<li>
<p><strong>ConvNeXt-B</strong> (Base):</p>
<ul>
<li>Channels C = (128, 256, 512, 1024)</li>
<li>Blocks per stage B = (3, 3, 27, 3)</li>
</ul>
</li>
<li>
<p><strong>ConvNeXt-L</strong> (Large):</p>
<ul>
<li>Channels C = (192, 384, 768, 1536)</li>
<li>Blocks per stage B = (3, 3, 27, 3)</li>
</ul>
</li>
<li>
<p><strong>ConvNeXt-XL</strong> (Extra Large):</p>
<ul>
<li>Channels C = (256, 512, 1024, 2048)</li>
<li>Blocks per stage B = (3, 3, 27, 3)</li>
</ul>
</li>
</ol>
<p>The number of channels doubles with each new stage, following a design philosophy similar to ResNet and Swin Transformers.</p>
<p>This hierarchical design allows different model sizes to provide flexible and efficient performance across varying dataset sizes or complexities.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="performance-on-imagenet">Performance on ImageNet<a href="#performance-on-imagenet" class="hash-link" aria-label="Direct link to Performance on ImageNet" title="Direct link to Performance on ImageNet">​</a></h3>
<p><img decoding="async" loading="lazy" alt="imagenet" src="/en/assets/images/img4-189f56cd65d8f2dd9b959ae84cc4c667.jpg" width="2575" height="2475" class="img_ev3q"></p>
<ol>
<li>
<p><strong>ImageNet-1K Performance Comparison</strong>:</p>
<ul>
<li><strong>ConvNeXt vs. ConvNet Benchmarks</strong>: ConvNeXt competes with strong ConvNet benchmarks (like RegNet and EfficientNet) in terms of accuracy and inference throughput.</li>
<li><strong>Comparison with Swin Transformers</strong>: ConvNeXt generally outperforms Swin Transformers under similar complexity conditions, e.g., ConvNeXt-T outperforms Swin-T by 0.8%. ConvNeXt also demonstrates higher throughput without relying on specialized modules like sliding windows or relative positional bias.</li>
<li><strong>Specific Model Performance</strong>: For instance, ConvNeXt-B shows a significant advantage in FLOPs/throughput when resolution increases (from 224² to 384²) compared to Swin-B, with a 0.6% accuracy improvement (85.1% vs. 84.5%) and 12.5% higher inference throughput.</li>
</ul>
</li>
<li>
<p><strong>ImageNet-22K Performance Comparison</strong>:</p>
<ul>
<li><strong>Impact of Large-Scale Pre-training</strong>: While it&#x27;s generally believed that Vision Transformers might perform better with large-scale pre-training, the results show that a well-designed ConvNet (like ConvNeXt) can match or even surpass similarly sized Swin Transformers after large-scale pre-training, with slightly higher throughput.</li>
<li><strong>Performance of ConvNeXt-XL</strong>: When further scaled up to ConvNeXt-XL, accuracy reaches 87.8%, demonstrating the scalability of the ConvNeXt architecture.</li>
</ul>
</li>
<li>
<p><strong>Advantages of the ConvNeXt Architecture</strong>:</p>
<ul>
<li>On ImageNet-1K, EfficientNetV2-L achieved the best performance under progressive training programs.</li>
<li>With ImageNet-22K pre-training, ConvNeXt can outperform EfficientNetV2, further proving the importance of large-scale training.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In the 2020s, with the rise of hierarchical Vision Transformers like Swin Transformers, traditional ConvNet seemed to be viewed as less superior.</p>
<p>However, the ConvNeXt model presented in this paper demonstrates that maintaining structural simplicity and efficiency can still compete with state-of-the-art Vision Transformers across various computer vision benchmarks.</p>
<p>This battle is just beginning!</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">Sep 11, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/vision-transformers/convmixer/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[22.01] ConvMixer</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/vision-transformers/caformer/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[22.10] CAFormer</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#making-convolutions-great-again" class="table-of-contents__link toc-highlight">Making Convolutions Great Again</a></li><li><a href="#defining-the-problem" class="table-of-contents__link toc-highlight">Defining the Problem</a></li><li><a href="#solving-the-problem" class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href="#redesigning-network-architecture" class="table-of-contents__link toc-highlight">Redesigning Network Architecture</a></li><li><a href="#introducing-modern-training-techniques" class="table-of-contents__link toc-highlight">Introducing Modern Training Techniques</a></li><li><a href="#macro-design" class="table-of-contents__link toc-highlight">Macro Design</a></li><li><a href="#resnext-ify" class="table-of-contents__link toc-highlight">ResNeXt-ify</a></li><li><a href="#inverted-bottleneck" class="table-of-contents__link toc-highlight">Inverted Bottleneck</a></li><li><a href="#large-kernel-convolution" class="table-of-contents__link toc-highlight">Large Kernel Convolution</a></li><li><a href="#micro-design" class="table-of-contents__link toc-highlight">Micro Design</a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#summary-of-model-configurations" class="table-of-contents__link toc-highlight">Summary of Model Configurations</a></li><li><a href="#performance-on-imagenet" class="table-of-contents__link toc-highlight">Performance on ImageNet</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>