<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multi-model/2020/ernievil" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.3.2">
<title data-rh="true">ERNIE-ViL | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/multi-model/2020/ernievil"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="ERNIE-ViL | DOCSAID"><meta data-rh="true" name="description" content="知識的雙面刃"><meta data-rh="true" property="og:description" content="知識的雙面刃"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/multi-model/2020/ernievil"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multi-model/2020/ernievil" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/multi-model/2020/ernievil" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multi-model/2020/ernievil" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel="stylesheet" href="/en/assets/css/styles.c06b1feb.css">
<script src="/en/assets/js/runtime~main.90f406c0.js" defer="defer"></script>
<script src="/en/assets/js/main.a68da885.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/multi-model/2020/ernievil" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/multi-model/2020/ernievil" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/multimodel">MultiModel</a><button aria-label="Collapse sidebar category &#x27;MultiModel&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/en/papers/category/2019">2019</a><button aria-label="Expand sidebar category &#x27;2019&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/papers/category/2020">2020</a><button aria-label="Collapse sidebar category &#x27;2020&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2020/pixelbert">Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2020/villa">VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/multi-model/2020/ernievil">ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2020/unimo">UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2020/oscar">Oscar</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/en/papers/category/2021">2021</a><button aria-label="Expand sidebar category &#x27;2021&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/featurefusion">FeatureFusion</a><button aria-label="Expand sidebar category &#x27;FeatureFusion&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/objectdetection">ObjectDetection</a><button aria-label="Expand sidebar category &#x27;ObjectDetection&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/llm">LLM</a><button aria-label="Expand sidebar category &#x27;LLM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/multimodel"><span itemprop="name">MultiModel</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/2020"><span itemprop="name">2020</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">ERNIE-ViL</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>ERNIE-ViL</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="知識的雙面刃">知識的雙面刃<a href="#知識的雙面刃" class="hash-link" aria-label="Direct link to 知識的雙面刃" title="Direct link to 知識的雙面刃">​</a></h2>
<p><strong><a href="https://arxiv.org/abs/2006.16934" target="_blank" rel="noopener noreferrer">ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph (2020.06)</a></strong></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>以下內容由 ChatGPT-4 彙整，並經過人工校對編輯與補充說明。</p></div></div>
<hr>
<p>在 BERT 之後的許多故事，你大概也都略有耳聞，能多少說出幾個差異，例如：</p>
<p>ERNIE 模型在預訓練時融合了豐富的先驗知識，像是實體知識圖，從而獲得更好的語義理解。另外，ERNIE 採用了片段級的 masking。除了常見的 Masked Language Model (MLM) 任務，ERNIE 還加入了其他的預訓練任務，進一步提高模型的表示能力等。</p>
<p>既然 ERNIE 都優化了 BERT，那研究者們肯定是要把這份優化理念，同理類推到其他領域，也是個意料之內的操作了。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="Direct link to 定義問題" title="Direct link to 定義問題">​</a></h2>
<p>作者著重於視覺語言預訓練模型的問題，特別是當前模型在詳細語義對齊方面的不足。</p>
<p>目前的模型往往無法區分常見詞和描述詳細語義的詞，如對象、屬性對象、物件之間的關係等。這使得這些模型在處理真實場景時，難以有效地表示和捕捉到細微語義。</p>
<p>作者定義並試圖解決的幾個要點：</p>
<ol>
<li>
<p><strong>當前視覺語言預訓練模型的不足</strong></p>
<p>現有模型常基於隨機屏蔽和預測子詞的方法，而沒有有效地區分常見詞和描述詳細語義的詞，如對象、屬性對象、物件之間的關係等。</p>
</li>
<li>
<p><strong>詳細語義對齊的重要性</strong></p>
<p>當前的方法往往忽略了跨視覺和語言建構詳細語義對齊的重要性。這意味著模型在處理真實場景時，可能無法充分捕捉和表示細微的語義差異。</p>
</li>
<li>
<p><strong>視覺語言預訓練的資料來源問題</strong></p>
<p>與文字預訓練模型不同，視覺語言模型需要高品質對齊的圖像文字資料，這些資料通常難以獲得。</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="Direct link to 解決問題" title="Direct link to 解決問題">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ernie-vil-模型設計">ERNIE-ViL 模型設計<a href="#ernie-vil-模型設計" class="hash-link" aria-label="Direct link to ERNIE-ViL 模型設計" title="Direct link to ERNIE-ViL 模型設計">​</a></h3>
<p><img decoding="async" loading="lazy" alt="ERNIE-ViL 模型架構" src="/en/assets/images/ernie_vil_1-7aa05503f954d682d9f2c55c1fb92360.jpg" width="1680" height="776" class="img_ev3q"></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="雙流跨模態網路">雙流跨模態網路<a href="#雙流跨模態網路" class="hash-link" aria-label="Direct link to 雙流跨模態網路" title="Direct link to 雙流跨模態網路">​</a></h3>
<p>雙流跨模態 Transformer 結構確保了這兩種模態的資訊能夠有效地組合在一起，提供一個全面的、統一的視覺語言表示。</p>
<ol>
<li>
<p><strong>雙流結構</strong></p>
<p>「雙流」意味著有兩個獨立的數據流或途徑。在 ERNIE-ViL 的情境中，這兩個流分別為文本（或語言）和圖像。這兩種模態都有其自己的 Transformer 結構，這意味著模型可以分別專注於每種模態的特定特徵，而不是混合它們在一起。</p>
</li>
<li>
<p><strong>處理文本和圖像資料</strong></p>
<p>因為視覺和語言資料的性質有很大的差異（例如：文本是序列化的，而圖像是二維的），所以分別處理它們使得模型可以專注於每種模態的獨特性質，然後使用專門設計的方法來解析和學習這些資料。</p>
</li>
<li>
<p><strong>跨模態 Transformer 區塊</strong></p>
<p>這些區塊的目的是促進視覺和語言數據之間的互動和對齊。一旦文本和圖像都被它們各自的 Transformer 結構處理，跨模態的區塊就會開始工作，將這兩種模態的資訊融合在一起。這是通過注意力機制和其他特定的策略來完成的，目的是找到文本和圖像之間的相關性和語境。</p>
</li>
</ol>
<p>整個雙流結構的主要目的。當你有一個圖像和一個相關的句子或描述，理想的情況是你的模型能夠理解它們之間的關聯性。例如：如果文本說「紅色的球」，模型應該能夠識別圖像中的紅色球體。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="場景圖預測任務">場景圖預測任務<a href="#場景圖預測任務" class="hash-link" aria-label="Direct link to 場景圖預測任務" title="Direct link to 場景圖預測任務">​</a></h3>
<p>場景圖（Scene Graph）本身不是一個可訓練的模型，而是一種資料結構或表示法。它是用來描述圖像中物件的存在、物件之間的關係以及物件的特定屬性的。</p>
<p>場景圖是一種視覺表示法，描述了圖像中物件的存在、物件之間的關係以及物件的特定屬性。舉例來說，對於一張有「紅蘋果在桌子上」的圖片，場景圖將會包含：「蘋果」這個物件、「桌子」這個物件、蘋果的屬性「紅色」、以及蘋果和桌子之間的關係「在…上」。</p>
<p>三大預測任務：</p>
<ol>
<li><strong>物件預測</strong>：這部分的目的是預測或識別文本中提到的特定物件。例如：在句子「紅蘋果在桌子上」中，它應該能夠識別「蘋果」和「桌子」為主要的物件。</li>
<li><strong>屬性預測</strong>：這部分著重於物件的特定特徵或描述。在上述例子中，物件「蘋果」的屬性是「紅色」。這部分的目的是識別和預測這些屬性。</li>
<li><strong>關係預測</strong>：這部分的目標是識別物件之間的關係。在我們的例子中，蘋果和桌子之間的關係是「在…上」。</li>
</ol>
<p>藉由這三個預測任務，ERNIE-ViL 被訓練去深入地對視覺（圖像）和語言（文本描述）之間的語義進行建模。這意味著模型不僅僅識別圖像中的物件和其描述，還能夠理解物件之間的關係以及它們的特定屬性，從而提供更豐富、更深入的視覺語言表示。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="語義對齊">語義對齊<a href="#語義對齊" class="hash-link" aria-label="Direct link to 語義對齊" title="Direct link to 語義對齊">​</a></h3>
<p>ERNIE-ViL 的核心目標之一是確保視覺（即圖像）和語言（即文本）之間存在深入的語義對齊。語義對齊可以理解為模型對圖像中的物件和它在句子中的語義描述之間的深入理解。</p>
<p>例如：當圖像中有一只貓在汽車上，模型不僅要識別出圖像中的「貓」和「汽車」，還要理解「在…之上」這一關係。這樣的語義理解確保模型能夠在描述和圖像之間建立正確的連接。</p>
<p>物件、屬性和關係的預測任務在此過程中起到了關鍵作用。這些任務要求模型不僅要識別圖像中的物件和其屬性，還要理 解物件之間的關係，這有助於建立更強大的跨模態理解。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="編碼方法">編碼方法<a href="#編碼方法" class="hash-link" aria-label="Direct link to 編碼方法" title="Direct link to 編碼方法">​</a></h3>
<p>編碼是機器學習中的一種技術，目的是將高維的輸入數據轉化為低維的形式，這有助於模型更好地理解和處理數據。ERNIE-ViL 使用了一些先進的編碼技術來處理其輸入的文本和圖像數據：</p>
<ol>
<li>
<p><strong>句子編碼</strong></p>
<ul>
<li>使用 WordPiece 方法進行句子分解。這是一種將詞語分解成更小的子單位或片段的方法。例如：「playing」可以分解為「play」和「ing」。</li>
<li>生成的每個子詞的編碼是基於多種信息源的組合，包括：原始的詞編碼（基於詞的語義）、分段編碼（區分不同的句子或段落）以及序列位置編碼（確定詞在句子中的位置）。</li>
</ul>
</li>
<li>
<p><strong>圖像編碼</strong></p>
<ul>
<li>使用預訓練的物件偵測器來識別圖像中的主要物件和特征。例如：它可以識別圖像中的「貓」或「汽車」。</li>
<li>對於每個識別出的物件或區域，模型還會編碼其在圖像中的位置資訊。這有助於模型了解物件之間的相對位置和關係，例如：「貓」在「汽車」的上方。</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="預訓練任務">預訓練任務<a href="#預訓練任務" class="hash-link" aria-label="Direct link to 預訓練任務" title="Direct link to 預訓練任務">​</a></h3>
<p>在深度學習的領域中，預訓練任務是一種常見的策略，旨在利用大量未標記的數據提前訓練模型，使其在後續的特定任務上具有更好的泛化能力。ERNIE-ViL針對視覺語言模型提出了一系列預訓練任務，包括物件預測、屬性預測、關係預測以及掩碼語言建模，以下我們進一步探討每一個預訓練任務的特點及其意義：</p>
<ol>
<li>
<p><strong>物件預測</strong></p>
<p>物件作為視覺場景的核心元素，對於理解該場景至關重要。想像一張只有蘋果和桌子的圖片，若蘋果被遮罩，整張圖片的主要元素將被隱藏，這會對模型的理解造成困難。在物件預測的預訓練任務中，部分物件會被這樣遮罩，要求模型基於其他可見的視覺和文本信息進行預測。這迫使模型學習在圖像和文本之間建立連接，並提高對場景的整體認知。</p>
</li>
<li>
<p><strong>屬性預測</strong></p>
<p>物件除了基本的概念外，還有許多相關的屬性，如顏色、大小和形狀。例如：「紅色的蘋果」中的「紅色」就是蘋果的一個屬性。屬性預測任務要求模型預測被遮罩的物件的這些屬性，從而使模型能夠更精確地描述和理解圖像中的物件，而不只是基本的分類。</p>
</li>
<li>
<p><strong>關係預測</strong></p>
<p>物件之間的關係提供了更多的場景信息。例如：在「蘋果在桌子上」中，「在…之上」描述了蘋果和桌子之間的相對位置關係。在關係預測任務中，模型將學習如何從圖像中識別這些關係，並將其正確地映射到語言描述中，從而提供對視覺場景的深入理解。</p>
</li>
<li>
<p><strong>掩碼語言建模 (MLM)</strong></p>
<p>MLM是一種文本預訓練任務。例如：從「蘋果是紅色的」這句話中，遮住「紅色」，然後要求模型填補這個遺失的部分。ERNIE-ViL 通過這種策略學習文本的句法和語義信息，使其在後續的任務中具有更強大的語言處理能力。</p>
</li>
<li>
<p><strong>掩碼區域預測和圖像文字匹配</strong></p>
<p>這些任務專注於圖像部分，就是我們之前提到過的 ITM (Image-Text Matching) 和 MRM (Masked Region Modeling)。例如：一張圖片中可能包含一個「紅色的蘋果」和「木製的桌子」。模型可能會遮  住蘋果的部分，然後試圖根據桌子和相關的文字描述來預測遮住的部分。這種預訓練任務不僅增強了模型對單一模態的理解，還加強了跨模態間的聯系，幫助模型在真實場景中更好地整合視覺和語言信息。</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="Direct link to 討論" title="Direct link to 討論">​</a></h2>
<p>ERNIE-ViL 的實驗結果在多種視覺語言任務上展現了其卓越的性能，特別是當它和其他先進的跨模態預訓練模型進行比較時。以下幾點為作者對此模型的主要觀察和討論：</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="域外訓練資料的優勢">域外訓練資料的優勢<a href="#域外訓練資料的優勢" class="hash-link" aria-label="Direct link to 域外訓練資料的優勢" title="Direct link to 域外訓練資料的優勢">​</a></h3>
<p><img decoding="async" loading="lazy" alt="ERNIE-ViL 在不同資料集上的性能" src="/en/assets/images/ernie_vil_2-366b8fa885506457759c6a0514053983.jpg" width="1024" height="656" class="img_ev3q"></p>
<p>根據表 1 的數據，ERNIE-ViL 模型在多個視覺語言任務上展現出卓越的性能，特別是當它在域外的大型資料集 CC 和 SBU 上進行預訓練時。</p>
<p>以下是一些主要的亮點：</p>
<ol>
<li>比較其他在相同域外資料集上預訓練的方法，ERNIE-ViL 在 5 個主要方面上均取得了最佳性能。</li>
<li>在視覺推理部分，ERNIE-ViL-large 相比 VLBERT-large 在 VCR 任務上有了6.60% 的提升，而在 VQA 任務上提高了 1.74%。</li>
<li>對於視覺基礎任務，ERNIE-ViL-large 在 RefCOCO+ 的 testA 和 testB 部分相比 VLBERT-large 分別提高了2.40%。</li>
<li>在跨模態檢索的部分，ERNIE-ViLbase 在影像檢索和文字檢索任務中分別取得了2.94% 和 0.50% 的提升，超越了 Unicoder-VL-base。</li>
<li>當使用域外和域內的所有資料集進行預訓練時，ERNIE-ViL-large 依然在多個任務上超越了其他先進模型，如 UNITER、OSCAR 和 VILLA。</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="場景圖預測的重要性">場景圖預測的重要性<a href="#場景圖預測的重要性" class="hash-link" aria-label="Direct link to 場景圖預測的重要性" title="Direct link to 場景圖預測的重要性">​</a></h3>
<p><img decoding="async" loading="lazy" alt="ERNIE-ViL 在不同任務上的性能" src="/en/assets/images/ernie_vil_3-2005f7d64bb742d62b5a52bc76588a78.jpg" width="1024" height="151" class="img_ev3q"></p>
<p>場景圖預測（Scene Graph Prediction, SGP）任務在模型的表現中起到了至關重要的作用。從實驗中可以觀察到：</p>
<ol>
<li><strong>場景圖預測的引入</strong>
<ul>
<li>ERNIE-ViL 在引入 SGP 任務後，整體表現得到了顯著的提升。這個結果突顯了 SGP 在預訓練階段的價值。</li>
</ul>
</li>
<li><strong>對特定任務的改進</strong>
<ul>
<li>在基礎引用表達式上，尤其是需要深入的語義對齊的部分，SGP 使模型在 RefCOCO+ 上的準確率提高了 0.69%。</li>
<li>對於圖像檢索任務，模型在 Flickr30K 數據集上的 R@1 提高了 2.22%。</li>
</ul>
</li>
<li><strong>ERNIE 2.0的影響</strong>
<ul>
<li>從 ERNIE 2.0 開始的文字初始化進一步提高了模型的表現。特別是在視覺推理任務，如 VCR，其效果尤為明顯。這可能是因為 ERNIE 2.0 在預訓練過程中學到了更多的常識知識。</li>
</ul>
</li>
</ol>
<p>ERNIE-ViL 在加入 SGP 任務後，其性能有顯著的提升，尤其是在需要詳細語義對齊的任務上，如基礎引用表達和跨模態檢索。這突顯了場景圖的建模對於理解圖像和文本之間的關聯的重要性。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="完形填空測驗">完形填空測驗<a href="#完形填空測驗" class="hash-link" aria-label="Direct link to 完形填空測驗" title="Direct link to 完形填空測驗">​</a></h3>
<p><img decoding="async" loading="lazy" alt="ERNIE-ViL 在完形填空測驗上的性能" src="/en/assets/images/ernie_vil_4-8d270533806f2e78d8a1e81e82dcfcfd.jpg" width="1024" height="398" class="img_ev3q"></p>
<p>使用視覺模態為條件的語言完形填空測驗來評估 SGP 任務的效果。此測驗需要模型根據文字和圖像的上下文推斷被隱藏的詳細語意標記。</p>
<ol>
<li><strong>數據集構建</strong>
<ul>
<li>從 Flickr30K 資料集中隨機選取了 15,000 個圖像文字對。分別選擇了 5,000 個物件、屬性和關係標記作為隱藏目標。</li>
</ul>
</li>
<li><strong>評估指標</strong>
<ul>
<li>使用前一準確率（ACC@1）和前五準確率（ACC@5）作為評估標準。</li>
</ul>
</li>
<li><strong>比較結果（根據表3）</strong>
<ul>
<li>使用 SGP預訓練的模型相比不使用 SGP 的模型在 ACC@1 上顯示出了顯著改進：物件改進了 1.20%，關係改進了 3.08%，屬性改進了 1.84%。</li>
<li>兩個模型的文字參數都是基於 BERT 初始化的。</li>
</ul>
</li>
<li><strong>觀察結果</strong>
<ul>
<li>有些情況下，不使用 SGP 任務預訓練的模型無法正確預測，因其沒有學習詳細語義對齊且在預訓練時無法區分常用詞和詳細語義詞。在其他情境下，儘管模型能夠進行預測，但其置信度低於使用 SGP 任務預訓練的模型。</li>
</ul>
</li>
</ol>
<p>作者從完形填空測驗表明，引入 SGP 任務可以使 ERNIE-ViL 模型更有效地學習跨模式的詳細語義對齊。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="Direct link to 結論" title="Direct link to 結論">​</a></h2>
<p>ERNIE 架構在多模態學習的過程中，雖然能有效提取知識，但其計算量和存儲需求都相對較大。這意味著需要更多的計算資源，這在部分設備或情境中可能會受到限制。其次，儘管 ERNIE 有能力  利用事先學習的知識，但它的泛化能力在一些特定的、較為罕見的跨模態場景中可能仍有待加強。</p>
<p>延伸至 ERNIE-ViL，該模型雖然成功地引入了場景圖預測任務以優化跨模態詳細語義對齊，但同時也帶來了新的挑戰。舉例來說，如果場景圖中的物件如「樹」和「人」之間的關係被錯誤地標記為「坐在」而非「站在」，這種誤解可能會導致模型在下游任務上的失效。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>場景圖的準確性和完整性都會直接影響模型的性能！</p></div></div>
<p>這增加了在實際應用中的不確定性。再者，考慮一個照片中呈現的是一個模糊的背影，或是多重重疊的物件，模型可能難以確定那是「人」還是「影子」或其他物體。在這些情況下，模型在遇到複雜或模糊的場景時，可能會面臨解讀困難，這也限制了其在某些特定場合，例如當需要精確物件識別或場景解讀時的適用性。</p>
<p>使用場景圖，其實背後隱含的意涵就是：「人類理解知識的方式」比「模型理解知識的方式」還要好，所以讓模型來學習人類理解世界的方式。</p>
<p>但，這樣好嗎？</p>
<p>我們也建議你也可以試著想想，這樣好在哪裡？不好在哪裡 ？想清楚之後，你就會更清楚地認識到本篇文章中使用的「知識增強」技法會遇到的機遇與困難。</p>
<p>儘管如此，ERNIE-ViL 在跨模態預訓練的領域仍展現出令人矚目的優勢。它不僅優化了詳細語義的對齊，而且還提供了新的方向，譬如：我們能進一步整合從影像中提取的場景圖，以及利用圖神經網路（GNN）來整合更多的結構化知識。這不僅是技術進步的象徵，也為未來的研究者提供了新的研究方向和思考空間。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-05-05T09:57:21.000Z" itemprop="dateModified">May 5, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/multi-model/2020/villa"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">VILLA</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/multi-model/2020/unimo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">UNIMO</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#知識的雙面刃" class="table-of-contents__link toc-highlight">知識的雙面刃</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#ernie-vil-模型設計" class="table-of-contents__link toc-highlight">ERNIE-ViL 模型設計</a></li><li><a href="#雙流跨模態網路" class="table-of-contents__link toc-highlight">雙流跨模態網路</a></li><li><a href="#場景圖預測任務" class="table-of-contents__link toc-highlight">場景圖預測任務</a></li><li><a href="#語義對齊" class="table-of-contents__link toc-highlight">語義對齊</a></li><li><a href="#編碼方法" class="table-of-contents__link toc-highlight">編碼方法</a></li><li><a href="#預訓練任務" class="table-of-contents__link toc-highlight">預訓練任務</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#域外訓練資料的優勢" class="table-of-contents__link toc-highlight">域外訓練資料的優勢</a></li><li><a href="#場景圖預測的重要性" class="table-of-contents__link toc-highlight">場景圖預測的重要性</a></li><li><a href="#完形填空測驗" class="table-of-contents__link toc-highlight">完形填空測驗</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>