<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multi-model/2021/mdetr" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.3.2">
<title data-rh="true">MDETR | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/multi-model/2021/mdetr"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="MDETR | DOCSAID"><meta data-rh="true" name="description" content="連續之藝"><meta data-rh="true" property="og:description" content="連續之藝"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/multi-model/2021/mdetr"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multi-model/2021/mdetr" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/multi-model/2021/mdetr" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multi-model/2021/mdetr" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel="stylesheet" href="/en/assets/css/styles.8d9f1582.css">
<script src="/en/assets/js/runtime~main.4dff2d10.js" defer="defer"></script>
<script src="/en/assets/js/main.22d47fc6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/multi-model/2021/mdetr" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/multi-model/2021/mdetr" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/multimodel">MultiModel</a><button aria-label="Collapse sidebar category &#x27;MultiModel&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/en/papers/category/2019">2019</a><button aria-label="Expand sidebar category &#x27;2019&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/en/papers/category/2020">2020</a><button aria-label="Expand sidebar category &#x27;2020&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/en/papers/category/2021">2021</a><button aria-label="Collapse sidebar category &#x27;2021&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2021/clip">CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2021/vinvl">VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2021/vilt">ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2021/vlt5">VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/multi-model/2021/mdetr">MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2021/albef">ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2021/simvlm">SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multi-model/2021/meter">METER</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/featurefusion">FeatureFusion</a><button aria-label="Expand sidebar category &#x27;FeatureFusion&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/objectdetection">ObjectDetection</a><button aria-label="Expand sidebar category &#x27;ObjectDetection&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/languagemodel">LanguageModel</a><button aria-label="Expand sidebar category &#x27;LanguageModel&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/multimodel"><span itemprop="name">MultiModel</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/2021"><span itemprop="name">2021</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">MDETR</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>MDETR</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="連續之藝">連續之藝<a href="#連續之藝" class="hash-link" aria-label="Direct link to 連續之藝" title="Direct link to 連續之藝">​</a></h2>
<p><strong><a href="https://arxiv.org/abs/2104.12763" target="_blank" rel="noopener noreferrer">MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding (2021.04)</a></strong></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>以下內容由 ChatGPT-4 彙整，並經過人工校對編輯與補充說明。</p></div></div>
<hr>
<p>在近年的電腦視覺領域，物件偵測一直站在前線，扮演著眾多先進多模態理解系統的核心角色。然而，傳統的方法將偵測系統作為一個黑盒子，進行固定概念的影像偵測，這樣的方法明顯受到了其固有局限性。</p>
<p>一個很明顯的問題，就是它不能有效地利用多模態上下文進行協同訓練，下游模型也只能存取已偵測到的對象。而且，這樣的偵測系統往往是凍結不變的，意味著它們缺乏進一步的細化和適應性。更重要的是，這些偵測系統的詞彙受到嚴重的限制，對於以自由格式文字表達的新穎概念組合通常是盲目的。</p>
<p>簡單來說，本論文的作者也想要把 VL 模型裡的物件偵測的架構換掉。</p>
<p>你應該還記得之前有一篇論文是用 ViT 來換，但結果沒有很好。</p>
<ul>
<li>傳送門：<strong><a href="/en/papers/multi-model/2021/vilt">ViLT：你方唱罷我登場</a></strong></li>
</ul>
<p>既然抽換 ViT 感覺不是很好（畢竟 ViT 不是專職用來做物件偵測的），所以這次就用另外一個「專職」做物件偵測模型，DETR，來進行抽換吧！</p>
<ul>
<li>傳送門：<strong><a href="/en/papers/object-detection/detr">DETR：跨領域的奠基者</a></strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="定義問題">定義問題<a href="#定義問題" class="hash-link" aria-label="Direct link to 定義問題" title="Direct link to 定義問題">​</a></h2>
<p>目前大多數先進的多模態理解系統依賴於物件偵測作為其核心部分，但這些設計明顯充滿了各種問題：</p>
<ol>
<li>
<p><strong>協同訓練的局限性</strong></p>
<p>在多模態系統中，協同訓練意味著同時使用來自多種輸入源（例如：影像、文字、聲音等）的資料進行模型訓練。當一個系統的部分無法與其他部分進行這種協同訓練時，它可能無法充分利用所有可用的資訊。</p>
<p>試想如果有一個影像和語音輸入的模型，只獨立地訓練影像偵測器而不考慮語音輸入，那麼當語音輸入提供有關影像中對象的重要資訊時，模型可能無法正確地識別該對象。</p>
</li>
<li>
<p><strong>偵測範圍的限制</strong></p>
<p>偵測系統的主要目的是識別影像中的特定對象。但是，如果這些系統只專注於已知對象，並忽略了影像的其他部分，則可能會遺漏重要的上下文資訊。像是  在一張含有多人和一隻狗的圖片中，偵測器可能只識別人和狗，但忽略了背景中的公園場景，這個場景可能提供了有關為什麼人和狗在那裡的重要資訊。</p>
</li>
<li>
<p><strong>模型固化</strong></p>
<p>一旦模型訓練完成並「凍結」，它就不再更新或學習。這可能會阻礙模型在面對新的情境或資料時適應和優化。如果你的偵測器是在夏天的圖片上訓練的。如果它在冬天的圖片上不能進行微調，它可能會在雪地或穿著厚重外套的人上表現不佳。</p>
</li>
<li>
<p><strong>詞彙局限性</strong></p>
<p>物件偵測系統基於其訓練資料識別特定的類別或屬性。如果遇到未在訓練資料中見過的新對象或概念，它可能無法識別。</p>
</li>
<li>
<p><strong>不是端對端的設計</strong></p>
<p>端對端系統允許從輸入直接到輸出的連續學習和優化，沒有中間步驟。如果偵測器不是端對端的，那麼它和其他任務之間的協同訓練可能會受到限制。就數學上來說，這個系統無法微分呀，不能微分就沒機會優化啦！</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解決問題">解決問題<a href="#解決問題" class="hash-link" aria-label="Direct link to 解決問題" title="Direct link to 解決問題">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mdetr-模型設計">MDETR 模型設計<a href="#mdetr-模型設計" class="hash-link" aria-label="Direct link to MDETR 模型設計" title="Direct link to MDETR 模型設計">​</a></h3>
<p><img decoding="async" loading="lazy" alt="MDETR 模型架構" src="/en/assets/images/mdetr_1-9fe82970ec37a8117f878d76e4271882.jpg" width="1024" height="338" class="img_ev3q"></p>
<p>這個模型很簡單，首先文字的部分，用改良後的 Encoder 模型， RoBERTa。</p>
<p>產出文字的特徵編碼之後，接著用 Concat 的方式，塞進原本的 DETR 架構。</p>
<p>整體結構分為幾個部分：</p>
<ul>
<li><strong>主幹卷積編碼</strong>：圖像首先由卷積主幹編碼並攤平。</li>
<li><strong>空間訊息</strong>：透過將二維位置編碼加到攤平的向量中，模型保存了空間信息。</li>
<li><strong>文字編碼</strong>：使用預先訓練的 Transformer 語言模型對文字進行編碼，以產生與輸入大小相同的隱藏向量序列。</li>
<li><strong>模態相關的線性投影</strong>：對影像和文字特徵應用模態相關的線性投影，將這些特徵投影到一個共享的編碼空間。</li>
<li><strong>交叉編碼器</strong>：串聯影像和文字特徵的序列被饋送到一個聯合 Transformer Encoder，這是模型中的核心部分。</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="訓練方式">訓練方式<a href="#訓練方式" class="hash-link" aria-label="Direct link to 訓練方式" title="Direct link to 訓練方式">​</a></h3>
<ol>
<li>Soft token 預測</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Soft token 預測" src="/en/assets/images/mdetr_2-f522827e2c5a6a2fd3d51965193cbabb.jpg" width="1224" height="380" class="img_ev3q"></p>
<p>Soft token 的想法很有趣，soft token 關注於預測原始文本中對應於每個匹配物件的「範圍」，而不是預測每個檢測物件的分類類別。這是該方法與標準物件檢測的主要區別。</p>
<p>假設描述句子為「一隻黑貓和一隻白狗」，模型在檢測到黑色動物時，會試圖預測其與「黑貓」這部分文本的關聯性。這不是僅僅關於一個獨立的 Token 或類別標籤，而是關於文本中一系列的 Token，這些 Token 合在一起形成一個「範圍」，一起描述一個特定的物件。</p>
<p>這種方法的好處是它可以處理同一文本中對多個物件的多重引用，或者多個物件對應於同一文本的情況。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="對比對齊">對比對齊<a href="#對比對齊" class="hash-link" aria-label="Direct link to 對比對齊" title="Direct link to 對比對齊">​</a></h3>
<p>對比對齊旨在確保視覺對象的編碼表示與其相應的文本 Token 在特徵空間中是接近的。這種對齊比僅基於位置信息的”Soft token prediction”更強大，因為它直接運作在特徵表示上。</p>
<p><img decoding="async" loading="lazy" alt="對比對齊" src="/en/assets/images/mdetr_3-4c1659405e91212323e7fdfafd0ef89b.jpg" width="1024" height="169" class="img_ev3q"></p>
<p>參考論文中所提供的數學式，其中：</p>
<ul>
<li>L：最大的 Token 數量。</li>
<li>N：最大的物件數量。</li>
<li>T+​i：給定物件 oi​ 應對齊的 Token 集合。</li>
<li>Oi+​：與給定 Token ti​ 對齊的物件集合。</li>
<li>τ：是溫度參數，直接設定為 0.07。</li>
</ul>
<p>整個數學式的概念很簡單，意思就是：物件和文字，你們倆要好好對齊，長得愈像愈好。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="所有損失">所有損失<a href="#所有損失" class="hash-link" aria-label="Direct link to 所有損失" title="Direct link to 所有損失">​</a></h3>
<p>MDETR 的訓練涉及多個損失函數，除了上面提到的對比損失之外，還包括原本 DETR 論文中提到的那些損失，像是二分匹配的框預測損失、L1 loss、GIoU 等，都要一起算進來。</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="資料集">資料集<a href="#資料集" class="hash-link" aria-label="Direct link to 資料集" title="Direct link to 資料集">​</a></h3>
<ul>
<li><strong>CLEVR</strong>：用於評估方法的結果。</li>
<li><strong>Flickr30k</strong>：被用於建立組合資料集的影像。</li>
<li><strong>MS COCO</strong>：被用於建立組合資料集的影像。</li>
<li><strong>Visual Genome (VG)</strong>：被用於建立組合資料集的影像。</li>
<li>註釋資料來自於引用表達式資料集、VG區域、Flickr實體和GQA訓練平衡集。</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="技術細節">技術細節<a href="#技術細節" class="hash-link" aria-label="Direct link to 技術細節" title="Direct link to 技術細節">​</a></h3>
<ul>
<li>
<p><strong>Pre-training Modulated Detection</strong></p>
<p>預訓練階段，目標是檢測對齊的自由形式文字中引用的所有物件。</p>
</li>
<li>
<p><strong>資料組合技巧與其重要性</strong></p>
<ul>
<li>對於每一張圖片，從所提到的資料集中取得其所有相關的文本註釋。若不同的註釋引用了相同的圖片，則這些註釋會被合併。為了保證訓練集和驗證/測試集的獨立性，所有出現在驗證或測試集中的圖像都從訓練集中移除。</li>
<li>這種演算法用於組合句子，確保組合的短語或文字區塊之間的重疊不太大（GIoU ≤ 0.5）。GIoU是一種用於評估兩個矩形區域重疊程度的指標。組合後的句子總長度被限制在250個字元以下。透過這種方法，形成了一個擁有130萬對齊的圖像-文字對的大資料集。</li>
<li>使用這種資料組合技巧有兩個主要原因：<!-- -->
<ul>
<li>資料效率：通過將更多的信息打包到單一的訓練樣本中，可以更有效地利用資料。</li>
<li>更好的學習信號：<!-- -->
<ul>
<li>當模型在學習時，需要識別和解決文本中多次出現的相同物件類別之間的歧義。</li>
<li>在只有一個句子的情境中，「Soft Token 預測」的任務變得相對簡單，因為模型通常可以輕易地預測該句子的主題或核心意義，而無需太多依賴圖像。</li>
<li>通過組合多個句子，模型被迫更加深入地探索圖像和文字之間的關聯，從而提高其預測能力。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>模型架構選用</strong></p>
<ul>
<li>文字編碼器使用的是預先訓練的 RoBERTa-base，具有 12 個 Transformer 編碼器層。</li>
<li>視覺主幹有兩種選擇：<!-- -->
<ul>
<li><strong>ResNet-101</strong>：這是從 Torchvision 獲得的，並在 ImageNet 上預先訓練的。</li>
<li><strong>EfficientNet 系列</strong>：使用了 EfficientNetB3 和 EfficientNetB5。其中EfficientNetB3 在 ImageNet上達到了 84.1% 的 top 1 準確率，而 EfficientNetB5 達到了86.1%的準確率。</li>
</ul>
</li>
<li>另外也使用了一種基於大量未標記資料的訓練模型，這是使用 Noisy-Student 的偽標記技術。</li>
<li>訓練的細節：使用 32 個 V100 GPU，進行了 40 個 epoch 的預訓練，有效批量大小為 64，訓練時間大約需要一周。</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="討論">討論<a href="#討論" class="hash-link" aria-label="Direct link to 討論" title="Direct link to 討論">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="下游任務表現分析">下游任務表現分析<a href="#下游任務表現分析" class="hash-link" aria-label="Direct link to 下游任務表現分析" title="Direct link to 下游任務表現分析">​</a></h3>
<ol>
<li>
<p><strong>Phrase grounding</strong></p>
<p><img decoding="async" loading="lazy" alt="Phrase grounding" src="/en/assets/images/mdetr_4-c8426ce0c50ebdd7a392e1127821d2fb.jpg" width="972" height="872" class="img_ev3q"></p>
<p>作者使用了 Flickr30k 數據集和特定的訓練/驗證/測試分割。在進行評估時，他們採用了兩種不同的評估協議，這兩種評估協議都旨在解決當一個短語參考圖片中的多個物件時的問題，但它們採取了不同的方法，並具有各自的優點和缺點。</p>
<ol>
<li>
<p><strong>ANY-BOX-Protocol（任意協議）</strong></p>
<p>在這個協議下，當給定的短語參考圖片中的多個不同的物件時，預測的邊界框被認為是正確的，只要它與任何一個真實的邊界框的交集超過聯合（IoU）大於預設閾值，通常是0.5。這意味著只要模型能夠正確識別圖片中的任何一個參考物件，該預測就被認為是正確的。但此方法的問題在於，  它無法評估模型是否找到了所有被參考的實例。</p>
</li>
<li>
<p><strong>MERGED-BOXES-Protocol（合併框協議）</strong></p>
<p>在這個協議下，如果一個短語參考圖片中的多個物件，那麼所有與該短語相關的真實邊界框首先會被合併成一個包含它們所有的最小邊界框。然後，如常規方法，使用這個合併後的邊界框作為真實的邊界框來計算 IoU。這意味著模型的預測需要與這個合併後的邊界框匹配，而不是與單獨的真實邊界框匹配。此方法的問題是它可能會失去對每個單獨實例的細緻了解，特別是當這些實例在圖片中相距很遠時，合併的框可能會不合理地太過巨大。</p>
</li>
<li>
<p><strong>結果比較</strong></p>
<ul>
<li>在 ANY-BOX 設定下，與目前最先進的技術相比，MDETR 在驗證集上的 Recall@1 測量中取得了 8.5 點的提升，且不需要使用任何額外的預訓練數據。</li>
<li>通過進行預訓練，使用相同的主幹網路，MDETR 在測試集上的最佳模型效能進一步提高了 12.1 點。</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>Referring expression comprehension</strong></p>
<p><img decoding="async" loading="lazy" alt="Referring expression comprehension" src="/en/assets/images/mdetr_5-363d71e92179f27e926b68e38e1cde4d.jpg" width="1024" height="271" class="img_ev3q"></p>
<p>大部分之前的研究和方法針對這項任務都是對一組預先提取的、與圖像相關的邊界框進行排序。這些邊界框是透過使用預先訓練好的對象檢測器得到的。</p>
<p>相對於先前的方法，本文提出了更具挑戰性的目標：在給定引用表達式和相應的圖像下，直接訓練模型去預測邊界框，而不是簡單地排序預先提取的框。</p>
<p>本文的模型在預訓練期間已被訓練以識別文本中所引用的每一個對象。例如：對於標題「穿著藍色連衣裙的女人站在玫瑰叢旁邊。」，模型會被訓練去預測所有引用的  對象（如女人、藍色連衣裙和玫瑰叢）的框。然而，當涉及到引用表達式時，任務的目的僅僅是返回一個邊界框，代表整個表達式所引用的物件。為了適應這種變化，模型在這三個特定的資料集上進行了微調。</p>
<p>上表中展示了結果，指出這種方法在所有的資料集上都比最新的方法有了顯著的進步。</p>
</li>
<li>
<p><strong>Visual Question Answering</strong></p>
<p><img decoding="async" loading="lazy" alt="Visual Question Answering" src="/en/assets/images/mdetr_6-fb30a864ffbf83db984cd2dfdc55dc52.jpg" width="1024" height="368" class="img_ev3q"></p>
<p>這個模型架構也是可以應用到 VQA 的任務，但需要經過一些設計。</p>
<ul>
<li><strong>模型設計</strong>
<ul>
<li>查詢類型：除了用於檢測的 100 個查詢之外，還引入了針對問題類型的查詢和用於預測問題類型的查詢。在 GQA 中，這些問題類型被定義為 REL、OBJ、GLOBAL、CAT 和 ATTR。</li>
<li>訓練：進行了 40 個 epoch 的預訓練，然後在不平衡的 GQA 分割上微調 125 個 epoch，最後在平衡分割上進行了 10 個 epoch 的微調。</li>
<li>損失策略: 在前 125 個 epoch 中，同時訓練檢測損失和 QA，但對 QA 損失給予更大的權重。</li>
</ul>
</li>
</ul>
<p>模型利用物件查詢作為輸入到解碼器的學習編碼。這些編碼被用於物件檢測。在推理時，模型的特定部分會預測問題的類型並從該部分獲得答案。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>在這篇論文中，不是使用之前常見的 VQA v2，而是用 GQA。</p><p>GQA 和 VQA v2 是兩個被廣泛使用於視覺問答（Visual Question Answering，簡稱 VQA）的資料集。儘管它們都著重於給定一張圖片並回答相關的問題，但這兩者有幾個關鍵的差異：</p><ol>
<li><strong>資料規模和來源</strong>
<ul>
<li>
<p>GQA：GQA 資料集基於 Visual Genome 數據集，它包含大約 2200 萬個問題-答案對。</p>
</li>
<li>
<p>VQA v2：VQA v2 資料集是在原始 VQA 資料集上的改進，它包含大約 120 萬個問題-答案對，並基於 MS COCO 和 Abstract Scenes 數據集。</p>
</li>
</ul>
</li>
<li><strong>問題和答案的性質</strong>
<ul>
<li>
<p>GQA：著重於複雜和組合的問題，通常涉及多個物件和其之間的關係。答案通常更具描述性，可以是多字的回答。</p>
</li>
<li>
<p>VQA v2：質量更加多樣，問題可以從非常簡單（例如：「這是什麼顏色？」）到較為複雜的問題。答案通常是一個或兩個詞。</p>
</li>
</ul>
</li>
<li><strong>資料不平衡性</strong>
<ul>
<li>
<p>GQA：資料集的設計目的之一是解決 VQA 中的一些不平衡性問題，這些問題可能導致模型在沒有真正理解圖片內容的情況下猜測答案。</p>
</li>
<li>
<p>VQA v2：相較於其前身 VQA v1，VQA v2 特意加入了具有挑戰性的對照圖片，以解決原始資料集中的資料偏見問題。</p>
</li>
</ul>
</li>
<li><strong>場景圖</strong>
<ul>
<li>
<p>GQA：GQA 包含了一個豐富的場景圖，詳細描述了圖片中物件的類型、屬性和它們之間的關係。</p>
</li>
<li>
<p>VQA v2：VQA v2 並沒有內建的場景圖，但研究者可以結合其他資料來源或技術來提供這些資訊。</p>
</li>
</ul>
</li>
<li><strong>任務的目的</strong>
<ul>
<li>
<p>GQA：除了基本的視覺問答任務外，GQA 還著重於多模態推理，促使模型更加深入地理解圖片內容和問題的上下文。</p>
</li>
<li>
<p>VQA v2：主要著重於基本的視覺問答任務，旨在改進模型的性能並解決資料偏見問題。</p>
</li>
</ul>
</li>
</ol><p>簡單來說，GQA 傾向於提供更複雜的問題和答案，並且具有更深入的物件和關係描述，而 VQA v2 則更加多樣化，著重於解決資料偏見問題。</p></div></div>
</li>
<li>
<p><strong>效能比較</strong></p>
<p><img decoding="async" loading="lazy" alt="效能比較" src="/en/assets/images/mdetr_7-e8febec9925a06c95bcae4030fb3b9df.jpg" width="860" height="576" class="img_ev3q"></p>
<ul>
<li>使用具有 Resnet-101 主幹的模型，其表現優於 LXMERT 和 VL-T5。</li>
<li>該模型的表現甚至超過了使用更多預訓練資料的 OSCAR。</li>
<li>使用 EfficientNet-B5 主幹的 MDETR 模型可以達到更高的效能，詳見上表。</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="few-shot-可以做得到嗎">Few-shot 可以做得到嗎？<a href="#few-shot-可以做得到嗎" class="hash-link" aria-label="Direct link to Few-shot 可以做得到嗎？" title="Direct link to Few-shot 可以做得到嗎？">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Few-shot 可以做得到嗎？" src="/en/assets/images/mdetr_8-b119933acbc0bffe8251f1e7133e3b5c.jpg" width="812" height="400" class="img_ev3q"></p>
<p>作者們受到 CLIP 在 Zero-shot 影像分類的成功啟發，進一步探索了如何基於預訓練的 MDETR模型在少量標記資料上進行物件偵測。不同於 CLIP，MDETR 的預訓練資料集不保證所有目標類別的平衡性。這意味著在其資料集中，沒有框是與文字對齊的，導致模型對於給定的文字，會始終預測一個框。</p>
<p>由於這種設計，MDETR 無法在真正的零樣本傳輸設定中被評估。因此，作者選擇  了一個替代的策略，即在少樣本設定中進行評估。此次的實驗選擇了 LVIS 資料集，它包含了 1.2k 的類別，其中多數類別的訓練樣本非常少，呈現長尾分佈。</p>
<p>為了適應這種分佈，MDETR 的訓練策略是：對於每一個正類別，它採用圖像和該類別的文字名稱作為訓練實例，同時使用該類別的所有註解。而對於負類別，只提供類別名稱和空註解。在做推論時，MDETR會對每一個可能的類別名稱進行查詢，然後合併所有文字提示上偵測到的框。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>舉個例子：</p><p>假設我們有一個簡單的資料集，只有三個類別：「狗」、「貓」和「魚」。</p><p>我們手上的標記資料如下：</p><p>一張圖片顯示一隻狗，標籤是「狗」。
另一張圖片顯示一隻貓和一條魚，標籤分別是「貓」和「魚」。
基於MDETR的訓練策略：</p><p>對於第一張圖片：</p><p>因為圖片裡有「狗」，所以將該圖片和文字「狗」作為訓練實例，並使用「狗」這個標籤。
由於圖片中沒有「貓」和「魚」，我們會提供「貓」和「魚」這兩個類別名稱，但是不會提供標籤（即空標籤）。
對於第二張圖片：</p><p>因為圖片裡有「貓」和「魚」，所以將該  圖片、文字「貓」和「魚」作為訓練實例，並使用對應的標籤「貓」和「魚」。
由於圖片中沒有「狗」，我們會提供「狗」這個類別名稱，但不會提供標籤（即空標籤）。
當 MDETR 進行推論時，對於一個新圖片，它會分別查詢三個類別名稱「狗」、「貓」和「魚」，並合併在每個文字提示上偵測到的結果。例如：如果它在查詢「狗」時偵測到一個框，在查詢「貓」時沒有偵測到框，而在查詢「魚」時偵測到一個框，則最終的結果將包含一個「狗」的框和一個「魚」的框。</p></div></div>
<p>作者在 LVIS 資料集的三個子集（包括1%、10%和100%的影像）上進行了 MDETR 的微調。結果與兩個基線方法進行了比較：一個是直接在 LVIS 的完整訓練集上訓練的 Mask-RCNN，另一個是先在 MSCOCO 上預訓練，再在 LVIS 的子集上微調的 DETR。令人驚訝的是，即使在只有1% 的資料上，MDETR 都能利用它的文字預訓練，在稀有類別上超越完全微調的 DETR。</p>
<p>此外，有一個顯著的觀察是：當在全部的訓練資料上進行微調時，對於稀有的物體偵測表現從使用 10% 的資料的 20.9 AP 驟降到使用 100% 的資料的 7.5 AP。這一大幅下降可能是由於資料中的極端類別不平衡造成的。</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="結論">結論<a href="#結論" class="hash-link" aria-label="Direct link to 結論" title="Direct link to 結論">​</a></h2>
<p>MDETR 最吸引人的特點之一就是它的「完全可微性」。</p>
<p>這種設計讓整個模型可以進行端到端的訓練，這種一致性帶來的效果是：更緊密的模型協同工作，進而有機會提高整體性能和訓練效率。其次，在實際的性能表現上，它在多種資料集上都展現出了令人難以置信的效果，這使得它在多模態學習的領域中站穩了腳跟。</p>
<p>再者，MDETR 的多功能性也是一大亮點。它不僅在調製檢測上展現出色，還在其他如 Few-show 偵測和視覺問答等下游應用中證明了自己的價值。</p>
<p>MDETR 提供了一條道路，在其不依賴黑盒物件偵測器的設計思路可能會啟發更多的研究者去創建準確且高效的模型。</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>為什麼以前的論文架構不也把 Faster RCNN 都串起來呢？完全可微分，多好？</p><p>因為完全可微的模型雖然聽起來很美好，但它也可能需要更多的計算資源。特別是當你沒有經過一些巧妙的設計，就單純地、直接地、暴力地串串串的時候，你很大的概率會受到來自模型對你的制裁：</p><ul>
<li><strong>Train 不起來。</strong></li>
</ul><p>當整個模型都是可微的時，其內部結構很可能複雜，這也意味著更高的計算成本和更多的訓練難度。研究人員可能需要花更多時間進行調參，而這可能不是每個團隊都能夠承受的。</p></div></div></div><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/multi-model/2021/vlt5"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">VL-T5</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/multi-model/2021/albef"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">ALBEF</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#連續之藝" class="table-of-contents__link toc-highlight">連續之藝</a></li><li><a href="#定義問題" class="table-of-contents__link toc-highlight">定義問題</a></li><li><a href="#解決問題" class="table-of-contents__link toc-highlight">解決問題</a><ul><li><a href="#mdetr-模型設計" class="table-of-contents__link toc-highlight">MDETR 模型設計</a></li><li><a href="#訓練方式" class="table-of-contents__link toc-highlight">訓練方式</a></li><li><a href="#對比對齊" class="table-of-contents__link toc-highlight">對比對齊</a></li><li><a href="#所有損失" class="table-of-contents__link toc-highlight">所有損失</a></li><li><a href="#資料集" class="table-of-contents__link toc-highlight">資料集</a></li><li><a href="#技術細節" class="table-of-contents__link toc-highlight">技術細節</a></li></ul></li><li><a href="#討論" class="table-of-contents__link toc-highlight">討論</a><ul><li><a href="#下游任務表現分析" class="table-of-contents__link toc-highlight">下游任務表現分析</a></li><li><a href="#few-shot-可以做得到嗎" class="table-of-contents__link toc-highlight">Few-shot 可以做得到嗎？</a></li></ul></li><li><a href="#結論" class="table-of-contents__link toc-highlight">結論</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator"> ·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>