<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-visualbert/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">[19.08] VisualBERT | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/visualbert/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[19.08] VisualBERT | DOCSAID"><meta data-rh="true" name="description" content="Gaze at the Prelude"><meta data-rh="true" property="og:description" content="Gaze at the Prelude"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/visualbert/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/visualbert/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/visualbert/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/visualbert/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.1fe4c5ae.css">
<script src="/en/assets/js/runtime~main.ac22c07c.js" defer="defer"></script>
<script src="/en/assets/js/main.61daef6c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" href="/en/papers/visualbert/"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/visualbert/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/visualbert/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/alexnet/">[12.09] AlexNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vgg/">[14.09] VGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/hourglass/">[16.03] Hourglass</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/densenet/">[16.08] DenseNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/resnext/">[16.11] ResNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/fpn/">[16.12] FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v1/">[17.04] MobileNet-V1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/nasnet/">[17.07] NASNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/shufflenet/">[17.07] ShuffleNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/senet/">[17.09] SENet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v2/">[18.01] MobileNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/panet/">[18.03] PANet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/nasfpn/">[19.04] NAS-FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet/">[19.05] EfficientNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v3/">[19.05] MobileNet-V3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/en/papers/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/unetpp/">[19.12] UNet++</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/detr/">[20.05] DETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vit/">[20.10] ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/deit/">[20.12] DeiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pvt/">[21.02] PVT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet-v2/">[21.04] EfficientNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/roformer/">[21.04] RoFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mlp-mixer/">[21.05] MLP-Mixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pp-lcnet/">[21.09] PP-LCNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/meter/">[21.11] METER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/poolformer/">[21.11] PoolFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/convnext/">[22.01] ConvNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/frvt-distinguishing-twins/">[22.09] FRVT-Twins</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/caformer/">[22.10] CAFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/tivc/">[23.09] TIVC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v4/">[24.04] MobileNet-V4</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[19.08] VisualBERT</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>[19.08] VisualBERT</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="gaze-at-the-prelude">Gaze at the Prelude<a class="hash-link" aria-label="Direct link to Gaze at the Prelude" title="Direct link to Gaze at the Prelude" href="/en/papers/visualbert/#gaze-at-the-prelude">​</a></h2>
<p><a href="https://arxiv.org/abs/1908.03557" target="_blank" rel="noopener noreferrer"><strong>VisualBERT: A Simple and Performant Baseline for Vision and Language</strong></a></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>The following content has been compiled by ChatGPT-4 and manually proofread, edited, and supplemented.</p></div></div>
<hr>
<p>Around 2015, many multimodal models based on LSTM architecture were being explored, as mentioned by the authors: &quot;Multimodal model research is nothing new!&quot; With the advent of the Transformer architecture and its attention mechanism in 2017, significant advancements were made in natural language processing. Notably, BERT successfully pre-trained a general language encoder capable of predicting masked words in a text.</p>
<p>By 2019, the application of attention mechanisms in the multimodal domain had also advanced significantly, refocusing research on combining language and vision to extract deeper semantic details from images, including objects, attributes, parts, spatial relationships, actions, and intentions.</p>
<p>Inspired by this, the authors sought to capture implicit relationships in images using attention mechanisms and believed that pre-training could effectively learn these relationships. Based on previous research, they identified several current issues:</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-definition">Problem Definition<a class="hash-link" aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition" href="/en/papers/visualbert/#problem-definition">​</a></h2>
<ul>
<li>
<p><strong>Complex Interaction Between Vision and Language:</strong></p>
<ul>
<li>Current vision-language tasks (e.g., object recognition, visual captioning, visual question answering, and visual reasoning) require systems to understand detailed image semantics, including objects, attributes, parts, spatial relationships, actions, and intentions, and how these concepts are referenced and established in language.</li>
</ul>
</li>
<li>
<p><strong>Unified Model Architecture for Vision and Language:</strong></p>
<ul>
<li>Many existing models are designed for specific vision-language tasks and lack a universal model that can be applied across various tasks.</li>
</ul>
</li>
<li>
<p><strong>Importance of Pre-Training:</strong></p>
<ul>
<li>How to effectively pre-train models on vision and language data to enhance their performance in downstream tasks.</li>
</ul>
</li>
<li>
<p><strong>Challenges in Understanding Image Semantics:</strong></p>
<ul>
<li>Capturing and understanding the detailed semantics described in images and associating them with textual descriptions.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solution">Solution<a class="hash-link" aria-label="Direct link to Solution" title="Direct link to Solution" href="/en/papers/visualbert/#solution">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="visualbert-model-design">VisualBERT Model Design<a class="hash-link" aria-label="Direct link to VisualBERT Model Design" title="Direct link to VisualBERT Model Design" href="/en/papers/visualbert/#visualbert-model-design">​</a></h3>
<p><img decoding="async" loading="lazy" alt="VisualBERT Model Architecture" src="/en/assets/images/arch_visual_bert-8032691edcd02251fc604c7557f2ea4e.jpg" width="1816" height="672" class="img_ev3q"></p>
<ol>
<li>
<p><strong>Attention Mechanism:</strong></p>
<ul>
<li>The core idea of VisualBERT is to use the attention mechanism in Transformers to implicitly align elements of the input text with regions in the input image.</li>
</ul>
</li>
<li>
<p><strong>Visual Features:</strong></p>
<ul>
<li>In addition to all components of BERT, VisualBERT introduces a set of visual features called F to model images.</li>
<li>Each feature in F corresponds to an object region in the image, derived from an object detector (possibly Faster RCNN or others).</li>
<li>Each feature f in F is calculated as the sum of the following three features:<!-- -->
<ul>
<li>(f_o): The visual feature representation of the object region f, computed by a convolutional neural network.</li>
<li>(f_s): Segment feature indicating it is an image feature as opposed to a textual feature.</li>
<li>(f_p): Positional feature, used when alignment between words and object regions is provided as part of the input and set to the sum of the positional features corresponding to the aligned words.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Combining Visual and Text Features:</strong></p>
<ul>
<li>The visual features F are passed along with the original text feature set E through multiple layers of Transformers. This design allows the model to implicitly discover useful alignments between the two sets of inputs (text and image) and build new joint representations.</li>
</ul>
</li>
</ol>
<p>This architecture enables VisualBERT to capture rich semantic relationships between images and corresponding texts when handling multimodal tasks, leveraging the powerful capabilities of Transformers for deep representation learning.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pre-training-mechanism">Pre-Training Mechanism<a class="hash-link" aria-label="Direct link to Pre-Training Mechanism" title="Direct link to Pre-Training Mechanism" href="/en/papers/visualbert/#pre-training-mechanism">​</a></h3>
<p>The pre-training process of VisualBERT can be divided into three main stages:</p>
<ol>
<li>
<p>Task-Agnostic Pre-Training:</p>
<ul>
<li>
<p><strong>Data Source:</strong></p>
<ul>
<li>Assume a photo in the COCO dataset shows a little boy playing with his dog in a park. Five possible captions for this photo might be:<!-- -->
<ul>
<li>A little boy playing in the park.</li>
<li>A dog chasing a ball on the grass.</li>
<li>A child and his pet enjoying time outdoors.</li>
<li>A boy and a dog having fun in the sun.</li>
<li>A kid and a dog interacting in the park.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Masked Language Modeling:</strong></p>
<ul>
<li>Using the first caption &quot;A little boy playing in the park&quot; as an example, randomly mask the word &quot;playing,&quot; resulting in &quot;A little boy in the park [MASK].&quot; VisualBERT&#x27;s task is to predict the masked word &quot;playing&quot; based on the context and the corresponding image (a boy and a dog in the park).</li>
</ul>
</li>
<li>
<p><strong>Sentence-Image Prediction:</strong></p>
<ul>
<li>Given the same photo, provide the model with two captions:<!-- -->
<ul>
<li>(a) A little boy playing in the park (describes the image)</li>
<li>(b) An old lady shopping at the market (a random unrelated caption)</li>
</ul>
</li>
<li>VisualBERT receives these two captions and the photo as input and must determine which caption matches the image. The correct answer is caption (a).</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Task-Specific Pre-Training:</p>
<ul>
<li>Before fine-tuning VisualBERT for specific downstream tasks, this pre-training stage helps the model adapt better to the target domain. This stage primarily involves masked language modeling with image targets, training on specific task data to accustom the model to the new target domain.</li>
</ul>
</li>
<li>
<p>Fine-Tuning:</p>
<ul>
<li>This step is similar to BERT&#x27;s fine-tuning strategy. First, introduce the corresponding input, output layers, and objectives for the specific task. Then train the Transformer to maximize performance on the task.</li>
</ul>
</li>
</ol>
<p>By combining these three stages of pre-training, the authors aim to make the model more generalized and adaptable to various vision-language tasks.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion" href="/en/papers/visualbert/#discussion">​</a></h2>
<p>In this study, the authors observed that VisualBERT not only performed well across various tasks but also provided unique insights into training strategies and architectural design. Specifically, how to integrate image and text information and establish deep semantic connections between the two.</p>
<p>Next, let&#x27;s explore the core advantages of VisualBERT, the chosen pre-training strategies, and how it effectively captures the detailed relationships between images and language.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-well-does-the-model-perform">How Well Does the Model Perform?<a class="hash-link" aria-label="Direct link to How Well Does the Model Perform?" title="Direct link to How Well Does the Model Perform?" href="/en/papers/visualbert/#how-well-does-the-model-perform">​</a></h3>
<ul>
<li>
<p><strong>VQA</strong></p>
<ul>
<li>
<p><strong>Task Description:</strong></p>
<ul>
<li>Goal: Provide correct answers to given images and questions.</li>
<li>Dataset: VQA 2.0, proposed by Goyal et al. in 2017.</li>
<li>Dataset Characteristics: Contains over 1 million questions related to COCO images.</li>
</ul>
</li>
<li>
<p><strong>Model Training:</strong></p>
<ul>
<li>Answer Selection: Train the model to predict 3,129 most common answers.</li>
<li>Image Feature Source: Based on ResNeXt Faster RCNN, pre-trained on Visual Genome.</li>
</ul>
</li>
<li>
<p><strong>Part One:</strong></p>
<ul>
<li>Baseline models using the same visual features (in terms of feature dimensions) and object region proposals (in terms of the number of selected regions) as in the study.</li>
</ul>
</li>
<li>
<p><strong>Part Two:</strong></p>
<ul>
<li>Model results of VisualBERT.</li>
</ul>
</li>
<li>
<p><strong>Part Three:</strong></p>
<ul>
<li>Results of other non-comparable methods, including those using external QA pairs, multiple detectors, and model ensembles.</li>
</ul>
</li>
<li>
<p><strong>Summary:</strong></p>
<ul>
<li>Performs well against comparable baselines.</li>
<li>For non-comparable methods, the proposed method is simple and outperforms existing methods in efficiency and performance.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>VCR</strong></p>
<ul>
<li>
<p><strong>Task Description:</strong></p>
<ul>
<li>VCR includes 290,000 questions from 110,000 movie scenes.</li>
<li>These questions focus mainly on visual commonsense.</li>
</ul>
</li>
<li>
<p><strong>Subtasks:</strong></p>
<ul>
<li>VCR is divided into two multiple-choice subtasks.</li>
<li>These are Question Answering (Q → A) and Answer Justification (QA → R).</li>
<li>Separate models are trained for both subtasks.</li>
</ul>
</li>
<li>
<p><strong>Image Features:</strong></p>
<ul>
<li>ResNet50 (proposed by He et al. in 2016) extracts image features.</li>
<li>Use &quot;gold&quot; object bounding boxes and segmentation provided in the dataset.</li>
</ul>
</li>
<li>
<p><strong>Text-Image Alignment:</strong></p>
<ul>
<li>VCR provides alignment between words referenced in the text and object regions.</li>
<li>Using corresponding positional features to match words and regions, the model utilizes this alignment.</li>
</ul>
</li>
<li>
<p><strong>Comparison Baseline:</strong></p>
<ul>
<li>Compared with the dataset&#x27;s baseline model based on BERT (R2C).</li>
<li>Also compared with the top single model on the leaderboard (B2T2).</li>
</ul>
</li>
<li>
<p><strong>Summary:</strong></p>
<ul>
<li>A trimmed-down version of VisualBERT without COCO pre-training performs significantly better than R2C with the same resource allocation.</li>
<li>The full version of VisualBERT further improves performance.</li>
</ul>
</li>
</ul>
<p>Although there is a significant domain difference between VCR (mainly covering movie scenes) and COCO, pre-training on COCO is still very beneficial for VCR.</p>
</li>
<li>
<p><strong>NLVR2</strong></p>
<ul>
<li>
<p><strong>Task Description:</strong></p>
<ul>
<li>NLVR2 focuses on joint reasoning between natural language and images.</li>
<li>Major challenges include semantic diversity, compositionality, and visual reasoning.</li>
<li>The task is to determine whether a given natural language description accurately describes a pair of images.</li>
<li>Contains over 100,000 English sentence examples paired with web images.</li>
</ul>
</li>
<li>
<p><strong>Segment Feature Adjustment:</strong></p>
<ul>
<li>The segment feature mechanism in VisualBERT is adjusted.</li>
<li>Used to assign features from different images using different segment features.</li>
</ul>
</li>
<li>
<p><strong>Image Features:</strong></p>
<ul>
<li>Utilizes Detectron&#x27;s pre-trained detector to obtain image features.</li>
<li>Each image uses 144 proposals to provide features.</li>
</ul>
</li>
<li>
<p><strong>Summary:</strong></p>
<ul>
<li>VisualBERT shows superior performance.</li>
<li>PhBERT without early fusion and VisualBERT without COCO pre-training significantly outperform the previous leading model MaxEnt.</li>
<li>The full version of VisualBERT further extends its performance gap with other models.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>FLICKR30K</strong></p>
<ul>
<li>
<p><strong>Task Description:</strong></p>
<ul>
<li>The main goal of the Flickr30K dataset is to test a system&#x27;s ability to locate specific object regions in an image based on phrases in captions. - Given part or a segment of a sentence, the system needs to select the corresponding image object region. - The dataset contains 30k images and nearly 250k annotations.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Model Configuration:</strong></p>
<ul>
<li>Based on the BAN setup (proposed by Kim et al. in 2018).</li>
<li>Image features are obtained using Faster R-CNN pre-trained on Visual Genome.</li>
<li>During fine-tuning, additional attention blocks are added, and the average weight of the attention heads is used to predict alignment between object boxes and phrases.</li>
<li>During system prediction, the box most attended to in the last sub-word of the phrase is selected as the result.</li>
</ul>
</li>
<li>
<p><strong>Summary:</strong></p>
<ul>
<li>VisualBERT outperforms the current leading model BAN on this task.</li>
<li>Interestingly, models without early fusion do not show significant performance differences from the full version of VisualBERT, suggesting that simpler or shallower model structures may suffice for this task.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-most-important-in-this-model-design">What Is Most Important in This Model Design?<a class="hash-link" aria-label="Direct link to What Is Most Important in This Model Design?" title="Direct link to What Is Most Important in This Model Design?" href="/en/papers/visualbert/#what-is-most-important-in-this-model-design">​</a></h3>
<p>The authors explore which components or design choices in the VisualBERT model contribute most to its performance.</p>
<p>They selected the following four core components/strategies for ablation studies:</p>
<ol>
<li>Task-agnostic pre-training (C1).</li>
<li>Early fusion, i.e., early interaction between image and text features (C2).</li>
<li>Initialization strategy of BERT (C3).</li>
<li>Sentence-image prediction objective (C4).</li>
</ol>
<p>Experimental results show:</p>
<ol>
<li>Task-agnostic pre-training (C1) is crucial. Pre-training on paired visual and language data significantly improves model performance.</li>
<li>Early fusion (C2) is also important. Allowing early interaction between image and text features enhances mutual influence between visual and language components across multiple interaction layers.</li>
<li>The initialization strategy of BERT (C3) has some importance. Although performance declines without BERT pre-trained weights, the decline is not as severe as expected, suggesting that the model also learns substantial language grounding knowledge during COCO pre-training.</li>
<li>The sentence-image prediction objective (C4) has a certain impact but is less significant than other components.</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-the-model-really-focus-on-the-right-areas">Does the Model Really Focus on the Right Areas?<a class="hash-link" aria-label="Direct link to Does the Model Really Focus on the Right Areas?" title="Direct link to Does the Model Really Focus on the Right Areas?" href="/en/papers/visualbert/#does-the-model-really-focus-on-the-right-areas">​</a></h3>
<p>The authors investigate whether the attention heads in VisualBERT can correctly map entities in sentences to corresponding object regions in images. Additionally, they examine whether the attention heads can identify syntactic relationships in sentences, especially when these syntactic relationships have clear correspondences with image regions.</p>
<ol>
<li>
<p>Entity Recognition:</p>
<ul>
<li>Many attention heads in VisualBERT exhibit high accuracy without direct supervision for entity recognition.</li>
<li>The accuracy appears to improve in higher layers of the model, suggesting that early layers may be less certain about entity recognition, while later layers become increasingly confident.</li>
</ul>
</li>
<li>
<p>Syntactic Basis:</p>
<ul>
<li>Many attention heads in VisualBERT seem to capture syntactic relationships, especially the associations between verbs and their corresponding arguments.</li>
<li>For various syntactic dependency relationships, at least one attention head in VisualBERT performs significantly better than a baseline based on guessing.</li>
<li>This indicates that VisualBERT can implicitly identify and map syntactic structures without explicit syntactic supervision.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-attention-distribution-evolve">How Does Attention Distribution Evolve?<a class="hash-link" aria-label="Direct link to How Does Attention Distribution Evolve?" title="Direct link to How Does Attention Distribution Evolve?" href="/en/papers/visualbert/#how-does-attention-distribution-evolve">​</a></h3>
<p>The authors explore how VisualBERT progressively changes its attention distribution across multiple Transformer layers to more accurately align entities or concepts in text with corresponding regions in images.</p>
<ul>
<li>Attention Refinement: VisualBERT incrementally refines the alignment between text and images across its successive Transformer layers. For example, as illustrated in the bottom left of the image, initially, both &quot;husband&quot; and &quot;woman&quot; might strongly focus on the &quot;woman&quot; region in the image, but this alignment becomes more precise and correct in later layers of the model.</li>
<li>Syntactic Alignment: VisualBERT can align entities not only based on semantics but also based on syntax. For example, in the image, the word &quot;teasing&quot; focuses on both the man and woman, while the word &quot;by&quot; focuses only on the man.</li>
<li>Coreference Resolution: VisualBERT seems capable of resolving coreference in language, correctly aligning the word &quot;her&quot; to the &quot;woman&quot; in the image.</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" href="/en/papers/visualbert/#conclusion">​</a></h2>
<p>VisualBERT demonstrates outstanding performance across various vision-language tasks. These results not only validate the model&#x27;s effectiveness but, more importantly, through its built-in attention mechanism, VisualBERT provides an interpretable and intuitive way to capture and understand information.</p>
<p>However, one thing remains unavoidable:</p>
<ul>
<li>When combining object detection models, the model architecture becomes highly complex and challenging to use.</li>
<li>This excessive complexity may inhibit the model&#x27;s potential in practical applications, increasing the difficulty of deployment and adjustment.</li>
</ul>
<p>Therefore, optimizing and simplifying this architecture should be considered a crucial direction for future research.</p>
<p>Of course, many issues still require further exploration and clarification. For instance, can VisualBERT perform equally well on purely visual tasks, such as scene graph parsing and contextual recognition? Additionally, can its capabilities be further expanded through pre-training on larger caption datasets, such as Visual Genome and Conceptual Captions?</p>
<p>At this stage, despite many questions that warrant further investigation, this research provides clear directions for future researchers.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-07-11T23:16:50.000Z" itemprop="dateModified">Jul 11, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/vilbert/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[19.08] ViLBERT</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/vlbert/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[19.08] VL-BERT</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#gaze-at-the-prelude">Gaze at the Prelude</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#problem-definition">Problem Definition</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#solution">Solution</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#visualbert-model-design">VisualBERT Model Design</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#pre-training-mechanism">Pre-Training Mechanism</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#discussion">Discussion</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#how-well-does-the-model-perform">How Well Does the Model Perform?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#what-is-most-important-in-this-model-design">What Is Most Important in This Model Design?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#does-the-model-really-focus-on-the-right-areas">Does the Model Really Focus on the Right Areas?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#how-does-attention-distribution-evolve">How Does Attention Distribution Evolve?</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/visualbert/#conclusion">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>