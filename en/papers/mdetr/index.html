<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-mdetr/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">[21.04] MDETR | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/mdetr/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[21.04] MDETR | DOCSAID"><meta data-rh="true" name="description" content="The Art of Continuity"><meta data-rh="true" property="og:description" content="The Art of Continuity"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/mdetr/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/mdetr/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/mdetr/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/mdetr/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.1fe4c5ae.css">
<script src="/en/assets/js/runtime~main.801c4bfe.js" defer="defer"></script>
<script src="/en/assets/js/main.9d527964.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" href="/en/papers/mdetr/"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/mdetr/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/mdetr/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/alexnet/">[12.09] AlexNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vgg/">[14.09] VGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/hourglass/">[16.03] Hourglass</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/densenet/">[16.08] DenseNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/resnext/">[16.11] ResNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/fpn/">[16.12] FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v1/">[17.04] MobileNet-V1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/nasnet/">[17.07] NASNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/shufflenet/">[17.07] ShuffleNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/senet/">[17.09] SENet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/cosface/">[18.01] CosFace</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v2/">[18.01] MobileNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/panet/">[18.03] PANet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/nasfpn/">[19.04] NAS-FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet/">[19.05] EfficientNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v3/">[19.05] MobileNet-V3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/unetpp/">[19.12] UNet++</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/detr/">[20.05] DETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vit/">[20.10] ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/deit/">[20.12] DeiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/repvgg/">[21.01] RepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pvt/">[21.02] PVT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet-v2/">[21.04] EfficientNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/en/papers/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/roformer/">[21.04] RoFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mlp-mixer/">[21.05] MLP-Mixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pp-lcnet/">[21.09] PP-LCNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/meter/">[21.11] METER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/poolformer/">[21.11] PoolFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/convnext/">[22.01] ConvNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/frvt-distinguishing-twins/">[22.09] FRVT-Twins</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/caformer/">[22.10] CAFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/tivc/">[23.09] TIVC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v4/">[24.04] MobileNet-V4</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[21.04] MDETR</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>[21.04] MDETR</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-art-of-continuity">The Art of Continuity<a class="hash-link" aria-label="Direct link to The Art of Continuity" title="Direct link to The Art of Continuity" href="/en/papers/mdetr/#the-art-of-continuity">​</a></h2>
<p><a href="https://arxiv.org/abs/2104.12763" target="_blank" rel="noopener noreferrer"><strong>MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding</strong></a></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>The following content is summarized by ChatGPT-4 and has been manually checked, edited, and supplemented with additional explanations.</p></div></div>
<hr>
<p>In recent years, object detection has been at the forefront of computer vision, serving as the core of many advanced multi-modal understanding systems. However, traditional methods treat detection systems as black boxes, conducting fixed concept image detection, which clearly has inherent limitations.</p>
<p>A significant issue is the inability to effectively utilize multi-modal context for collaborative training, limiting downstream models to accessing only detected objects. Moreover, these detection systems are often frozen, meaning they lack further refinement and adaptability. Most importantly, the vocabulary of these detection systems is severely limited, making them blind to novel concept combinations expressed in free-form text.</p>
<p>Simply put, the authors of this paper aim to replace the object detection architecture in VL models.</p>
<p>You might remember a previous paper that attempted to replace the detection architecture with ViT, which didn’t turn out well.</p>
<ul>
<li>Portal: <strong><a href="/en/papers/vilt/">ViLT: Passing the Baton</a></strong></li>
</ul>
<p>Since replacing with ViT wasn’t very successful (because ViT isn’t specialized for object detection), this time the authors used another &quot;specialist&quot; object detection model, DETR, for the replacement.</p>
<ul>
<li>Portal: <strong><a href="/en/papers/detr/">DETR: The Pioneer Across Fields</a></strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-definition">Problem Definition<a class="hash-link" aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition" href="/en/papers/mdetr/#problem-definition">​</a></h2>
<p>Most advanced multi-modal understanding systems currently rely on object detection as a core component, but these designs are fraught with various issues:</p>
<ol>
<li>
<p><strong>Limitations in Collaborative Training</strong></p>
<p>In multi-modal systems, collaborative training means using data from multiple input sources (e.g., images, text, audio) simultaneously for model training. If a system component cannot collaborate with other parts for such training, it might fail to fully leverage all available information.</p>
<p>Imagine a model with both image and audio inputs that only trains the image detector independently, ignoring the audio input. If the audio provides important information about objects in the image, the model might fail to identify them correctly.</p>
</li>
<li>
<p><strong>Detection Scope Limitation</strong></p>
<p>The main purpose of detection systems is to identify specific objects in images. However, if these systems focus solely on known objects and ignore other parts of the image, they might miss important contextual information. For instance, in an image with multiple people and a dog, the detector might only identify the people and dog, ignoring the park setting, which provides important context.</p>
</li>
<li>
<p><strong>Model Fixation</strong></p>
<p>Once a model is trained and &quot;frozen,&quot; it no longer updates or learns. This hinders its ability to adapt and optimize in new situations or with new data. For example, a detector trained on summer images might perform poorly on winter images with snow or people in heavy coats if it cannot be fine-tuned.</p>
</li>
<li>
<p><strong>Vocabulary Limitation</strong></p>
<p>Object detection systems identify specific classes or attributes based on their training data. If they encounter new objects or concepts not seen in the training data, they might fail to recognize them.</p>
</li>
<li>
<p><strong>Lack of End-to-End Design</strong></p>
<p>End-to-end systems allow continuous learning and optimization from input to output without intermediate steps. If the detector is not end-to-end, its collaborative training with other tasks might be limited. Mathematically speaking, this system cannot be differentiated, and without differentiation, it cannot be optimized.</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solution">Solution<a class="hash-link" aria-label="Direct link to Solution" title="Direct link to Solution" href="/en/papers/mdetr/#solution">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mdetr-model-design">MDETR Model Design<a class="hash-link" aria-label="Direct link to MDETR Model Design" title="Direct link to MDETR Model Design" href="/en/papers/mdetr/#mdetr-model-design">​</a></h3>
<p><img decoding="async" loading="lazy" alt="MDETR Architecture" src="/en/assets/images/mdetr_1-9fe82970ec37a8117f878d76e4271882.jpg" width="1024" height="338" class="img_ev3q"></p>
<p>The model is straightforward: for the text part, it uses a modified RoBERTa encoder. After generating text feature encodings, they are concatenated and fed into the original DETR architecture.</p>
<p>The overall structure includes:</p>
<ul>
<li><strong>Main Convolutional Encoder</strong>: The image is first encoded by a convolutional backbone and flattened.</li>
<li><strong>Spatial Information</strong>: By adding 2D positional encoding to the flattened vectors, the model retains spatial information.</li>
<li><strong>Text Encoding</strong>: The text is encoded using a pretrained Transformer language model, producing a sequence of hidden vectors of the same size as the input.</li>
<li><strong>Modality-Specific Linear Projections</strong>: Modality-specific linear projections are applied to image and text features to project them into a shared encoding space.</li>
<li><strong>Cross-Encoder</strong>: The concatenated sequence of image and text features is fed into a joint Transformer encoder, the core part of the model.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="training-method">Training Method<a class="hash-link" aria-label="Direct link to Training Method" title="Direct link to Training Method" href="/en/papers/mdetr/#training-method">​</a></h3>
<ol>
<li>Soft Token Prediction</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Soft Token Prediction" src="/en/assets/images/mdetr_2-f522827e2c5a6a2fd3d51965193cbabb.jpg" width="1224" height="380" class="img_ev3q"></p>
<p>The idea behind soft token prediction is intriguing. Soft token prediction focuses on predicting the &quot;range&quot; of each matched object in the original text rather than predicting the class label of each detected object. This is the main difference between this method and standard object detection.</p>
<p>Suppose the description sentence is &quot;a black cat and a white dog.&quot; When the model detects a black animal, it tries to predict its association with the phrase &quot;black cat.&quot; This is not just about an individual token or class label but about a range of tokens in the text that collectively describe a specific object.</p>
<p>The benefit of this approach is that it can handle multiple references to objects in the same text or cases where multiple objects correspond to the same text.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="contrastive-alignment">Contrastive Alignment<a class="hash-link" aria-label="Direct link to Contrastive Alignment" title="Direct link to Contrastive Alignment" href="/en/papers/mdetr/#contrastive-alignment">​</a></h3>
<p>Contrastive alignment aims to ensure that the encoded representations of visual objects are close to their corresponding text tokens in the feature space. This alignment is stronger than just relying on positional information from &quot;Soft Token Prediction&quot; as it directly operates on feature representations.</p>
<p><img decoding="async" loading="lazy" alt="Contrastive Alignment" src="/en/assets/images/mdetr_3-4c1659405e91212323e7fdfafd0ef89b.jpg" width="1024" height="169" class="img_ev3q"></p>
<p>Referencing the mathematical equation provided in the paper:</p>
<ul>
<li>L: The maximum number of tokens.</li>
<li>N: The maximum number of objects.</li>
<li>T+​i: The set of tokens that should align with a given object oi​.</li>
<li>Oi+​: The set of objects that should align with a given token ti​.</li>
<li>τ: The temperature parameter, set to 0.07 directly.</li>
</ul>
<p>The concept of the entire equation is simple: objects and text, align well, the closer the better.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="all-losses">All Losses<a class="hash-link" aria-label="Direct link to All Losses" title="Direct link to All Losses" href="/en/papers/mdetr/#all-losses">​</a></h3>
<p>Training MDETR involves multiple loss functions. In addition to the contrastive loss mentioned above, it includes the original losses from the DETR paper, such as bipartite matching, bounding box prediction loss, L1 loss, GIoU, etc.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dataset">Dataset<a class="hash-link" aria-label="Direct link to Dataset" title="Direct link to Dataset" href="/en/papers/mdetr/#dataset">​</a></h3>
<ul>
<li><strong>CLEVR</strong>: Used to evaluate the method&#x27;s results.</li>
<li><strong>Flickr30k</strong>: Used to build the composite dataset images.</li>
<li><strong>MS COCO</strong>: Used to build the composite dataset images.</li>
<li><strong>Visual Genome (VG)</strong>: Used to build the composite dataset images.</li>
<li>Annotation data is sourced from referring expression datasets, VG regions, Flickr entities, and the GQA training balanced set.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="technical-details">Technical Details<a class="hash-link" aria-label="Direct link to Technical Details" title="Direct link to Technical Details" href="/en/papers/mdetr/#technical-details">​</a></h3>
<ul>
<li>
<p><strong>Pre-training Modulated Detection</strong></p>
<p>During the pre-training phase, the goal is to detect all objects referenced in the aligned free-form text.</p>
</li>
<li>
<p><strong>Data Composition Technique and Its Importance</strong></p>
<ul>
<li>For each image, all relevant text annotations are retrieved from the mentioned datasets. If different annotations reference the same image, they are merged. To ensure the independence of the training and validation/test sets, all images that appear in the validation or test sets are removed from the training set.</li>
<li>This algorithm combines sentences, ensuring the overlap between combined phrases or text blocks is minimal (GIoU ≤ 0.5). GIoU is a metric used to assess the overlap between two rectangular regions. The total length of the combined sentence is limited to fewer than 250 characters. Using this method, a large dataset of 1.3 million aligned image-text pairs is formed.</li>
<li>There are two main reasons for using this data composition technique:<!-- -->
<ul>
<li>Data efficiency: By packing more information into a single training sample, the data can be utilized more effectively.</li>
<li>Better learning signal:<!-- -->
<ul>
<li>When the model learns, it needs to identify and resolve ambiguities between multiple references to the same object class in the text.</li>
<li>In a single-sentence context, the task of &quot;Soft Token Prediction&quot; becomes relatively simple, as the model can usually easily predict the subject or core meaning of the sentence without much reliance on the image.</li>
<li>By combining multiple sentences, the model is forced to explore the associations between images and text more deeply, thereby improving its prediction capabilities.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Model Architecture Choices</strong></p>
<ul>
<li>The text encoder uses a pretrained RoBERTa-base with 12 Transformer encoder layers.</li>
<li>Visual backbones include:<!-- -->
<ul>
<li><strong>ResNet-101</strong>: Obtained from Torchvision and pretrained on ImageNet.</li>
<li><strong>EfficientNet Series</strong>: EfficientNetB3 and EfficientNetB5 were used. EfficientNetB3 achieved 84.1% top-1 accuracy on ImageNet, while EfficientNetB5 achieved 86.1%.</li>
</ul>
</li>
<li>Additionally, a model trained on large amounts of unlabeled data using the Noisy-Student pseudo-labeling technique was used.</li>
<li>Training details: Using 32 V100 GPUs, pretraining was conducted for 40 epochs with an effective batch size of 64, taking approximately one week.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion" href="/en/papers/mdetr/#discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="downstream-task-performance-analysis">Downstream Task Performance Analysis<a class="hash-link" aria-label="Direct link to Downstream Task Performance Analysis" title="Direct link to Downstream Task Performance Analysis" href="/en/papers/mdetr/#downstream-task-performance-analysis">​</a></h3>
<ol>
<li>
<p><strong>Phrase Grounding</strong></p>
<p><img decoding="async" loading="lazy" alt="Phrase Grounding" src="/en/assets/images/mdetr_4-c8426ce0c50ebdd7a392e1127821d2fb.jpg" width="972" height="872" class="img_ev3q"></p>
<p>The authors used the Flickr30k dataset with specific training/validation/test splits. For evaluation, they adopted two different protocols to address the issue of multiple objects being referenced by a single phrase, each with its pros and cons.</p>
<ol>
<li>
<p><strong>ANY-BOX Protocol</strong></p>
<p>Under this protocol, if a given phrase references multiple different objects in the image, the predicted bounding box is considered correct if it intersects with any of the ground truth boxes above a set threshold, typically 0.5 IoU. This means the prediction is correct as long as the model correctly identifies any of the referenced objects. However, this method does not evaluate if the model has found all referenced instances.</p>
</li>
<li>
<p><strong>MERGED-BOXES Protocol</strong></p>
<p>Under this protocol, if a phrase references multiple objects in the image, all ground truth boxes associated with the phrase are first merged into a single bounding box that encompasses all of them. Then, the merged box is used to calculate IoU as usual. This means the model&#x27;s prediction needs to match the merged box rather than individual ground truth boxes. The issue with this method is that it may lose detailed understanding of each instance, especially when these instances are far apart in the image, making the merged box unreasonably large.</p>
</li>
<li>
<p><strong>Results Comparison</strong></p>
<ul>
<li>Under the ANY-BOX setting, MDETR achieved an 8.5 point improvement in Recall@1 on the validation set compared to the current state-of-the-art techniques, without using any additional pre-training data.</li>
<li>With pre-training and using the same backbone, MDETR further improved its performance on the test set by 12.1 points.</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>Referring Expression Comprehension</strong></p>
<p><img decoding="async" loading="lazy" alt="Referring Expression Comprehension" src="/en/assets/images/mdetr_5-363d71e92179f27e926b68e38e1cde4d.jpg" width="1024" height="271" class="img_ev3q"></p>
<p>Most previous research and methods for this task involve ranking a set of pre-extracted image-related bounding boxes. These boxes are obtained using pretrained object detectors.</p>
<p>In contrast, this paper aims for a more challenging goal: training the model to predict bounding boxes directly given a referring expression and the corresponding image, rather than simply ranking pre-extracted boxes.</p>
<p>The model in this paper is pretrained to identify every object referenced in the text during pre-training. For example, for the caption &quot;a woman in a blue dress standing next to a rose bush,&quot; the model is trained to predict boxes for all referenced objects (woman, blue dress, and rose bush). However, when it comes to referring expressions, the task is only to return a single bounding box representing the object referenced by the entire expression. To adapt to this change, the model was fine-tuned on these three specific datasets.</p>
<p>The results shown in the table indicate significant improvements over state-of-the-art methods across all datasets.</p>
</li>
<li>
<p><strong>Visual Question Answering</strong></p>
<p><img decoding="async" loading="lazy" alt="Visual Question Answering" src="/en/assets/images/mdetr_6-fb30a864ffbf83db984cd2dfdc55dc52.jpg" width="1024" height="368" class="img_ev3q"></p>
<p>This model architecture can also be applied to the VQA task but requires some design modifications.</p>
<ul>
<li><strong>Model Design</strong>
<ul>
<li>Query Types: In addition to the 100 detection queries, query types for question types and queries for predicting question types are introduced. In GQA, these question types are defined as REL, OBJ, GLOBAL, CAT, and ATTR.</li>
<li>Training: Pretraining was conducted for 40 epochs, followed by fine-tuning on the unbalanced GQA split for 125 epochs and final fine-tuning on the balanced split for 10 epochs.</li>
<li>Loss Strategy: During the first 125 epochs, both detection loss and QA loss were trained, with QA loss given more weight.</li>
</ul>
</li>
</ul>
<p>The model utilizes object queries as learned encodings input into the decoder for object detection. During inference, specific parts of the model predict the question type and derive the answer from that part.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Unlike commonly used VQA v2, this paper uses GQA.</p><p>GQA and VQA v2 are widely used datasets for Visual Question Answering (VQA). While both focus on answering questions given an image, there are key differences:</p><ol>
<li>
<p><strong>Data Scale and Source</strong></p>
<ul>
<li>
<p>GQA: Based on the Visual Genome dataset, containing approximately 22 million question-answer pairs.</p>
</li>
<li>
<p>VQA v2: An improved version of the original VQA dataset, containing about 1.2 million question-answer pairs, based on MS COCO and Abstract Scenes datasets.</p>
</li>
</ul>
</li>
<li>
<p><strong>Nature of Questions and Answers</strong></p>
<ul>
<li>
<p>GQA: Focuses on complex and compositional questions, often involving multiple objects and their relationships. Answers are usually more descriptive and can be multi-word responses.</p>
</li>
<li>
<p>VQA v2: More diverse, with questions ranging from very simple (e.g., &quot;What color is this?&quot;) to more complex. Answers are usually one or two words.</p>
</li>
</ul>
</li>
<li>
<p><strong>Data Imbalance</strong></p>
<ul>
<li>
<p>GQA: Designed to address some imbalance issues in VQA that might lead models to guess answers without truly understanding the image content.</p>
</li>
<li>
<p>VQA v2: Introduced challenging counterexample images to address data bias issues found in the original dataset.</p>
</li>
</ul>
</li>
<li>
<p><strong>Scene Graphs</strong></p>
<ul>
<li>
<p>GQA: Contains rich scene graphs, detailing the types, attributes, and relationships of objects in the images.</p>
</li>
<li>
<p>VQA v2: Does not include built-in scene graphs, but researchers can use other data sources or techniques to provide this information.</p>
</li>
</ul>
</li>
<li>
<p><strong>Task Goals</strong></p>
<ul>
<li>
<p>GQA: In addition to basic VQA tasks, GQA focuses on multi-modal reasoning, pushing models to deeply understand image content and question context.</p>
</li>
<li>
<p>VQA v2: Primarily focuses on basic VQA tasks, aiming to improve model performance and address data bias issues.</p>
</li>
</ul>
</li>
</ol><p>In summary, GQA tends to provide more complex questions and answers, with richer object and relationship descriptions, while VQA v2 is more diverse and focuses on addressing data bias issues.</p></div></div>
</li>
<li>
<p><strong>Performance Comparison</strong></p>
<p><img decoding="async" loading="lazy" alt="Performance Comparison" src="/en/assets/images/mdetr_7-e8febec9925a06c95bcae4030fb3b9df.jpg" width="860" height="576" class="img_ev3q"></p>
<ul>
<li>Using a model with a ResNet-101 backbone, its performance exceeds that of LXMERT and VL-T5.</li>
<li>The model&#x27;s performance even surpasses OSCAR, which uses more pre-training data.</li>
<li>The MDETR model with an EfficientNet-B5 backbone achieves even higher performance, as shown in the table.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="few-shot-capabilities">Few-Shot Capabilities?<a class="hash-link" aria-label="Direct link to Few-Shot Capabilities?" title="Direct link to Few-Shot Capabilities?" href="/en/papers/mdetr/#few-shot-capabilities">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Few-Shot Capabilities" src="/en/assets/images/mdetr_8-b119933acbc0bffe8251f1e7133e3b5c.jpg" width="812" height="400" class="img_ev3q"></p>
<p>Inspired by the success of CLIP in zero-shot image classification, the authors explored how to fine-tune the pretrained MDETR model for object detection with a small amount of labeled data. Unlike CLIP, MDETR&#x27;s pretraining dataset does not ensure all target classes are balanced. This means that in its dataset, no bounding boxes are aligned with text, leading the model always to predict a box for the given text.</p>
<p>Due to this design, MDETR cannot be evaluated in a true zero-shot transfer setting. Therefore, the authors chose an alternative strategy, evaluating in a few-shot setting. The LVIS dataset was chosen for this experiment, containing 1.2k categories, most of which have very few training samples, exhibiting a long-tail distribution.</p>
<p>To adapt to this distribution, MDETR&#x27;s training strategy was: for each positive class, it used the image and the class name as the training instance while using all annotations for that class. For negative classes, only the class name and empty annotations were provided. During inference, MDETR queried for each possible class name and merged all detected boxes across text prompts.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Example:</p><p>Assume we have a simple dataset with three classes: &quot;dog,&quot; &quot;cat,&quot; and &quot;fish.&quot;</p><p>Our labeled data includes:</p><p>An image showing a dog, labeled &quot;dog.&quot;
Another image showing a cat and a fish, labeled &quot;cat&quot; and &quot;fish,&quot; respectively.
Based on MDETR&#x27;s training strategy:</p><p>For the first image:</p><p>Since it contains a &quot;dog,&quot; this image and the text &quot;dog&quot; are used as the training instance with the label &quot;dog.&quot;
Since there are no &quot;cats&quot; or &quot;fish&quot; in the image, the class names &quot;cat&quot; and &quot;fish&quot; are provided without labels (i.e., empty labels).
For the second image:</p><p>Since it contains a &quot;cat&quot; and a &quot;fish,&quot; the image, and the texts &quot;cat&quot; and &quot;fish&quot; are used as training instances with the labels &quot;cat&quot; and &quot;fish.&quot;
Since there are no &quot;dogs&quot; in the image, the class name &quot;dog&quot; is provided without a label (i.e., empty label).
During inference, MDETR queries each class name &quot;dog,&quot; &quot;cat,&quot; and &quot;fish&quot; and merges the results detected across text prompts. For example, if it detects a box when querying &quot;dog&quot; and &quot;fish&quot; but none when querying &quot;cat,&quot; the final result will contain boxes for &quot;dog&quot; and &quot;fish.&quot;</p></div></div>
<p>The authors fine-tuned MDETR on three subsets of the LVIS dataset (1%, 10%, and 100% of images). The results were compared with two baselines: a Mask-RCNN trained on the full LVIS training set and a DETR pretrained on MSCOCO and fine-tuned on LVIS subsets. Surprisingly, even with only 1% of the data, MDETR leveraged its text pretraining to outperform fully fine-tuned DETR on rare categories.</p>
<p>Moreover, a significant observation was that fine-tuning on the full training data led to a sharp drop in detection performance on rare objects, from 20.9 AP with 10% data to 7.5 AP with 100% data. This steep decline is likely due to extreme class imbalance in the data.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" href="/en/papers/mdetr/#conclusion">​</a></h2>
<p>One of the most appealing features of MDETR is its &quot;complete differentiability.&quot;</p>
<p>This design allows the entire model to be trained end-to-end, fostering tighter model collaboration and potentially improving overall performance and training efficiency. Additionally, its practical performance on multiple datasets has demonstrated impressive results, establishing its position in the multi-modal learning domain.</p>
<p>Furthermore, MDETR&#x27;s versatility is a major highlight. Not only does it excel in modulated detection, but it has also proven its value in downstream applications such as few-shot detection and visual question answering.</p>
<p>MDETR provides a pathway, with its non-reliance on black-box object detectors potentially inspiring more researchers to create accurate and efficient models.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Why didn&#x27;t previous papers string Faster RCNN together? Complete differentiability, how great would that be?</p><p>While a fully differentiable model sounds appealing, it can also require more computational resources. Especially if you do it directly and brutally without clever design, there&#x27;s a high probability that the model will retaliate against you:</p><ul>
<li><strong>It won&#x27;t train.</strong></li>
</ul><p>When the entire model is differentiable, its internal structure is likely complex, meaning higher computational costs and more training difficulty. Researchers might need to spend more time tuning hyperparameters, which might not be feasible for every team.</p></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-07-15T04:47:10.000Z" itemprop="dateModified">Jul 15, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/efficientnet-v2/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[21.04] EfficientNet-V2</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/roformer/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[21.04] RoFormer</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#the-art-of-continuity">The Art of Continuity</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#problem-definition">Problem Definition</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#solution">Solution</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#mdetr-model-design">MDETR Model Design</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#training-method">Training Method</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#contrastive-alignment">Contrastive Alignment</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#all-losses">All Losses</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#dataset">Dataset</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#technical-details">Technical Details</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#discussion">Discussion</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#downstream-task-performance-analysis">Downstream Task Performance Analysis</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#few-shot-capabilities">Few-Shot Capabilities?</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/mdetr/#conclusion">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>