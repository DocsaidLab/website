<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-object-detection/yolov4/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[20.04] YOLOv4 | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/object-detection/yolov4/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[20.04] YOLOv4 | DOCSAID"><meta data-rh=true name=description content="Model Design Overview"><meta data-rh=true property=og:description content="Model Design Overview"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/object-detection/yolov4/><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov4/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/object-detection/yolov4/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/object-detection/yolov4/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov4/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/object-detection-13","name":"Object Detection (13)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/object-detection/yolov4/","name":"[20.04] YOLOv4","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.7747fe92.js defer></script><script src=/en/assets/js/main.46218941.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/object-detection/yolov4/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/object-detection/yolov4/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/object-detection/yolov4/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-14>Contrastive Learning (14)</a><button aria-label="Expand sidebar category 'Contrastive Learning (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-42>Face Anti-Spoofing (42)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (42)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/object-detection-13>Object Detection (13)</a><button aria-label="Collapse sidebar category 'Object Detection (13)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov1/>[15.06] YOLOv1</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/ssd/>[15.12] SSD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov2/>[16.12] YOLOv2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/retinanet/>[17.08] RetinaNet</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov3/>[18.04] YOLOv3</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/object-detection/yolov4/>[20.04] YOLOv4</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/detr/>[20.05] DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/deformable-detr/>[20.10] Deformable DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/fast-detr/>[21.01] Fast DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov7/>[22.07] YOLOv7</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov6/>[22.09] YOLOv6</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-world/>[24.01] YOLO-World</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-tiny/>[24.12] YOLO-Tiny</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/retail-product-1>Retail Product (1)</a><button aria-label="Expand sidebar category 'Retail Product (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-13>Vision Transformers (13)</a><button aria-label="Expand sidebar category 'Vision Transformers (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 225 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/object-detection-13><span>Object Detection (13)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[20.04] YOLOv4</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[20.04] YOLOv4</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-design-overview>Model Design Overview<a href=#model-design-overview class=hash-link aria-label="Direct link to Model Design Overview" title="Direct link to Model Design Overview">​</a></h2>
<p><a href=https://arxiv.org/abs/2004.10934 target=_blank rel="noopener noreferrer"><strong>YOLOv4: Optimal Speed and Accuracy of Object Detection</strong></a></p>
<hr>
<p>This paper adopts an industrial style, reading like a user manual for an object detection model.</p>
<p>Let's start from the overall design and learn how to build a good object detection model.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>The original YOLO model author only developed up to v3; subsequent versions are inherited by other branches. Therefore, the later version numbers are not necessarily related, nor are their publication years strictly chronological.<p>YOLOv4 comes from a development team in Taiwan.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-architecture-overview>Model Architecture Overview<a href=#model-architecture-overview class=hash-link aria-label="Direct link to Model Architecture Overview" title="Direct link to Model Architecture Overview">​</a></h2>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=object-detection src=/en/assets/images/img2-617b6c380cf668d031bc474aa9bd9e93.jpg width=1588 height=770 class=img_ev3q></figure></div>
<p>An object detector typically consists of four main modules: <code>Input</code>, <code>Backbone</code>, <code>Neck</code>, and <code>Head</code>.</p>
<p>Each module has its own role, connecting with others while allowing flexible combinations, like stacking building blocks.</p>
<ul>
<li>
<p><strong>Input</strong></p>
<p>Besides simple image input, it may include various enhancement signals such as multi-scale image pyramids, data augmentation, image patching, or resolution changes. The goal is to strengthen the model’s ability to recognize targets starting from the input stage.</p>
</li>
<li>
<p><strong>Backbone</strong></p>
<p>The main feature extraction module, mostly adapted from classic image classification architectures like VGG, ResNet, DenseNet, as well as detection-tailored variants such as CSPDarknet and DetNet. Its purpose is to convert input images into deep feature maps containing semantic and visual information.</p>
</li>
<li>
<p><strong>Neck</strong></p>
<p>Attempts to integrate feature maps from different levels, balancing shallow localization information with deep semantic features. From FPN to PAN, then BiFPN and NAS-FPN, each generation aims for better fusion of information flow and transmission efficiency.</p>
</li>
<li>
<p><strong>Head</strong></p>
<p>The final classification and bounding box regression module, divided into dense prediction (e.g., YOLO, SSD) and sparse prediction (e.g., R-CNN series). This is the main battlefield for performance and accuracy.</p>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=two-design-paradigms>Two Design Paradigms<a href=#two-design-paradigms class=hash-link aria-label="Direct link to Two Design Paradigms" title="Direct link to Two Design Paradigms">​</a></h2>
<p>Object detection architectures have gradually diverged into two mainstream strategies: two-stage and one-stage.</p>
<ul>
<li>
<p><strong>Two-stage architectures</strong>, such as Faster R-CNN and Libra R-CNN, first generate region proposals, then perform classification and bounding box regression. This approach provides higher accuracy and stronger region modeling ability, suitable for tasks requiring precise detection. Anchor-free versions like RepPoints have appeared later to alleviate anchor design limitations.</p>
</li>
<li>
<p><strong>One-stage architectures</strong>, like YOLO, SSD, and RetinaNet, perform dense predictions directly on the whole image without generating region proposals, trading for higher efficiency and faster inference. With the emergence of anchor-free designs (e.g., CenterNet, FCOS, CornerNet), these methods have begun challenging the accuracy advantage traditionally held by two-stage architectures.</p>
</li>
</ul>
<p>The divergence between these two routes essentially reflects the trade-off between "speed" and "accuracy," and YOLOv4’s design attempts to find the best balance on this spectrum.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=neck-for-feature-fusion>Neck for Feature Fusion<a href=#neck-for-feature-fusion class=hash-link aria-label="Direct link to Neck for Feature Fusion" title="Direct link to Neck for Feature Fusion">​</a></h2>
<p>With the increasing depth and breadth of the Backbone, effectively integrating multi-level features becomes critical for subsequent predictions.</p>
<p>This is the original purpose of the Neck module: to complement semantic information and localization cues, providing the Head with input rich in semantics and spatial details. Common approaches include:</p>
<ul>
<li><strong>FPN</strong> (Feature Pyramid Network) pioneered the top-down fusion structure, progressively upsampling and merging high-level semantic signals.</li>
<li><strong>PAN</strong> (Path Aggregation Network) adds a bottom-up path to enhance shallow feature feedback.</li>
<li><strong>BiFPN / NAS-FPN</strong> further pursue efficiency and performance extremes, using learnable weights for bidirectional fusion and neural architecture search (NAS) for optimal configurations.</li>
<li><strong>ASPP / RFB / SAM</strong> modules strengthen feature representation through spatial pyramids, receptive field design, or attention mechanisms.</li>
</ul>
<p>The core challenge behind these designs is how to improve multi-scale understanding and feature expressiveness without excessively sacrificing speed.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=backbones-designed-for-detection>Backbones Designed for Detection<a href=#backbones-designed-for-detection class=hash-link aria-label="Direct link to Backbones Designed for Detection" title="Direct link to Backbones Designed for Detection">​</a></h2>
<p>As model design moves towards modularity and interchangeability, some researchers revisit the backbone itself, redesigning feature extractors specifically tailored for detection tasks.</p>
<ul>
<li><strong>DetNet</strong> and <strong>DetNAS</strong> shift the backbone from classification-oriented to detection-oriented, emphasizing maintaining high resolution and detection-awareness.</li>
<li><strong>SpineNet</strong> highlights diverse data flows and flexible feature combinations, integrating NAS to find the best network configuration.</li>
<li><strong>HitDetector</strong> designs the entire pipeline from input to prediction based on task requirements, allowing modules to synergize effectively.</li>
</ul>
<p>This direction signifies that task demands have shifted from image classification to localization and bounding box regression, and simply applying classification backbones no longer meets detection complexity. Therefore, redesigning the backbone may be a more efficient solution.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=training-optimization-techniques>Training Optimization Techniques<a href=#training-optimization-techniques class=hash-link aria-label="Direct link to Training Optimization Techniques" title="Direct link to Training Optimization Techniques">​</a></h2>
<p>In object detection training, a class of techniques collectively called "Bag of Freebies" is widely used.</p>
<p>These refer to techniques that <strong>only increase training cost but do not affect inference efficiency</strong>. In other words, their purpose is to improve model accuracy and generalization without compromising inference speed.</p>
<p>These techniques can be categorized into three main types: data augmentation, label processing, and loss function optimization.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=data-augmentation>Data Augmentation<a href=#data-augmentation class=hash-link aria-label="Direct link to Data Augmentation" title="Direct link to Data Augmentation">​</a></h3>
<p>The generalization ability of object detection models largely depends on the diversity of input data. To enable the model to adapt to images in different environments, data augmentation is the most common and effective strategy.</p>
<ul>
<li><strong>Pixel-level transformations</strong>: including brightness, contrast, saturation, hue, noise distortions, as well as geometric transformations like random scaling, cropping, flipping, and rotation. These methods preserve original pixel information but change their arrangement and visual presentation.</li>
<li><strong>Occlusion-based augmentation</strong>: to simulate occlusion scenarios, methods like Random Erase, CutOut, Hide-and-Seek, and GridMask randomly mask image regions, training the model to recognize targets under occlusion.</li>
<li><strong>Feature masking</strong>: similar concepts apply on feature maps, such as DropOut, DropConnect, and DropBlock, enhancing the model’s robustness and stability on intermediate representations.</li>
<li><strong>Mixing multiple images</strong>: strategies like MixUp and CutMix blend or concatenate two images and adjust labels with weighted or area-based assignments, further improving model understanding of complex scenes.</li>
<li><strong>Style transfer</strong>: Style Transfer GANs are used to generate images with different styles to reduce CNN over-reliance on specific materials and textures.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=label-smoothing-and-knowledge-distillation>Label Smoothing and Knowledge Distillation<a href=#label-smoothing-and-knowledge-distillation class=hash-link aria-label="Direct link to Label Smoothing and Knowledge Distillation" title="Direct link to Label Smoothing and Knowledge Distillation">​</a></h3>
<p>Beyond input image processing, label manipulation is also part of the free optimization techniques.</p>
<p>In classification tasks, labels are usually presented in a one-hot format. Although these "hard labels" are clear, they lack semantic-level soft flexibility. To address this, Label Smoothing was introduced to convert original labels into "soft labels," preventing the model from being overconfident in any one class and improving its stability on unseen data.</p>
<p>A further approach involves knowledge distillation, where a higher-performing Teacher model provides more refined output distributions. Through a Label Refinement Network, the student model is guided to learn the relative relationships between classes.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=loss-function-optimization>Loss Function Optimization<a href=#loss-function-optimization class=hash-link aria-label="Direct link to Loss Function Optimization" title="Direct link to Loss Function Optimization">​</a></h3>
<p>Finally, for the crucial bounding box regression problem in object detection, the traditional approach uses MSE or L1/L2 Loss to directly regress coordinate values (e.g., center points or diagonals). However, such methods neglect the geometric structure of the entire box and are easily affected by object scale variations.</p>
<p>Therefore, IoU Loss has become the mainstream choice in recent years, using the overlap area between predicted and ground-truth boxes as the core of the loss function. It offers scale invariance and semantic consistency advantages.</p>
<p>Several improved versions have since emerged:</p>
<ul>
<li><strong>GIoU (Generalized IoU)</strong>: incorporates the smallest enclosing box to solve the no-gradient issue when boxes do not overlap.</li>
<li><strong>DIoU (Distance IoU)</strong>: adds center point distance to enhance localization accuracy.</li>
<li><strong>CIoU (Complete IoU)</strong>: considers overlap, center distance, and aspect ratio simultaneously, achieving the best overall performance and faster convergence.</li>
</ul>
<p>These improvements in regression losses not only stabilize convergence but also elevate localization precision, becoming indispensable for high-performance detectors.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=sophisticated-module-design>Sophisticated Module Design<a href=#sophisticated-module-design class=hash-link aria-label="Direct link to Sophisticated Module Design" title="Direct link to Sophisticated Module Design">​</a></h2>
<p>In contrast to Bag of Freebies, another accuracy-boosting strategy is called <strong>Bag of Specials</strong>.</p>
<p>These methods typically slightly increase inference cost but <strong>the accuracy gains far outweigh the additional computational expense</strong>, making them highly cost-effective techniques in object detection.</p>
<p>These techniques can be categorized into four aspects: <strong>receptive field expansion, attention mechanisms, feature integration modules, and non-maximum suppression (NMS) post-processing</strong>, plus one often overlooked but highly influential aspect: <strong>activation function design</strong>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=receptive-field-expansion>Receptive Field Expansion<a href=#receptive-field-expansion class=hash-link aria-label="Direct link to Receptive Field Expansion" title="Direct link to Receptive Field Expansion">​</a></h3>
<p>To help the model perceive contextual information earlier and improve spatial understanding, many modules are designed to expand the receptive field:</p>
<ul>
<li><strong>SPP (Spatial Pyramid Pooling)</strong> module originates from the classical SPM concept, initially used in image classification to build multi-scale region representations. YOLOv3 integrated it into the convolutional network as multi-scale MaxPooling (e.g., k = 1, 5, 9, 13) concatenations, greatly expanding the receptive field without changing spatial dimensions. On YOLOv3-608, it only adds 0.5% computation but improves AP50 by 2.7%.</li>
<li><strong>ASPP (Atrous Spatial Pyramid Pooling)</strong> uses multiple 3×3 atrous convolutions with different dilation rates, achieving multi-scale receptive field spatial awareness.</li>
<li><strong>RFB (Receptive Field Block)</strong> goes further by parallel computation of multiple convolution groups with varying dilation rates, achieving denser and wider spatial coverage. On the SSD architecture, it adds only 7% inference time but brings a 5.7% AP50 improvement.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=attention-mechanisms>Attention Mechanisms<a href=#attention-mechanisms class=hash-link aria-label="Direct link to Attention Mechanisms" title="Direct link to Attention Mechanisms">​</a></h3>
<p>Attention modules help the model dynamically adjust the importance of signals and have become core techniques widely applied in visual tasks:</p>
<ul>
<li><strong>SE (Squeeze-and-Excitation)</strong> module focuses on channel-level reweighting, helping the model concentrate more on discriminative feature dimensions but has relatively higher inference overhead on GPUs.</li>
<li><strong>SAM (Spatial Attention Module)</strong> introduces attention at the spatial level, performing spatial weighting on input feature maps with very low cost and almost no effect on GPU inference speed.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=cross-scale-integration>Cross-Scale Integration<a href=#cross-scale-integration class=hash-link aria-label="Direct link to Cross-Scale Integration" title="Direct link to Cross-Scale Integration">​</a></h3>
<p>Traditionally, skip connections or hypercolumns concatenate shallow and deep features. With the rise of multi-scale fusion architectures like FPN, more efficient fusion modules have been proposed:</p>
<ul>
<li><strong>SFAM</strong>: strengthens channel attention based on the SE module.</li>
<li><strong>ASFF (Adaptive Spatial Feature Fusion)</strong>: determines fusion weights of different scales through point-wise softmax.</li>
<li><strong>BiFPN</strong>: proposes multi-input weighted residual connections, achieving learned fusion across scales, balancing accuracy and efficiency well.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=activation-function-evolution>Activation Function Evolution<a href=#activation-function-evolution class=hash-link aria-label="Direct link to Activation Function Evolution" title="Direct link to Activation Function Evolution">​</a></h3>
<p>A good activation function enables more stable gradient propagation without additional burden:</p>
<ul>
<li><strong>ReLU</strong> solved the gradient vanishing problem of early sigmoid/tanh.</li>
<li><strong>LReLU / PReLU</strong> addressed the zero-gradient issue in negative ranges.</li>
<li><strong>ReLU6 / hard-Swish</strong> were tailored for quantized networks.</li>
<li><strong>SELU</strong> supports self-normalization.</li>
<li><strong>Swish / Mish</strong> are smooth, differentiable functions improving convergence and accuracy in deep networks.</li>
</ul>
<p>Though these activation functions are micro-designs, they often accumulate significant performance gains in large networks.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=nms-post-processing>NMS Post-processing<a href=#nms-post-processing class=hash-link aria-label="Direct link to NMS Post-processing" title="Direct link to NMS Post-processing">​</a></h3>
<p>Non-maximum suppression (NMS) filters redundant predicted boxes and is the final step in object detection:</p>
<ul>
<li><strong>Traditional NMS</strong> keeps the best box based on IoU and confidence score ranking but cannot handle confidence degradation in occluded objects well.</li>
<li><strong>Soft-NMS</strong> attempts to reduce the overly harsh suppression by penalizing scores instead of outright removal.</li>
<li><strong>DIoU-NMS</strong> incorporates center distance information, making the selection more geometrically intuitive.</li>
</ul>
<p>However, with the rise of anchor-free architectures, some models (e.g., FCOS) have removed NMS entirely, relying on loss design or post-processing conditions to finalize predictions.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=solving-the-problem>Solving the Problem<a href=#solving-the-problem class=hash-link aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<p>Now, we finally get to look at YOLOv4 itself.</p>
<p>With the introduction of all the above methods, it is clear the authors aimed to find an architecture that is both fast and effective.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=architecture-choice>Architecture Choice<a href=#architecture-choice class=hash-link aria-label="Direct link to Architecture Choice" title="Direct link to Architecture Choice">​</a></h3>
<p>When designing the Backbone, the authors started from the core concept that "classification models ≠ detection models," re-evaluating various architectures’ performance across different tasks.</p>
<p>They found that although <strong>CSPResNeXt50</strong> performs excellently on ImageNet, <strong>CSPDarknet53</strong> outperforms it on the MS COCO detection task. This is because detection tasks demand more rigor than classification:</p>
<ul>
<li><strong>Higher input resolution:</strong> To recognize small objects.</li>
<li><strong>Deeper layers and larger receptive fields:</strong> To cover broader contextual relationships.</li>
<li><strong>More parameter capacity:</strong> To handle multi-object, multi-scale scenarios simultaneously.</li>
</ul>
<p>For example, CSPResNeXt50 has only 16 layers of 3×3 convolutions with a receptive field of 425×425; CSPDarknet53 has 29 layers and a receptive field of 725×725, combined with a larger parameter count (27.6M), making it better suited for complex detection scenes.</p>
<p><img decoding=async loading=lazy alt=arch-choose src=/en/assets/images/img3-20fd8ec8a502602b1f1adf73ba9755eb.jpg width=1634 height=258 class=img_ev3q></p>
<p>Therefore, YOLOv4 selects <strong>CSPDarknet53</strong> as the backbone, enhances the receptive field with an <strong>SPP module</strong>, aggregates multi-level features through <strong>PANet</strong>, and finally connects to the <strong>YOLOv3 Head</strong> to complete predictions.</p>
<p>The core configuration of this architecture is:</p>
<ul>
<li><strong>Backbone:</strong> CSPDarknet53</li>
<li><strong>Neck:</strong> SPP + PANet</li>
<li><strong>Head:</strong> YOLOv3 (anchor-based)</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=training-strategy-choice>Training Strategy Choice<a href=#training-strategy-choice class=hash-link aria-label="Direct link to Training Strategy Choice" title="Direct link to Training Strategy Choice">​</a></h3>
<p>YOLOv4’s training strategy builds on two classic concepts:</p>
<ul>
<li><strong>Bag of Freebies (BoF):</strong> Adds training cost without inference overhead</li>
<li><strong>Bag of Specials (BoS):</strong> Slightly increases inference cost but significantly boosts accuracy</li>
</ul>
<p>In training, YOLOv4 discards activations hard to converge on (e.g., PReLU, SELU) and quantization-tailored ReLU6, opting instead for a balanced-effect <strong>Mish activation</strong>; for normalization, it replaces SyncBN, which requires multi-GPU, with a design better suited for single-GPU training called <strong>Cross mini-Batch Normalization (CmBN)</strong>.</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=cmbn src=/en/assets/images/img5-caa9f54d9cc87b3e41001d71b8d0bea2.jpg width=936 height=664 class=img_ev3q></figure></div>
<p>In deep learning, Batch Normalization (BN) is a critical technique for stable training.</p>
<p>However, BN implicitly assumes each mini-batch is large enough to obtain representative mean and variance statistics. When model size grows, GPU memory limits, or single-GPU setups are used, mini-batch sizes often become small, drastically reducing BN’s effectiveness and causing unstable convergence.</p>
<p>Previously, SyncBN (Cross-GPU BN) was proposed to share statistics across GPUs to improve stability, but it requires multi-GPU hardware, unsuitable for single-GPU training.</p>
<p>CmBN introduces a "cross mini-batch" statistical aggregation mechanism:</p>
<p>Suppose a data augmentation method like Mosaic is used, composing four images into one training image. CmBN treats this batch, made from different source images, as an "expanded sample set." When calculating BN statistics, it separately aggregates stats from these sub-samples and then averages them.</p>
<p>In other words, a batch contains multiple mini-batch statistical cues, making BN less dependent on the data bias of a single sub-sample. This strategy resembles a data-level "aggregation correction" for small batch sizes, improving BN’s generalization stability without multi-GPU synchronization.</p>
<p>Additionally, DropBlock is selected as the main regularization method, combined with CutMix, MixUp, Label Smoothing, and other augmentation strategies, forming a representative free enhancement toolkit during training.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=additional-enhancement-designs>Additional Enhancement Designs<a href=#additional-enhancement-designs class=hash-link aria-label="Direct link to Additional Enhancement Designs" title="Direct link to Additional Enhancement Designs">​</a></h3>
<p>YOLOv4 incorporates three key optimizations in training:</p>
<ol>
<li>
<p><strong>Mosaic Data Augmentation:</strong></p>
<p>Mixes four images into one to increase scene diversity and lets BatchNorm process more image information simultaneously, improving stability under small batches.</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=mosaic src=/en/assets/images/img4-aea38baff57d8f2756b5a4cd8193aca1.jpg width=928 height=572 class=img_ev3q></figure></div>
</li>
<li>
<p><strong>SAT (Self-Adversarial Training):</strong></p>
<p>At training start, the model adversarially modifies its own images and learns to recognize these disguised images, enhancing robustness against occlusion and camouflage attacks.</p>
</li>
<li>
<p><strong>Module Improvements:</strong></p>
<ul>
<li>
<p>SAM changed to point-wise attention, increasing focus on fine detail.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=sam src=/en/assets/images/img6-9d0a27036a4b6585933ac15bf5728997.jpg width=932 height=620 class=img_ev3q></figure></div>
</li>
<li>
<p>PANet’s shortcut connection changed to concatenation, strengthening feature fusion integrity.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=pan src=/en/assets/images/img6_1-c7ed42bb754d25de6b2a0855ef37aba9.jpg width=936 height=456 class=img_ev3q></figure></div>
</li>
</ul>
</li>
</ol>
<p>These designs, from enhancing data representativeness and expanding model recognition boundaries to reshaping training stability, form a comprehensive optimization strategy adapted to single-GPU training and inference environments.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=component-overview-and-technology-choices>Component Overview and Technology Choices<a href=#component-overview-and-technology-choices class=hash-link aria-label="Direct link to Component Overview and Technology Choices" title="Direct link to Component Overview and Technology Choices">​</a></h3>
<p>Integrating the above designs, YOLOv4’s final module selection is:</p>
<p><strong>Architecture Components:</strong></p>
<ul>
<li><strong>Backbone:</strong> CSPDarknet53</li>
<li><strong>Neck:</strong> SPP, PAN</li>
<li><strong>Head:</strong> YOLOv3 Head</li>
</ul>
<p><strong>Training Techniques (BoF for Backbone & Detector):</strong></p>
<ul>
<li>CutMix / Mosaic data augmentation</li>
<li>DropBlock regularization</li>
<li>Label smoothing</li>
<li>CIoU Loss</li>
<li>CmBN</li>
<li>SAT</li>
<li>Grid sensitivity elimination strategy</li>
<li>Multiple anchors assigned to a single ground truth</li>
<li>Cosine annealing scheduler</li>
<li>Random training resolution</li>
<li>Optimized parameter combinations (via genetic algorithm)</li>
</ul>
<p><strong>Inference Techniques (BoS for Backbone & Detector):</strong></p>
<ul>
<li>Mish activation</li>
<li>CSP module</li>
<li>Multi-input weighted residual connections</li>
<li>SPP module</li>
<li>Improved SAM</li>
<li>PAN path aggregation</li>
<li>DIoU-NMS post-processing strategy</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=experimental-setup>Experimental Setup<a href=#experimental-setup class=hash-link aria-label="Direct link to Experimental Setup" title="Direct link to Experimental Setup">​</a></h3>
<p>To verify the actual impact of various training strategies on model performance, the authors conducted large-scale experiments on two standard datasets, training and evaluating separately for classification and detection tasks.</p>
<p>For the <strong>ImageNet (ILSVRC 2012 val)</strong> image classification experiments, the following default training settings were used:</p>
<ul>
<li><strong>Training steps:</strong> 8,000,000 steps</li>
<li><strong>Batch size / mini-batch size:</strong> 128 / 32</li>
<li><strong>Learning rate schedule:</strong> Polynomial decay, initial learning rate 0.1</li>
<li><strong>Warm-up steps:</strong> 1,000</li>
<li><strong>Momentum / Weight decay:</strong> 0.9 / 0.005</li>
</ul>
<p>For the <strong>Bag of Freebies (BoF)</strong> settings, the authors validated the following strategies:</p>
<ul>
<li>MixUp</li>
<li>CutMix</li>
<li>Mosaic</li>
<li>Blurring</li>
<li>Label smoothing regularization</li>
</ul>
<p>For the <strong>MS COCO (test-dev 2017)</strong> object detection experiments, the default training settings were:</p>
<ul>
<li>
<p><strong>Training steps:</strong> 500,500 steps</p>
</li>
<li>
<p><strong>Learning rate schedule:</strong> Step decay</p>
<ul>
<li>Initial learning rate: 0.01</li>
<li>Multiply by 0.1 at steps 400,000 and 450,000</li>
</ul>
</li>
<li>
<p><strong>Momentum / Weight decay:</strong> 0.9 / 0.0005</p>
</li>
<li>
<p><strong>Batch size / mini-batch size:</strong> 64 / 8 or 4 (adjusted according to model and memory capacity)</p>
</li>
</ul>
<p>Except for some hyperparameter search experiments using genetic algorithms, all other experiments used the same default settings.</p>
<p>In the hyperparameter search, the authors used the YOLOv3-SPP architecture with GIoU loss, searching over 300 epochs on a min-val 5k subset. The final best combination adopted was:</p>
<ul>
<li><strong>Learning rate:</strong> 0.00261</li>
<li><strong>Momentum:</strong> 0.949</li>
<li><strong>IoU assignment threshold:</strong> 0.213</li>
<li><strong>Loss normalization coefficient:</strong> 0.07</li>
</ul>
<p>All experiments were conducted on a <strong>single GPU</strong>, without using multi-GPU optimizations such as SyncBN.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=impact-of-different-features-during-classifier-training>Impact of Different Features During Classifier Training<a href=#impact-of-different-features-during-classifier-training class=hash-link aria-label="Direct link to Impact of Different Features During Classifier Training" title="Direct link to Impact of Different Features During Classifier Training">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="different classifier" src=/en/assets/images/img8-a960ee1d79a88fdb1ac53cd38f23af88.jpg width=956 height=508 class=img_ev3q></figure></div>
<p>The authors first investigated how different training strategies affect final accuracy during classifier training, focusing on common enhancement techniques: <strong>class label smoothing</strong>, various data augmentation methods (such as <strong>blurring, MixUp, CutMix, Mosaic</strong>), and different types of activation functions (<strong>Leaky ReLU, Swish, Mish</strong>).</p>
<p>As shown in the above table, the following features significantly improved model accuracy during classifier training:</p>
<ul>
<li><strong>CutMix data augmentation</strong></li>
<li><strong>Mosaic data augmentation</strong></li>
<li><strong>Class label smoothing regularization</strong></li>
<li><strong>Mish activation function</strong></li>
</ul>
<p>Therefore, in YOLOv4’s classifier training strategy, the chosen <strong>BoF-backbone (Bag of Freebies for classifier)</strong> includes:</p>
<ul>
<li><strong>CutMix</strong></li>
<li><strong>Mosaic</strong></li>
<li><strong>Class label smoothing</strong></li>
</ul>
<p>Meanwhile, according to experimental results, <strong>Mish activation</strong> was incorporated as a supplementary activation function option, working synergistically with the above strategies to further improve classification accuracy.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=impact-of-different-features-during-detector-training>Impact of Different Features During Detector Training<a href=#impact-of-different-features-during-detector-training class=hash-link aria-label="Direct link to Impact of Different Features During Detector Training" title="Direct link to Impact of Different Features During Detector Training">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="different feat" src=/en/assets/images/img10-9a0bab56bddc1ffb65d994542b7ba98d.jpg width=1224 height=668 class=img_ev3q></figure></div>
<p>The authors further evaluated the impact of different training strategies on detector accuracy, particularly systematic experiments on the <strong>Bag of Freebies for detector (BoF-detector)</strong> as shown above.</p>
<p>YOLOv4 greatly expanded the BoF list, focusing on training techniques that <strong>improve accuracy without sacrificing inference speed (FPS)</strong>.</p>
<p>Verified BoF-detector items include:</p>
<ul>
<li>
<p><strong>S: Grid Sensitivity Elimination</strong></p>
<p>In YOLOv3, target coordinates are defined by <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>b</mi><mi>x</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy=false>(</mo><msub><mi>t</mi><mi>x</mi></msub><mo stretchy=false>)</mo><mo>+</mo><msub><mi>c</mi><mi>x</mi></msub></mrow><annotation encoding=application/x-tex>b_x = \sigma(t_x) + c_x</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8444em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">b</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.03588em>σ</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">t</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">c</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>, where <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>c</mi><mi>x</mi></msub></mrow><annotation encoding=application/x-tex>c_x</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">c</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> is an integer. When <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>t</mi><mi>x</mi></msub></mrow><annotation encoding=application/x-tex>t_x</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7651em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">t</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> is extremely large or small, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>b</mi><mi>x</mi></msub></mrow><annotation encoding=application/x-tex>b_x</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8444em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">b</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> approaches the grid boundary (<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>c</mi><mi>x</mi></msub></mrow><annotation encoding=application/x-tex>c_x</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5806em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">c</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> or <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>c</mi><mi>x</mi></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding=application/x-tex>c_x + 1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">c</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>1</span></span></span></span>), causing detection difficulty near grid edges.</p>
<p>To address this, the authors multiply the sigmoid output by a factor greater than 1, effectively removing grid constraints on detection feasibility.</p>
</li>
<li>
<p><strong>M: Mosaic Data Augmentation</strong>
Combines four images into one training input, allowing the model to learn targets in varied scenes and scales, enhancing generalization.</p>
</li>
<li>
<p><strong>IT: IoU Threshold</strong>
Assigns multiple anchors to each ground truth where IoU(anchor, truth) > threshold to increase sample usage.</p>
</li>
<li>
<p><strong>GA: Genetic Algorithms</strong>
Uses genetic algorithms in early training (first 10%) to search optimal hyperparameters, improving convergence stability and model performance.</p>
</li>
<li>
<p><strong>LS: Class Label Smoothing</strong>
Applies label smoothing to classification to reduce model overconfidence and improve discrimination on fuzzy class boundaries.</p>
</li>
<li>
<p><strong>CBN: Cross mini-Batch Normalization (CmBN)</strong>
Aggregates statistics across entire batches instead of single mini-batches, boosting stability under small batch training.</p>
</li>
<li>
<p><strong>CA: Cosine Annealing Scheduler</strong>
Dynamically adjusts learning rate using a cosine function for smoother training curves and convergence.</p>
</li>
<li>
<p><strong>DM: Dynamic mini-batch Size</strong>
Dynamically increases mini-batch size during low-resolution training stages, aligned with random input size training.</p>
</li>
<li>
<p><strong>OA: Optimized Anchors</strong>
Optimizes anchor locations and sizes based on input resolution (e.g., 512×512) to improve anchor assignment efficiency.</p>
</li>
<li>
<p><strong>BBox Regression Loss</strong>
Compares multiple bounding box loss functions, including GIoU, CIoU, DIoU, and traditional MSE, validating regression quality in different scenarios.</p>
</li>
</ul>
<hr>
<p>Additionally, the authors validated designs for the <strong>Bag of Specials for detector</strong>, shown below:</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=bos-detector src=/en/assets/images/img7-f147f96dfd51b07e810af6aef5ff2ce7.jpg width=964 height=312 class=img_ev3q></figure></div>
<p>Experimented items include:</p>
<ul>
<li><strong>PAN:</strong> Enhances parameter aggregation and feature flow</li>
<li><strong>RFB:</strong> Multi-scale dilated convolutions for receptive field expansion</li>
<li><strong>SAM:</strong> Spatial attention mechanism</li>
<li><strong>Gaussian YOLO (G):</strong> Predicts bounding boxes as Gaussian distributions instead of point estimates</li>
<li><strong>ASFF:</strong> Cross-scale attention-based fusion strategy</li>
</ul>
<p>According to results, combining <strong>SPP, PAN, and SAM</strong> achieves the best overall performance.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=impact-of-different-backbones>Impact of Different Backbones<a href=#impact-of-different-backbones class=hash-link aria-label="Direct link to Impact of Different Backbones" title="Direct link to Impact of Different Backbones">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=diff-backbone src=/en/assets/images/img9-d85f1976bf64e98971f602e38656c880.jpg width=1000 height=484 class=img_ev3q></figure></div>
<p>The authors explored the impact of different backbones on detector accuracy, as shown above.</p>
<p>Results indicate that <strong>the best-performing classifier model does not necessarily excel in object detection tasks</strong>.</p>
<p>Specifically, although <strong>CSPResNeXt50</strong> achieves higher classification accuracy than <strong>CSPDarknet53</strong> after various training enhancements, using these pretrained weights as detector backbones results in worse detection performance for CSPResNeXt50.</p>
<p>Conversely, <strong>CSPDarknet53</strong>, when trained with the same BoF and Mish activation, not only improves classifier accuracy but also yields better detection performance when used as a backbone.</p>
<p>This finding reveals a fundamental difference in feature learning requirements between classification and detection tasks.</p>
<p>The structural characteristics and receptive field design of CSPDarknet53 make it more suitable as a feature extractor for object detection.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=impact-of-different-mini-batch-sizes>Impact of Different Mini-Batch Sizes<a href=#impact-of-different-mini-batch-sizes class=hash-link aria-label="Direct link to Impact of Different Mini-Batch Sizes" title="Direct link to Impact of Different Mini-Batch Sizes">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=diff-batchsize src=/en/assets/images/img11-27d305262d83be0d9f354ff04ab81ca6.jpg width=988 height=448 class=img_ev3q></figure></div>
<p>Finally, the authors analyzed the effects of varying mini-batch sizes during training; results are shown above.</p>
<p>It is clear that <strong>after introducing BoF and BoS training strategies, mini-batch size barely affects the final detector performance</strong>.</p>
<p>This finding has significant practical implications.</p>
<p>Previously, improving training stability and model performance often required large GPU memory to support big batch sizes. This study shows that by combining Mosaic, CmBN, DropBlock, Label smoothing, SPP, PAN, and other training and architectural optimizations in YOLOv4, <strong>stable and excellent learning can be maintained even with small mini-batches</strong>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=benchmark>Benchmark<a href=#benchmark class=hash-link aria-label="Direct link to Benchmark" title="Direct link to Benchmark">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=benchmark src=/en/assets/images/img12-3d60e09de8eac6845341f4d3350b0b17.jpg width=1160 height=1080 class=img_ev3q></figure></div>
<p>YOLOv4’s final results, compared to other state-of-the-art object detectors, lie on the Pareto optimal curve between speed and accuracy, demonstrating the ability to balance efficiency and precision.</p>
<p>Compared to popular fast and high-accuracy models, YOLOv4 <strong>outperforms both</strong>, achieving high inference speed while matching or surpassing previous high-accuracy benchmarks.</p>
<p>Considering differences in GPU architectures used for inference time testing, the authors tested YOLOv4 on common GPUs across three major architectures:</p>
<ul>
<li><strong>Maxwell architecture:</strong> GTX Titan X (Maxwell), Tesla M40</li>
<li><strong>Pascal architecture:</strong> Titan X (Pascal), Titan Xp, GTX 1080 Ti, Tesla P100</li>
<li><strong>Volta architecture:</strong> Titan Volta, Tesla V100</li>
</ul>
<p>These experiments further confirm YOLOv4’s scalability and general applicability.</p>
<p>Whether on older or latest hardware, YOLOv4 maintains good computational efficiency and stable performance, reflecting its hardware-friendly design.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>YOLOv4 surpasses all existing mainstream models on the MS COCO AP50 metric while maintaining leading inference speed (FPS). More importantly, YOLOv4 can be trained and deployed on consumer-grade GPUs with only 8–16GB VRAM, significantly lowering technical barriers and enabling widespread applicability.</p>
<p>Every developer interested in object detection should definitely study this paper—it’s well worth the read.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-06-11T13:46:29.000Z itemprop=dateModified>Jun 11, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/object-detection/yolov3/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[18.04] YOLOv3</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/object-detection/detr/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[20.05] DETR</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#model-design-overview class="table-of-contents__link toc-highlight">Model Design Overview</a><li><a href=#model-architecture-overview class="table-of-contents__link toc-highlight">Model Architecture Overview</a><li><a href=#two-design-paradigms class="table-of-contents__link toc-highlight">Two Design Paradigms</a><li><a href=#neck-for-feature-fusion class="table-of-contents__link toc-highlight">Neck for Feature Fusion</a><li><a href=#backbones-designed-for-detection class="table-of-contents__link toc-highlight">Backbones Designed for Detection</a><li><a href=#training-optimization-techniques class="table-of-contents__link toc-highlight">Training Optimization Techniques</a><ul><li><a href=#data-augmentation class="table-of-contents__link toc-highlight">Data Augmentation</a><li><a href=#label-smoothing-and-knowledge-distillation class="table-of-contents__link toc-highlight">Label Smoothing and Knowledge Distillation</a><li><a href=#loss-function-optimization class="table-of-contents__link toc-highlight">Loss Function Optimization</a></ul><li><a href=#sophisticated-module-design class="table-of-contents__link toc-highlight">Sophisticated Module Design</a><ul><li><a href=#receptive-field-expansion class="table-of-contents__link toc-highlight">Receptive Field Expansion</a><li><a href=#attention-mechanisms class="table-of-contents__link toc-highlight">Attention Mechanisms</a><li><a href=#cross-scale-integration class="table-of-contents__link toc-highlight">Cross-Scale Integration</a><li><a href=#activation-function-evolution class="table-of-contents__link toc-highlight">Activation Function Evolution</a><li><a href=#nms-post-processing class="table-of-contents__link toc-highlight">NMS Post-processing</a></ul><li><a href=#solving-the-problem class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href=#architecture-choice class="table-of-contents__link toc-highlight">Architecture Choice</a><li><a href=#training-strategy-choice class="table-of-contents__link toc-highlight">Training Strategy Choice</a><li><a href=#additional-enhancement-designs class="table-of-contents__link toc-highlight">Additional Enhancement Designs</a><li><a href=#component-overview-and-technology-choices class="table-of-contents__link toc-highlight">Component Overview and Technology Choices</a><li><a href=#experimental-setup class="table-of-contents__link toc-highlight">Experimental Setup</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#impact-of-different-features-during-classifier-training class="table-of-contents__link toc-highlight">Impact of Different Features During Classifier Training</a><li><a href=#impact-of-different-features-during-detector-training class="table-of-contents__link toc-highlight">Impact of Different Features During Detector Training</a><li><a href=#impact-of-different-backbones class="table-of-contents__link toc-highlight">Impact of Different Backbones</a><li><a href=#impact-of-different-mini-batch-sizes class="table-of-contents__link toc-highlight">Impact of Different Mini-Batch Sizes</a><li><a href=#benchmark class="table-of-contents__link toc-highlight">Benchmark</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>