<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-object-detection/yolov7/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[22.07] YOLOv7 | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/object-detection/yolov7/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[22.07] YOLOv7 | DOCSAID"><meta data-rh=true name=description content="The Underdog’s Comeback"><meta data-rh=true property=og:description content="The Underdog’s Comeback"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/object-detection/yolov7/><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov7/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/object-detection/yolov7/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/object-detection/yolov7/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov7/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/object-detection","name":"Object Detection (16)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/object-detection/yolov7/","name":"[22.07] YOLOv7","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.ef02043f.css><script src=/en/assets/js/runtime~main.c1cb7176.js defer></script><script src=/en/assets/js/main.a0cc1a69.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/object-detection/yolov7/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/object-detection/yolov7/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/object-detection/yolov7/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning>Contrastive Learning (14)</a><button aria-label="Expand sidebar category 'Contrastive Learning (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-antispoofing>Face Anti-Spoofing (43)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (43)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/object-detection>Object Detection (16)</a><button aria-label="Collapse sidebar category 'Object Detection (16)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov1/>[15.06] YOLOv1</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/ssd/>[15.12] SSD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov2/>[16.12] YOLOv2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/retinanet/>[17.08] RetinaNet</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov3/>[18.04] YOLOv3</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov4/>[20.04] YOLOv4</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/detr/>[20.05] DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/deformable-detr/>[20.10] Deformable DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/smca-detr/>[21.01] SMCA DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/h-detr/>[22.07] H-DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/object-detection/yolov7/>[22.07] YOLOv7</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov6/>[22.09] YOLOv6</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-world/>[24.01] YOLO-World</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov9/>[24.02] YOLOv9</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov10/>[24.05] YOLOv10</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-tiny/>[24.12] YOLO-Tiny</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/retail-product>Retail Product (6)</a><button aria-label="Expand sidebar category 'Retail Product (6)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers>Vision Transformers (13)</a><button aria-label="Expand sidebar category 'Vision Transformers (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 235 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/object-detection><span>Object Detection (16)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[22.07] YOLOv7</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[22.07] YOLOv7</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=the-underdogs-comeback>The Underdog’s Comeback<a href=#the-underdogs-comeback class=hash-link aria-label="Direct link to The Underdog’s Comeback" title="Direct link to The Underdog’s Comeback">​</a></h2>
<p><a href=https://arxiv.org/abs/2207.02696 target=_blank rel="noopener noreferrer"><strong>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</strong></a></p>
<hr>
<p>The author team behind YOLOv7 comes from Academia Sinica, Taiwan.</p>
<p>Wait, why was v7 released two months earlier than v6? This is because many teams worldwide are competing to become the true successor of the original YOLO. Welcome to the star-studded YOLO universe!</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-definition>Problem Definition<a href=#problem-definition class=hash-link aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>The field of real-time object detection has always been balancing four factors: <strong>speed, accuracy, parameter count, and deployability</strong>.</p>
<p>These trade-offs are reordered depending on the scenario: in autonomous driving, latency is critical; in embedded devices, power consumption matters; and in medical imaging, accuracy is paramount.</p>
<p>In recent years, real-time detector design has generally followed two main paths:</p>
<p>The first is pursuing <strong>extreme lightweighting</strong>, targeting microprocessors and edge CPUs, such as MCUNet and NanoDet; the other focuses on <strong>squeezing performance from efficient architectures</strong>, like YOLOX and YOLOR, by backbone modification, neck optimization, and feature fusion design to improve inference speed and accuracy.</p>
<p>What these methods have in common is:</p>
<ul>
<li><strong>They almost all start from the inference architecture, trying to achieve better accuracy via faster pathways.</strong></li>
</ul>
<p>However, YOLOv7 takes a reverse path.</p>
<p>The authors propose a relatively less-explored angle in this paper:</p>
<blockquote>
<p><strong>What if we don’t change the model architecture but redesign the training pipeline instead?</strong></p>
</blockquote>
<p>This starting point gave birth to YOLOv7’s core concept: the <strong>trainable bag-of-freebies</strong>.</p>
<p>The so-called bag-of-freebies refers to techniques that increase training cost but do not add any inference cost.</p>
<p>YOLOv7’s innovation lies in upgrading these techniques into “trainable” modules, further boosting model performance without altering any inference structure.</p>
<p>Although this design sounds simple, it immediately raises two challenging questions at the structural level:</p>
<ol>
<li><strong>How should module re-parameterization be designed to benefit both training and inference simultaneously?</strong></li>
<li><strong>When using dynamic label assignment techniques, how to handle the alignment problem across multiple output layers?</strong></li>
</ol>
<p>These two issues were often dismissed as mere “training details” in the past, but along YOLOv7’s path, these <strong>implicit bottlenecks during training become the main battlefield of design</strong>.</p>
<p>This time, YOLOv7 shifts the focus from “inference performance” to “training efficiency.”</p>
<p>The order of thinking has quietly reversed at this moment.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-solving>Problem Solving<a href=#problem-solving class=hash-link aria-label="Direct link to Problem Solving" title="Direct link to Problem Solving">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=extended-elan>Extended-ELAN<a href=#extended-elan class=hash-link aria-label="Direct link to Extended-ELAN" title="Direct link to Extended-ELAN">​</a></h3>
<p><img decoding=async loading=lazy alt=Extended-ELAN src=/en/assets/images/img1-0c1dc7c792538843c31a3034ddab63b4.jpg width=1736 height=644 class=img_ev3q></p>
<p>The first key design introduced by YOLOv7 is the backbone architecture called <strong>E-ELAN (Extended Efficient Layer Aggregation Networks)</strong>.</p>
<p>Its predecessor is the ELAN architecture used in YOLOX, and the core design principle of E-ELAN is:</p>
<blockquote>
<p><strong>To improve parameter utilization and feature representation diversity without changing the gradient path length.</strong></p>
</blockquote>
<p>From the above figure, it is clear that E-ELAN fully preserves the original ELAN’s gradient propagation paths.</p>
<p>As we know, effective gradient paths in neural networks directly affect training efficiency and convergence capability.</p>
<p>While maintaining this main backbone unchanged, E-ELAN implements several key modifications:</p>
<ul>
<li><strong>Expand</strong>: Expands channels and cardinality via group convolution.</li>
<li><strong>Shuffle</strong>: Performs grouped rearrangement of feature maps.</li>
<li><strong>Merge</strong>: Fuses the grouped features to promote cross-group learning.</li>
</ul>
<p>These operations enable each block to learn more diverse features, while also increasing memory and computational density efficiency without compromising the original ELAN architecture’s stability.</p>
<p>Overall, E-ELAN is a technique that optimizes both <strong>computational density and learning diversity</strong> specifically for the backbone.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=scaling-for-cascaded-architectures>Scaling for Cascaded Architectures<a href=#scaling-for-cascaded-architectures class=hash-link aria-label="Direct link to Scaling for Cascaded Architectures" title="Direct link to Scaling for Cascaded Architectures">​</a></h3>
<p><img decoding=async loading=lazy alt="model scaling" src=/en/assets/images/img2-d8c07e4af3aa3116255e4428bb1c311c.jpg width=1880 height=396 class=img_ev3q></p>
<p>The second key design is a <strong>compound model scaling strategy for cascaded architectures</strong>.</p>
<p>Previously, the concept of model scaling was seen in EfficientNet, which scales network <strong>width, depth, and resolution</strong> simultaneously.</p>
<p>However, such scaling methods mostly target “single-path” architectures like ResNet or PlainNet.</p>
<p>YOLO series, on the other hand, is a typical “multi-branch, inter-layer cascaded” architecture. When you independently increase the depth of a certain block, it changes the input channels of the transition layer; once the channel numbers before and after are asymmetric, it causes wasted computational resources and memory fragmentation issues.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>When performing convolutions, GPUs/TPUs schedule computations based on fixed input channel counts and tensor shapes. Once certain layers have an explosive increase in channel numbers but subsequent operations do not effectively utilize these channels, it results in <strong>redundant computations</strong>, and at the same time reduces the operational efficiency of some core modules.<p>Modern deep learning systems (such as PyTorch and TensorRT) attempt to pre-allocate memory. If tensor sizes change frequently—for example, some blocks have channels of 128 while others have 256—this causes the memory allocator to create irregular memory blocks. Ultimately, this can lead to inefficient packing of GPU memory, resulting in waste or allocation failures.</div></div>
<p>Therefore, the authors propose the following solution:</p>
<blockquote>
<p><strong>When scaling depth, the width should be adjusted accordingly to maintain stable channel ratios.</strong></p>
</blockquote>
<p>This strategy avoids resource waste caused by asymmetric layer scaling and preserves the hardware-friendly characteristics of the original model design.</p>
<p>The specific approach is:</p>
<ol>
<li>First perform depth scaling by increasing the number of stacked layers within a block;</li>
<li>According to the changes in output channels after expansion, correspondingly scale the width of the transition layers;</li>
<li>Maintain the overall channel ratio unchanged to prevent misaligned memory allocation or computational bottlenecks on hardware.</li>
</ol>
<p>This is a <strong>coupled scaling method</strong> that considers parameter changes across different levels simultaneously, allowing model scaling while retaining efficient computation.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=trainable-bof>Trainable BoF<a href=#trainable-bof class=hash-link aria-label="Direct link to Trainable BoF" title="Direct link to Trainable BoF">​</a></h3>
<p>The <strong>trainable bag-of-freebies</strong> proposed by YOLOv7 is not merely a stacking of existing techniques but advances the training pipeline into the main arena of model optimization.</p>
<p>Let’s take a closer look at three key innovations.</p>
<ol>
<li>
<p><strong>Planned Re-parameterized Convolution</strong></p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=re-parameterized src=/en/assets/images/img3-4c2d0718489c323a7d343b9d41d19a56.jpg width=1072 height=824 class=img_ev3q></figure></div>
<p>Re-parameterized convolution (RepConv) is a training/inference separated structural design:</p>
<ul>
<li>During training, it contains three branches: a 3×3 convolution, a 1×1 convolution, plus an identity shortcut (direct input pass-through);</li>
<li>During inference, these three paths are merged into a single path, achieving zero additional cost.</li>
</ul>
<p>This design performs well in some flat structures (e.g., VGG), accelerating convergence and improving accuracy.</p>
<p>However, the authors discovered:</p>
<blockquote>
<p><strong>When applying the original RepConv in networks like ResNet or DenseNet, which already have shortcuts or feature concatenation, accuracy significantly drops.</strong></p>
</blockquote>
<p>This is because ResNet inherently includes a residual shortcut; adding RepConv’s identity branch causes overlapping gradients on multiple identity paths, generating interference.</p>
<p>In DenseNet’s case, its architecture concatenates outputs across layers; inserting identity branches in every layer disrupts the learning paths of feature diversity, preventing branches from learning distinct features.</p>
<p>Thus, the authors proposed:</p>
<blockquote>
<p><strong>Depending on internal network structure (whether residual/concat exists), decide whether to use RepConvN without the identity branch.</strong></p>
</blockquote>
<p>This approach is called Planned Re-parameterized Convolution, aiming for “planned replacement and configuration based on model structure.” For example:</p>
<ul>
<li>In a PlainNet (a basic network without residuals), using full RepConv with 3×3, 1×1, and identity branches is beneficial;</li>
<li>For ResNet or DenseNet, RepConvN (keeping only 3×3 + 1×1, removing the identity branch) should be used to avoid gradient transmission confusion.</li>
</ul>
</li>
</ol>
<hr>
<ol start=2>
<li>
<p><strong>Coarse-to-Fine Label Design</strong></p>
<p><img decoding=async loading=lazy alt=Coarse-to-Fine src=/en/assets/images/img4-355b1a556f4bd31cfedaf61f949df5d7.jpg width=1938 height=396 class=img_ev3q></p>
<p>When training multiple detection heads, the authors introduced an innovative supervision mechanism:</p>
<ul>
<li><strong>Coarse-to-Fine Label Assignment</strong>.</li>
</ul>
<p>This mechanism involves two important roles:</p>
<ul>
<li><strong>Lead Head</strong>: the main output head responsible for the final detection task;</li>
<li><strong>Auxiliary Head</strong>: the auxiliary output head that helps shallow layers converge.</li>
</ul>
<p>Traditional deep supervision assigns labels to each head independently, but here the authors propose a novel perspective:</p>
<blockquote>
<p><strong>Use the Lead Head’s predictions as the leader for overall training.</strong></p>
</blockquote>
<p>There are two design strategies in practice:</p>
<ul>
<li><strong>Lead Head Guided Label Assigner</strong>: generates soft labels based on Lead Head predictions plus ground truth, applied to all heads;</li>
<li><strong>Coarse-to-Fine Label Assigner</strong>: produces two kinds of soft labels—coarse and fine—assigned respectively to the Auxiliary Head and Lead Head, improving recall for the former and precision for the latter.</li>
</ul>
<p>The logic behind this strategy is:</p>
<ul>
<li><strong>Lead Head has strong learning ability and suits being the indicator;</strong></li>
<li><strong>Auxiliary Head has weaker learning ability and requires relaxed positive sample conditions to improve recall;</strong></li>
<li><strong>Inspired by residual learning, Lead Head focuses on parts not yet learned.</strong></li>
</ul>
<p>To prevent coarse labels from causing bad priors, the authors further restrict the decoder to progressively converge during training, forming a dynamically adjusted supervision mechanism.</p>
</li>
</ol>
<hr>
<ol start=3>
<li>
<p><strong>Other Trainable Freebies</strong></p>
<p>Finally, the authors integrate several previously considered “training tricks” and elevate them into <strong>trainable design units</strong>:</p>
<ul>
<li><strong>BatchNorm and convolution fusion</strong>: merging BatchNorm mean and variance into convolution weights and bias to simplify inference computations;</li>
<li><strong>YOLOR-style implicit knowledge injection</strong>: combining implicit vectors into convolution layers, precomputable at inference as fixed vector weights;</li>
<li><strong>EMA (Exponential Moving Average) model</strong>: smoothing training parameters via moving averages and using the EMA model for final inference to enhance stability and generalization.</li>
</ul>
<p>Though these techniques are not new, in YOLOv7 their role is redefined as <strong>accelerators for model training performance</strong>, no longer just black-box tuning tools.</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=experimental-setup>Experimental Setup<a href=#experimental-setup class=hash-link aria-label="Direct link to Experimental Setup" title="Direct link to Experimental Setup">​</a></h3>
<p>All YOLOv7 experiments are conducted on the <strong>Microsoft COCO 2017</strong> dataset, with the following emphasis:</p>
<ul>
<li><strong>Training from scratch</strong>, without any ImageNet pretrained weights;</li>
<li>Using <code>train2017</code> for training, <code>val2017</code> for validation and hyperparameter tuning, and <code>test2017</code> for final evaluation.</li>
</ul>
<p>Moreover, to cater to different application scenarios, the YOLOv7 series models are categorized as follows:</p>
<div style=white-space:nowrap;overflow-x:auto;font-size:1rem;line-height:0.8;justify-content:center;display:flex><table><thead><tr><th>Model Name<th>Design Goal<th>Scaling Technique<th>Notes<tbody><tr><td>v7-tiny<td>Edge devices<td>Lightweight design<td>Uses LeakyReLU<tr><td>v7<td>General GPU<td>Neck expansion + compound scaling<td>Uses SiLU<tr><td>v7-W6<td>High-performance GPU<td>Compound scaling + W6 design<td><tr><td>v7-E6/E6E<td>E-ELAN structure<td>Deepening & channel expansion + group shuffle<td>E6E is enhanced version<tr><td>v7-D6/X<td>Cloud models<td>-<td>-</table></div>
<p>All scaling strategies use the previously introduced compound model scaling method to avoid computational resource waste caused by channel mismatches.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=comparison-with-previous-yolo-versions>Comparison with Previous YOLO Versions<a href=#comparison-with-previous-yolo-versions class=hash-link aria-label="Direct link to Comparison with Previous YOLO Versions" title="Direct link to Comparison with Previous YOLO Versions">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=compare src=/en/assets/images/img5-0fb8f01bcbb1be815f52dc6ecb9b3c37.jpg width=1224 height=796 class=img_ev3q></figure></div>
<p>First, looking at the comparison with previous YOLO series and YOLOR results as shown above:</p>
<p>It is evident that YOLOv7 achieves the goal of being "<strong>smaller, faster, and more accurate</strong>" in nearly all comparison metrics, especially standing out in small models (tiny) and mid-tier models (W6).</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=comparison-with-latest-state-of-the-art-models>Comparison with Latest State-of-the-Art Models<a href=#comparison-with-latest-state-of-the-art-models class=hash-link aria-label="Direct link to Comparison with Latest State-of-the-Art Models" title="Direct link to Comparison with Latest State-of-the-Art Models">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=compare src=/en/assets/images/img6-a87123d1e13424ff90c1bb1b7769d07c.jpg width=1224 height=976 class=img_ev3q></figure></div>
<p>Regarding speed and accuracy trade-offs:</p>
<ul>
<li><strong>YOLOv7-tiny-SiLU</strong> vs <strong>YOLOv5-N (r6.1)</strong>: +127 fps, +10.7% AP accuracy improvement</li>
<li><strong>YOLOv7</strong> achieves <strong>51.4% AP</strong> at 161 fps, while PPYOLOE-L reaches the same accuracy at only <strong>78 fps</strong></li>
</ul>
<p>For high-end models comparison:</p>
<ul>
<li><strong>YOLOv7-X</strong> vs <strong>YOLOv5-L (r6.1)</strong>: 15 fps faster inference, 3.9% higher accuracy</li>
<li><strong>YOLOv7-X</strong> vs <strong>YOLOv5-X (r6.1)</strong>: 31 fps faster, 22% fewer parameters, 8% less computation, yet 2.2% AP improvement</li>
</ul>
<p>For large-scale models:</p>
<ul>
<li><strong>YOLOv7-E6</strong> vs <strong>YOLOv5-X6 (r6.1)</strong>: 0.9% AP improvement, with 45% fewer parameters, 63% less computation, and 47% faster</li>
<li><strong>YOLOv7-D6</strong> vs <strong>YOLOR-E6</strong>: comparable speed, 0.8% AP improvement</li>
<li><strong>YOLOv7-E6E</strong> vs <strong>YOLOR-D6</strong>: comparable speed, 0.3% AP improvement</li>
</ul>
<p>From these comparisons, several key observations emerge:</p>
<ol>
<li><strong>YOLOv7 maintains high efficiency across lightweight to large models;</strong></li>
<li>Even without pretrained weights, YOLOv7 outperforms most SOTA models;</li>
<li>Model scaling strategies and module training designs (e.g., RepConvN, Coarse-to-Fine Label Assigner) directly translate into practical performance gains.</li>
</ol>
<p>YOLOv7 is not just a showcase of training tricks but a comprehensive architectural design paradigm addressing real-world deployment needs.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=ablation-study-model-scaling>Ablation Study: Model Scaling<a href=#ablation-study-model-scaling class=hash-link aria-label="Direct link to Ablation Study: Model Scaling" title="Direct link to Ablation Study: Model Scaling">​</a></h3>
<p>Common traditional scaling approaches include:</p>
<ul>
<li><strong>Width-only scaling</strong>: increasing channels per layer;</li>
<li><strong>Depth-only scaling</strong>: increasing the number of layers or blocks;</li>
<li><strong>Compound scaling</strong> (e.g., EfficientNet): simultaneously adjusting depth, width, and resolution.</li>
</ul>
<p>The authors adopt a method specialized for <strong>concatenation-based architectures</strong>:</p>
<blockquote>
<p><strong>Increase computational block depth by 1.5× and simultaneously scale transition block width by 1.25×.</strong></p>
</blockquote>
<p>The rationale is to maintain:</p>
<ul>
<li>stable output feature map structure within blocks;</li>
<li>channel alignment between layers;</li>
<li>reduced memory fragmentation to enhance hardware computation efficiency.</li>
</ul>
<p>The results are shown below:</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=ablation src=/en/assets/images/img7-b879791b1da0265d7d7366f02aa7e85b.jpg width=808 height=256 class=img_ev3q></figure></div>
<p>From the data:</p>
<ul>
<li><strong>Width-only scaling</strong> is simple but least efficient, with increased parameters but no clear accuracy gain;</li>
<li><strong>Depth-only scaling</strong> yields some accuracy gain but causes output misalignment and reduces compute utilization;</li>
<li><strong>Compound scaling strategy</strong> achieves the greatest AP increase (+0.5%) with fewer parameters and computation.</li>
</ul>
<p>This compound scaling is not merely scaling but a geometry-stable scaling method tailored for specific architectures like ELAN and YOLO-style networks.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=label-assignment-strategy>Label Assignment Strategy<a href=#label-assignment-strategy class=hash-link aria-label="Direct link to Label Assignment Strategy" title="Direct link to Label Assignment Strategy">​</a></h3>
<p>In past multi-head detectors, the <strong>Auxiliary Head</strong> was often just an additional intermediate supervision point to accelerate gradient flow and convergence.</p>
<p>YOLOv7’s design goes further:</p>
<blockquote>
<p><strong>Why not let the learning content of the auxiliary head be guided by the main output (Lead Head)?</strong></p>
</blockquote>
<p>This is the core idea behind <strong>Assistant Loss for Auxiliary Head</strong>, building a semantically layered supervision structure via Lead Head predictions.</p>
<ol>
<li>
<p><strong>Experiment 1: Comparison of Three Label Assignment Strategies</strong></p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="label assign" src=/en/assets/images/img8-34175c3218372839359e45d06e5817c7.jpg width=782 height=262 class=img_ev3q></figure></div>
<p>Results show:</p>
<ul>
<li><strong>Adding assistant loss improves performance;</strong></li>
<li><strong>Lead Head guided soft labels outperform traditional supervision;</strong></li>
<li>The best result comes from <strong>Coarse-to-Fine guidance</strong>, assigning fine labels to the Lead Head and relaxed, recall-oriented labels to the Auxiliary Head, achieving hierarchical supervision.</li>
</ul>
<p>The authors further visualize the <strong>Objectness Map</strong> under different strategies:</p>
<p><img decoding=async loading=lazy alt=vis src=/en/assets/images/img9-1301f47eb294d83c83f5519a70edf6c9.jpg width=1826 height=416 class=img_ev3q></p>
<p>Under traditional methods, the Auxiliary Head often learns inconsistent features; after applying Lead-guided soft labels, the Auxiliary Head’s features align better with the Lead Head and even help learn residual information.</p>
<p>This mid-to-high-level guided information flow forms a special “residual learning architecture,” enhancing overall network learning division and efficiency.</p>
</li>
</ol>
<hr>
<ol start=2>
<li>
<p><strong>Experiment 2: Decoder Upper Bound Constraint</strong></p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="decoder limit" src=/en/assets/images/img10-366735b37a9a3ecc2d8efac8f47bf0c8.jpg width=736 height=224 class=img_ev3q></figure></div>
<p>To prevent negative effects of Coarse Labels on final outputs, the authors added a constraint in the decoder:</p>
<blockquote>
<p><strong>Limit the objectness score upper bound of coarse labels, decaying it based on the target center distance.</strong></p>
</blockquote>
<p>The results show this design not only provides tolerance for coarse labels but also helps the model focus on what the auxiliary head should learn, avoiding unnecessary noise.</p>
</li>
</ol>
<hr>
<ol start=3>
<li>
<p><strong>Experiment 3: Partial Auxiliary Head Design</strong></p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="Partial Auxiliary Head" src=/en/assets/images/img11-3b35309d750019ade52114b64f933e64.jpg width=640 height=224 class=img_ev3q></figure></div>
<p>YOLOv7 also proposes a new design:</p>
<blockquote>
<p><strong>Connect the Auxiliary Head to intermediate layers of the E-ELAN pyramid rather than the final output.</strong></p>
</blockquote>
<p>This serves two purposes:</p>
<ol>
<li><strong>Avoid direct interference of Assistant Loss on the final feature update weights;</strong></li>
<li><strong>Allow objects of different sizes to be learned at different layers, maintaining pyramid learning distribution for small, medium, and large targets.</strong></li>
</ol>
<p>Results show the <strong>Partial Coarse-to-Fine strategy performs best</strong>, preserving clear learning boundaries for each layer.</p>
</li>
</ol>
<p>Letting the Lead Head guide supervision is not just reuse of information but forms a semantically consistent, gradient-complementary training path for the whole network.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>YOLOv7 presents an object detection design philosophy centered on <strong>the training pipeline as the core, with inference architecture as background</strong>.</p>
<p>From model scaling and module replacement to label assignment, the authors bring those previously considered “training details” to the forefront of design, demonstrating that training itself is the key factor in model optimization.</p>
<p>For practitioners, YOLOv7’s greatest insight is realizing how important <strong>memory alignment, channel matching, and gradient flow</strong>—hardware-oriented designs—are for practical deployment. Moreover, module replacements (like RepConv) must consider information paths and architectural logic beyond mere parameter counts.</p>
<p>This is the intersection of engineering and research, rich with concepts worth learning.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-06-14T04:21:55.000Z itemprop=dateModified>Jun 14, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/object-detection/h-detr/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[22.07] H-DETR</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/object-detection/yolov6/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[22.09] YOLOv6</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#the-underdogs-comeback class="table-of-contents__link toc-highlight">The Underdog’s Comeback</a><li><a href=#problem-definition class="table-of-contents__link toc-highlight">Problem Definition</a><li><a href=#problem-solving class="table-of-contents__link toc-highlight">Problem Solving</a><ul><li><a href=#extended-elan class="table-of-contents__link toc-highlight">Extended-ELAN</a><li><a href=#scaling-for-cascaded-architectures class="table-of-contents__link toc-highlight">Scaling for Cascaded Architectures</a><li><a href=#trainable-bof class="table-of-contents__link toc-highlight">Trainable BoF</a><li><a href=#experimental-setup class="table-of-contents__link toc-highlight">Experimental Setup</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#comparison-with-previous-yolo-versions class="table-of-contents__link toc-highlight">Comparison with Previous YOLO Versions</a><li><a href=#comparison-with-latest-state-of-the-art-models class="table-of-contents__link toc-highlight">Comparison with Latest State-of-the-Art Models</a><li><a href=#ablation-study-model-scaling class="table-of-contents__link toc-highlight">Ablation Study: Model Scaling</a><li><a href=#label-assignment-strategy class="table-of-contents__link toc-highlight">Label Assignment Strategy</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>