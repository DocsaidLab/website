<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-object-detection/yolov6/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[22.09] YOLOv6 | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/object-detection/yolov6/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[22.09] YOLOv6 | DOCSAID"><meta data-rh=true name=description content="The Crossroad of Re-parameterization"><meta data-rh=true property=og:description content="The Crossroad of Re-parameterization"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/object-detection/yolov6/><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov6/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/object-detection/yolov6/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/object-detection/yolov6/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov6/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/object-detection","name":"Object Detection (19)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/object-detection/yolov6/","name":"[22.09] YOLOv6","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.ef02043f.css><script src=/en/assets/js/runtime~main.4e3e805b.js defer></script><script src=/en/assets/js/main.00053b7d.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/object-detection/yolov6/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/object-detection/yolov6/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/object-detection/yolov6/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning>Contrastive Learning (14)</a><button aria-label="Expand sidebar category 'Contrastive Learning (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-antispoofing>Face Anti-Spoofing (43)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (43)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/object-detection>Object Detection (19)</a><button aria-label="Collapse sidebar category 'Object Detection (19)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov1/>[15.06] YOLOv1</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/ssd/>[15.12] SSD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov2/>[16.12] YOLOv2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/retinanet/>[17.08] RetinaNet</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov3/>[18.04] YOLOv3</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/atss/>[19.12] ATSS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov4/>[20.04] YOLOv4</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/detr/>[20.05] DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/gfl/>[20.06] GFL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/deformable-detr/>[20.10] Deformable DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/smca-detr/>[21.01] SMCA DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/h-detr/>[22.07] H-DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov7/>[22.07] YOLOv7</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/object-detection/yolov6/>[22.09] YOLOv6</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-world/>[24.01] YOLO-World</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov9/>[24.02] YOLOv9</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov10/>[24.05] YOLOv10</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov11/>[24.10] YOLOv11</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-tiny/>[24.12] YOLO-Tiny</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/retail-product>Retail Product (6)</a><button aria-label="Expand sidebar category 'Retail Product (6)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers>Vision Transformers (13)</a><button aria-label="Expand sidebar category 'Vision Transformers (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 238 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/object-detection><span>Object Detection (19)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[22.09] YOLOv6</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[22.09] YOLOv6</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=the-crossroad-of-re-parameterization>The Crossroad of Re-parameterization<a href=#the-crossroad-of-re-parameterization class=hash-link aria-label="Direct link to The Crossroad of Re-parameterization" title="Direct link to The Crossroad of Re-parameterization">​</a></h2>
<p><a href=https://arxiv.org/abs/2209.02976 target=_blank rel="noopener noreferrer"><strong>YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications</strong></a></p>
<hr>
<p>Since YOLOv5 was never formally published, we directly move on to v6.</p>
<p>YOLOv6 is developed under the leadership of the computer vision team at Meituan, a Chinese lifestyle service platform.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-definition>Problem Definition<a href=#problem-definition class=hash-link aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>The YOLO series has always enjoyed extremely high popularity in industrial applications.</p>
<p>The reason is simple: it’s straightforward, fast, and accurate enough.</p>
<p>However, as task types expand and deployment scenarios become more complex, the foundational assumptions once considered “good enough” gradually become insufficient.</p>
<p>For example, the myth of architectural consistency causes a performance gap between small and large models; the instability of quantized inference results in re-parameterized models producing adverse effects during practical deployment.</p>
<p>Moreover, current speed reports mostly rely on high-end GPUs, causing a large discrepancy between expected and actual latency in deployment; advanced training strategies such as label assignment, loss design, and knowledge distillation often remain theoretical or limited to small-scale experiments, making it hard to form a complete training pipeline.</p>
<p>At the core lies a fundamental question:</p>
<blockquote>
<p><strong>When we say a model is fast and accurate enough, what do we really mean?</strong></p>
</blockquote>
<p>Is it the benchmark score on an A100 GPU, or real-time response on a T4? Is it the endpoint of the training accuracy curve, or the ability to run stably in industrial settings for a month?</p>
<p>These accumulated questions are not just technical details but also reflect differing design philosophies.</p>
<p>Looking back at the YOLO architecture, past development focused on module stacking, feature fusion, and data augmentation. However, before reaching the threshold of industrial deployment, the core that needs reconsideration is:</p>
<p><strong>model quantizability, module re-parameterizability, training strategies, and hardware friendliness.</strong></p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-solving>Problem Solving<a href=#problem-solving class=hash-link aria-label="Direct link to Problem Solving" title="Direct link to Problem Solving">​</a></h2>
<p>YOLOv6 breaks down its design philosophy into six strategic aspects addressing six long-standing engineering bottlenecks:</p>
<ol>
<li><strong>Network Design</strong></li>
<li><strong>Label Assignment</strong></li>
<li><strong>Loss Function</strong></li>
<li><strong>Training Enhancements (Industry-handy Improvements)</strong></li>
<li><strong>Quantization and Deployment</strong></li>
<li><strong>Self-distillation</strong></li>
</ol>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>The YOLO series introduces re-parameterized architectures starting from this paper, a concept that gained popularity following the 2021 RepVGG paper. For readers who haven't yet read RepVGG, you can refer to our previous notes:<ul>
<li><a href=/en/papers/reparameterization/repvgg/><strong>[21.01] RepVGG: Making VGG Great Again</strong></a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=network-design>Network Design<a href=#network-design class=hash-link aria-label="Direct link to Network Design" title="Direct link to Network Design">​</a></h3>
<p>An object detection model can basically be decomposed into three core modules:</p>
<ul>
<li><strong>Backbone, Neck, and Head.</strong></li>
</ul>
<p>These terms should be familiar to everyone.</p>
<p>The Backbone dominates feature extraction, determining the semantic representation ability of the model and accounting for most of the computational cost; the Neck integrates feature maps at different levels, building a multi-scale pyramid structure; the Head performs classification and bounding box prediction based on these feature maps.</p>
<p>YOLOv6 redesigns all three modules with differentiated adjustments according to different model sizes. The core idea is:</p>
<blockquote>
<p><strong>Design different architectures for different model scales.</strong></p>
</blockquote>
<p>For small models, YOLOv6 uses the <strong>RepBlock</strong> re-parameterizable module as the main Backbone. This conversion mechanism is inspired by RepVGG’s design philosophy: <strong>training is for learning, inference is optimized for deployment.</strong></p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=RepBlock src=/en/assets/images/img1-b0329cfb2268fba5474240d16190cf03.jpg width=1068 height=488 class=img_ev3q></figure></div>
<p>The RepBlock module, shown in figure (a) above, contains a multi-branch structure during training, which facilitates feature representation learning; during inference, it is converted into a single-path, stacked 3×3 convolution form that maximizes hardware computation density and parallel efficiency, performing particularly well on mainstream GPUs and CPUs.</p>
<p>However, when scaling up the model size, the single-path architecture causes parameters and computations to grow exponentially. Therefore, YOLOv6 introduces a new design for medium and large models: the <strong>CSPStackRep Block</strong>, shown in figure (c). This is a hybrid module combining the <strong>CSP structure</strong> with the <strong>RepBlock re-parameterization mechanism.</strong></p>
<p>Internally, it consists of three 1×1 convolutions and a stack of RepBlocks, incorporating cross-stage residual connections (CSP) to maintain performance stability while reducing redundant computation. This structure achieves a better trade-off between accuracy and speed, especially suitable for medium to large models.</p>
<hr>
<p>Next, looking at the Neck.</p>
<p>YOLOv6 continues to use the <strong>PAN (Path Aggregation Network)</strong> architecture from v4/v5 as the main Neck. The difference is that small models use RepBlock, while larger models use CSPStackRep Block to replace v5’s original CSPBlock.</p>
<p>This architecture is named <strong>Rep-PAN</strong> by the authors. Its width and depth dynamically adjust according to model scale, allowing features to flow effectively across different levels and spatial scales, while maintaining inference efficiency.</p>
<p>The overall architecture is shown below:</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=RepPAN src=/en/assets/images/img2-2c5897c1b525b656392ca5cf0c734d26.jpg width=1552 height=360 class=img_ev3q></figure></div>
<hr>
<p>Finally, the Head design optimization.</p>
<p>Compared to v5’s coupled design or YOLOX’s decoupled Head with two extra 3×3 convolution layers, YOLOv6 adopts a decoupled design called <strong>Efficient Decoupled Head</strong>, which retains only one intermediate convolutional layer and adjusts overall channel counts based on Backbone and Neck widths.</p>
<p>Additionally, YOLOv6 uses an <strong>anchor-free</strong> approach based on anchor points. The model no longer assumes a preset set of anchor boxes but directly predicts the distance from each point to the object boundary. This method offers simple decoding, strong generalization, and low post-processing cost. It is also a mainstream design in recent anchor-free detectors such as FCOS and YOLOX.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=label-assignment>Label Assignment<a href=#label-assignment class=hash-link aria-label="Direct link to Label Assignment" title="Direct link to Label Assignment">​</a></h3>
<p>Label assignment is a critical step in the object detection training pipeline.</p>
<p>It is responsible for assigning each ground-truth object to a set of predefined anchors or prediction points during training. This process not only determines the distribution of positive and negative samples but also directly affects the learning dynamics of both classification and regression branches.</p>
<p>Early methods mostly relied on geometric criteria, such as IoU-based strategies or rules based on whether a point lies inside a bounding box. These approaches are intuitive but often insufficient for handling multi-scale objects and uneven backgrounds.</p>
<p>In recent years, advanced methods have emerged that treat label assignment as an optimization or task alignment problem.</p>
<ol>
<li>
<p><strong>SimOTA: An ideal transport problem with suboptimal convergence</strong></p>
<p>Early versions of YOLOv6 adopted <strong>SimOTA</strong> as the label assignment strategy.</p>
<p>This method is derived from OTA (Optimal Transport Assignment), modeling label assignment as a global transport problem and using the loss function as the distance metric to achieve an overall optimal allocation between classification and regression results.</p>
<p>SimOTA simplifies OTA by retaining its core idea while reducing hyperparameter complexity. However, in practice, the authors observed several issues:</p>
<ul>
<li>Introducing SimOTA <strong>significantly lengthened training time</strong>;</li>
<li>The model tended to exhibit <strong>unstable oscillations</strong> during early training stages and was more sensitive to initialization and learning rates.</li>
</ul>
<p>These observations led them to seek a more stable and training-friendly alternative.</p>
</li>
<li>
<p><strong>TAL: Task alignment from geometry to semantics</strong></p>
<p>Ultimately, the authors adopted <strong>Task Alignment Learning (TAL)</strong>.</p>
<p>Proposed by TOOD, unlike SimOTA, TAL <strong>no longer relies solely on geometric metrics (e.g., IoU) as assignment criteria</strong>, but establishes a unified metric considering both classification scores and predicted box quality, enabling synchronous alignment of classification and regression branches during label assignment.</p>
<p>This design brings two clear advantages:</p>
<ol>
<li><strong>Mitigates task misalignment between classification and regression</strong>, enhancing overall model consistency;</li>
<li><strong>Demonstrates stability and accelerated convergence from early training stages</strong>, with less reliance on hyperparameter tuning.</li>
</ol>
<p>TOOD’s original architecture also introduced T-head and TAP (Task-Aligned Predictor) modules. Later, PP-YOLOE simplified this to the ET-head.</p>
<p>However, the authors found that incorporating ET-head in YOLOv6 did not yield significant accuracy improvements and slowed inference speed. Therefore, they retained their Efficient Decoupled Head design and adopted TAL only for label assignment.</p>
</li>
</ol>
<hr>
<p>Experiments confirmed that TAL not only outperforms SimOTA in accuracy but more importantly, <strong>significantly stabilizes the training process</strong>, becoming YOLOv6’s default label assignment mechanism.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=loss-function>Loss Function<a href=#loss-function class=hash-link aria-label="Direct link to Loss Function" title="Direct link to Loss Function">​</a></h3>
<p>The object detection task can essentially be decomposed into two subtasks: <strong>classification</strong> and <strong>localization</strong>, corresponding to classification loss and bounding box regression loss. These two loss paths influence prediction accuracy as well as the overall convergence speed and stability.</p>
<p>In YOLOv6, the authors conducted extensive experiments on loss design for these subtasks and ultimately selected a balanced combination strategy.</p>
<ol>
<li>
<p><strong>Classification Loss: Balancing signal strength between positive and negative samples</strong></p>
<p>For the classification branch, traditional cross-entropy loss tends to cause the model to overfit easy samples and ignore a small number of hard samples due to class imbalance. Focal Loss was proposed to address this by down-weighting easy samples, focusing training on harder examples.</p>
<p>Further extensions include:</p>
<ul>
<li><strong>Quality Focal Loss (QFL)</strong>, which integrates classification confidence with predicted box quality for supervision;</li>
<li><strong>Poly Loss</strong>, which expands the loss polynomially to improve generalization;</li>
<li><strong>VariFocal Loss (VFL)</strong>, which asymmetrically handles positives and negatives by assigning stronger signals to positive samples, further balancing the signal distribution.</li>
</ul>
<p>Through experiments, <strong>YOLOv6 ultimately selects VFL as the classification loss</strong>, primarily due to its high sensitivity to hard samples and stable training behavior.</p>
</li>
</ol>
<hr>
<ol start=2>
<li>
<p><strong>Bounding Box Loss: From IoU to probability distribution</strong></p>
<p>The core of localization loss design is to make predicted boxes as close as possible to ground truth. Early methods used L1 or L2 loss but found inconsistencies with evaluation metrics (such as IoU), causing misdirected learning.</p>
<p>Therefore, IoU-based losses were proposed, including:</p>
<ul>
<li><strong>GIoU, DIoU, CIoU</strong>: incorporating geometric information such as enclosing box area, center distance, and aspect ratio;</li>
<li><strong>SIoU</strong>: further considering convergence behaviors along diagonal and angular directions.</li>
</ul>
<p>YOLOv6 adopts the following strategy:</p>
<ul>
<li>Small models (YOLOv6-N/T) use <strong>SIoU</strong>;</li>
<li>Medium and large models use <strong>GIoU</strong> to balance stability and performance.</li>
</ul>
<p>Additionally, YOLOv6 introduces <strong>Distribution Focal Loss (DFL)</strong> in medium and large models. This loss treats box regression as a probability distribution estimation, better handling ambiguous or uncertain object boundaries. Considering its computational cost, <strong>DFL is only used in YOLOv6-M/L</strong>, not in smaller models.</p>
</li>
</ol>
<hr>
<ol start=3>
<li>
<p><strong>Objectness Loss: A design without gains</strong></p>
<p>The authors also experimented with adding an <strong>objectness loss</strong> similar to FCOS and YOLOX, aimed at suppressing low-quality predicted boxes. However, experiments showed that in YOLOv6’s anchor-free architecture, <strong>this design brought no significant benefits</strong> and thus was excluded from the final model.</p>
</li>
</ol>
<hr>
<p>Overall, YOLOv6’s loss design follows a <strong>task-alignment plus signal-enhancement</strong> strategy:</p>
<ul>
<li>The classification branch emphasizes signal asymmetry, using VFL to stabilize discrimination;</li>
<li>The regression branch prioritizes stable and fast learning in small models, while allowing complex but precise DFL in large models;</li>
<li>All designs aim to <strong>improve overall learning quality without sacrificing inference efficiency</strong>.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=quantization-and-deployment>Quantization and Deployment<a href=#quantization-and-deployment class=hash-link aria-label="Direct link to Quantization and Deployment" title="Direct link to Quantization and Deployment">​</a></h3>
<p>In industrial applications, speed is essential.</p>
<p>Therefore, <strong>how to accelerate inference without severely sacrificing performance</strong> becomes a practical challenge beyond model design and training.</p>
<p>Quantization is the primary solution to this problem.</p>
<p>However, for architectures like YOLOv6 that heavily utilize re-parameterization blocks, traditional quantization workflows are not directly applicable.</p>
<p>Using Post-Training Quantization (PTQ) leads to a significant drop in accuracy; switching to Quantization-Aware Training (QAT) introduces mismatches between the quantization simulator during training and actual inference, causing additional issues.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p>Before moving on, let’s pause here.<p>Shortly after this paper was published, another classic study on quantization issues for re-parameterized architectures appeared: QARepVGG.<p>This paper points out that the cause of quantization problems lies in BatchNorm collapse during the re-parameterization process due to certain statistical parameter issues. Interested readers can refer to our previous notes:<ul>
<li><a href=/en/papers/reparameterization/qarepvgg/><strong>[22.12] QARepVGG: Making RepVGG Great Again</strong></a></li>
</ul></div></div>
<p>At this moment, YOLOv6 tackles the problem step-by-step.</p>
<ol>
<li>
<p><strong>RepOptimizer: Solving quantization issues at the optimizer level for re-parameterized blocks</strong></p>
<p>YOLOv6 uses <strong>RepOptimizer</strong> as its training optimizer, a gradient optimization strategy designed specifically for re-parameterized architectures. The core idea is to perform structural-level re-parameterization during every gradient update, aligning training and deployment behaviors from the source.</p>
<p>Experiments (see figure below) show that models trained this way have a narrower feature distribution range, making them more suitable for direct Post-Training Quantization (PTQ). This enables producing a quantized model with stable inference performance without requiring additional large-scale annotated data.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=rep-optimizer src=/en/assets/images/img3-58f19366000e0707661d8b219423a44e.jpg width=808 height=588 class=img_ev3q></figure></div>
</li>
</ol>
<hr>
<ol start=2>
<li>
<p><strong>Sensitivity Analysis: Protecting fragile parts of the model with float precision</strong></p>
<p>Even when trained with RepOptimizer, some layers may still suffer significant accuracy drops after quantization. Therefore, YOLOv6 applies <strong>sensitivity analysis</strong> to evaluate the quantization impact on feature maps layer-by-layer and identify the most sensitive layers.</p>
<p>Evaluation metrics used include:</p>
<ul>
<li><strong>MSE (Mean Squared Error)</strong></li>
<li><strong>SNR (Signal-to-Noise Ratio)</strong></li>
<li><strong>Cosine Similarity</strong></li>
</ul>
<p>After comprehensive analysis on the YOLOv6-S model, the authors choose to keep the top six most sensitive layers in full float precision, while safely quantizing the rest. This mixed-precision deployment strategy effectively improves stability without adding extra complexity.</p>
</li>
</ol>
<hr>
<ol start=3>
<li>
<p><strong>QAT + Channel-wise Distillation</strong></p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="Channel-wise Distillation" src=/en/assets/images/img4-f401e4abf6312af1beda4f780ade7d1a.jpg width=908 height=584 class=img_ev3q></figure></div>
<p>In some deployment scenarios, even RepOptimizer combined with mixed precision is insufficient, thus further Quantization-Aware Training (QAT) is needed. YOLOv6 makes two improvements on this basis:</p>
<ol>
<li>QAT must be built on top of the RepOptimizer framework to solve inconsistencies caused by fake quantizers;</li>
<li>It incorporates Channel-wise Distillation, where the FP32 model outputs serve as teacher signals to guide the quantized training process.</li>
</ol>
<p>This self-distillation design further improves channel-level distribution learning, compensating for details lost during quantization.</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=implementation-details>Implementation Details<a href=#implementation-details class=hash-link aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details">​</a></h3>
<p>YOLOv6’s training pipeline largely follows YOLOv5’s setup, including these techniques and strategies:</p>
<ul>
<li>Optimizer: Stochastic Gradient Descent (SGD) with momentum;</li>
<li>Learning rate schedule: Cosine decay;</li>
<li>Other techniques: Warm-up initialization, grouped weight decay, and exponential moving average (EMA).</li>
</ul>
<p>For data augmentation, the authors adopt two widely validated strong augmentations: <strong>Mosaic</strong> and <strong>Mixup</strong>, continuing the practice from YOLOv4/YOLOv5.</p>
<p>Training data comes from COCO 2017 training set; validation uses the COCO 2017 validation set.</p>
<p>All models are trained on 8 NVIDIA A100 GPUs. Unless otherwise noted, speed tests are performed on Tesla T4 using TensorRT 7.2.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p>This paper dedicates considerable space to planning and experiments on self-distillation, but due to space constraints, we highlight only a few key results here. Readers interested in self-distillation techniques are encouraged to consult the original paper.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=comparison-with-other-architectures>Comparison with Other Architectures<a href=#comparison-with-other-architectures class=hash-link aria-label="Direct link to Comparison with Other Architectures" title="Direct link to Comparison with Other Architectures">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=result src=/en/assets/images/img0-02b723e213f54c6b4846f1ee22f3b92d.jpg width=1408 height=482 class=img_ev3q></figure></div>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=result src=/en/assets/images/img5-ad609b9750e34bb11f93af2a568c0614.jpg width=1224 height=872 class=img_ev3q></figure></div>
<p>Since YOLOv6’s core goal is to build an efficient detector for industrial deployment, the authors focus not on FLOPs or parameter counts but on actual inference efficiency after deployment, including:</p>
<ul>
<li><strong>Throughput (FPS):</strong> inference frame rate at batch sizes 1 and 32;</li>
<li><strong>Latency:</strong> average per-image processing delay.</li>
</ul>
<p>Experiments compare the YOLOv6 series with other YOLO-family models:</p>
<ul>
<li>YOLOv5</li>
<li>YOLOX</li>
<li>PP-YOLOE</li>
<li>YOLOv7</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p><strong>Wait, why can v6 be compared with v7?</strong><p>Because v7 was released earlier than v6! Welcome to the chaotic world of YOLO! 😱</div></div>
<p>All models were tested under identical conditions: <strong>FP16 precision + TensorRT deployment + Tesla T4 GPU</strong>.</p>
<p>YOLOv7-Tiny results were re-tested using its open-source models and official weights at input sizes 416 and 640.</p>
<p>Key takeaways from the experiments:</p>
<ul>
<li><strong>YOLOv6-N</strong> achieves <strong>7.9% / 2.6% AP improvements over YOLOv5-N / YOLOv7-Tiny</strong> at input size 416, with best throughput and latency metrics.</li>
<li><strong>YOLOv6-S</strong> outperforms YOLOX-S / PPYOLOE-S by <strong>3.0% / 0.4% AP</strong>, also running faster.</li>
<li>At input size 640, <strong>YOLOv6-T</strong> improves accuracy by <strong>2.9%</strong> over YOLOv5-S / YOLOv7-Tiny and is faster by <strong>73 / 25 FPS</strong> at batch size 1.</li>
<li><strong>YOLOv6-M</strong> gains <strong>4.2% AP over YOLOv5-M</strong> and <strong>2.7% / 0.6% over YOLOX-M / PPYOLOE-M</strong>, while maintaining speed advantages.</li>
<li>In large model comparisons, <strong>YOLOv6-L</strong> surpasses YOLOX-L / PPYOLOE-L by <strong>2.8% / 1.1% AP</strong> while leading in latency.</li>
</ul>
<p>The authors also propose a faster variant, <strong>YOLOv6-L-ReLU</strong>, which replaces SiLU activation with ReLU. Without increasing latency, it achieves <strong>51.7% AP with only 8.8ms delay</strong>, outperforming YOLOX-L, PPYOLOE-L, and YOLOv7 in both accuracy and speed.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=ablation-studies>Ablation Studies<a href=#ablation-studies class=hash-link aria-label="Direct link to Ablation Studies" title="Direct link to Ablation Studies">​</a></h3>
<p>To verify the impact of design choices on final performance, systematic ablation experiments cover three aspects: <strong>network architecture, label assignment, and loss functions</strong>. Experiments mainly use YOLOv6-N/S/M models under consistent training and testing setups.</p>
<p>First, performance of single-path vs. multi-branch (CSPStackRep Block) structures is compared across model scales.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="backbone ablation" src=/en/assets/images/img6-dab3d2a9c793d5ff7aeaaddc8ae45416.jpg width=1036 height=504 class=img_ev3q></figure></div>
<p>Results show:</p>
<ul>
<li>On YOLOv6-N, <strong>single-path structure</strong> outperforms multi-branch in accuracy and speed. Despite higher FLOPs and parameters, lower memory footprint and better parallelism lead to faster runtime.</li>
<li>YOLOv6-S shows comparable performance between both structures.</li>
<li>For YOLOv6-M/L, <strong>multi-branch structure</strong> performs better; authors select CSPStackRep with channel coefficients of 2/3 (M) and 1/2 (L).</li>
</ul>
<p>Analysis of YOLOv6-L’s Neck width and depth reveals that narrow-and-deep design yields 0.2% higher AP with similar speed, aligning with expectations that deeper architectures have larger receptive fields suited for detection.</p>
<p>Additional conclusions include:</p>
<ul>
<li>Conv + SiLU achieves best accuracy.</li>
<li>RepConv + ReLU balances speed and accuracy well.</li>
<li>Adding Decoupled Head (DH) improves AP by 1.4% with only 5% more computation.</li>
<li>Anchor-free design increases inference speed by 51% compared to anchor-based, mainly due to reduced output dimensions.</li>
<li>Unified Backbone and Neck redesign (EB+RN) improves AP by 3.6% and speed by 21%.</li>
<li>Hybrid-channel Decoupled Head (HC) adds 0.2% AP and boosts FPS by 6.8%.</li>
</ul>
<p>For label assignment, <strong>TAL</strong> proves best, achieving <strong>35.0% AP on YOLOv6-N</strong>, 0.5% higher than SimOTA, with more stable training.</p>
<p>Loss function comparisons among VFL, QFL, Focal Loss, and Poly Loss show VFL yields AP improvements of 0.2% / 0.3% / 0.1% on YOLOv6-N/S/M, becoming the default classification loss.</p>
<p>Tests on IoU-based and distribution-based losses find:</p>
<ul>
<li><strong>SIoU</strong> best for YOLOv6-N and YOLOv6-T;</li>
<li><strong>CIoU</strong> better for YOLOv6-M;</li>
<li>DFL adds 0.1–0.2% gain but slows small models, thus used only in YOLOv6-M/L.</li>
</ul>
<p>Objectness loss experiments show introducing it on YOLOv6-N/S/M causes up to 1.1% AP drop. The authors attribute this to TAL’s alignment logic becoming disrupted by adding a third task branch, hence opting to exclude objectness loss.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=quantization-results>Quantization Results<a href=#quantization-results class=hash-link aria-label="Direct link to Quantization Results" title="Direct link to Quantization Results">​</a></h3>
<p>To validate YOLOv6’s quantization design in deployment, authors conduct complete PTQ and QAT experiments on <strong>YOLOv6-S</strong>, testing both v1.0 and v2.0 models trained for 300 epochs by default.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>The biggest improvement in v2.0 is the removal of quantization-sensitive layers, such as certain composite activations, shortcut structures, and misaligned convolution modules.</div></div>
<ol>
<li>
<p><strong>Post-Training Quantization (PTQ)</strong></p>
<p>Comparison of PTQ effects with and without RepOptimizer shows:</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=ptq src=/en/assets/images/img7-181ecae8f459f478849553e3ea770bca.jpg width=956 height=380 class=img_ev3q></figure></div>
<p>RepOptimizer does not affect FP32 accuracy but <strong>significantly improves INT8 quantized accuracy</strong>, especially in v1.0, from 35.0% to 40.9%. This demonstrates its ability to converge feature distributions, aiding quantization error tolerance.</p>
</li>
</ol>
<hr>
<ol start=2>
<li>
<p><strong>Quantization-Aware Training (QAT)</strong></p>
<p>For YOLOv6 v1.0, <strong>Partial QAT</strong> is applied to non-sensitive layers, comparing Full QAT and Channel-wise Distillation (CW Distill):</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=qat src=/en/assets/images/img8-1ae3fb0a04ea0f8a246a394a1b7179a3.jpg width=940 height=216 class=img_ev3q></figure></div>
<p>Partial QAT outperforms Full QAT overall, improving accuracy by 7.3% with only slight throughput reduction. This confirms the effectiveness of selectively choosing quantization blocks to minimize accuracy loss.</p>
<p>For v2.0, with quantization-sensitive layers removed, QAT can be applied network-wide. Combined with graph optimizations and quantization module simplifications, accuracy and speed improve further.</p>
<p>Comparison with PaddleSlim’s quantized models shows:</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=qat src=/en/assets/images/img9-4caa25b96621e3710c71f555a93b4434.jpg width=952 height=340 class=img_ev3q></figure></div>
<p>Results indicate the proposed quantization method outperforms other publicly available quantized models in both accuracy and speed, even surpassing YOLOv6 FP16 in speed while maintaining only 0.1% lower accuracy.</p>
</li>
</ol>
<hr>
<p>These results demonstrate that through <strong>RepOptimizer + selective QAT + graph optimization + channel-wise distillation</strong>, YOLOv6 not only achieves quantization but does so efficiently, truly balancing speed and accuracy for deployment.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>YOLOv6’s main contributions include:</p>
<ul>
<li>Designing re-parameterizable architectures for small models and efficient multi-branch architectures for large models based on model scale differences;</li>
<li>Employing TAL for label assignment and VFL + SIoU/CIoU + DFL loss design, balancing stability and accuracy;</li>
<li>Incorporating engineering techniques such as extended training, self-distillation, gray border handling, and Mosaic augmentation to enhance performance;</li>
<li>Combining RepOptimizer with mixed-precision PTQ/QAT strategies to successfully build deployable quantized detectors.</li>
</ul>
<p>YOLOv6 surpasses existing models in accuracy and speed, maintaining strong performance after quantization, providing a practical solution for real-time industrial applications.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-06-12T11:36:55.000Z itemprop=dateModified>Jun 12, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/object-detection/yolov7/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[22.07] YOLOv7</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/object-detection/yolo-world/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[24.01] YOLO-World</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#the-crossroad-of-re-parameterization class="table-of-contents__link toc-highlight">The Crossroad of Re-parameterization</a><li><a href=#problem-definition class="table-of-contents__link toc-highlight">Problem Definition</a><li><a href=#problem-solving class="table-of-contents__link toc-highlight">Problem Solving</a><ul><li><a href=#network-design class="table-of-contents__link toc-highlight">Network Design</a><li><a href=#label-assignment class="table-of-contents__link toc-highlight">Label Assignment</a><li><a href=#loss-function class="table-of-contents__link toc-highlight">Loss Function</a><li><a href=#quantization-and-deployment class="table-of-contents__link toc-highlight">Quantization and Deployment</a><li><a href=#implementation-details class="table-of-contents__link toc-highlight">Implementation Details</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#comparison-with-other-architectures class="table-of-contents__link toc-highlight">Comparison with Other Architectures</a><li><a href=#ablation-studies class="table-of-contents__link toc-highlight">Ablation Studies</a><li><a href=#quantization-results class="table-of-contents__link toc-highlight">Quantization Results</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>