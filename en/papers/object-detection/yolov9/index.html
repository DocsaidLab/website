<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-object-detection/yolov9/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[24.02] YOLOv9 | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/object-detection/yolov9/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[24.02] YOLOv9 | DOCSAID"><meta data-rh=true name=description content="Programmable Gradient Information"><meta data-rh=true property=og:description content="Programmable Gradient Information"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/object-detection/yolov9/><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov9/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/object-detection/yolov9/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/object-detection/yolov9/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/object-detection/yolov9/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/object-detection-15","name":"Object Detection (15)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/object-detection/yolov9/","name":"[24.02] YOLOv9","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.7ddd3182.js defer></script><script src=/en/assets/js/main.0a27bfa3.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/object-detection/yolov9/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/object-detection/yolov9/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/object-detection/yolov9/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-14>Contrastive Learning (14)</a><button aria-label="Expand sidebar category 'Contrastive Learning (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-43>Face Anti-Spoofing (43)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (43)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/object-detection-15>Object Detection (15)</a><button aria-label="Collapse sidebar category 'Object Detection (15)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov1/>[15.06] YOLOv1</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/ssd/>[15.12] SSD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov2/>[16.12] YOLOv2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/retinanet/>[17.08] RetinaNet</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov3/>[18.04] YOLOv3</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov4/>[20.04] YOLOv4</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/detr/>[20.05] DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/deformable-detr/>[20.10] Deformable DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/smca-detr/>[21.01] SMCA DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/h-detr/>[22.07] H-DETR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov7/>[22.07] YOLOv7</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolov6/>[22.09] YOLOv6</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-world/>[24.01] YOLO-World</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/object-detection/yolov9/>[24.02] YOLOv9</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/object-detection/yolo-tiny/>[24.12] YOLO-Tiny</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/retail-product-3>Retail Product (3)</a><button aria-label="Expand sidebar category 'Retail Product (3)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-13>Vision Transformers (13)</a><button aria-label="Expand sidebar category 'Vision Transformers (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 230 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/object-detection-15><span>Object Detection (15)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[24.02] YOLOv9</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[24.02] YOLOv9</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=programmable-gradient-information>Programmable Gradient Information<a href=#programmable-gradient-information class=hash-link aria-label="Direct link to Programmable Gradient Information" title="Direct link to Programmable Gradient Information">​</a></h2>
<p><a href=https://arxiv.org/abs/2402.13616 target=_blank rel="noopener noreferrer"><strong>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</strong></a></p>
<hr>
<p>YOLOv8 didn’t publish a paper either?</p>
<p>If so, let's continue reading the YOLOv9 paper, which is also a work from Taiwan's Academia Sinica.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-definition>Problem Definition<a href=#problem-definition class=hash-link aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>In recent years, the YOLO series has become the de facto standard in real-time object detection. From YOLOv3 to YOLOv7, the continuously evolving architectures have achieved a good balance between accuracy and inference speed, widely applied in surveillance, traffic, AR/VR, and many other scenarios.</p>
<p>Most of these models adopt CSPNet or ELAN as backbone modules, assisted by PAN/FPN for feature fusion, and use YOLO heads or FCOS heads for multi-scale prediction. Even emerging Transformer architectures like RT-DETR have yet to truly surpass the YOLO series in terms of efficiency and deployment.</p>
<p>However, as models become increasingly complex, researchers gradually realize that the issues affecting convergence quality do not stem solely from architectural design or loss function choice, but deeper bottlenecks actually lie in the information transmission paths.</p>
<blockquote>
<p><strong>The real challenge of modern deep neural networks lies in the information bottleneck during training.</strong></p>
</blockquote>
<p>As input data passes through deep networks layer by layer, the original semantic information is repeatedly compressed and transformed, often diluted or lost by the intermediate layers. This semantic degradation leads to features obtained at the later stages being poorly correlated with the target, causing unreliable gradients that result in slow convergence, low accuracy, or even ineffective training.</p>
<p>Although the industry has proposed three major technical approaches to address this problem:</p>
<ul>
<li>
<p><strong>Reversible Architectures</strong>: Designs with inverse mapping functions that preserve complete input information after each layer operation. Representative works include RevNet or Res2Net. In theory, they avoid information loss, but their complex structures and extra computation cost limit inference efficiency.</p>
</li>
<li>
<p><strong>Masked Modeling</strong>: Strategies like those used in MAE or BERT, where the model learns key semantics by reconstructing masked regions. However, these methods are mostly designed for large Transformer architectures and tend to fail at stable semantic restoration when applied to lightweight models due to insufficient parameters.</p>
</li>
<li>
<p><strong>Deep Supervision</strong>: Inserting auxiliary supervision signals into intermediate layers to guide the model to maintain semantic consistency. While this can improve learning stability, it is prone to accumulating erroneous information and is practically effective only in very deep networks, often harming smaller models.</p>
</li>
</ul>
<p>Though each technique has merits, none can simultaneously satisfy both "information retention" and "lightweight efficiency" demands. This bottleneck remains a major obstacle especially for parameter-constrained real-time models.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p><strong>What is a "Reversible Architecture"?</strong><p>A reversible architecture is a neural network design that guarantees no information loss in the forward pass. That is, given the output of a layer, you can perfectly recover its input.<p>For detailed design ideas, see:<ul>
<li><a href=https://arxiv.org/abs/2212.11696 target=_blank rel="noopener noreferrer"><strong>[22.12] Reversible Column Networks</strong></a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=why-does-semantic-information-get-lost>Why Does Semantic Information Get Lost?<a href=#why-does-semantic-information-get-lost class=hash-link aria-label="Direct link to Why Does Semantic Information Get Lost?" title="Direct link to Why Does Semantic Information Get Lost?">​</a></h3>
<p>Traditionally, model convergence difficulties were attributed to "vanishing gradients" or "gradient saturation," but these issues have been largely mitigated by BatchNorm, ReLU, and other techniques. The real problem is that <strong>the semantic information on which gradients are based no longer exists</strong>.</p>
<p>According to the <strong>Information Bottleneck principle</strong>, after transformation by each network layer, the mutual information between data and the target task decreases:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi>I</mi><mo stretchy=false>(</mo><mi>X</mi><mo separator=true>,</mo><mi>X</mi><mo stretchy=false>)</mo><mo>≥</mo><mi>I</mi><mo stretchy=false>(</mo><mi>Y</mi><mo separator=true>,</mo><mi>X</mi><mo stretchy=false>)</mo><mo>≥</mo><mi>I</mi><mo stretchy=false>(</mo><mi>Y</mi><mo separator=true>,</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=false>(</mo><mi>X</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo><mo>≥</mo><mo>⋯</mo><mo>≥</mo><mi>I</mi><mo stretchy=false>(</mo><mi>Y</mi><mo separator=true>,</mo><mover accent=true><mi>Y</mi><mo>^</mo></mover><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>I(X, X) \geq I(Y, X) \geq I(Y, f_θ(X)) \geq \dots \geq I(Y, \hat{Y})</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>≥</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.22222em>Y</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>≥</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.22222em>Y</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10764em>f</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.1076em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.02778em>θ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>))</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>≥</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.7719em;vertical-align:-0.136em></span><span class=minner>⋯</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>≥</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.1968em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.22222em>Y</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord accent"><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.9468em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.22222em>Y</span></span><span style=top:-3.2523em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.25em><span class=mord>^</span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span></span>
<p>where <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>I</mi><mo stretchy=false>(</mo><mi>Y</mi><mo separator=true>,</mo><mi>X</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>I(Y, X)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.22222em>Y</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>)</span></span></span></span> denotes the semantic information in the input data relevant to the target.</p>
<p>If this information is diluted during forward propagation, even with little total information loss, the model cannot learn truly useful target mappings. This phenomenon is especially pronounced in <strong>lightweight models</strong>, since their limited parameters cannot support full feature representation, making semantic signals more prone to compression and erasure.</p>
<p>Expanding model capacity can alleviate symptoms, but this "wider instead of deeper" approach cannot fundamentally solve semantic loss and gradient degradation.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=the-possibility-of-reversible-functions>The Possibility of Reversible Functions<a href=#the-possibility-of-reversible-functions class=hash-link aria-label="Direct link to The Possibility of Reversible Functions" title="Direct link to The Possibility of Reversible Functions">​</a></h3>
<p>Reversible functions offer a theoretical solution.</p>
<p>If a transformation function <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>r</mi><mi>ψ</mi></msub></mrow><annotation encoding=application/x-tex>r_ψ</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7167em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>r</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>ψ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span> has an inverse <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>v</mi><mi>ζ</mi></msub></mrow><annotation encoding=application/x-tex>v_ζ</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7167em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.07378em>ζ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span> satisfying:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi>X</mi><mo>=</mo><msub><mi>v</mi><mi>ζ</mi></msub><mo stretchy=false>(</mo><msub><mi>r</mi><mi>ψ</mi></msub><mo stretchy=false>(</mo><mi>X</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo><mo separator=true>,</mo><mspace width=1em /><mi>I</mi><mo stretchy=false>(</mo><mi>X</mi><mo separator=true>,</mo><mi>X</mi><mo stretchy=false>)</mo><mo>=</mo><mi>I</mi><mo stretchy=false>(</mo><mi>X</mi><mo separator=true>,</mo><msub><mi>r</mi><mi>ψ</mi></msub><mo stretchy=false>(</mo><mi>X</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo><mo>=</mo><mi>I</mi><mo stretchy=false>(</mo><mi>X</mi><mo separator=true>,</mo><msub><mi>v</mi><mi>ζ</mi></msub><mo stretchy=false>(</mo><msub><mi>r</mi><mi>ψ</mi></msub><mo stretchy=false>(</mo><mi>X</mi><mo stretchy=false>)</mo><mo stretchy=false>)</mo><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>X = v_ζ(r_ψ(X)),\quad I(X, X) = I(X, r_ψ(X)) = I(X, v_ζ(r_ψ(X)))</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.0361em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.07378em>ζ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>r</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>ψ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>))</span><span class=mpunct>,</span><span class=mspace style=margin-right:1em></span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.0361em;vertical-align:-0.2861em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>r</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>ψ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>))</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.0361em;vertical-align:-0.2861em></span><span class="mord mathnormal" style=margin-right:0.07847em>I</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.07378em>ζ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>r</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.03588em>ψ</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07847em>X</span><span class=mclose>)))</span></span></span></span></span>
<p>then the transformation loses no information, guaranteeing complete semantic signal flow.</p>
<p>However, most reversible architectures still face practical challenges like high complexity, unstable learning, and poor generalization, making them difficult to deploy in real-time tasks.</p>
<p>Therefore, we need a novel method that not only preserves key semantics and improves gradient reliability but also:</p>
<ul>
<li><strong>Applies to lightweight architectures</strong></li>
<li><strong>Incurs zero extra inference cost</strong></li>
<li><strong>Can flexibly combine with task losses and structural modules</strong></li>
</ul>
<p>Considering these requirements, the authors propose a novel concept to address the information bottleneck:</p>
<blockquote>
<p><strong>Programmable Gradient Information (PGI).</strong></p>
</blockquote>
<p>Let's continue.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=solution>Solution<a href=#solution class=hash-link aria-label="Direct link to Solution" title="Direct link to Solution">​</a></h2>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=model_arch src=/en/assets/images/img3-e7db217ecea9642a9425b98ce31aa102.jpg width=1224 height=508 class=img_ev3q></figure></div>
<p>To solve the information bottleneck and gradient degradation issues, the authors propose a new training framework: <strong>Programmable Gradient Information (PGI)</strong>.</p>
<p>As shown above, this framework draws on experiences from prior methods (such as PAN, RevCol, Deep Supervision), designing a training strategy that <strong>supplements semantic loss during training yet deploys with zero cost during inference</strong>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=design-concept>Design Concept<a href=#design-concept class=hash-link aria-label="Direct link to Design Concept" title="Direct link to Design Concept">​</a></h3>
<p>PGI consists of three key components:</p>
<ol>
<li>
<p><strong>Main Branch</strong></p>
<ul>
<li>The original model architecture responsible for inference.</li>
<li>Only this branch is kept during actual deployment, maintaining inference efficiency.</li>
</ul>
</li>
<li>
<p><strong>Auxiliary Reversible Branch</strong></p>
<ul>
<li>Provides additional gradients during training to compensate for semantic features lost due to information bottlenecks in the main branch.</li>
</ul>
</li>
<li>
<p><strong>Multi-level Auxiliary Information</strong></p>
<ul>
<li>Integrates and reorganizes supervision signals from prediction heads at different scales, enabling the main branch to learn gradients consistent with global semantics.</li>
</ul>
</li>
</ol>
<p>In Figure (b), reversible architectures like RevCol can preserve information integrity but significantly increase inference latency. The authors observe that <strong>instead of making the main branch reversible, it is better to introduce an auxiliary reversible branch during training</strong>.</p>
<p>Figure (d) illustrates this design.</p>
<p>During training, this auxiliary branch simulates a complete semantic path of the input data, providing clear and reliable gradient signals to the main branch. Even if the main branch’s features have been compressed or diluted, it can still receive correct corrections from the auxiliary branch.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=multi-level-semantic-integration>Multi-level Semantic Integration<a href=#multi-level-semantic-integration class=hash-link aria-label="Direct link to Multi-level Semantic Integration" title="Direct link to Multi-level Semantic Integration">​</a></h3>
<p>Traditional deep supervision, as shown in Figure (c), assigns features at each layer to different tasks (e.g., small object detection, large object detection), which can cause semantic conflicts. For example, a layer focusing on small objects while ignoring large ones may cause the backbone features to mistakenly treat important regions as background, resulting in distorted training signals.</p>
<p>To solve this problem, PGI proposes a <strong>Multi-level Auxiliary Information module</strong> that integrates semantic information via a semantic fusion network before sending gradients back from the prediction branches to the backbone.</p>
<p>The advantages of this design are:</p>
<ul>
<li>Features at all layers learn complete semantics rather than correspond to a single object scale only.</li>
<li>Eliminates supervisory conflicts between different prediction heads.</li>
<li>Allows customizable fusion architectures to flexibly adjust semantic depth and scope.</li>
</ul>
<p>PGI transforms “deep supervision” from simple multi-point losses into a <strong>multi-semantic consistency fusion training strategy</strong>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=generalized-elan>Generalized ELAN<a href=#generalized-elan class=hash-link aria-label="Direct link to Generalized ELAN" title="Direct link to Generalized ELAN">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=gelan_arch src=/en/assets/images/img4-232ebbeae551d4e6a30b112c4512ce86.jpg width=1224 height=504 class=img_ev3q></figure></div>
<p>To fully unleash the potential of the PGI framework, YOLOv9 concurrently introduces a new backbone architecture:</p>
<ul>
<li><strong>Generalized ELAN (GELAN).</strong></li>
</ul>
<p>As shown above, GELAN draws inspiration from CSPNet and ELAN, where:</p>
<ul>
<li><strong>CSPNet</strong> provides branch-wise gradient control to enhance feature reuse;</li>
<li><strong>ELAN</strong> uses deep convolutional stacking for effective aggregation.</li>
</ul>
<p>GELAN further <strong>generalizes ELAN’s computational unit design</strong>, supporting arbitrary computational modules (such as standard convolution, depthwise convolution, attention modules, etc.), enabling optimization according to different platform constraints.</p>
<p>Its advantages include:</p>
<ul>
<li><strong>Modular design</strong>: adaptable for mobile, embedded devices, or server inference.</li>
<li><strong>Stable gradient paths</strong>: structurally built-in signal flow mechanisms that synergize well with PGI.</li>
<li><strong>Lightweight yet efficient</strong>: can achieve higher parameter utilization efficiency even without using depthwise convolutions.</li>
</ul>
<p>As the backbone network, GELAN not only facilitates PGI’s effectiveness but also lays a foundation for efficient and scalable models.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p>To verify the practicality and generalization capability of the proposed <strong>PGI training mechanism</strong> and <strong>GELAN architecture</strong>, the authors conducted comprehensive experiments on the MS COCO 2017 dataset, comparing against multiple current state-of-the-art real-time object detection models.</p>
<p>This study follows the YOLOv7 training protocol and uses the standard MS COCO 2017 dataset split. All models were trained from scratch for a total of <strong>500 epochs</strong>, with the first 3 epochs using <strong>linear warm-up</strong> and subsequent learning rates decayed according to model scale.</p>
<p>To stabilize final training, <strong>Mosaic data augmentation</strong> was disabled during the last 15 epochs.</p>
<p>YOLOv9 is implemented in two versions:</p>
<ul>
<li><strong>YOLOv9-C / YOLOv9-E</strong>: built respectively upon YOLOv7 and Dynamic YOLOv7.</li>
<li>The original YOLOv7’s <strong>ELAN modules</strong> were replaced by the proposed <strong>GELAN</strong>, using <strong>RepConv and CSPNet blocks</strong> as computational units.</li>
<li>Downsampling modules were simplified, and the anchor-free prediction head further optimized.</li>
<li>PGI’s auxiliary supervision settings fully retained YOLOv7’s multi-branch feature learning design.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=comparison-with-existing-methods>Comparison with Existing Methods<a href=#comparison-with-existing-methods class=hash-link aria-label="Direct link to Comparison with Existing Methods" title="Direct link to Comparison with Existing Methods">​</a></h3>
<p><img decoding=async loading=lazy alt=sota_comparison src=/en/assets/images/img5-e248d175fabf95182a2a222f9b97958f.jpg width=1394 height=1294 class=img_ev3q></p>
<p>Experimental results shown above demonstrate that YOLOv9 outperforms existing real-time object detectors of similar scale across multiple tasks, with advantages in parameter count, computation cost, and final accuracy:</p>
<ul>
<li>
<p>For <strong>lightweight models</strong> (e.g., YOLO MS-S), YOLOv9 reduces parameters by about <strong>10%</strong> and computation by <strong>5–15%</strong> while maintaining accuracy, highlighting its suitability for edge computing.</p>
</li>
<li>
<p>For <strong>mid-tier models</strong> (YOLO MS), YOLOv9 retains cost advantages and further improves accuracy by <strong>0.4 to 0.6 AP percentage points</strong>, demonstrating better model efficiency despite smaller overall size.</p>
</li>
<li>
<p>Compared with the mainstream <strong>YOLOv7 AF</strong>, YOLOv9-C reduces parameters by <strong>42%</strong> and FLOPs by <strong>22%</strong>, while achieving the same AP (53%), underscoring PGI’s effective training quality improvement.</p>
</li>
<li>
<p>Compared to the large <strong>YOLOv8-X</strong>, YOLOv9-E cuts parameters by <strong>16%</strong> and FLOPs by <strong>27%</strong>, yet improves AP by <strong>1.7%</strong>, fully showcasing the synergy of efficient design and semantics-driven training.</p>
</li>
</ul>
<p>These results indicate that YOLOv9 achieves highly competitive performance in various real-time detection scenarios <strong>without relying on any pretrained models or external datasets</strong>, confirming the practicality and generalizability of PGI training and GELAN architecture.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=gelan-architecture-analysis>GELAN Architecture Analysis<a href=#gelan-architecture-analysis class=hash-link aria-label="Direct link to GELAN Architecture Analysis" title="Direct link to GELAN Architecture Analysis">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=gelan_analysis src=/en/assets/images/img6-ea87b7b30047393bbb811f3b41e586d3.jpg width=1224 height=480 class=img_ev3q></figure></div>
<p>First, the authors examined the substitutability of GELAN’s computational modules, comparing three alternatives:</p>
<ul>
<li><strong>ResNet Block</strong></li>
<li><strong>DarkNet Block</strong></li>
<li><strong>CSP Block</strong></li>
</ul>
<p>Results show that even replacing ELAN’s original convolutional units with these modules maintains stable overall performance, demonstrating GELAN’s high flexibility and portability. Among them, <strong>CSP Block performed best</strong>, improving accuracy by <strong>0.7 AP</strong> while reducing parameters and computation.</p>
<p>Hence, YOLOv9 ultimately selects <strong>CSP</strong> as GELAN’s core unit, balancing performance and lightweight design.</p>
<p>Further, the authors tested the impact of <strong>ELAN module depth</strong> and <strong>CSP stacking depth</strong> on performance across different model scales.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=elan_depth_analysis src=/en/assets/images/img7-0f8e9b2731bfda1e2e58c976c7c6458b.jpg width=1224 height=928 class=img_ev3q></figure></div>
<p>The experiments show:</p>
<ul>
<li>Increasing ELAN depth from 1 to 2 yields significant accuracy gains.</li>
<li>For depths ≥ 2, increasing either ELAN or CSP stack layers results in linear scaling of performance, parameters, and computation, with no explosive gains.</li>
</ul>
<p>In other words, <strong>GELAN achieves stable performance without excessive fine-tuning</strong>, suitable for flexible design according to hardware or task needs.</p>
<p>YOLOv9 versions use the following depth configurations:</p>
<ul>
<li>YOLOv9-S: ELAN depth 2, CSP depth 3</li>
<li>YOLOv9-M / C: ELAN depth 2, CSP depth 1</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=pgi-training-strategy-analysis>PGI Training Strategy Analysis<a href=#pgi-training-strategy-analysis class=hash-link aria-label="Direct link to PGI Training Strategy Analysis" title="Direct link to PGI Training Strategy Analysis">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=pgi_analysis src=/en/assets/images/img8-5b8fb767cb60bceeaaab8ddd0e47f461.jpg width=1224 height=832 class=img_ev3q></figure></div>
<p>PGI consists of two main components: the auxiliary reversible branch and multi-level semantic integration.</p>
<p>The authors conducted ablation studies independently and jointly, finding:</p>
<ul>
<li>The <strong>auxiliary reversible branch (ICN)</strong> uses <strong>DHLC linkage</strong> design to provide multi-level reversible information, stably enhancing model performance.</li>
<li><strong>Multi-level semantic integration</strong> was tested with FPN and PAN structures, where PAN resembles traditional Deep Supervision (PFH) but shows notable effect only in very deep models.</li>
<li><strong>PGI contributes across all model sizes</strong>, with the ICN combination providing the most stable improvements, confirming its capability to alleviate the information bottleneck problem.</li>
</ul>
<p>Additionally, YOLOv7’s <strong>Lead-Head Guided Assignment technique</strong> was applied to PGI’s auxiliary supervision path, further improving supervision quality and final learning outcomes.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-scale-comparison>Model Scale Comparison<a href=#model-scale-comparison class=hash-link aria-label="Direct link to Model Scale Comparison" title="Direct link to Model Scale Comparison">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=model_size_comparison src=/en/assets/images/img9-dc8a3f25e7ec5bf313b3a4d67ab797b2.jpg width=1224 height=880 class=img_ev3q></figure></div>
<p>The authors applied PGI and traditional Deep Supervision on different model scales, observing:</p>
<ul>
<li><strong>Deep supervision reduces accuracy in shallow models</strong>, due to over-supervision causing feature misguidance.</li>
</ul>
<p>In general models, deep supervision performs unstably and may induce overfitting. PGI, however, <strong>stably improves accuracy across scales</strong>, showing significant advantage especially in lightweight models.</p>
<p>This validates PGI’s two main contributions:</p>
<ol>
<li><strong>Successfully applying auxiliary supervision to lightweight/shallow models</strong></li>
<li><strong>Injecting more reliable semantic gradients in deep model training, enhancing learning efficiency and convergence quality</strong></li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=overall-component-integration>Overall Component Integration<a href=#overall-component-integration class=hash-link aria-label="Direct link to Overall Component Integration" title="Direct link to Overall Component Integration">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=component_integration src=/en/assets/images/img10-39635723accc994053468513a1f50081.jpg width=1224 height=348 class=img_ev3q></figure></div>
<p>Finally, the authors present an ablation from the <strong>YOLOv7 baseline</strong> to the full <strong>YOLOv9-E version</strong>.</p>
<p>After gradually adding GELAN and PGI, the model shows clear improvements in accuracy, efficiency, and computational cost, proving these two designs bring comprehensive breakthroughs to YOLOv9.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>The key innovation of YOLOv9 lies in transforming the "gradient flow" into a <strong>programmable information transmission mechanism</strong>.</p>
<p>Through PGI, training is no longer passive backpropagation but an active planning of:</p>
<ul>
<li>How semantics should flow</li>
<li>Where gradients should come from</li>
<li>At which layer the target should be learned</li>
</ul>
<p>Together with the GELAN backbone, YOLOv9 simultaneously achieves three major goals: high accuracy, low parameter count, and deployment flexibility.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-07-19T06:37:31.000Z itemprop=dateModified>Jul 19, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/object-detection/yolo-world/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[24.01] YOLO-World</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/object-detection/yolo-tiny/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[24.12] YOLO-Tiny</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#programmable-gradient-information class="table-of-contents__link toc-highlight">Programmable Gradient Information</a><li><a href=#problem-definition class="table-of-contents__link toc-highlight">Problem Definition</a><ul><li><a href=#why-does-semantic-information-get-lost class="table-of-contents__link toc-highlight">Why Does Semantic Information Get Lost?</a><li><a href=#the-possibility-of-reversible-functions class="table-of-contents__link toc-highlight">The Possibility of Reversible Functions</a></ul><li><a href=#solution class="table-of-contents__link toc-highlight">Solution</a><ul><li><a href=#design-concept class="table-of-contents__link toc-highlight">Design Concept</a><li><a href=#multi-level-semantic-integration class="table-of-contents__link toc-highlight">Multi-level Semantic Integration</a><li><a href=#generalized-elan class="table-of-contents__link toc-highlight">Generalized ELAN</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#comparison-with-existing-methods class="table-of-contents__link toc-highlight">Comparison with Existing Methods</a><li><a href=#gelan-architecture-analysis class="table-of-contents__link toc-highlight">GELAN Architecture Analysis</a><li><a href=#pgi-training-strategy-analysis class="table-of-contents__link toc-highlight">PGI Training Strategy Analysis</a><li><a href=#model-scale-comparison class="table-of-contents__link toc-highlight">Model Scale Comparison</a><li><a href=#overall-component-integration class="table-of-contents__link toc-highlight">Overall Component Integration</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>