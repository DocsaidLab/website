<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-object-detection/detr/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">[20.05] DETR | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/object-detection/detr/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[20.05] DETR | DOCSAID"><meta data-rh="true" name="description" content="A Foundation Across Domains"><meta data-rh="true" property="og:description" content="A Foundation Across Domains"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/object-detection/detr/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/object-detection/detr/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/object-detection/detr/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/object-detection/detr/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.1fe4c5ae.css">
<script src="/en/assets/js/runtime~main.b5f82560.js" defer="defer"></script>
<script src="/en/assets/js/main.092819f0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" href="/en/papers/object-detection/detr/"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/object-detection/detr/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/object-detection/detr/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/alexnet/">[12.09] AlexNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vgg/">[14.09] VGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/densenet/">[16.08] DenseNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/resnext/">[16.11] ResNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v1/">[17.04] MobileNet-V1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/nasnet/">[17.07] NASNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/shufflenet/">[17.07] ShuffleNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/senet/">[17.09] SENet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v2/">[18.01] MobileNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet/">[19.05] EfficientNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v3/">[19.05] MobileNet-V3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/ghostnet/">[19.11] GhostNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vit/">[20.10] ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/deit/">[20.12] DeiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/repvgg/">[21.01] RepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pvt/">[21.02] PVT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet-v2/">[21.04] EfficientNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mlp-mixer/">[21.05] MLP-Mixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/beit/">[21.06] BEiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pp-lcnet/">[21.09] PP-LCNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/poolformer/">[21.11] PoolFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/convnext/">[22.01] ConvNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/replknet/">[22.03] RepLKNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobileone/">[22.06] MobileOne</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/caformer/">[22.10] CAFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/qarepvgg/">[22.12] QARepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/fastvit/">[23.03] FastViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vanillanet/">[23.05] VanillaNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/repvit/">[23.07] RepViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v4/">[24.04] MobileNet-V4</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition">Face Recognition</a><button aria-label="Expand sidebar category &#x27;Face Recognition&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion">Feature Fusion</a><button aria-label="Expand sidebar category &#x27;Feature Fusion&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/language-model">Language Model</a><button aria-label="Expand sidebar category &#x27;Language Model&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/multimodality">Multimodality</a><button aria-label="Expand sidebar category &#x27;Multimodality&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/object-detection">Object Detection</a><button aria-label="Collapse sidebar category &#x27;Object Detection&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/object-detection/yolov1/">[15.06] YOLO-V1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/object-detection/detr/">[20.05] DETR</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/object-detection"><span itemprop="name">Object Detection</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[20.05] DETR</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>[20.05] DETR</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-foundation-across-domains">A Foundation Across Domains<a class="hash-link" aria-label="Direct link to A Foundation Across Domains" title="Direct link to A Foundation Across Domains" href="/en/papers/object-detection/detr/#a-foundation-across-domains">​</a></h2>
<p><strong><a href="https://arxiv.org/abs/2005.12872" target="_blank" rel="noopener noreferrer">End-to-End Object Detection with Transformers</a></strong></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>The following content has been compiled by ChatGPT-4 and manually proofread, edited, and supplemented.</p></div></div>
<hr>
<p>Object detection has always been a core task in computer vision.</p>
<p>Anchor-based methods like Faster R-CNN, SSD, and YOLOv2 use predefined bounding boxes (anchors) to predict the locations of objects. These anchors are manually selected for their size and aspect ratios, aiming to cover all possible object shapes. Anchor-based methods have inherent advantages, especially in detecting objects of various scales and shapes, as they generalize well to objects with different scales and aspect ratios.</p>
<p>However, choosing appropriate anchor sizes and ratios often requires prior knowledge and manual tuning, which may not be suitable for all applications or datasets. Additionally, using multiple anchors at each position can lead to numerous redundant predictions, particularly in background regions without objects, necessitating additional non-maximum suppression (NMS) steps to clean up.</p>
<p>Meanwhile, in the NLP domain, the development of Transformer architecture has advanced rapidly, yielding significant breakthroughs. These factors led the authors of this paper to decide:</p>
<ul>
<li><strong>Why not apply Transformers in object detection too?</strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-definition">Problem Definition<a class="hash-link" aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition" href="/en/papers/object-detection/detr/#problem-definition">​</a></h2>
<p>The authors define several current issues in the paper:</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="anchor-based-methods-are-complicated">Anchor-based Methods are Complicated<a class="hash-link" aria-label="Direct link to Anchor-based Methods are Complicated" title="Direct link to Anchor-based Methods are Complicated" href="/en/papers/object-detection/detr/#anchor-based-methods-are-complicated">​</a></h3>
<p>Anchor-based methods have become popular for object detection. These methods use predefined anchors to predict bounding boxes and categories. These anchors are typically predefined bounding boxes of various sizes and aspect ratios, uniformly placed across the image.</p>
<p>However, anchor-based methods pose several challenges:</p>
<ol>
<li>
<p><strong>Redundant Predictions</strong></p>
<p>Due to the numerous anchors covering the image, a single object may be detected multiple times, resulting in multiple overlapping bounding boxes for the same object.</p>
</li>
<li>
<p><strong>Complex Post-processing</strong></p>
<p>To address the redundant prediction problem, post-processing techniques such as non-maximum suppression (NMS) are required. NMS removes overlapping bounding boxes, keeping only the best prediction for each object, adding computational and parameter adjustment complexity.</p>
</li>
<li>
<p><strong>Anchor Design and Matching</strong></p>
<p>Designing the size and aspect ratios of anchors is critical for model performance. Inappropriate anchor design can lead to inaccurate detections. Additionally, matching new predictions with the closest anchors involves extra computation.</p>
</li>
</ol>
<p>If anchor-based methods are complex, can we avoid using anchors?</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="anchor-free-methods-lack-accuracy">Anchor-free Methods Lack Accuracy<a class="hash-link" aria-label="Direct link to Anchor-free Methods Lack Accuracy" title="Direct link to Anchor-free Methods Lack Accuracy" href="/en/papers/object-detection/detr/#anchor-free-methods-lack-accuracy">​</a></h3>
<p>Accuracy remains paramount in practical scenarios. Users may tolerate slower speeds or complex systems, but:</p>
<ul>
<li><strong>Inaccurate models will lead to complaints.</strong></li>
</ul>
<p>Direct prediction has been an attractive concept in object detection. Unlike anchor-based strategies, direct prediction aims to predict object bounding boxes and category labels directly from image pixels without intermediate anchors.</p>
<p>Despite the theoretical appeal, previous attempts at direct prediction often fell short in performance. These methods might not perform well on common object detection benchmarks compared to leading methods.</p>
<p>Although direct prediction methods offer theoretical advantages with simplified model structures and fewer manual priors, their practical performance and competitiveness need improvement. Researchers continue to seek ways to improve these strategies to achieve true end-to-end object detection.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solution">Solution<a class="hash-link" aria-label="Direct link to Solution" title="Direct link to Solution" href="/en/papers/object-detection/detr/#solution">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="detr-model-design">DETR Model Design<a class="hash-link" aria-label="Direct link to DETR Model Design" title="Direct link to DETR Model Design" href="/en/papers/object-detection/detr/#detr-model-design">​</a></h3>
<p><img decoding="async" loading="lazy" alt="DETR Model Architecture" src="/en/assets/images/detr_1-20bf18537c142000ca4626cbd3cbbe88.jpg" width="1024" height="266" class="img_ev3q"></p>
<ol>
<li>
<p><strong>Backbone</strong></p>
<p>The backbone extracts features from the input image using a convolutional neural network (CNN). The input image produces a set of feature maps that capture various details and contextual information, albeit at a reduced resolution.</p>
</li>
<li>
<p><strong>Transformer Encoder</strong></p>
<p>The Transformer Encoder receives the output feature maps from the backbone. To help the Transformer understand the relative positions of each feature, positional encodings are added. Unlike CNNs, Transformers are not inherently sensitive to input order, and positional encodings provide a way to input positional information, allowing the Transformer to consider the relative positions of features.</p>
</li>
<li>
<p><strong>Transformer Decoder</strong></p>
<p>The Decoder input consists of fixed-size vectors called &quot;object queries,&quot; representing general expectations for detected objects. Through multiple layers of the decoder, these queries attend to the encoder outputs, identifying features corresponding to specific objects.</p>
</li>
<li>
<p><strong>Feed-Forward Network (FFN)</strong></p>
<p>Each output from the decoder goes through the FFN, converting the decoder outputs into specific predictions, including object center coordinates (x, y), height (H), and width (W). The number of predictions is fixed at N, typically much larger than the actual number of objects in the image. If no object is found, it can output a special &quot;no object&quot; category.</p>
<p>The overall architecture is illustrated below:</p>
<p><img decoding="async" loading="lazy" alt="DETR Model Architecture" src="/en/assets/images/detr_3-69a48ad1663edee8a24b38940ed6001f.jpg" width="908" height="1000" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>I was curious about how the spatial positional encoding in the paper differs from the typical Transformer, so I looked into the implementation. It turns out it&#x27;s based on learnable parameters for rows and columns.</p><p><img decoding="async" loading="lazy" alt="DETR Model Architecture" src="/en/assets/images/detr_4-d53787cc476c8c3404d9a3bab0d5270d.jpg" width="1024" height="782" class="img_ev3q"></p></div></div>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="loss-function-design">Loss Function Design<a class="hash-link" aria-label="Direct link to Loss Function Design" title="Direct link to Loss Function Design" href="/en/papers/object-detection/detr/#loss-function-design">​</a></h3>
<ol>
<li>
<p><strong>Set Prediction Loss</strong></p>
<p>This enforces a unique matching loss between predicted boxes and ground truth boxes.</p>
<ul>
<li>
<p><strong>Matching Loss (L_match)</strong></p>
<p>DETR outputs a fixed-size set of N predictions. Here, N is set to be significantly larger than the typical number of objects in an image. The loss produces the best bipartite matching between predicted and ground truth objects. If an image has 5 ground truth objects but the model predicts 8, we still need to find the best match between these 5 ground truths and 8 predictions.</p>
</li>
<li>
<p><strong>Hungarian Algorithm</strong></p>
<p>The Hungarian algorithm finds the best match, considering class predictions and similarity between predicted and ground truth boxes.</p>
<p>The Hungarian algorithm solves the bipartite matching problem, commonly used in assignment problems. It finds the optimal assignment minimizing the total cost of assigning n tasks to n workers.</p>
<p>Additionally, the log probability term for the &quot;no object&quot; class is down-weighted by a factor of 10 to avoid excessive background or non-object region interference during training.</p>
</li>
</ul>
</li>
<li>
<p><strong>Bounding Box Loss</strong></p>
<p>In object detection, the model predicts the location and extent of objects, usually represented as a &quot;bounding box.&quot; Bounding boxes are typically described by four coordinates (e.g., top-left and bottom-right x, y coordinates) or a center point and width-height.</p>
<ul>
<li>
<p><strong>Bounding Box Loss (L_box)</strong></p>
<p>Unlike traditional methods that adjust candidate boxes (anchors or proposals), DETR directly predicts object bounding boxes.</p>
<p>Using common losses like L1 loss may yield different values for objects of varying sizes even with the same relative position deviation.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>If a model predicts a small and a large object, both deviating 10 pixels from their true positions, the 10-pixel deviation might be significant for the small object but negligible for the large one. Using L1 loss, these cases might have significantly different loss values.</p></div></div>
</li>
<li>
<p><strong>Problem Solving</strong></p>
<p>To address this, DETR combines L1 loss with Generalized IoU (Intersection over Union) loss. Generalized IoU loss mitigates the impact of object size by considering the overlap between predicted and ground truth boxes, aiming to maximize their intersection. Combining these losses provides a consistent loss scale across different object sizes, aiding the model in learning bounding boxes for objects of various sizes more uniformly.</p>
</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dataset">Dataset<a class="hash-link" aria-label="Direct link to Dataset" title="Direct link to Dataset" href="/en/papers/object-detection/detr/#dataset">​</a></h3>
<ul>
<li><strong>Dataset Used</strong>: The COCO 2017 detection and panoptic segmentation datasets were used for experiments.</li>
<li><strong>Dataset Size</strong>: The dataset contains 118,000 training images and 5,000 validation images.</li>
<li><strong>Data Annotation</strong>: Each image is annotated with bounding boxes and panoptic segmentation information.</li>
<li><strong>Image Details</strong>: On average, each image contains 7 object instances, with a maximum of 63 instances in the training set, ranging from small to large objects.</li>
<li><strong>Evaluation Metric</strong>: Average Precision (AP) was used as the evaluation metric, with AP based on bounding boxes by default. To compare with Faster R-CNN, the paper reports validation AP of the final training epoch.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="technical-details">Technical Details<a class="hash-link" aria-label="Direct link to Technical Details" title="Direct link to Technical Details" href="/en/papers/object-detection/detr/#technical-details">​</a></h3>
<ul>
<li><strong>Training Optimizer</strong>: AdamW was used to train the DETR model.</li>
<li><strong>Learning Rate Settings</strong>:<!-- -->
<ul>
<li>Transformer initial learning rate: 1e-4</li>
<li>Backbone network learning rate: 1e-5</li>
<li>Weight decay: 1e-4.</li>
</ul>
</li>
<li><strong>Weight Initialization</strong>: Transformer weights were initialized using Xavier init. The backbone network used an ImageNet pre-trained ResNet model with frozen BatchNorm layers.</li>
<li><strong>Model Architecture</strong>: The paper reports results for two backbone networks: ResNet50 and ResNet-101, named DETR and DETR-R101, respectively. A modified version, DETR-DC5 and DETR-DC5-R101, was also tested to improve small object detection at the cost of increased computation.</li>
<li><strong>Image Preprocessing</strong>: Images were resized with scale augmentation, ensuring the shortest side was between 480 and 800 pixels, with a maximum longest side of 1333 pixels. Random cropping augmentation during training improved performance by about 1 AP.</li>
<li><strong>Other Details</strong>: Dropout of 0.1 was used for training. During inference, some predictions were empty, and confidence scores were used to cover these empty predictions, increasing AP by 2.</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion" href="/en/papers/object-detection/detr/#discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="is-it-effective">Is It Effective?<a class="hash-link" aria-label="Direct link to Is It Effective?" title="Direct link to Is It Effective?" href="/en/papers/object-detection/detr/#is-it-effective">​</a></h3>
<p><img decoding="async" loading="lazy" alt="DETR Performance" src="/en/assets/images/detr_2-d433d652fadb40a1808d7383ce261218.jpg" width="1024" height="425" class="img_ev3q"></p>
<p>Small object detection performance drops significantly (AP 27.2 -&gt; AP 23.7), but overall, it looks promising!</p>
<p>When comparing DETR to Faster R-CNN, several key points emerge:</p>
<ol>
<li>
<p><strong>Differences in Training Methods</strong></p>
<ul>
<li>DETR uses a Transformer architecture, typically with Adam or Adagrad optimizers. Longer training schedules and dropout are often employed to allow the model to learn more in-depth representations.</li>
<li>Faster R-CNN mainly uses SGD for training, with relatively fewer data augmentations.</li>
</ul>
</li>
<li>
<p><strong>Enhancing the Faster R-CNN Baseline</strong></p>
<ul>
<li>To make Faster R-CNN more comparable to DETR, researchers added generalized IoU to its loss function, applied the same random crop augmentation, and adopted longer training schedules.</li>
<li>These adjustments improved Faster R-CNN&#x27;s performance on the COCO detection task by 1-2 AP.</li>
</ul>
</li>
<li>
<p><strong>Model Comparison</strong></p>
<ul>
<li>The authors reported results for Faster R-CNN trained with a 3x schedule and enhanced with augmentations and a 9x schedule.</li>
<li>For DETR, models with similar parameter counts were considered, meaning DETR and Faster R-CNN had similar model complexity.</li>
</ul>
</li>
<li>
<p><strong>Performance Comparison</strong></p>
<ul>
<li>DETR achieved 42 AP on the COCO val subset with the same number of parameters as Faster R-CNN, demonstrating competitive performance.</li>
<li>DETR improved its overall performance mainly through better large object detection (APL). However, it still lagged in small object detection (APS).</li>
<li>DETR-DC5, despite higher overall AP, still underperformed Faster R-CNN in small object detection.</li>
<li>Faster R-CNN and DETR showed similar trends when using ResNet-101 as the backbone.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="component-analysis">Component Analysis<a class="hash-link" aria-label="Direct link to Component Analysis" title="Direct link to Component Analysis" href="/en/papers/object-detection/detr/#component-analysis">​</a></h3>
<p>The authors delved into the importance of various components in the DETR architecture:</p>
<ol>
<li>
<p><strong>Encoder Layers</strong></p>
<ul>
<li>Increasing encoder layers affects global image-level self-attention.</li>
<li>Removing encoder layers entirely causes a 3.9 AP drop, particularly impacting large objects (6.0 AP).</li>
<li>Encoders are crucial for object unmixing, providing global scene reasoning.</li>
<li>Encoders can separate instances, benefiting object extraction and localization in decoders.</li>
</ul>
</li>
<li>
<p><strong>Decoder Layers</strong></p>
<ul>
<li>Increasing decoder layers improves AP and AP50.</li>
<li>Due to set-based loss, DETR does not require NMS.</li>
<li>Running NMS after the first decoder layer improves performance.</li>
<li>With more layers, NMS&#x27;s improvement diminishes.</li>
<li>Decoder attention is local, focusing on object extremities like heads or limbs.</li>
</ul>
</li>
<li>
<p><strong>Importance of FFN</strong></p>
<ul>
<li>FFN acts as a 1x1 convolution layer.</li>
<li>Removing FFN entirely reduces performance by 2.3 AP, indicating its significance for good results.</li>
</ul>
</li>
<li>
<p><strong>Importance of Positional Encoding</strong></p>
<ul>
<li>There are two types of positional encodings in the model: spatial and output positional encodings.</li>
<li>Removing spatial positional encoding entirely leads to a 7.8 AP drop.</li>
<li>Using sinusoidal or learnable encoding results in only a slight 1.3 AP drop.</li>
<li>Positional encoding significantly impacts model performance.</li>
</ul>
</li>
<li>
<p><strong>Importance of Loss Components</strong></p>
<ul>
<li>Classification Loss is indispensable.</li>
<li>GIoU Loss significantly contributes to model performance, with its absence causing a 0.7 AP drop.</li>
<li>Using L1 Loss alone degrades results.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="application-to-panoptic-segmentation">Application to Panoptic Segmentation<a class="hash-link" aria-label="Direct link to Application to Panoptic Segmentation" title="Direct link to Application to Panoptic Segmentation" href="/en/papers/object-detection/detr/#application-to-panoptic-segmentation">​</a></h3>
<p>Panoptic segmentation has garnered significant attention recently. This section is not the main focus of the paper, but the authors briefly explain its application, highlighting its future importance.</p>
<ul>
<li>
<p><strong>Experimental Setup</strong></p>
<p>DETR was tested on the COCO dataset for panoptic segmentation. This dataset includes 80 object categories and 53 stuff categories. During DETR training, the model must predict bounding boxes around objects and stuff classes. Bounding box prediction is essential for training as the Hungarian matching algorithm relies on box distance calculations.</p>
</li>
<li>
<p><strong>New Mask Head</strong></p>
<p><img decoding="async" loading="lazy" alt="DETR Mask Architecture" src="/en/assets/images/detr_5-51a03b557e1ca0be4d41e9c32c207fe5.jpg" width="1024" height="321" class="img_ev3q"></p>
<p>Besides the basic DETR architecture, a new mask head was added to predict a binary mask for each predicted box. This mask head takes the Transformer decoder output for each object and computes multi-headed attention scores on encoder outputs, generating attention heatmaps for each object. To get final predictions and increase resolution, an FPN-like structure was used.</p>
<p>During training, DETR was first trained to predict bounding boxes, followed by 25 epochs of mask head training. This could be done end-to-end or with separate training stages, with similar results. For final panoptic segmentation, the model uses argmax on mask scores for each pixel and assigns corresponding classes to the resulting masks, ensuring no overlap between final masks.</p>
</li>
<li>
<p><strong>Results</strong></p>
<p><img decoding="async" loading="lazy" alt="DETR Mask Results 1" src="/en/assets/images/detr_6-0dc5b20dbe6154ca746c614398c4b008.jpg" width="1024" height="298" class="img_ev3q"></p>
<p>DETR showed strong performance in panoptic segmentation, especially for stuff classes, likely due to the encoder&#x27;s global reasoning ability. Despite a performance gap in stuff class mask prediction, DETR achieved competitive PQ scores, reaching 46 PQ on the COCO test set.</p>
<p><img decoding="async" loading="lazy" alt="DETR Mask Results 2" src="/en/assets/images/detr_7-25e486fc58029679c5dabe4728b1f18c.jpg" width="1024" height="281" class="img_ev3q"></p>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" href="/en/papers/object-detection/detr/#conclusion">​</a></h2>
<p>No technology is perfect.</p>
<p>DETR&#x27;s primary challenge is detecting small objects, where it still has significant room for improvement.</p>
<p>Moreover, compared to Faster R-CNN, which has undergone years of refinement, DETR only achieves comparable results in some scenarios, indicating it may not always be the best choice.</p>
<p>However, this does not diminish the significance and importance of this paper. DETR is not just a new technology but a new way of thinking. By combining advanced Transformer architecture with bipartite matching loss, it offers a novel, direct set prediction approach.</p>
<p>DETR&#x27;s simplicity and intuitiveness are major selling points. In many object detection methods, we need to go through complex annotation and specific anchor selection processes, but DETR eliminates these constraints, making the entire process simpler. Furthermore, its application range is broad, extending beyond object detection to panoptic segmentation, achieving notable results in this area. Most impressively, when dealing with large objects, its self-attention mechanism effectively captures global information in the image, surpassing other traditional methods in this aspect.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-08-01T00:11:53.000Z" itemprop="dateModified">Aug 1, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/object-detection/yolov1/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[15.06] YOLO-V1</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#a-foundation-across-domains">A Foundation Across Domains</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#problem-definition">Problem Definition</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#anchor-based-methods-are-complicated">Anchor-based Methods are Complicated</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#anchor-free-methods-lack-accuracy">Anchor-free Methods Lack Accuracy</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#solution">Solution</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#detr-model-design">DETR Model Design</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#loss-function-design">Loss Function Design</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#dataset">Dataset</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#technical-details">Technical Details</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#discussion">Discussion</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#is-it-effective">Is It Effective?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#component-analysis">Component Analysis</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#application-to-panoptic-segmentation">Application to Panoptic Segmentation</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/object-detection/detr/#conclusion">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>