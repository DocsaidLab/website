<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-clip/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">[21.03] CLIP | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/clip/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[21.03] CLIP | DOCSAID"><meta data-rh="true" name="description" content="Breaking the Dimensional Barrier"><meta data-rh="true" property="og:description" content="Breaking the Dimensional Barrier"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/clip/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/clip/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/clip/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/clip/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.1fe4c5ae.css">
<script src="/en/assets/js/runtime~main.bbf0d240.js" defer="defer"></script>
<script src="/en/assets/js/main.9ff1fd8f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" href="/en/papers/clip/"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/clip/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/clip/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/alexnet/">[12.09] AlexNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vgg/">[14.09] VGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/hourglass/">[16.03] Hourglass</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/densenet/">[16.08] DenseNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/resnext/">[16.11] ResNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/fpn/">[16.12] FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v1/">[17.04] MobileNet-V1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/nasnet/">[17.07] NASNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/shufflenet/">[17.07] ShuffleNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/senet/">[17.09] SENet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/cosface/">[18.01] CosFace</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v2/">[18.01] MobileNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/panet/">[18.03] PANet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/nasfpn/">[19.04] NAS-FPN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet/">[19.05] EfficientNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v3/">[19.05] MobileNet-V3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/unetpp/">[19.12] UNet++</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/detr/">[20.05] DETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vit/">[20.10] ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/deit/">[20.12] DeiT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/repvgg/">[21.01] RepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pvt/">[21.02] PVT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/en/papers/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/efficientnet-v2/">[21.04] EfficientNet-V2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/roformer/">[21.04] RoFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mlp-mixer/">[21.05] MLP-Mixer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/pp-lcnet/">[21.09] PP-LCNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/meter/">[21.11] METER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/poolformer/">[21.11] PoolFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/convnext/">[22.01] ConvNeXt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/frvt-distinguishing-twins/">[22.09] FRVT-Twins</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/caformer/">[22.10] CAFormer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/tivc/">[23.09] TIVC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/mobilenet-v4/">[24.04] MobileNet-V4</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[21.03] CLIP</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>[21.03] CLIP</h1>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="breaking-the-dimensional-barrier">Breaking the Dimensional Barrier<a class="hash-link" aria-label="Direct link to Breaking the Dimensional Barrier" title="Direct link to Breaking the Dimensional Barrier" href="/en/papers/clip/#breaking-the-dimensional-barrier">​</a></h2>
<p><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer"><strong>Learning Transferable Visual Models From Natural Language Supervision</strong></a></p>
<hr>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>The following content was compiled by ChatGPT-4, edited and supplemented with manual corrections and explanations.</p></div></div>
<hr>
<p>In our daily lives, information is everywhere, appearing in various forms such as text, images, sound, or video. You might have described a painting with words or used pictures to make a story more vivid.</p>
<p>This time, we discuss Multi-Model Machine Learning (MMML).</p>
<p>Multi-modal learning is a machine learning method that uses multiple sources or modalities of information, such as text, images, and audio. By combining different types of information, multi-modal learning can capture richer data features to improve model accuracy and performance. This is a relatively new but rapidly developing field of research, achieving significant progress in applications like visual question answering, sentiment analysis, and automatic caption generation.</p>
<p>Vision-Language is a term often associated with multi-modal learning, focusing on the interaction between vision (such as images or videos) and language (such as text or speech). For example, a vision-language model can take an image and a question to generate an answer about the image or generate descriptive text from an image.</p>
<p>Multi-modal learning allows for describing broader concepts. Though not new, it has gained more attention recently due to the significant performance improvements brought by attention mechanisms.</p>
<p>Now, let&#x27;s talk about CLIP, &quot;Contrastive Language–Image Pre-training.&quot; Its core idea is simple: using large amounts of text and image data, the model learns to understand images through text or understand text through images.</p>
<p>This &quot;bi-directional understanding&quot; enhances the model&#x27;s comprehension and opens new possibilities for multi-modal learning. By pretraining on vast amounts of data, CLIP learns to closely associate text with images, enabling it to generate descriptive text from an image or find matching images from a description.</p>
<p>Before diving into the model details, let&#x27;s first look at the problems CLIP addresses and its proposed solutions.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="defining-the-problem">Defining the Problem<a class="hash-link" aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem" href="/en/papers/clip/#defining-the-problem">​</a></h2>
<p>Try describing the following image:</p>
<p><img decoding="async" loading="lazy" alt="Coffee" src="/en/assets/images/coffee-740001a9c0f56f99363cdd405c10b4f7.jpg" width="1024" height="1024" class="img_ev3q"></p>
<ul>
<li>There&#x27;s a cup of coffee... um, a book under the cup... they are on a table?</li>
<li>Is it a corner of a coffee shop in the morning?</li>
<li>Brown table and chairs, and brown coffee? (Seriously, focus!)</li>
<li>...</li>
</ul>
<p>In fact, this image was generated using this description:</p>
<blockquote>
<p><em>On a peaceful morning, sunlight gently spills through the curtains onto a minimalist wooden table. A freshly brewed cup of coffee sits on the table, its aroma mingling with the sunlight, evoking warmth and hope for the new day. The cup casts a long shadow, forming a beautiful scene with the green plants by the window. The surface of the coffee ripples slightly, seemingly narrating the morning&#x27;s tranquility and life&#x27;s beauty. Next to the cup, an open book lies quietly, waiting to be read. In this serene morning, coffee, sunlight, greenery, and books together create a warm and tranquil scene, telling of the simplicity and beauty of life.</em></p>
</blockquote>
<p>If you also tried to describe this image, you might clearly feel the limitations of the ImageNet dataset used in academia and industry, despite its extensive categories and millions of images.</p>
<ul>
<li>There are too many ways to describe the same image.</li>
</ul>
<p>ImageNet often provides only a hierarchical category or limited labels to describe an image&#x27;s content. In contrast, human language and perception can interpret and understand an image from countless perspectives, such as color, shape, emotion, and narrative. This is the key difference between single-modal and multi-modal learning: the former usually analyzes information from a single perspective, while the latter combines different types of information for a richer and more diverse interpretation.</p>
<p>In single-label categories like ImageNet, much meaningful information can be overlooked, such as object interactions, contextual backgrounds, or the emotions evoked by the image. CLIP aims to break these limitations by combining information from various sources and types, such as text and images, to enrich the model&#x27;s understanding and expression capabilities, making it closer to human perception and understanding.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-problem">Solving the Problem<a class="hash-link" aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem" href="/en/papers/clip/#solving-the-problem">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="clip-model-architecture">CLIP Model Architecture<a class="hash-link" aria-label="Direct link to CLIP Model Architecture" title="Direct link to CLIP Model Architecture" href="/en/papers/clip/#clip-model-architecture">​</a></h3>
<p><img decoding="async" loading="lazy" alt="CLIP Architecture" src="/en/assets/images/arch_clip-ef4a34c3ec1c19ee45e598ca12ff5459.jpg" width="1726" height="622" class="img_ev3q"></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pretraining-process">Pretraining Process<a class="hash-link" aria-label="Direct link to Pretraining Process" title="Direct link to Pretraining Process" href="/en/papers/clip/#pretraining-process">​</a></h3>
<p>Suppose we have a set of image-text pairs, such as a picture of a dog and the text &quot;a cute puppy.&quot; In a training batch, CLIP receives multiple such pairs. The image encoder processes the image through ResNet or ViT to extract features, while the text encoder processes the text through a Transformer to extract text features.</p>
<p>The model then compares these features to ensure the cosine similarity between correctly paired images and texts (e.g., the dog image and &quot;a cute puppy&quot;) is maximized, while minimizing the cosine similarity between incorrectly paired images and texts (e.g., the dog image and &quot;an apple&quot;).</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="zero-shot-inference">Zero-Shot Inference<a class="hash-link" aria-label="Direct link to Zero-Shot Inference" title="Direct link to Zero-Shot Inference" href="/en/papers/clip/#zero-shot-inference">​</a></h3>
<p>To classify fruits using CLIP without additional labeled data, we can create a set of natural language prompts, such as &quot;A photo of an apple,&quot; &quot;A photo of a banana,&quot; &quot;A photo of a cherry,&quot; etc. When we have an image of a fruit to classify, we compare its features with the text features of these prompts, finding the text with the highest cosine similarity to infer the image&#x27;s category.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="natural-language-supervision">Natural Language Supervision<a class="hash-link" aria-label="Direct link to Natural Language Supervision" title="Direct link to Natural Language Supervision" href="/en/papers/clip/#natural-language-supervision">​</a></h3>
<p>By using natural language as a supervision signal, CLIP can easily expand its training dataset. Traditional supervised learning requires extensive labeling, but by leveraging natural language, one only needs to collect a large number of image-text pairs. This method not only learns multi-modal representations but also links these representations with natural language, enabling flexible zero-shot transfer learning.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="large-scale-dataset-wit">Large-Scale Dataset: WIT<a class="hash-link" aria-label="Direct link to Large-Scale Dataset: WIT" title="Direct link to Large-Scale Dataset: WIT" href="/en/papers/clip/#large-scale-dataset-wit">​</a></h3>
<p>A significant part of CLIP&#x27;s strong performance comes from its use of a large-scale dataset.</p>
<p>Initially, researchers used three datasets: MS-COCO, Visual Genome, and YFCC100M. However, they soon realized these datasets were too small for modern requirements. For example, MS-COCO and Visual Genome are high-quality datasets but only have about 100,000 training photos, which is minuscule compared to the 3.5 billion Instagram photos other computer vision systems handle. While YFCC100M has 100 million photos, many have low-quality captions and descriptions, such as autogenerated filenames or camera settings, providing little training help.</p>
<p>Thus, researchers created a new large-scale dataset called WIT (WebImageText), comprising 400 million image-text pairs from publicly available web resources.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>For example, finding a beautiful sunset picture online with the caption &quot;The red sunset reflects on the tranquil lake,&quot; such image-text pairs would be collected in the WIT dataset.</p></div></div>
<p>Researchers collected diverse image-text pairs across 500,000 different search queries, ensuring the dataset&#x27;s variety.</p>
<p>Creating the WIT dataset aimed to address the small scale and low quality of existing datasets, allowing CLIP to learn richer and more diverse visual and language knowledge. The WIT dataset&#x27;s total word count is similar to the WebText dataset used for training the GPT-2 model, highlighting its vast scale. This rich dataset enables CLIP to learn the correlations between images and text across numerous instances, achieving new heights in image and text understanding.</p>
<p>It must be noted that OpenAI&#x27;s substantial funding enables bold research advances, creating many advanced and practical technologies and models.</p>
<p>Here is the dataset download link: <a href="https://github.com/google-research-datasets/wit#wit--wikipedia-based-image-text-dataset" target="_blank" rel="noopener noreferrer">WIT: Wikipedia-based Image Text Dataset</a></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="training-details">Training Details<a class="hash-link" aria-label="Direct link to Training Details" title="Direct link to Training Details" href="/en/papers/clip/#training-details">​</a></h3>
<p>For model training, the authors selected a series of 5 ResNet and 3 Vision Transformer models. They trained a ResNet-50 and a ResNet-101, then scaled the ResNet-50 following the EfficientNet style to create three larger models, RN50x4, RN50x16, and RN50x64, with computation costs 4, 16, and 64 times that of ResNet-50.</p>
<p>For ViT models, they trained ViT-B/32, ViT-B/16, and ViT-L/14, each for 32 epochs using the Adam optimizer with decoupled weight decay regularization and cosine learning rate scheduling.</p>
<p>Initial hyperparameters were set through grid search, random search, and manual tuning on the baseline ResNet-50 model. Due to computational constraints, these hyperparameters were heuristically adjusted for larger models.</p>
<p>To ensure training stability, a learnable temperature parameter τ was initialized at 0.07, and measures were taken to prevent logits scaling above 100. Training and memory efficiency were enhanced using a large batch size of 32,768, mixed precision, gradient checkpointing, half-precision Adam statistics, and half-precision stochastic rounding text encoder weights.</p>
<p>Regarding hardware, training the largest ResNet model RN50x64 required 592 V100 GPUs for 18 days, while the largest Vision Transformer model needed 256 V100 GPUs for 12 days. To improve ViT-L/14 performance, they pre-trained for an additional epoch at a higher resolution of 336 pixels, labeled as ViT-L/14@336px.</p>
<p>The following table illustrates the configuration and requirements for training different models:</p>
<table><thead><tr><th>Model Name</th><th>Base Model</th><th>Compute Scale</th><th>GPU Count</th><th>Training Days</th><th>Special Configuration</th></tr></thead><tbody><tr><td>ResNet-50</td><td>ResNet-50</td><td>1x</td><td>(unspecified)</td><td>(unspecified)</td><td>None</td></tr><tr><td>ResNet-101</td><td>ResNet-101</td><td>1x</td><td>(unspecified)</td><td>(unspecified)</td><td>None</td></tr><tr><td>RN50x4</td><td>ResNet-50</td><td>4x</td><td>(unspecified)</td><td>(unspecified)</td><td>None</td></tr><tr><td>RN50x16</td><td>ResNet-50</td><td>16x</td><td>(unspecified)</td><td>(unspecified)</td><td>None</td></tr><tr><td>RN50x64</td><td>ResNet-50</td><td>64x</td><td>592</td><td>18</td><td>None</td></tr><tr><td>ViT-B/32</td><td>ViT-B</td><td>(unspecified)</td><td>(unspecified)</td><td>(unspecified)</td><td>None</td></tr><tr><td>ViT-B/16</td><td>ViT-B</td><td>(unspecified)</td><td>(unspecified)</td><td>(unspecified)</td><td>None</td></tr><tr><td>ViT-L/14</td><td>ViT-L</td><td>(unspecified)</td><td>256</td><td>12</td><td>Extra pre-training 1 epoch @336px</td></tr></tbody></table>
<p>This table outlines the model name, base model, compute scale, GPU count, training days, and special configuration. &quot;Unspecified&quot; indicates the original paper did not provide specific information.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>The scale of using 592 V100s is impressive and enviable.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="model-inference">Model Inference<a class="hash-link" aria-label="Direct link to Model Inference" title="Direct link to Model Inference" href="/en/papers/clip/#model-inference">​</a></h3>
<p>In image classification, dataset labels often reflect an afterthought approach. Many standard datasets use numerical IDs for labeling, with a file mapping these IDs back to their English names. Some datasets, like Flowers102 and GTSRB, seemingly do not include such mapping in their release, severely limiting zero-shot transfer potential.</p>
<p>The authors note that many datasets have somewhat arbitrary class label choices, lacking the descriptive information needed for successful zero-shot transfer. For example, providing only class names to the CLIP text encoder might result in ambiguity. In some cases, the same word can have different meanings, such as &quot;crane&quot; referring to either a construction crane or a flying crane in ImageNet. Another example is the term &quot;boxer&quot; in the Oxford-IIIT Pet dataset, where it refers to a dog breed but might be interpreted as an athlete by the text encoder without context.</p>
<p>To address these issues, the authors experimented with prompt templates like &quot;a photo of a &#x27;label&#x27;&quot; to provide textual context about the image. This method improved ImageNet accuracy by 1.3%, demonstrating its effectiveness.</p>
<p>Additionally, customizing prompt texts for each task significantly enhanced zero-shot performance. Examples include using &quot;a photo of a &#x27;label&#x27;, a pet&quot; for Oxford-IIIT Pets, specifying a type of food for Food101, or specifying an aircraft type for FGVC Aircraft.</p>
<p>Lastly, the authors explored boosting classification performance through ensembling multiple zero-shot classifiers. They constructed ensemble predictions in embedding space rather than probability space, using different context prompts like &quot;a big &#x27;label&#x27; photo&quot; and &quot;a small &#x27;label&#x27; photo&quot; to compute classifications. This ensemble prediction method improved performance, achieving an additional 3.5% accuracy gain on ImageNet, demonstrating the potential of prompt engineering and ensemble prediction methods to improve ImageNet accuracy by nearly 5%.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion" href="/en/papers/clip/#discussion">​</a></h2>
<p>In this section, the authors raise a series of thought-provoking questions regarding the performance and effectiveness of zero-shot learning. Zero-shot learning is a method that uses existing knowledge to understand new situations through abstraction and reasoning without prior examples.</p>
<p>First, the authors question whether zero-shot learning truly meets expectations and accurately recognizes new classes without prior examples. They then examine the model&#x27;s abstraction capability to see if it can capture and represent the core characteristics of data.</p>
<p>In today&#x27;s data-rich environment, a model&#x27;s ability to operate across different data domains is crucial. For example, can a model trained mainly on animal images effectively handle mechanical parts image recognition? They also discuss whether the model&#x27;s performance matches human judgment.</p>
<p>The accuracy and completeness of self-constructed datasets are another focus. The authors are concerned about potential &quot;cheating&quot; during dataset creation, such as improper labeling or omission of certain classes.</p>
<p>Lastly, the potential harm hidden in data is significant. If data contains incorrect or harmful information, how does it affect the model? Does it lead the model in a wrong direction?</p>
<p>Through these questions, the authors aim to provide a comprehensive and in-depth discussion of zero-shot learning.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Due to the extensive number of charts in the original paper, this article primarily focuses on conclusions, only including selected charts, with references to others as indicated in the paper.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-zero-shot-meet-expectations">Does Zero-Shot Meet Expectations?<a class="hash-link" aria-label="Direct link to Does Zero-Shot Meet Expectations?" title="Direct link to Does Zero-Shot Meet Expectations?" href="/en/papers/clip/#does-zero-shot-meet-expectations">​</a></h3>
<p>CLIP meets expectations in some areas but reveals areas needing improvement:</p>
<ol>
<li>
<p>Exceptional Performance on Specific Datasets:</p>
<p>On specific datasets, such as those with well-defined underlying feature representations, zero-shot CLIP&#x27;s performance can match or even surpass fully supervised classifiers. This highlights zero-shot learning as an efficient alternative in certain cases.</p>
</li>
<li>
<p>Performance Matching:</p>
<p>Comparing CLIP to few-shot logistic regression classifiers underscores CLIP&#x27;s robust performance. For instance, in an animal image classification task, zero-shot CLIP can identify unseen classes (e.g., &quot;zebra&quot; not present in training data) through text descriptions, while few-shot classifiers might need more training data for similar performance.</p>
</li>
<li>
<p>Zero-Shot Transfer Efficiency:</p>
<p>Differences in zero-shot transfer efficiency across datasets reflect dataset complexity and diversity. For example, zero-shot transfer might need fewer examples in simpler datasets, but more in complex, varied datasets for similar performance.</p>
</li>
<li>
<p>Comparison with Fully Supervised Classifiers:</p>
<p>In many cases, fully supervised classifiers outperform zero-shot CLIP, indicating supervised learning may remain a more stable choice in traditional classification tasks.</p>
</li>
<li>
<p>Predictive Trends in Performance:</p>
<p>The positive correlation between fully supervised and zero-shot performance suggests potential for improving zero-shot learning. Enhancing fully supervised data quantity or learning underlying features might improve zero-shot performance.</p>
</li>
<li>
<p>Scaling Patterns:</p>
<p>CLIP&#x27;s logarithmic linear scaling trend in zero-shot performance may be influenced by factors like model size and training data diversity. Further analysis could identify specific factors and provide methods to improve zero-shot learning performance.</p>
</li>
</ol>
<p>Zero-shot CLIP&#x27;s performance meets expectations to some extent, particularly when compared to few-shot logistic regression, matching its performance on some datasets and achieving fully supervised levels in specific cases. However, comparisons with fully supervised classifiers and performance differences across datasets indicate significant room for improvement in CLIP&#x27;s zero-shot transfer capabilities, with potential unexplored performance-impacting factors.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="is-its-representation-capability-good">Is Its Representation Capability Good?<a class="hash-link" aria-label="Direct link to Is Its Representation Capability Good?" title="Direct link to Is Its Representation Capability Good?" href="/en/papers/clip/#is-its-representation-capability-good">​</a></h3>
<p>CLIP shows strong potential in representation learning and generalization across various aspects:</p>
<ol>
<li>
<p>Representation Quality Assessment:</p>
<p>CLIP&#x27;s ability to fit linear classifiers and perform well on multiple datasets showcases its representation learning capability. However, further exploration is needed to see if it maintains this performance on unseen datasets or extreme conditions.</p>
</li>
<li>
<p>Utility of Linear Classifiers:</p>
<p>Evaluating with linear classifiers highlights the general and transferable representations learned during pretraining. This feedback reveals potential flaws, such as over-reliance on specific features, impacting adaptability to different tasks.</p>
</li>
<li>
<p>Model Scale and Performance:</p>
<p>CLIP&#x27;s various model scales, like ResNet-50x64 and CLIP ViT, indicate a relationship between model size, representation capability, and computational efficiency. Larger models may require more computational resources, limiting application on low-resource devices.</p>
</li>
<li>
<p>Task Diversity:</p>
<p>CLIP performs well on diverse tasks like geo-localization, optical character recognition (OCR), facial emotion recognition, and action recognition. For instance, in OCR tasks, CLIP effectively recognizes text in different fonts and colors. However, specialized models might outperform CLIP in specific tasks, warranting further investigation.</p>
</li>
<li>
<p>Broad Dataset Evaluation:</p>
<p>Evaluating on 27 datasets demonstrates CLIP&#x27;s strong generalization and computational efficiency but may still have data bias issues, requiring further verification on diverse datasets.</p>
</li>
<li>
<p>Self-Supervised System Performance:</p>
<p>Compared to self-supervised systems like SimCLRv2, CLIP shows relatively better performance. Whether self-supervised learning efficiency and effectiveness can continually improve needs time and extensive testing.</p>
</li>
<li>
<p>Comparison with Advanced Models:</p>
<p>Compared to advanced models like Noisy Student EfficientNet-L2, CLIP excels on most datasets but may still have shortcomings in specific tasks or datasets.</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="can-it-work-across-data-domains">Can It Work Across Data Domains?<a class="hash-link" aria-label="Direct link to Can It Work Across Data Domains?" title="Direct link to Can It Work Across Data Domains?" href="/en/papers/clip/#can-it-work-across-data-domains">​</a></h3>
<p><img decoding="async" loading="lazy" alt="CLIP Cross-Domain Performance" src="/en/assets/images/clip_demo-22093a2af3b3c9b43d2dcf4dde81e7a0.jpg" width="1770" height="698" class="img_ev3q"></p>
<p>In evaluating deep learning models&#x27; performance and robustness, cross-domain ability is crucial. Ideally, a model should maintain good performance across different datasets and distributions. However, as mentioned, many ImageNet-based models significantly drop in performance when faced with new or unseen data distributions.</p>
<p>For example, evaluating different natural distribution shift datasets (ImageNetV2, ImageNet Sketch, Youtube-BB, etc.) reveals traditional ImageNet models (e.g., ResNet-101) perform far worse on these new datasets than on the original ImageNet validation set. This indicates significant shortcomings in these models&#x27; cross-domain performance and susceptibility to data distribution shifts.</p>
<p>Zero-shot CLIP models perform markedly better on these new datasets than traditional ImageNet models, effectively narrowing the gap between ImageNet accuracy and accuracy under distribution shifts. Analysis shows zero-shot classifiers significantly improve after adapting to the ImageNet distribution, with accuracy increasing by 9.2%. However, under distribution shift scenarios, accuracy shows a slight decline.</p>
<p>This raises an important question: How to interpret CLIP&#x27;s improved accuracy on the ImageNet distribution and its unremarkable improvement under distribution shifts? Are these gains primarily due to &quot;leveraging spurious correlations&quot;? Moreover, is this phenomenon unique to CLIP, ImageNet datasets, and distribution shifts, or does it reflect a broader trend? Does it also apply to end-to-end fine-tuning and linear classifiers?</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="can-it-compare-to-humans">Can It Compare to Humans?<a class="hash-link" aria-label="Direct link to Can It Compare to Humans?" title="Direct link to Can It Compare to Humans?" href="/en/papers/clip/#can-it-compare-to-humans">​</a></h3>
<p>To understand the differences between CLIP and human performance, researchers compared specific tasks. They selected the Oxford IIT Pets dataset, asking 5 participants to label each of the 3669 images without seeing any breed examples. In another experiment, participants were given one or two image examples of each breed for reference. This aimed to understand how humans improve their classification accuracy with given examples.</p>
<p>Results show that on another dataset, STL-10, humans achieved up to 94% accuracy, with some subsets reaching 97-100%, demonstrating strong human ability in these tasks. When given one example per class, human average performance improved from 54% to 76%. Additional examples did not help much overall but were beneficial for confusing images. This indicates humans excel at recognizing uncertainty and adjusting judgments based on new information.</p>
<p>Despite CLIP&#x27;s potential in zero-shot learning and good performance in natural distribution shift tests, it still significantly lags behind humans in few-shot learning. Researchers note humans effectively use few examples to improve classification accuracy, while CLIP struggles in this area. Humans clearly outperform CLIP in leveraging prior knowledge.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="did-the-model-cheat">Did the Model Cheat?<a class="hash-link" aria-label="Direct link to Did the Model Cheat?" title="Direct link to Did the Model Cheat?" href="/en/papers/clip/#did-the-model-cheat">​</a></h3>
<p>During pretraining on large-scale web datasets, an unavoidable issue is unintentional overlap with downstream evaluation datasets. This overlap might lead to unusually good model performance in evaluations but does not reflect true generalization ability.</p>
<p>As previously discussed, some datasets, due to their uniqueness or synthetic nature, showed no overlap, such as MNIST, CLEVR, and GTSRB. However, other datasets exhibited significant overlap, especially those built from public datasets like YFCC100M. For instance, Country211 showed a 21.5% overlap rate.</p>
<p>Despite this, overlap had minimal impact on accuracy improvement, as in Country211, accuracy only increased by 0.2%. This might be because the training text in the overlapping items did not cover specific information relevant to downstream tasks. In some cases, like the Kinetics-700 dataset, overlapping items were actually irrelevant black transition frames, reducing accuracy in the overlapping subset by 20%.</p>
<p>Overlap analysis is nuanced and complex, requiring consideration of both data duplication and potential changes in underlying data distribution and other confounding factors. For instance, in the CIFAR-100 dataset, due to very low image resolution, many overlapping images were misidentified small objects like birds or planes. This might affect category distribution or overlap difficulty, impacting accuracy variation.</p>
<p>These observations align with previous large-scale pretraining studies, such as Mahajan et al. (2018) and Kolesnikov et al. (2019), which reported similar overlap rates and minimal overall performance changes. Importantly, these studies also compared different overlap data removal strategies and found almost no difference between the chosen methods and alternative strategies. This underscores the importance of duplication analysis in understanding and improving model generalization performance and the necessity of identifying and handling overlap before training.</p>
<p>The authors noted that while the detector showed near 100% accuracy on the proxy training task and achieved high precision through manual inspection and threshold adjustment, the recall rate&#x27;s perfection is limited. Additionally, underlying data distribution changes between overlapping and clean subsets are potential confounders affecting analysis. For example, in Kinetics-700, many &quot;overlapping&quot; items were black transition frames, reducing accuracy in the overlapping part by 20%.</p>
<p>The authors also mentioned potential subtle distribution changes, such as in CIFAR-100, where very low image resolution caused many overlapping images to be misidentified small objects like birds or planes. Accuracy changes might result from category distribution or overlap difficulty changes, possibly obscuring overfitting effects.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-are-the-limitations">What Are the Limitations?<a class="hash-link" aria-label="Direct link to What Are the Limitations?" title="Direct link to What Are the Limitations?" href="/en/papers/clip/#what-are-the-limitations">​</a></h3>
<p>Despite CLIP&#x27;s promising capabilities, it has several serious limitations:</p>
<ol>
<li>
<p>Performance Limitations:</p>
<p>On some supervised split datasets, zero-shot CLIP&#x27;s performance matches linear classifiers based on ResNet-50 features. However, on most datasets, its performance is far below state-of-the-art. Even with scaling improvements, achieving top performance would require increasing CLIP&#x27;s computation by about 1000 times, which is impractical with current hardware.</p>
</li>
<li>
<p>Task Learning and Transferability:</p>
<p>CLIP underperforms in certain tasks, such as fine-grained classification (e.g., car models, flower species, and aircraft variants) and more abstract and systematic tasks (e.g., counting objects in images).</p>
</li>
<li>
<p>Generalization Ability:</p>
<p>Although CLIP performs well on various natural image distributions, its zero-shot performance significantly drops when facing truly out-of-distribution data. For example, CLIP achieves only 88% accuracy on MNIST handwritten digit recognition, while logistic regression on raw pixels outperforms zero-shot CLIP. This highlights CLIP&#x27;s inability to address the fundamental issue of deep learning models&#x27; generalization weakness. Relying solely on massive diversified datasets to effectively cover all distributions is naive, as MNIST proves this assumption easily violated.</p>
</li>
<li>
<p>Data and Computational Efficiency:</p>
<p>CLIP requires vast amounts of data and computational resources, with its training process being too extensive for current hardware, taking hundreds of years to complete the full training cycle. Although existing self-supervised and self-training methods show potential for improving data efficiency, combining these methods might guide CLIP&#x27;s further development.</p>
</li>
<li>
<p>Evaluation and Optimization:</p>
<ul>
<li>The study&#x27;s approach has several obvious limitations. Although focusing on zero-shot models, researchers repeatedly queried full validation set performance to guide CLIP development. These validation sets typically have thousands of examples, impractical for true zero-shot scenarios. Similar concerns have been raised in the semi-supervised learning field (Oliver et al., 2018).</li>
<li>Performance counterintuitively drops when transitioning from zero-shot to few-shot settings, indicating CLIP&#x27;s few-shot learning optimization needs improvement.</li>
</ul>
</li>
<li>
<p>Social Bias and Ethical Considerations:</p>
<p>Trained on web image-text pairs, CLIP might learn societal biases due to unfiltered and unsupervised data.</p>
</li>
</ol>
<p>Future research needs to explore combining CLIP&#x27;s strong zero-shot performance with efficient few-shot learning methods to achieve more comprehensive and reliable visual and language understanding models.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="malicious-data">Malicious Data<a class="hash-link" aria-label="Direct link to Malicious Data" title="Direct link to Malicious Data" href="/en/papers/clip/#malicious-data">​</a></h3>
<ul>
<li>
<p>Bias:</p>
<p>The article deeply explores the potential biases and malice from data, highlighting these issues&#x27; societal and individual impacts. Through analyzing data sources and potential biases, the authors call for greater awareness and understanding of data&#x27;s importance and potential risks.</p>
<p>Firstly, the authors point out that data collection and processing may be influenced by varying degrees of subjectivity, losing objectivity. For example, data might be skewed due to the collector&#x27;s biases or important information might be overlooked due to specific processing methods.</p>
<p>They provide examples of how these biases manifest in different contexts, discussing issues like racial bias, gender discrimination, and economic inequality, explaining how these issues interplay with data bias, exacerbating social problems.</p>
<p>The authors remind readers that even if data is inherently innocent, improper handling might lead to severe consequences. They emphasize the need for comprehensive data governance mechanisms and data literacy among users and policymakers to understand and mitigate potential risks.</p>
<p>Finally, they urge data-related individuals and institutions to responsibly handle and use data, striving to create a fair, transparent, and sustainable data ecosystem to promote social justice and progress.</p>
</li>
<li>
<p>Surveillance:</p>
<p>The article delves into the application and impact of widely used computer vision models in surveillance, especially focusing on performance in CCTV image classification and zero-shot celebrity identification. By analyzing the model&#x27;s effectiveness and limitations, the authors aim to provide insights into these models&#x27; potential future impacts, assisting in forming norms and checks around such systems.</p>
<p>The authors first tested the model&#x27;s performance on low-resolution images captured by surveillance cameras (e.g., CCTV). Using the VIRAT dataset and data extracted by Varadarajan &amp; Odobez, they tested 515 images from 12 different video sequences in coarse and fine-grained classification. Coarse classification involved identifying the main subject of an image, such as determining if the image was an empty parking lot or a school campus. Fine-grained classification required the model to choose between two options to identify smaller features like a person standing in a corner. In the coarse classification task, the model achieved a 91.8% Top-1 accuracy, but its performance dropped significantly in fine-grained detection and &quot;stress tests,&quot; with accuracy falling to 51.1% and an error rate of 40.7% for &quot;close&quot; answers.</p>
<p>Further, the authors explored the effectiveness of zero-shot celebrity identification. Using the CelebA dataset to test CLIP&#x27;s zero-shot capability, they aimed to evaluate identity detection performance using only pre-trained public data. Results showed 59.2% Top-1 accuracy in the task of identifying 8k celebrity images &quot;in the wild&quot; among 100 possible classes. However, accuracy dropped to 43.3% when the number of classes increased to 1,000. While this performance does not compete with production-grade models like Google&#x27;s celebrity recognition, it demonstrates the feasibility and relatively robust performance of zero-shot identification without additional task-specific datasets.</p>
<p>Ultimately, the authors note that although CLIP shows significant advantages in zero-shot functionality, making it appealing for tasks with relatively few data, it has lower attractiveness for many on-demand surveillance tasks like face recognition, where large datasets and high-performance supervised models exist. Additionally, CLIP is limited in common surveillance-related tasks like object detection and semantic segmentation. However, by reducing the need for training data, CLIP unlocks certain usability aspects, allowing customized niche surveillance use cases without specialized models or datasets, lowering the skill requirements for building such applications.</p>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" href="/en/papers/clip/#conclusion">​</a></h2>
<p>This study explores transferring the success of large-scale pretraining in natural language processing (NLP) to computer vision (CV), demonstrating CLIP&#x27;s strong capabilities in multi-task learning and zero-shot transfer.</p>
<p>In conclusion, the paper&#x27;s contributions and future prospects are summarized:</p>
<ol>
<li>
<p>Contributions:</p>
<ul>
<li>Multi-Modal Learning: CLIP&#x27;s innovation lies in processing both image and text information, achieving multi-task learning and adaptation through large-scale pretraining and natural language prompts.</li>
<li>Zero-Shot Transfer Learning: CLIP demonstrates strong zero-shot transfer capability, performing well even without task-specific training data through natural language prompts.</li>
<li>Scalability: CLIP&#x27;s performance can match task-specific supervised models at sufficient scale, proving its training and application scalability.</li>
</ul>
</li>
<li>
<p>Future Prospects:</p>
<ul>
<li>Lack of Explainability: While CLIP can merge text and image information and provide some form of explanation through natural language descriptions, it still has explainability limitations. For example, the model might offer high-level abstract explanations but lack detailed decision-making processes. Additionally, due to the complexity of the multi-layer neural network structure and potential training data biases, explanations might not always be accurate or reliable.</li>
<li>Performance Improvement Space: Despite CLIP&#x27;s strong performance across tasks, there is room for improvement, especially compared to task-specific supervised models.</li>
</ul>
</li>
</ol>
<p>The authors also deeply explore potential societal impacts, thanking all project participants and software developers for their technical support. This study provides valuable insights for cross-domain technical exploration and opens new possibilities for further multi-modal learning development.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-07-15T05:02:43.000Z" itemprop="dateModified">Jul 15, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/vlt5/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[21.02] VL-T5</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/efficientnet-v2/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[21.04] EfficientNet-V2</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#breaking-the-dimensional-barrier">Breaking the Dimensional Barrier</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#defining-the-problem">Defining the Problem</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#solving-the-problem">Solving the Problem</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#clip-model-architecture">CLIP Model Architecture</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#pretraining-process">Pretraining Process</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#zero-shot-inference">Zero-Shot Inference</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#natural-language-supervision">Natural Language Supervision</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#large-scale-dataset-wit">Large-Scale Dataset: WIT</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#training-details">Training Details</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#model-inference">Model Inference</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#discussion">Discussion</a><ul><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#does-zero-shot-meet-expectations">Does Zero-Shot Meet Expectations?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#is-its-representation-capability-good">Is Its Representation Capability Good?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#can-it-work-across-data-domains">Can It Work Across Data Domains?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#can-it-compare-to-humans">Can It Compare to Humans?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#did-the-model-cheat">Did the Model Cheat?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#what-are-the-limitations">What Are the Limitations?</a></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#malicious-data">Malicious Data</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/en/papers/clip/#conclusion">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>