<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-deepseek/deepseek-llm/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>[24.01] DeepSeek LLM | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/deepseek/deepseek-llm/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[24.01] DeepSeek LLM | DOCSAID"><meta data-rh=true name=description content="New Scaling Laws"><meta data-rh=true property=og:description content="New Scaling Laws"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/deepseek/deepseek-llm/><link data-rh=true rel=alternate href=https://docsaid.org/papers/deepseek/deepseek-llm/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/deepseek/deepseek-llm/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/deepseek/deepseek-llm/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/deepseek/deepseek-llm/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.7b7e12db.css><script src=/en/assets/js/runtime~main.8c7abe49.js defer></script><script src=/en/assets/js/main.28b1f3ba.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/docsaid_logo.png><link rel=preload as=image href=/en/img/docsaid_logo_white.png><link rel=preload as=image href=https://github.com/zephyr-sh.png><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/deepseek/deepseek-llm/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/deepseek/deepseek-llm/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/deepseek/deepseek-llm/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-7ny38l ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="Expand sidebar category 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Collapse sidebar category 'DeepSeek (5)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/deepseek/deepseek-llm/>[24.01] DeepSeek LLM</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/deepseek/deepseek-vl/>[24.03] DeepSeek-VL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/deepseek/deepseek-v2/>[24.05] DeepSeek-V2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/deepseek/deepseek-v3/>[24.12] DeepSeek-V3</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/deepseek/deepseek-r1/>[25.01] DeepSeek-R1</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-11>Face Anti-Spoofing (11)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="Expand sidebar category 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="Expand sidebar category 'Vision Transformers (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 185 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/en/papers/category/deepseek-5><span itemprop=name>DeepSeek (5)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[24.01] DeepSeek LLM</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[24.01] DeepSeek LLM</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=new-scaling-laws>New Scaling Laws<a href=#new-scaling-laws class=hash-link aria-label="Direct link to New Scaling Laws" title="Direct link to New Scaling Laws">​</a></h2>
<p><a href=https://arxiv.org/abs/2401.02954 target=_blank rel="noopener noreferrer"><strong>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</strong></a></p>
<hr>
<p>Recently, DeepSeek has been in the spotlight, generating a lot of buzz, with even OpenAI stepping in to comment.</p>
<p>Given the immense impact they’ve made, let’s take some time to review their recent publications.</p>
<p>This paper is 48 pages long, filled with extensive experimental data and technical details, so we’ll focus on the key points.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=defining-the-problem>Defining the Problem<a href=#defining-the-problem class=hash-link aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem">​</a></h2>
<p>The performance gap between open-source and closed-source models has long been a popular topic. Previous research has shown that open-source models generally perform significantly worse than closed-source models on most benchmarks, primarily due to the scale and resource limitations of open-source models.</p>
<p>To bridge this gap, the research team proposed the DeepSeek LLM series, a set of open-source language models ranging in size from 7B to 67B. To train these models, the research team challenged the existing Scaling Laws, arguing that past research findings may not be entirely accurate. Therefore, they introduced their own Scaling Law, which became the core of the paper, exploring the relationship between model size, data scale, and computational budget.</p>
<p>We will focus on the discussion of the Scaling Law and briefly introduce the design and experimental results of DeepSeek LLM.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=solving-the-problem>Solving the Problem<a href=#solving-the-problem class=hash-link aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=data-processing-strategy>Data Processing Strategy<a href=#data-processing-strategy class=hash-link aria-label="Direct link to Data Processing Strategy" title="Direct link to Data Processing Strategy">​</a></h3>
<p>The research team adopted a three-phase strategy: "Deduplication → Filtering → Remixing."</p>
<ul>
<li>
<p><strong>Deduplication</strong>:</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=decup src=/en/assets/images/img1-416d1b1da253361b4c2392042febf92d.jpg width=1224 height=160 class=img_ev3q></figure></div>
<p>The entire Common Crawl corpus was used for deduplication. As seen in the table above, deduplication across multiple dumps is more thorough than when applied to a single dump, providing the model with more unique learning samples.</p>
</li>
<li>
<p><strong>Filtering</strong>: A rigorous document quality evaluation standard was designed, combining linguistic and semantic analysis to ensure data quality from both local and global perspectives.</p>
</li>
<li>
<p><strong>Remixing</strong>: To address the issue of uneven data distribution, the team specifically enhanced data from underrepresented domains, ensuring that the dataset was more balanced and inclusive of diverse information.</p>
</li>
</ul>
<p>In terms of Tokenizer design, the research team employed Byte-level Byte-Pair Encoding (BBPE), using pre-tokenization to avoid mixing different character categories (such as newlines, punctuation, and CJK symbols). Additionally, they split numbers into individual digits. The vocabulary was initially set to 100,000, with 15 special tokens and extra space up to 102,400, balancing computational efficiency in model training with future scalability needs.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-architecture-design>Model Architecture Design<a href=#model-architecture-design class=hash-link aria-label="Direct link to Model Architecture Design" title="Direct link to Model Architecture Design">​</a></h3>
<p>The model largely follows the LLaMA architecture, using a Pre-Norm structure with RMSNorm and SwiGLU as the activation function for the Feedforward Neural Network (FFN). Rotary Embedding is employed for position encoding to enhance sequence processing capability.</p>
<p>To reduce inference costs, the 67B model uses Grouped-Query Attention (GQA) to replace traditional Multi-Head Attention.</p>
<p>The smaller (7B) and larger (67B) models have 30 and 95 layers, respectively. This adjustment not only maintains consistency with the parameters of other open-source models but also facilitates pipeline segmentation, optimizing training and inference workflows. Unlike traditional methods that increase the FFN width, the research team chose to extend the 67B model’s parameters by increasing network depth, which helps improve overall model performance.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>Here are a few key terms, for which you may need to read other papers:<ol>
<li>Rotary Embedding: Rotational position encoding, which can be referenced from our previous article<!-- -->
<ul>
<li><a href=/en/papers/transformers/roformer/><strong>[21.04] RoFormer: Rotary Position Embedding</strong></a></li>
</ul>
</li>
<li>Grouped-Query Attention: Grouped multi-head attention, refer to the GQA paper<!-- -->
<ul>
<li><a href=https://arxiv.org/abs/2305.13245 target=_blank rel="noopener noreferrer"><strong>[23.05] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</strong></a></li>
</ul>
</li>
</ol><p>We’ll add corresponding notes once we’ve reviewed these papers.</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=parameter-configuration>Parameter Configuration<a href=#parameter-configuration class=hash-link aria-label="Direct link to Parameter Configuration" title="Direct link to Parameter Configuration">​</a></h3>
<p>The model initialization standard deviation was set to 0.006, and the AdamW optimizer was used:</p>
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=application/x-tex>\beta_1=0.9</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8889em;vertical-align:-0.1944em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.05278em>β</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0528em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.9</span></span></span></span></li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=application/x-tex>\beta_2=0.95</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8889em;vertical-align:-0.1944em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.05278em>β</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0528em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.95</span></span></span></span></li>
<li>weight_decay=0.1</li>
</ul>
<p>The learning rate scheduler employed a multi-step configuration, reaching its peak after 2000 warmup steps, then gradually decaying to 31.6% and 10% of the maximum value at 80% and 90% of the training tokens, respectively. This strategy allows for adjusting training scales under a fixed model size while reusing the previous stage’s training results, which is convenient for continued training. Overall, experiments show that, while the loss curves differ, the final performance is consistent with the cosine scheduler.</p>
<p>Gradient clipping was set to 1.0 to prevent gradient explosion and stabilize training.</p>
<p>Training was conducted using the HAI-LLM framework, which integrates data, tensors, sequences, and pipeline parallelism technologies, similar to Megatron’s design, effectively sharing the computational load of large-scale models.</p>
<p>Additionally, flash attention technology was introduced to improve hardware utilization, and ZeRO-1 technology was used to partition the optimizer state, further reducing memory consumption. Some layers (such as LayerNorm, GEMM, and Adam updates) were fused to accelerate training; mixed precision (bf16 training, fp32 gradient accumulation) and in-place cross-entropy computation were used to optimize memory usage.</p>
<p>Model weights and optimizer states are automatically saved every 5 minutes, ensuring that at most 5 minutes of training is lost in case of hardware or network failures. Training can also be resumed from different 3D parallel configurations to adapt to dynamic changes in computational resources.</p>
<p>For evaluation, the vLLM was used for generative tasks, while non-generative tasks were processed using continuous batch handling to minimize manual adjustments of batch size and token padding.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=scaling-laws>Scaling Laws<a href=#scaling-laws class=hash-link aria-label="Direct link to Scaling Laws" title="Direct link to Scaling Laws">​</a></h2>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>For readers who haven’t seen the two well-known Scaling Laws papers, you can first check out our previous notes:<ul>
<li><a href=/en/papers/transformers/scaling_laws/><strong>[20.01] Scaling Laws: Scaling Laws for Neural Language Models</strong></a></li>
<li><a href=/en/papers/transformers/chinchilla/><strong>[22.03] Chinchilla: Chinchilla's Gaze</strong></a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=hyperparameter-configuration>Hyperparameter Configuration<a href=#hyperparameter-configuration class=hash-link aria-label="Direct link to Hyperparameter Configuration" title="Direct link to Hyperparameter Configuration">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="scaling laws" src=/en/assets/images/img2-a447fb337a5fea99d4cba6558574759f.jpg width=1444 height=584 class=img_ev3q></figure></div>
<p>The research team first performed a grid search for a specific model scale (177M FLOPs/token) under a computational budget of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>C</mi><mo>=</mo><mn>1</mn><mi mathvariant=normal>e</mi><mn>17</mn></mrow><annotation encoding=application/x-tex>C = 1\mathrm{e}17</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>1</span><span class="mord mathrm">e</span><span class=mord>17</span></span></span></span>, exploring different combinations of batch size <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>B</mi></mrow><annotation encoding=application/x-tex>B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.05017em>B</span></span></span></span> and learning rate <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>η</mi></mrow><annotation encoding=application/x-tex>\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em></span><span class="mord mathnormal" style=margin-right:0.03588em>η</span></span></span></span>.</p>
<p>Based on the experimental results shown in the figure above, the research team observed that within a wide range of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>B</mi></mrow><annotation encoding=application/x-tex>B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.05017em>B</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>η</mi></mrow><annotation encoding=application/x-tex>\eta</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em></span><span class="mord mathnormal" style=margin-right:0.03588em>η</span></span></span></span>, the model's generalization error remained stable. This suggests that within this larger parameter space, an "approximately optimal" set of hyperparameters could be found.</p>
<p>To further explore a larger computational budget range (from <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>1</mn><mi mathvariant=normal>e</mi><mn>17</mn></mrow><annotation encoding=application/x-tex>1\mathrm{e}17</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6444em></span><span class=mord>1</span><span class="mord mathrm">e</span><span class=mord>17</span></span></span></span> to <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>2</mn><mi mathvariant=normal>e</mi><mn>19</mn></mrow><annotation encoding=application/x-tex>2\mathrm{e}19</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6444em></span><span class=mord>2</span><span class="mord mathrm">e</span><span class=mord>19</span></span></span></span>), the research team adopted a multi-step learning rate scheduler. This scheduler allows for the reuse of previously trained results after the first stage of training, enabling efficient training of multiple hyperparameter configurations. Since there is redundancy in the parameter space, the team defined that the hyperparameters are considered "approximately optimal" when the model's generalization error exceeds the minimum by no more than 0.25%.</p>
<p>Through extensive experiments and data fitting, the results showed:</p>
<ul>
<li><strong>Optimal Batch Size <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>B</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub></mrow><annotation encoding=application/x-tex>B_{opt}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.05017em>B</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0502em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span>:</strong> Increases gradually with the computational budget <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>C</mi></mrow><annotation encoding=application/x-tex>C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span></span></span></span>. This aligns with intuition, as more computational resources allow for larger batches.</li>
<li><strong>Optimal Learning Rate <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>η</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub></mrow><annotation encoding=application/x-tex>\eta_{opt}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7167em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>η</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span>:</strong> Decreases as <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>C</mi></mrow><annotation encoding=application/x-tex>C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span></span></span></span> increases. In other words, as the model size grows and the computational budget increases, a lower learning rate is needed to maintain stability.</li>
</ul>
<p>Finally, the research team presented the following two formulas to describe this relationship:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>η</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>0.3118</mn><mo>⋅</mo><msup><mi>C</mi><mrow><mo>−</mo><mn>0.1250</mn></mrow></msup></mrow><annotation encoding=application/x-tex>\eta_{opt} = 0.3118 \cdot C^{-0.1250}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7167em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>η</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.3118</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8641em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8641em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">0.1250</span></span></span></span></span></span></span></span></span></span></span></span></span>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>B</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>0.2920</mn><mo>⋅</mo><msup><mi>C</mi><mn>0.3271</mn></msup></mrow><annotation encoding=application/x-tex>B_{opt} = 0.2920 \cdot C^{0.3271}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.05017em>B</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0502em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.2920</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8641em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8641em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0.3271</span></span></span></span></span></span></span></span></span></span></span></span></span>
<p>These formulas quantify the power-law relationships between optimal learning rate and optimal batch size with respect to the computational budget <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>C</mi></mrow><annotation encoding=application/x-tex>C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span></span></span></span>.</p>
<p>To validate the effectiveness of these formulas, the research team conducted a validation experiment with a larger computational budget of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>C</mi><mo>=</mo><mn>1</mn><mi mathvariant=normal>e</mi><mn>20</mn></mrow><annotation encoding=application/x-tex>C = 1\mathrm{e}20</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>1</span><span class="mord mathrm">e</span><span class=mord>20</span></span></span></span>, using a model scale of 2.94B FLOPs/token. The experimental results, shown in the figure (right), confirmed that the fitted hyperparameters were located at the center of the optimal parameter space, further validating the accuracy of the formulas.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=optimal-model-and-data-scaling-estimates>Optimal Model and Data Scaling Estimates<a href=#optimal-model-and-data-scaling-estimates class=hash-link aria-label="Direct link to Optimal Model and Data Scaling Estimates" title="Direct link to Optimal Model and Data Scaling Estimates">​</a></h3>
<p>After fitting the approximate optimal hyperparameter formulas, the research team began investigating how to optimally allocate model size and data scale:</p>
<blockquote>
<p><strong>That is, determining the best model size and data size to minimize error, given a specific computational budget <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>C</mi></mrow><annotation encoding=application/x-tex>C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span></span></span></span>.</strong></p>
</blockquote>
<p>The core goal was to identify two power-law relationships, where the optimal model size <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>N</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub></mrow><annotation encoding=application/x-tex>N_{opt}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span> is related to the computational budget as:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>N</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>∝</mo><msup><mi>C</mi><mi>a</mi></msup></mrow><annotation encoding=application/x-tex>N_{opt} \propto C^{a}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>∝</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.7144em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>
<p>And the optimal data size <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>D</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub></mrow><annotation encoding=application/x-tex>D_{opt}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span> (measured in the number of tokens in the data) follows:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>D</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>∝</mo><msup><mi>C</mi><mi>b</mi></msup></mrow><annotation encoding=application/x-tex>D_{opt} \propto C^{b}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>∝</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8991em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8991em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span></span></span></span></span></span>
<p>Here, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>a</mi></mrow><annotation encoding=application/x-tex>a</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">a</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>b</mi></mrow><annotation encoding=application/x-tex>b</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">b</span></span></span></span> are the scaling exponents for the model and data, respectively.</p>
<p>Early works often used the number of model parameters to represent model size, and the relationship between computational budget and model/data size was approximately:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi>C</mi><mo>=</mo><mn>6</mn><mi>N</mi><mo>⋅</mo><mi>D</mi></mrow><annotation encoding=application/x-tex>C = 6N \cdot D</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class=mord>6</span><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.02778em>D</span></span></span></span></span>
<p>Model size could be approximated by <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>6</mn><msub><mi>N</mi><mn>1</mn></msub></mrow><annotation encoding=application/x-tex>6N_1</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord>6</span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> or <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>6</mn><msub><mi>N</mi><mn>2</mn></msub></mrow><annotation encoding=application/x-tex>6N_2</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord>6</span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>. However, these representations failed to fully account for the computational burden of attention operations, leading to errors of up to 50% in different model scales, especially for smaller models. These errors can significantly impact the statistical fitting of subsequent Scaling curves.</p>
<p>To overcome this issue, the research team introduced a new model size representation <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>M</mi></mrow><annotation encoding=application/x-tex>M</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.10903em>M</span></span></span></span>, which includes the computational cost of attention operations but excludes vocabulary calculations. The formula is:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi>M</mi><mo>=</mo><mn>72</mn><mtext> </mtext><msub><mi>n</mi><mrow><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi></mrow></msub><mtext> </mtext><msubsup><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mn>12</mn><mtext> </mtext><msub><mi>n</mi><mrow><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi></mrow></msub><mtext> </mtext><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mtext> </mtext><msub><mi>l</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub></mrow><annotation encoding=application/x-tex>M = 72\, n_{layer}\, d_{model}^2 + 12\, n_{layer}\, d_{model}\, l_{seq}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.1502em;vertical-align:-0.2861em></span><span class=mord>72</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style=margin-right:0.02778em>yer</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8641em><span style=top:-2.453em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span></span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.9805em;vertical-align:-0.2861em></span><span class=mord>12</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style=margin-right:0.02778em>yer</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:-0.0197em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">se</span><span class="mord mathnormal mtight" style=margin-right:0.03588em>q</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span></span>
<p>Where:</p>
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>n</mi><mrow><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding=application/x-tex>n_{layer}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7167em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style=margin-right:0.02778em>yer</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span>: Number of layers</li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding=application/x-tex>d_{model}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8444em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style=margin-right:0.01968em>l</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span>: Model width</li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>l</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub></mrow><annotation encoding=application/x-tex>l_{seq}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9805em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.01968em>l</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:-0.0197em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">se</span><span class="mord mathnormal mtight" style=margin-right:0.03588em>q</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span>: Sequence length</li>
</ul>
<p>Using <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>M</mi></mrow><annotation encoding=application/x-tex>M</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.10903em>M</span></span></span></span>, the computational budget can be simplified as:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi>C</mi><mo>=</mo><mi>M</mi><mo>⋅</mo><mi>D</mi></mrow><annotation encoding=application/x-tex>C = M \cdot D</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.02778em>D</span></span></span></span></span>
<p>This makes it easier to estimate the optimal allocation of model and data across different configurations.</p>
<p>After introducing <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>M</mi></mrow><annotation encoding=application/x-tex>M</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.10903em>M</span></span></span></span>, the research team clarified their objective:</p>
<blockquote>
<p><strong>Given <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>C</mi><mo>=</mo><mi>M</mi><mo>⋅</mo><mi>D</mi></mrow><annotation encoding=application/x-tex>C = M \cdot D</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.02778em>D</span></span></span></span>, find the optimal model size <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>M</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub></mrow><annotation encoding=application/x-tex>M_{opt}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span> and optimal data size <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>D</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub></mrow><annotation encoding=application/x-tex>D_{opt}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span> such that the model's generalization error <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>L</mi><mo stretchy=false>(</mo><mi>N</mi><mo separator=true>,</mo><mi>D</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>L(N, D)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal">L</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=mclose>)</span></span></span></span> is minimized.</strong></p>
</blockquote>
<p>This can be expressed as:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>M</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo stretchy=false>(</mo><mi>C</mi><mo stretchy=false>)</mo><mo separator=true>,</mo><mtext> </mtext><msub><mi>D</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo stretchy=false>(</mo><mi>C</mi><mo stretchy=false>)</mo><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>M</mi><mo separator=true>,</mo><mi>D</mi><mtext> </mtext><mtext>s.t.</mtext><mtext> </mtext><mi>C</mi><mo>=</mo><mi>M</mi><mo>⋅</mo><mi>D</mi></mrow></munder><mi>L</mi><mo stretchy=false>(</mo><mi>N</mi><mo separator=true>,</mo><mi>D</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>M_{opt}(C),\, D_{opt}(C) = \arg\min_{M,D \, \text{s.t.} \, C = M \cdot D} L(N,D)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0361em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=mclose>)</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.6304em;vertical-align:-0.8804em></span><span class=mop>ar<span style=margin-right:0.01389em>g</span></span><span class=mspace style=margin-right:0.1667em></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6679em><span style=top:-2.3557em;margin-left:0em><span class=pstrut style=height:3em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.10903em>M</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style=margin-right:0.02778em>D</span><span class="mspace mtight" style=margin-right:0.1952em></span><span class="mord text mtight"><span class="mord mtight">s.t.</span></span><span class="mspace mtight" style=margin-right:0.1952em></span><span class="mord mathnormal mtight" style=margin-right:0.07153em>C</span><span class="mrel mtight">=</span><span class="mord mathnormal mtight" style=margin-right:0.10903em>M</span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style=margin-right:0.02778em>D</span></span></span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span><span class=mop>min</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.8804em><span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal">L</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.10903em>N</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=mclose>)</span></span></span></span></span>
<p>To reduce experimental costs and fitting complexity, the research team used the <strong>IsoFLOP profile</strong> method introduced in Chinchilla:</p>
<ul>
<li>Select 8 different computational budgets, ranging from <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>1</mn><mi mathvariant=normal>e</mi><mn>17</mn></mrow><annotation encoding=application/x-tex>1\mathrm{e}17</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6444em></span><span class=mord>1</span><span class="mord mathrm">e</span><span class=mord>17</span></span></span></span> to <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>3</mn><mi mathvariant=normal>e</mi><mn>20</mn></mrow><annotation encoding=application/x-tex>3\mathrm{e}20</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6444em></span><span class=mord>3</span><span class="mord mathrm">e</span><span class=mord>20</span></span></span></span>.</li>
<li>For each budget, design about 10 different model and data size configurations.</li>
<li>For each configuration, hyperparameters are determined by the previously established formulas and generalization errors are calculated on an independent validation set (with 100M tokens).</li>
</ul>
<p>Using this data, the research team plotted the IsoFLOP curve and the model/data scaling curves:</p>
<p><img decoding=async loading=lazy alt="scaling laws" src=/en/assets/images/img3-4617742ab8437afa1ece51ef8e9441de.jpg width=1398 height=512 class=img_ev3q></p>
<p>They finally obtained the optimal model and data scaling formulas:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>M</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>M</mi><mrow><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>⋅</mo><msup><mi>C</mi><mi>a</mi></msup></mrow><annotation encoding=application/x-tex>M_{opt} = M_{base} \cdot C^{a}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ba</span><span class="mord mathnormal mtight">se</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.7144em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>
<ul>
<li>Where <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>M</mi><mrow><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>=</mo><mn>0.1715</mn><mo separator=true>,</mo><mtext>  </mtext><mi>a</mi><mo>=</mo><mn>0.5243</mn></mrow><annotation encoding=application/x-tex>M_{base} = 0.1715,\; a = 0.5243</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em>M</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ba</span><span class="mord mathnormal mtight">se</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8389em;vertical-align:-0.1944em></span><span class=mord>0.1715</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.2778em></span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.5243</span></span></span></span>.</li>
</ul>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi>D</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>D</mi><mrow><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>⋅</mo><msup><mi>C</mi><mi>b</mi></msup></mrow><annotation encoding=application/x-tex>D_{opt} = D_{base} \cdot C^{b}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9694em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ba</span><span class="mord mathnormal mtight">se</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8991em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.07153em>C</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8991em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span></span></span></span></span></span>
<ul>
<li>Where <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>D</mi><mrow><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>=</mo><mn>5.8316</mn><mo separator=true>,</mo><mtext>  </mtext><mi>b</mi><mo>=</mo><mn>0.4757</mn></mrow><annotation encoding=application/x-tex>D_{base} = 5.8316,\; b = 0.4757</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.02778em>D</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-left:-0.0278em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ba</span><span class="mord mathnormal mtight">se</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8889em;vertical-align:-0.1944em></span><span class=mord>5.8316</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.2778em></span><span class=mspace style=margin-right:0.1667em></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.4757</span></span></span></span>.</li>
</ul>
<p>Based on these fitting results, the research team successfully predicted the generalization errors for the DeepSeek LLM 7B and 67B models, as shown below:</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="scaling laws testing" src=/en/assets/images/img4-350dd2d632fd75ddfe74daf9de718396.jpg width=944 height=576 class=img_ev3q></figure></div>
<p>Experimental results show that small-scale experiments can accurately predict model performance under 1000 times the computational budget, providing strong support for large-scale model training.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=impact-of-data-quality>Impact of Data Quality<a href=#impact-of-data-quality class=hash-link aria-label="Direct link to Impact of Data Quality" title="Direct link to Impact of Data Quality">​</a></h3>
<p>The research team selected three datasets for comparative analysis:</p>
<ul>
<li><strong>Early Data:</strong> Early internal data.</li>
<li><strong>Current Data:</strong> Current internal data.</li>
<li><strong>OpenWebText2:</strong> The dataset used in previous scaling laws research.</li>
</ul>
<p>Through comparisons, the evaluation showed that the quality of the current internal data is better than the early data, while OpenWebText2, due to its smaller size, can undergo more meticulous processing, and its data quality even surpasses that of the current internal data.</p>
<p>Using the Scaling Laws formulas from the previous section, the research team calculated the Scaling indices for different datasets, as shown in the table below:</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="scaling laws data" src=/en/assets/images/img5-e88004af16723278a3ead5da5c34e127.jpg width=1086 height=428 class=img_ev3q></figure></div>
<p>Specifically, OpenAI (using OpenWebText2) had coefficients of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>a</mi><mo>=</mo><mn>0.73</mn></mrow><annotation encoding=application/x-tex>a=0.73</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.73</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>b</mi><mo>=</mo><mn>0.27</mn></mrow><annotation encoding=application/x-tex>b=0.27</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.27</span></span></span></span>; Chinchilla (MassiveText) had coefficients of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>a</mi><mo>=</mo><mn>0.49</mn></mrow><annotation encoding=application/x-tex>a=0.49</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.49</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>b</mi><mo>=</mo><mn>0.51</mn></mrow><annotation encoding=application/x-tex>b=0.51</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.51</span></span></span></span>; and for their own data, the early internal data had coefficients of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>a</mi><mo>=</mo><mn>0.450</mn></mrow><annotation encoding=application/x-tex>a=0.450</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.450</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>b</mi><mo>=</mo><mn>0.476</mn></mrow><annotation encoding=application/x-tex>b=0.476</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.476</span></span></span></span>, while the data processed with OpenWebText2 had coefficients of <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>a</mi><mo>=</mo><mn>0.578</mn></mrow><annotation encoding=application/x-tex>a=0.578</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">a</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.578</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>b</mi><mo>=</mo><mn>0.422</mn></mrow><annotation encoding=application/x-tex>b=0.422</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">b</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.422</span></span></span></span>.</p>
<p>From these results, it can be seen that as data quality improves, the model expansion index <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>a</mi></mrow><annotation encoding=application/x-tex>a</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal">a</span></span></span></span> gradually increases, while the data expansion index <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>b</mi></mrow><annotation encoding=application/x-tex>b</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">b</span></span></span></span> decreases accordingly. This shows that "<strong>with the same computational budget, higher-quality data requires allocating more resources to model expansion rather than merely increasing the amount of data</strong>." This also explains why earlier studies showed significant differences in the optimal model and data allocation strategies across different datasets.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>Intuitively, one might think that high-quality data, due to its clear logic and minimal noise, could allow smaller models to perform well. In other words, higher-quality data could "save" model capacity.<p>However, the research team points out: not so!<p><strong>Because high-quality data reduces prediction difficulty, it means that the signals in the data are stronger and more stable. This allows the model to use additional capacity to capture more nuanced patterns, further improving performance</strong>.<p>In other words, when data quality is higher, the model is no longer limited by "noise" and can fully leverage its scaling effects, resulting in greater improvements as model parameters increase.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-alignment>Model Alignment<a href=#model-alignment class=hash-link aria-label="Direct link to Model Alignment" title="Direct link to Model Alignment">​</a></h2>
<p>The main goal of the model alignment (Alignment) process is to ensure that when the model generates responses, it provides useful information while avoiding harmful content.</p>
<p>Regarding the data, the research team collected approximately 1.5 million Chinese and English instruction data points, covering a wide range of topics. Of these, 1.2 million are considered helpful data, distributed as follows: general language tasks make up 31.2%, math problems 46.6%, and programming exercises 22.2%; an additional 300,000 data points focus on safety issues, covering various sensitive topics.</p>
<p>The alignment process is divided into two stages:</p>
<ul>
<li>The first stage is supervised fine-tuning (Supervised Fine-Tuning, SFT).</li>
<li>The second stage uses direct preference optimization (Direct Preference Optimization, DPO).</li>
</ul>
<p>In the supervised fine-tuning stage, different training epochs were used for different model sizes because larger models are more prone to overfitting. The experiment used benchmarks like GSM8K and HumanEval, finding that the performance of the 7B model continued to improve, while the 67B model reached its performance ceiling more quickly.</p>
<p>In the second stage, DPO was employed to further enhance the model's alignment. DPO has proven to be a simple and effective method, with its core being the adjustment of the model’s response generation direction using preference data.</p>
<p>In this stage, the research team constructed preference data for helpfulness and harmlessness. Specifically, they first collected multilingual prompts covering creative writing, Q&A, instruction following, and other areas, then used the DeepSeek Chat model to generate candidate responses, which were used to construct helpful preference data. Harmfulness preference data was constructed using a similar process.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p>The experiment results of DeepSeek LLM are abundant. We’ll focus on the model's generative abilities in open-domain and open-ended tasks, as these are closer to real-world usage, allowing us to assess the model’s performance in multi-turn dialogues, cross-lingual tasks, and other unstructured applications.</p>
<p>For other aspects, interested readers can refer to the original paper.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chinese-open-ended-evaluation>Chinese Open-Ended Evaluation<a href=#chinese-open-ended-evaluation class=hash-link aria-label="Direct link to Chinese Open-Ended Evaluation" title="Direct link to Chinese Open-Ended Evaluation">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=results src=/en/assets/images/img6-80892a2c97f572e3f892f812e863537a.jpg width=1392 height=866 class=img_ev3q></figure></div>
<p>The research team tested the model’s open-ended generative abilities on Chinese and English tasks.</p>
<p>For the Chinese evaluation, they used the AlignBench test set, which covers 8 major categories and 36 subcategories, with a total of 683 questions. Each question provided not only a prompt but also a professional reference answer and a scoring template, using GPT-4 for response quality assessment to ensure the objectivity and professionalism of the evaluation.</p>
<p>During the evaluation process, the research team used the official AlignBench GitHub repository to implement the model evaluation and strictly adjusted the generation temperature parameters according to the original settings. Specifically, for tasks such as role-playing, writing skills, and open-ended questions, the generation temperature was set to 0.7; for other tasks, it was set to 0.1 to ensure the generated results were sufficiently diverse yet stable.</p>
<p>From the AlignBench leaderboard, it can be seen that the DeepSeek 67B Chat model outperformed ChatGPT and other benchmark models in several Chinese tasks, second only to two versions of GPT-4, demonstrating its excellent performance in handling open-ended Chinese questions. Furthermore, after DPO (direct preference optimization) training, the model showed significant improvements in almost all evaluation metrics, proving the positive effect of DPO in improving model alignment.</p>
<p>In Chinese language tasks, its basic Chinese proficiency even surpassed the latest version of GPT-4; in more complex Chinese logical reasoning and mathematical problem-solving tasks, the model's performance clearly led other large Chinese language models.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=english-open-ended-evaluation>English Open-Ended Evaluation<a href=#english-open-ended-evaluation class=hash-link aria-label="Direct link to English Open-Ended Evaluation" title="Direct link to English Open-Ended Evaluation">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=results src=/en/assets/images/img7-a7bf02e885d09f5268f943bcbe27b355.jpg width=1616 height=480 class=img_ev3q></figure></div>
<p>In the English open-ended evaluation, the research team used the MT-Bench benchmark, which includes 8 different categories of multi-turn dialogue questions designed to test the model’s generative abilities in continuous conversations. This benchmark comprehensively assesses the model's performance in various open-ended scenarios, especially examining whether the model can maintain consistency and coherence in multi-turn interactions.</p>
<p>The experimental results showed that the DeepSeek LLM 67B Chat model outperformed several other open-source models, such as LLaMA-2-Chat 70B, Xwin 70b v0.1, and TÜLU 2+DPO 70B, with a score of 8.35, comparable to GPT-3.5-turbo. This indicates that in multi-turn English dialogue generation, DeepSeek LLM's performance has reached the level of mainstream commercial models.</p>
<p>Moreover, after further applying DPO training, the average score of DeepSeek LLM 67B Chat improved to 8.76, just behind GPT-4. This suggests that DPO not only further adjusts the model’s generation preferences but also effectively enhances its performance in open-ended multi-turn dialogues.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=other-interesting-findings>Other Interesting Findings<a href=#other-interesting-findings class=hash-link aria-label="Direct link to Other Interesting Findings" title="Direct link to Other Interesting Findings">​</a></h3>
<p>After completing the full evaluation process, the research team made several interesting discoveries:</p>
<ul>
<li>
<p><strong>(1) Model Safety Evaluation:</strong></p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=do-not-answer src=/en/assets/images/img8-c0563b88aba42cbbbdd42c554e325836.jpg width=1108 height=652 class=img_ev3q></figure></div>
<p>The research team first used the "Do-Not-Answer Score" to evaluate how well the model can correctly refuse to answer sensitive or inappropriate questions. A higher score indicates better safety performance.</p>
<p>From the data, DeepSeek-67B-Chat performed exceptionally well in safety and is considered one of the safer models.</p>
</li>
<li>
<p><strong>(2) Staged Fine-Tuning Strategy:</strong></p>
<p>During the fine-tuning process, the research team discovered that for smaller models (such as 7B), extended fine-tuning on mathematical and programming data can enhance their expertise, but it may harm dialogue performance, causing repetitive responses.</p>
<p>To address this, they proposed a staged fine-tuning strategy: the first stage uses all available data for fine-tuning, while the second stage focuses on fine-tuning dialogue data. Experimental results showed that this staged fine-tuning effectively balanced the needs of specialized tasks and smooth dialogues.</p>
</li>
<li>
<p><strong>(3) Impact of Multiple-Choice Question Data:</strong></p>
<p>During the alignment phase, the research team attempted to add 20 million Chinese multiple-choice question data to improve the model’s performance on multiple-choice tasks.</p>
<p>After adding the multiple-choice data, the model's scores on benchmarks like MMLU, C-Eval, and CMMLU significantly improved. However, this improvement did not extend to generative evaluations like TriviaQA and Chinese question answering. In other words, although multiple-choice data could enhance the model's problem-solving ability, it did not make the model appear "smarter" in real-world dialogue generation.</p>
<p>Therefore, to avoid the model overfitting to the multiple-choice question format and diminishing its overall intelligence performance, they ultimately decided not to include multiple-choice data in both the pre-training and fine-tuning stages.</p>
</li>
<li>
<p><strong>(4) Instruction Data in Pre-Training Stage:</strong></p>
<p>Additionally, the research team experimented by adding 5 million multiple-choice instruction data points in the later stages of pre-training (the last 10%) to observe the impact on the base model's performance.</p>
<p>The results showed that this did indeed improve the model’s performance on benchmark tasks, but the final effect was nearly identical to adding the same data during the supervised fine-tuning stage.</p>
<p>Thus, if the instruction data is large enough, including it in pre-training is feasible. However, since the research team preferred to exclude multiple-choice data and non-multiple-choice data was relatively limited, they ultimately decided not to include instruction data in the pre-training stage.</p>
</li>
<li>
<p><strong>(5) Impact of System Prompts:</strong></p>
<p>System prompts are used to guide the model in generating helpful, polite, and positive responses.</p>
<p>The research team used system prompts similar to those in LLaMA-2, such as clearly telling the model, “You are DeepSeek Chat, a helpful, respectful, and honest AI assistant,” and explaining the knowledge cutoff date, etc.</p>
<p>Interestingly, they found that for the 7B model, adding a system prompt slightly decreased the evaluation score (from 7.15 to 7.11), whereas for the 67B model, the system prompt significantly improved performance (from 8.35 to 8.58). This could be because larger models are better able to understand and follow the intent conveyed by the system prompt, while smaller models may be negatively impacted by inconsistencies between training and testing.</p>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>DeepSeek LLM is a large-scale language model trained from the ground up, built on a corpus of 2 trillion tokens in both Chinese and English. The paper not only records the choice of hyperparameters and Scaling Laws calibration during the model training process but also compares various fine-tuning strategies in depth and proposes a methodology that balances training efficiency and resource allocation.</p>
<p>This release is not just about the model itself but also marks an important milestone as DeepSeek-AI officially enters the open-source LLM space.</p>
<p>Currently, the DeepSeek series has released several core papers. We will take the time to read the subsequent research papers and continue to explore its full technical layout and evolution trajectory.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-03-27T05:19:51.000Z itemprop=dateModified>Mar 27, 2025</time></b> by <b>zephyr-sh</b></span></div></div><div style=margin-top:3rem> </div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/category/deepseek-5><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>DeepSeek (5)</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/deepseek/deepseek-vl/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[24.03] DeepSeek-VL</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#new-scaling-laws class="table-of-contents__link toc-highlight">New Scaling Laws</a><li><a href=#defining-the-problem class="table-of-contents__link toc-highlight">Defining the Problem</a><li><a href=#solving-the-problem class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href=#data-processing-strategy class="table-of-contents__link toc-highlight">Data Processing Strategy</a><li><a href=#model-architecture-design class="table-of-contents__link toc-highlight">Model Architecture Design</a><li><a href=#parameter-configuration class="table-of-contents__link toc-highlight">Parameter Configuration</a></ul><li><a href=#scaling-laws class="table-of-contents__link toc-highlight">Scaling Laws</a><ul><li><a href=#hyperparameter-configuration class="table-of-contents__link toc-highlight">Hyperparameter Configuration</a><li><a href=#optimal-model-and-data-scaling-estimates class="table-of-contents__link toc-highlight">Optimal Model and Data Scaling Estimates</a><li><a href=#impact-of-data-quality class="table-of-contents__link toc-highlight">Impact of Data Quality</a></ul><li><a href=#model-alignment class="table-of-contents__link toc-highlight">Model Alignment</a><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#chinese-open-ended-evaluation class="table-of-contents__link toc-highlight">Chinese Open-Ended Evaluation</a><li><a href=#english-open-ended-evaluation class="table-of-contents__link toc-highlight">English Open-Ended Evaluation</a><li><a href=#other-interesting-findings class="table-of-contents__link toc-highlight">Other Interesting Findings</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>