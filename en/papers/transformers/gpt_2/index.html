<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-transformers/gpt_2/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[19.02] GPT-2 | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/transformers/gpt_2/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[19.02] GPT-2 | DOCSAID"><meta data-rh="true" name="description" content="Forty-Eight Layers of Decoders"><meta data-rh="true" property="og:description" content="Forty-Eight Layers of Decoders"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/transformers/gpt_2/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/gpt_2/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/transformers/gpt_2/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/transformers/gpt_2/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.3057f3b6.css">
<script src="/en/assets/js/runtime~main.0bdac5f7.js" defer="defer"></script>
<script src="/en/assets/js/main.70a61a2a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/transformers/gpt_2/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/transformers/gpt_2/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns-8">Classic CNNs (8)</a><button aria-label="Expand sidebar category &#x27;Classic CNNs (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="Expand sidebar category &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="Expand sidebar category &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="Expand sidebar category &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/multimodality-18">Multimodality (18)</a><button aria-label="Expand sidebar category &#x27;Multimodality (18)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization-1">Normalization (1)</a><button aria-label="Expand sidebar category &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection-5">Object Detection (5)</a><button aria-label="Expand sidebar category &#x27;Object Detection (5)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="Expand sidebar category &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="Expand sidebar category &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection-8">Text Detection (8)</a><button aria-label="Expand sidebar category &#x27;Text Detection (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/transformers-13">Transformers (13)</a><button aria-label="Collapse sidebar category &#x27;Transformers (13)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/transformer/">[17.06] Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/gpt_1/">[18.06] GPT-1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/bert/">[18.10] BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/transformer-xl/">[19.01] Transformer-XL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/adapter/">[19.02] Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/transformers/gpt_2/">[19.02] GPT-2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/sparse-transformer/">[19.04] Sparse Transformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/scaling_laws/">[20.01] Scaling Laws</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/longformer/">[20.04] Longformer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/gpt_3/">[20.05] GPT-3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/bigbird/">[20.07] BigBird</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/autoprompt/">[20.10] AutoPrompt</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/transformers/roformer/">[21.04] RoFormer</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="Expand sidebar category &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/transformers-13"><span itemprop="name">Transformers (13)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[19.02] GPT-2</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[19.02] GPT-2</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="forty-eight-layers-of-decoders">Forty-Eight Layers of Decoders<a href="#forty-eight-layers-of-decoders" class="hash-link" aria-label="Direct link to Forty-Eight Layers of Decoders" title="Direct link to Forty-Eight Layers of Decoders">​</a></h2>
<p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener noreferrer"><strong>Language models are unsupervised multitask learners</strong></a></p>
<hr>
<p>The first generation of GPT stacked twelve layers of Transformer decoders. OpenAI felt this wasn&#x27;t enough, so they stacked forty-eight layers of Transformer decoders, resulting in GPT-2.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="defining-the-problem">Defining the Problem<a href="#defining-the-problem" class="hash-link" aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem">​</a></h2>
<p>The authors believe that existing machine learning systems, despite their excellent performance, are fraught with issues. The paper briefly outlines several areas of concern:</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="fragility">Fragility<a href="#fragility" class="hash-link" aria-label="Direct link to Fragility" title="Direct link to Fragility">​</a></h3>
<ul>
<li>Machine learning systems exhibit fragility when there are slight changes in data distribution and task specifications.</li>
<li>These systems lack stability in handling input diversity and variations, leading to unstable behaviors.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="limitations-of-narrow-experts">Limitations of Narrow Experts<a href="#limitations-of-narrow-experts" class="hash-link" aria-label="Direct link to Limitations of Narrow Experts" title="Direct link to Limitations of Narrow Experts">​</a></h3>
<ul>
<li>Current machine learning systems function more like narrow experts rather than general-purpose systems, limiting their application scope.</li>
<li>These systems rely on manually building and annotating training datasets for each specific task, a tedious and non-generalizable process.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lack-of-generalization">Lack of Generalization<a href="#lack-of-generalization" class="hash-link" aria-label="Direct link to Lack of Generalization" title="Direct link to Lack of Generalization">​</a></h3>
<ul>
<li>The prevalence of single-task training on single-domain datasets results in systems that lack generalization capabilities.</li>
<li>To achieve robust systems, training and evaluation need to span a broader range of domains and tasks.</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-problem">Solving the Problem<a href="#solving-the-problem" class="hash-link" aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<p>This paper does not propose a new model architecture but extends the existing GPT-1 model.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="model-design">Model Design<a href="#model-design" class="hash-link" aria-label="Direct link to Model Design" title="Direct link to Model Design">​</a></h3>
<p>First, the authors moved LayerNorm to the input of each sub-block, similar to pre-activation residual networks. Additionally, they added an extra layer normalization after the final self-attention block.</p>
<p>A modified initialization method was used, considering the cumulative effect on the residual path as model depth increases. The residual layers&#x27; weights were scaled by a factor of 1/√N during initialization, where N is the number of residual layers.</p>
<p>To enhance the model&#x27;s capabilities, the vocabulary size was expanded to 50,257, the context size increased from 512 to 1024 tokens, and a larger batch size of 512 was used.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>If you have experience, you will know that a batch size of 512 in this design is quite large.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="multitask-learning">Multitask Learning<a href="#multitask-learning" class="hash-link" aria-label="Direct link to Multitask Learning" title="Direct link to Multitask Learning">​</a></h3>
<p>A general-purpose system should be capable of performing many different tasks. This means modeling P(output|input, task) rather than just modeling the conditional distribution of a single task P(output|input).</p>
<p>Here, the authors propose a method of specifying all inputs and outputs as symbol sequences and training the model using language modeling. For example, a translation training example might be represented as (translate to French, English text, French text), while a reading comprehension training example might be (answer the question, document, question, answer).</p>
<p>In short, different tasks are distinguished using text descriptions.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="diverse-data">Diverse Data<a href="#diverse-data" class="hash-link" aria-label="Direct link to Diverse Data" title="Direct link to Diverse Data">​</a></h3>
<p>Most prior language model training focused on single text domains, such as news articles, Wikipedia, or book novels. The authors&#x27; approach encourages constructing the largest and most diverse dataset possible to cover a broader range of natural language contexts and demonstrations.</p>
<p>For this, the authors chose web scraping, specifically using the Common Crawl dataset. However, they found significant quality issues with Common Crawl.</p>
<ul>
<li>
<p><strong>Solution</strong>:</p>
<ul>
<li><strong>New Web Scraping</strong>: To improve data quality, they scraped all outbound links from the Reddit platform that received at least three upvotes, considering them clean data.</li>
<li><strong>WebText Dataset</strong>: The resulting dataset, WebText, includes a subset of texts from these 45 million links. The initial version of WebText excludes links created after December 2017, and after deduplication and heuristic cleaning, it contains just over 8 million documents, totaling 40 GB of text.</li>
<li><strong>Wikipedia Exclusion</strong>: All Wikipedia documents were removed from WebText to avoid overlap with evaluation tasks, ensuring more accurate analysis.</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>This indicates that pre-2017 data was not influenced by language models, representing genuine human language. Post-2017, it&#x27;s challenging to discern whether a piece of text was written by a human or a machine.</p><p>This poses a significant challenge for future model training, as learning from self-generated data can amplify any errors or biases. Over iterations, these errors accumulate, causing the model&#x27;s performance to degrade gradually until it eventually collapses.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="optimized-bpe-encoding">Optimized BPE Encoding<a href="#optimized-bpe-encoding" class="hash-link" aria-label="Direct link to Optimized BPE Encoding" title="Direct link to Optimized BPE Encoding">​</a></h3>
<p>The advantage of byte-level BPE lies in its flexibility and generality, as it can decompose any character into a sequence of bytes. However, directly applying BPE to byte sequences can result in suboptimal merges due to BPE&#x27;s greedy frequency algorithm.</p>
<p>To address this, the authors propose an improved method: preventing merges between different character classes within the byte sequences, except for spaces. This significantly improves compression efficiency while only adding minimal single-character fragments in multiple token representations.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>This can be likened to piecing together a jigsaw puzzle.</p><p>Ordinary BPE is like first assembling commonly occurring puzzle pieces, such as parts of the sky or grass, and then putting these larger assembled pieces together. The benefit is that you can quickly piece together large sections of the sky or grass, making it efficient.</p><p>However, if the puzzle&#x27;s image is highly complex with many different patterns (analogous to Unicode characters), you need to remember all these patterns&#x27; combinations. Consequently, your puzzle pieces become numerous, making management cumbersome.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="byte-level-bpe">Byte-Level BPE<a href="#byte-level-bpe" class="hash-link" aria-label="Direct link to Byte-Level BPE" title="Direct link to Byte-Level BPE">​</a></h4><p>The byte-level BPE approach focuses on each puzzle piece itself, breaking each piece down into smaller units, possibly based on color or shape. This means you only need to remember these basic units rather than the entire pattern&#x27;s combination, significantly reducing the number of puzzle pieces and simplifying management to just 256 basic units, far fewer than thousands of pattern combinations.</p><p>However, this method also has an issue: randomly combining these small units can result in more fragmented pieces. For example, you might mix parts of the sky and grass, making it hard to piece together a complete image.</p><p>To solve this, the authors proposed an improved method. They decided to avoid mixing different types of patterns when combining these small units, such as not mixing units of the sky and grass. This ensures that each part of the pattern is easier to piece together. Additionally, they allowed some space between patterns, maintaining each part&#x27;s integrity.</p><p>This way, they could combine the advantages of both methods: efficiently piecing together large patterns while not needing to remember too many pattern combinations. This method made their puzzle-solving more efficient and easier to see the complete picture.</p></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="model-sizes">Model Sizes<a href="#model-sizes" class="hash-link" aria-label="Direct link to Model Sizes" title="Direct link to Model Sizes">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Model Sizes" src="/en/assets/images/img1-c3737326094ba966db2b08b32ee80e8b.jpg" width="804" height="312" class="img_ev3q"></p>
<p>The paper proposes four model parameter designs, which were trained and benchmarked.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="zero-shot-performance">Zero-Shot Performance<a href="#zero-shot-performance" class="hash-link" aria-label="Direct link to Zero-Shot Performance" title="Direct link to Zero-Shot Performance">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Results" src="/en/assets/images/img2-ae04d7f1c1775b0a20e0982c681cefcd.jpg" width="1586" height="376" class="img_ev3q"></p>
<p>The authors first evaluated the model&#x27;s zero-shot performance, i.e., its performance on unseen tasks.</p>
<p>They found that GPT-2 performs well in cross-domain and cross-dataset transferability, improving existing technology levels in 7 out of 8 datasets in zero-shot settings. Significant improvements were seen on small datasets like Penn Treebank and WikiText-2, which contain only 1 to 2 million training tokens.</p>
<p>The dataset for measuring long-term dependencies, such as LAMBADA and the Children&#x27;s Book Test, also showed substantial progress.</p>
<table><thead><tr><th>Metric</th><th>Meaning</th><th>Interpretation</th></tr></thead><tbody><tr><td><strong>PPL</strong></td><td><strong>Perplexity</strong>: Measures model sequence prediction performance.</td><td>Lower score indicates better model prediction accuracy.</td></tr><tr><td><strong>ACC</strong></td><td><strong>Accuracy</strong>: Measures the proportion of correct predictions.</td><td>Higher score indicates higher prediction accuracy.</td></tr><tr><td><strong>BPB</strong></td><td><strong>Bits Per Byte</strong>: Average bits needed per byte in binary representation.</td><td>Lower score indicates better compression and prediction efficiency.</td></tr><tr><td><strong>BPC</strong></td><td><strong>Bits Per Character</strong>: Average bits needed per character in binary representation.</td><td>Lower score indicates better compression and prediction efficiency.</td></tr></tbody></table>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="various-experiments">Various Experiments<a href="#various-experiments" class="hash-link" aria-label="Direct link to Various Experiments" title="Direct link to Various Experiments">​</a></h3>
<p>The authors conducted experiments on various tasks, including language modeling, machine translation, question answering, text summarization, and text generation.</p>
<p>Here&#x27;s a summary table to quickly understand GPT-2&#x27;s performance on various tasks:</p>
<table><thead><tr><th><strong>Test Name</strong></th><th><strong>Purpose</strong></th><th><strong>Test Method</strong></th><th><strong>Results</strong></th></tr></thead><tbody><tr><td><strong>Children&#x27;s Book Test (CBT)</strong></td><td>Check model performance on different word categories.</td><td>Predict correct options in cloze test.</td><td>93.3% accuracy on common nouns, 89.1% accuracy on named entities.</td></tr><tr><td><strong>LAMBADA</strong></td><td>Test modeling long-term dependencies.</td><td>Predict the last word in a sentence.</td><td>Perplexity reduced from 99.8 to 8.6, accuracy improved from 19% to 52.66%; accuracy with stopword filter improved to 63.24%.</td></tr><tr><td><strong>Winograd Schema Challenge</strong></td><td>Measure ability to resolve text ambiguity.</td><td>Solve ambiguity problems.</td><td>Performed well on a small dataset of 273 examples.</td></tr><tr><td><strong>Reading Comprehension (CoQA)</strong></td><td>Test ability to answer questions relying on dialogue history.</td><td>Use conversational question-answering dataset.</td><td>Achieved 55 F1 score on the development set, approaching or exceeding three-quarters of baseline systems.</td></tr><tr><td><strong>Summarization</strong></td><td>Test summarization ability.</td><td>Generate summaries via top-k random sampling.</td><td>Generated summaries similar in quality to human-written ones, though occasionally confusing in details; marginally better than selecting 3 random sentences on ROUGE metrics.</td></tr><tr><td><strong>Translation</strong></td><td>Test translation ability.</td><td>Use English-French sentence translation.</td><td>Achieved 5 BLEU on English-French test set; 11.5 BLEU on French-English test set, lower than the best unsupervised methods&#x27; 33.5 BLEU.</td></tr><tr><td><strong>Question Answering</strong></td><td>Test frequency of producing correct answers to factual questions.</td><td>Embed example question-answer pairs, evaluate answer accuracy.</td><td>Correctly answered 4.1% of questions, 5.3 times more than baseline; 63.1% accuracy on the top 1% most confident questions.</td></tr></tbody></table>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This study aims to explore the application and potential of unsupervised multitask learning in natural language processing (NLP), particularly utilizing the GPT-2 model&#x27;s ability to perform various NLP tasks directly without supervised adaptation or modification.</p>
<p>GPT-2 demonstrates that when large language models are trained on sufficiently large and diverse datasets, they can perform well across many domains and datasets, capable of executing numerous tasks without explicit supervision.</p>
<p>These findings offer new perspectives and possibilities for unsupervised learning and model improvement in future NLP tasks.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-09-14T23:02:28.000Z" itemprop="dateModified">Sep 14, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/transformers/adapter/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[19.02] Adapter</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/transformers/sparse-transformer/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[19.04] Sparse Transformer</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#forty-eight-layers-of-decoders" class="table-of-contents__link toc-highlight">Forty-Eight Layers of Decoders</a></li><li><a href="#defining-the-problem" class="table-of-contents__link toc-highlight">Defining the Problem</a><ul><li><a href="#fragility" class="table-of-contents__link toc-highlight">Fragility</a></li><li><a href="#limitations-of-narrow-experts" class="table-of-contents__link toc-highlight">Limitations of Narrow Experts</a></li><li><a href="#lack-of-generalization" class="table-of-contents__link toc-highlight">Lack of Generalization</a></li></ul></li><li><a href="#solving-the-problem" class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href="#model-design" class="table-of-contents__link toc-highlight">Model Design</a></li><li><a href="#multitask-learning" class="table-of-contents__link toc-highlight">Multitask Learning</a></li><li><a href="#diverse-data" class="table-of-contents__link toc-highlight">Diverse Data</a></li><li><a href="#optimized-bpe-encoding" class="table-of-contents__link toc-highlight">Optimized BPE Encoding</a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#model-sizes" class="table-of-contents__link toc-highlight">Model Sizes</a></li><li><a href="#zero-shot-performance" class="table-of-contents__link toc-highlight">Zero-Shot Performance</a></li><li><a href="#various-experiments" class="table-of-contents__link toc-highlight">Various Experiments</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>