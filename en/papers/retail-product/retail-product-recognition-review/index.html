<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-retail-product/retail-product-recognition-review/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[20.11] RPR: Review | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/retail-product/retail-product-recognition-review/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[20.11] RPR: Review | DOCSAID"><meta data-rh=true name=description content="Retail Product Recognition"><meta data-rh=true property=og:description content="Retail Product Recognition"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/retail-product/retail-product-recognition-review/><link data-rh=true rel=alternate href=https://docsaid.org/papers/retail-product/retail-product-recognition-review/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/retail-product/retail-product-recognition-review/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/retail-product/retail-product-recognition-review/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/retail-product/retail-product-recognition-review/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/retail-product-4","name":"Retail Product (4)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/retail-product/retail-product-recognition-review/","name":"[20.11] RPR: Review","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.7c428b09.js defer></script><script src=/en/assets/js/main.a90830bf.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/retail-product/retail-product-recognition-review/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/retail-product/retail-product-recognition-review/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/retail-product/retail-product-recognition-review/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-14>Contrastive Learning (14)</a><button aria-label="Expand sidebar category 'Contrastive Learning (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-43>Face Anti-Spoofing (43)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (43)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-16>Object Detection (16)</a><button aria-label="Expand sidebar category 'Object Detection (16)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/retail-product-4>Retail Product (4)</a><button aria-label="Collapse sidebar category 'Retail Product (4)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/retail-product/rpc/>[19.01] RPC Dataset</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/retail-product/sku-110k/>[19.04] SKU-110K</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/retail-product/dpsnet/>[20.11] DPSNet</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/retail-product/retail-product-recognition-review/>[20.11] RPR: Review</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-13>Vision Transformers (13)</a><button aria-label="Expand sidebar category 'Vision Transformers (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 232 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/retail-product-4><span>Retail Product (4)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[20.11] RPR: Review</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[20.11] RPR: Review</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=retail-product-recognition>Retail Product Recognition<a href=#retail-product-recognition class=hash-link aria-label="Direct link to Retail Product Recognition" title="Direct link to Retail Product Recognition">​</a></h2>
<p><a href=https://onlinelibrary.wiley.com/doi/pdf/10.1155/2020/8875910 target=_blank rel="noopener noreferrer"><strong>Deep Learning for Retail Product Recognition: Challenges and Techniques</strong></a></p>
<hr>
<p>Recently, I revisited and organized references in the field of Automatic Check-Out (ACO).</p>
<p>We begin with several survey papers to quickly grasp the overall landscape of this domain.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-definition>Problem Definition<a href=#problem-definition class=hash-link aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>The main goal of Retail Product Recognition technology is to assist retailers in effective product management and to enhance the customer shopping experience. Traditionally, the most widely used method has been barcode recognition, which automatically captures product information by scanning the barcode printed on product packaging.</p>
<p>However, since barcode placement is not fixed, in practice it often requires manual rotation of the product to align with the scanner, causing delays in the process.</p>
<p>According to a survey by Digimarc, approximately 45% of customers report that barcode scanning is inconvenient.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>Digimarc Corporation is a publicly traded company headquartered in Beaverton, Oregon, USA, specializing in the development of digital watermarking and serialized QR code recognition technologies. These technologies are used to enhance product authentication, supply chain tracking, and recycling sorting applications.</div></div>
<p>In this context, RFID technology can be considered a possible alternative. It transmits data via radio waves and does not rely on line-of-sight scanning to complete recognition tasks, offering theoretical efficiency advantages. Each product is tagged individually, allowing remote reading without the need for precise alignment.</p>
<p>The drawback is cost.</p>
<p>In the retail industry, where margins are tight, affixing an RFID tag to every item accumulates into a significant expense over time. Additionally, when multiple items are identified simultaneously, RFID signals can suffer from occlusion or interference, leading to errors, which is unfavorable for cost control in high-volume product sales.</p>
<p>With the rapid digitalization of the retail sector, companies are increasingly seeking to improve operational efficiency and customer experience through artificial intelligence technologies.</p>
<p>According to a Juniper Research report, global retail spending on AI-related services is expected to grow from $3.6 billion in 2019 to $12 billion by 2023, demonstrating strong investment in such technologies. On the other hand, as supermarkets display an ever-growing number of products, labor management costs rise significantly, further driving retailers to pursue higher automation in recognition solutions.</p>
<p>The widespread adoption of digital imaging devices has facilitated the generation of large-scale product image datasets, forming a crucial foundation for developing computer vision recognition systems.</p>
<p>Product recognition tasks can be viewed as a combined problem of image classification and object detection, with the core objective of automatically identifying product categories and locations through images. This technology can be applied in multiple scenarios:</p>
<ol>
<li><strong>Shelf Management</strong>: Automatically detecting out-of-stock products and alerting staff to restock. Research indicates that fully implementing shelf plans can increase sales by 7.8% and gross profit by 8.1%.</li>
<li><strong>Self-Checkout Systems</strong>: Shortening checkout time via product image recognition, enhancing customer satisfaction. SCO system deployment grew steadily from 2014 to 2019 and is widely adopted to reduce labor costs.</li>
<li><strong>Assistance for the Visually Impaired</strong>: Helping visually impaired customers recognize product information (such as price, brand, and expiration date), lowering shopping barriers and improving autonomy and social participation.</li>
</ol>
<p>From a technical perspective, compared to traditional handcrafted feature methods, deep learning can automatically learn features directly from images, offering higher recognition capability and generalization. Its multilayer structure can extract more detailed semantic information, suitable for complex and multi-category product scenarios.</p>
<p>Currently, research teams have applied deep learning to the retail domain and achieved concrete results across multiple tasks. Industry applications such as Amazon Go and Walmart Intelligent Retail Lab have already emerged.</p>
<p>Despite the increasing volume of related research in recent years, systematic surveys focused on "deep learning for product recognition tasks" remain limited. Previously, only two surveys on retail shelf product detection have been published; both excluded checkout scenarios and did not focus on deep learning approaches.</p>
<p>The authors of this paper review over one hundred publications from leading conferences and journals like CVPR, ICCV, and AAAI, attempting to integrate existing techniques, challenges, and resources. They hope this paper can serve as an introductory guide for researchers and engineers in the field, helping them quickly grasp core problems and existing achievements.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>We especially appreciate authors who are so dedicated and helpful—our sincere gratitude.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=traditional-methods>Traditional Methods<a href=#traditional-methods class=hash-link aria-label="Direct link to Traditional Methods" title="Direct link to Traditional Methods">​</a></h2>
<p>The core of product image recognition lies in extracting representative features from packaging images to accomplish classification and identification tasks.</p>
<p>Early computer vision research commonly adopted a modular processing pipeline, decomposing the recognition system into several key steps:</p>
<ol>
<li><strong>Image Capture</strong>: Collecting product images via cameras or mobile devices.</li>
<li><strong>Preprocessing</strong>: Removing noise and simplifying information in the input images, including image segmentation, geometric transformations, and contrast enhancement.</li>
<li><strong>Feature Extraction</strong>: Analyzing image patches to identify stable features invariant to position or scale changes.</li>
<li><strong>Feature Classification</strong>: Mapping extracted features into vector space and applying specific classification algorithms for prediction.</li>
<li><strong>Recognition Output</strong>: Producing product category results from a pretrained classifier.</li>
</ol>
<p>The crucial step in this framework is <strong>feature extraction</strong>, as its accuracy directly affects the final recognition performance. Before deep learning became widespread, researchers heavily relied on handcrafted features to capture visual characteristics of images.</p>
<p>Two classic methods are:</p>
<ul>
<li><strong>SIFT (Scale-Invariant Feature Transform)</strong>: Proposed by David Lowe in 1999, it extracts local features at multiple scales using an image pyramid, offering invariance to rotation, translation, and scale, and has been widely used in object matching and classification.</li>
<li><strong>SURF (Speeded Up Robust Features)</strong>: Developed in 2006 based on SIFT, it optimizes computational efficiency and suits applications with real-time requirements.</li>
</ul>
<p>However, these handcrafted features depend on developers’ expertise and assumptions, limiting their ability to capture all potentially important information in images. Moreover, when product categories are large, packaging designs vary significantly, or shooting conditions change (e.g., angle, lighting), handcrafted features struggle to maintain recognition stability and scalability. This prompted the research community to gradually shift toward data-driven deep learning methods, which learn the most discriminative feature representations directly from images through end-to-end training.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=deep-learning-methods>Deep Learning Methods<a href=#deep-learning-methods class=hash-link aria-label="Direct link to Deep Learning Methods" title="Direct link to Deep Learning Methods">​</a></h2>
<p>Deep learning, a subfield of machine learning, aims to automatically learn multi-level representations from data to capture high-level semantic structures. This approach avoids the limitations of manual feature design and is particularly suitable for high-dimensional data such as images, speech, and text.</p>
<p>In image recognition tasks, deep learning advantages have been amplified by improved GPU computing power, gradually replacing traditional methods to become mainstream solutions. In retail product recognition, deep learning mainly covers two tasks:</p>
<ol>
<li><strong>Image Classification</strong>: Assigning input images to predefined categories. With sufficient training data, models achieve accuracy surpassing human-level performance.</li>
<li><strong>Object Detection</strong>: Beyond classification, models also predict object locations in images (represented by bounding boxes). This task demands higher model design and computational efficiency and is an indispensable module in product recognition.</li>
</ol>
<p>The breakthrough in deep learning for images primarily stems from convolutional neural networks (CNNs). Inspired by physiological studies of the cat visual cortex, LeCun et al. first proposed using CNNs for image classification in 1988, successfully applying them to handwritten digit and check recognition.</p>
<p>After 2010, the ImageNet challenge accelerated rapid evolution of CNN architectures, spawning various mainstream models:</p>
<ul>
<li><strong>AlexNet (2012)</strong>: Introduced ReLU and Dropout, breaking traditional image recognition bottlenecks and igniting the deep learning boom.</li>
<li><strong>GoogLeNet (2014)</strong>: Employed Inception modules to reduce parameter count while increasing model depth.</li>
<li><strong>VGG (2014)</strong>: Emphasized uniform 3x3 convolution kernels for easy architecture composition and reuse.</li>
<li><strong>ResNet (2015)</strong>: Proposed residual connections to solve the degradation problem in very deep networks, enabling training of models over 100 layers.</li>
</ul>
<p>Recent research has also extended CNN applications to 3D structure recognition, developing Multiview CNNs that input multi-angle images for more precise classification, suitable for advanced tasks like 3D product recognition.</p>
<p>In summary, deep learning’s two main driving forces are <strong>large-scale data and deeper network structures</strong>. Their synergy continuously advances model capabilities in visual recognition tasks, laying the technical foundation for product recognition systems.</p>
<p>Returning to object detection tasks:</p>
<p>Within deep learning, the core goal of object detection is:</p>
<blockquote>
<p><strong>Automatically identifying object categories and their locations (bounding boxes) within images.</strong></p>
</blockquote>
<p>Before deep learning, object detection largely relied on sliding window strategies that scanned fixed-size windows over the entire image, classifying each patch to determine target presence. This method was extremely time-consuming and inefficient for large images or scenes with multiple objects.</p>
<p>With deep learning, object detection algorithms are broadly categorized into:</p>
<ul>
<li>
<p><strong>Two-stage methods (region proposal first)</strong>
Represented by the R-CNN series, the process involves two stages: first, generating candidate object regions via algorithms like Selective Search; second, classifying and refining these regions with CNNs.</p>
<ul>
<li>R-CNN: Processes each candidate region individually with CNN, improving accuracy but slow in speed.</li>
<li>Fast R-CNN: Applies CNN to the entire image once, then uses ROI pooling on feature maps, greatly reducing redundant computation.</li>
<li>Faster R-CNN: Introduces a Region Proposal Network (RPN) to learn proposals automatically within the network, sharing features with the classifier, becoming the current mainstream for high accuracy.</li>
</ul>
</li>
<li>
<p><strong>One-stage methods (end-to-end regression)</strong>
Directly regress object locations and categories from the image, omitting the proposal stage, thus faster but initially less accurate.</p>
<ul>
<li>Representative models include YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector), which show clear advantages in real-time applications such as instant checkout and visual navigation.</li>
</ul>
</li>
</ul>
<p>Both methods have pros and cons: Two-stage models generally offer higher stability in complex backgrounds, while One-stage models suit deployment environments with low latency requirements. Practical applications choose based on the trade-off between accuracy and timeliness.</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="pipeline of image-based product recognition" src=/en/assets/images/img1-71e1d4e810038c545f9d4251984afc04.jpg width=1508 height=580 class=img_ev3q></figure></div>
<p>Technically, product recognition tasks can be regarded as specialized applications of object detection, with a typical approach as illustrated above:</p>
<ol>
<li><strong>Product Detection</strong>: Use an object detection model to generate multiple bounding boxes indicating product regions.</li>
<li><strong>Region Cropping</strong>: Crop each predicted region into a single product image.</li>
<li><strong>Image Classification</strong>: Input cropped images into classification models to infer product categories.</li>
</ol>
<p>In recent years, several companies have deployed deep learning technologies in retail environments:</p>
<ul>
<li><strong>Amazon Go (2018)</strong>: Utilizes dozens of cameras to capture customer trajectories and combines CNN models to recognize shopping behavior and products. To compensate for pure image recognition limitations, the system integrates Bluetooth and weight sensors to improve overall accuracy.</li>
<li><strong>Walmart IRL (2019)</strong>: Focuses on real-time shelf monitoring, using cameras and deep learning models to automatically detect out-of-stock situations and alert restocking personnel.</li>
<li><strong>Chinese Companies (DeepBlue Technology, Malong Technologies)</strong>: Offer integrated automatic vending machines, smart weighing systems, and product recognition modules that combine product image analysis, real-time checkout, and classification recommendation. Malong’s AI Fresh system specially targets fresh produce, addressing unstructured visual features such as variations in fruit and vegetable appearance.</li>
</ul>
<p>Currently, although initial commercial deployments exist, deep learning-based product recognition technologies still face many challenges:</p>
<ul>
<li>Trade-offs between accuracy and inference speed;</li>
<li>High visual similarity among different products leading to misclassification;</li>
<li>Imbalanced multi-class data and prominent long-tail distributions;</li>
<li>Deployment costs and multi-device stability require evaluation;</li>
<li>Handling non-ideal real-world factors such as occlusion, reflections, and hand interference remains difficult.</li>
</ul>
<p>Based on these observations, deep learning holds the most potential for product recognition tasks, but its practical application still needs further empirical research and large-scale field deployment to improve and optimize.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=challenges-in-product-recognition>Challenges in Product Recognition<a href=#challenges-in-product-recognition class=hash-link aria-label="Direct link to Challenges in Product Recognition" title="Direct link to Challenges in Product Recognition">​</a></h2>
<p>Although product recognition can be viewed as a variant of object detection tasks, the practical requirements and environments differ significantly from general object detection, making direct transfer of existing models difficult.</p>
<p>In this section, the authors summarize four major challenges faced by retail product recognition, as outlined below:</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=challenge-1-large-number-of-categories>Challenge 1: Large Number of Categories<a href=#challenge-1-large-number-of-categories class=hash-link aria-label="Direct link to Challenge 1: Large Number of Categories" title="Direct link to Challenge 1: Large Number of Categories">​</a></h3>
<p>Compared to typical object detection tasks, the most distinctive feature of product recognition is that the <strong>number of categories far exceeds standard datasets</strong>.</p>
<p>A medium-sized supermarket often stocks thousands of Stock Keeping Units (SKUs), far beyond common datasets. In practice, a single image frequently contains a dozen or more product categories, with subtle differences among categories (such as different specifications within the same brand), resulting in recognition difficulties much greater than usual detection tasks.</p>
<p>Moreover, mainstream models like Faster R-CNN, YOLO, and SSD assume a fixed number of classes in their classification heads. When the number of categories expands to thousands, both accuracy and recall drop significantly.</p>
<p>The paper’s experiments, as shown below, illustrate this:</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt="large classes" src=/en/assets/images/img2-d22281f4b59a1045a398a579f7c303de.jpg width=1176 height=856 class=img_ev3q></figure></div>
<p>Regardless of model architecture, <strong>accuracy noticeably decreases when classes increase from 20 to 80</strong>.</p>
<p>Thus, relying solely on traditional object detection architectures faces learning bottlenecks and inference instability due to the high dimensionality of classification in product recognition. This challenge extends beyond architecture design, involving data distribution, inter-class feature representation, and classification strategy design.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=challenge-2-domain-gap-in-data-distribution>Challenge 2: Domain Gap in Data Distribution<a href=#challenge-2-domain-gap-in-data-distribution class=hash-link aria-label="Direct link to Challenge 2: Domain Gap in Data Distribution" title="Direct link to Challenge 2: Domain Gap in Data Distribution">​</a></h3>
<p>Deep learning models depend heavily on large annotated datasets for training. However, product recognition faces three main data acquisition constraints:</p>
<ol>
<li>
<p><strong>High annotation cost:</strong>
Bounding box or segmentation annotation for product recognition usually requires manual labor. Creating tens of thousands of labeled training images demands significant time and human resources. Although tools like LabelImg and LabelMe aid annotation, overall production cost remains a barrier to large-scale expansion.</p>
<p><img decoding=async loading=lazy alt="domain gap" src=/en/assets/images/img3-029aad43816b45a288c2e637a287d32b.jpg width=1584 height=374 class=img_ev3q></p>
</li>
<li>
<p><strong>Domain gap between training data and real scenarios:</strong>
Existing product datasets are often captured under ideal conditions—fixed angles and simple backgrounds (e.g., rotating platforms). Test or deployment environments typically feature complex backgrounds, variable lighting, and frequent occlusions, causing a gap between training and real-world performance.</p>
</li>
</ol>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="domain gap" src=/en/assets/images/img4-1de51520d838b446530489d3331c1b25.jpg width=1224 height=464 class=img_ev3q></figure></div>
<ol start=3>
<li>
<p><strong>Imbalanced data and long-tail distribution:</strong>
Product datasets commonly exhibit “few samples per many classes.” Unlike VOC or COCO which have relatively balanced category distributions, retail datasets contain fewer images but many classes, making learning difficult.</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=unbalance src=/en/assets/images/img5-e3d6023a6002f0e5286859cefe24c186.jpg width=1196 height=680 class=img_ev3q></figure></div>
</li>
</ol>
<p>In summary, data insufficiency limits model performance and hampers generalization and rapid transfer learning for new products. Without systematic solutions for data scarcity and domain shifts, even advanced architectures may fail to meet deployment requirements.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=challenge-3-high-intraclass-variation>Challenge 3: High Intraclass Variation<a href=#challenge-3-high-intraclass-variation class=hash-link aria-label="Direct link to Challenge 3: High Intraclass Variation" title="Direct link to Challenge 3: High Intraclass Variation">​</a></h3>
<p>A major difficulty in product recognition lies in accurately distinguishing products with <strong>high intraclass heterogeneity</strong>, also known as <strong>sub-category recognition</strong> or <strong>fine-grained classification</strong>.</p>
<p>This challenge has the following characteristics:</p>
<ol>
<li><strong>Extremely subtle visual differences:</strong> Cookies of different flavors or packaging sizes within the same brand may differ only in color saturation or text placement, sometimes indistinguishable by the human eye.</li>
<li><strong>Diverse appearance variations:</strong> The same product may look quite different from varying angles or scales; models must be invariant to scale and viewpoint.</li>
<li><strong>Significant environmental interference:</strong> Lighting, background, and occlusion significantly impact recognition, posing challenges to model decision boundaries.</li>
</ol>
<p>Fine-grained classification is well-studied in other domains (e.g., bird species or car model recognition), often relying on extra annotations like keypoints or part alignment to help models learn subtle differences. However, in retail scenarios, challenges are even more pronounced:</p>
<ul>
<li>Visual similarity between products extends beyond shape to packaging structure, colors, and fonts.</li>
<li>Lack of dedicated fine-grained product datasets; most existing datasets only label overall categories without clear sub-category definitions.</li>
<li>Without additional labeled data or expert knowledge, models struggle to learn effective discrimination, resulting in increased misclassification or confusion rates.</li>
</ul>
<p>For example, (a) below shows two visually similar products with different flavors:</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="intraclass products" src=/en/assets/images/img6-326ad5eddf6574487948d7db965ccf5b.jpg width=1212 height=360 class=img_ev3q></figure></div>
<p>Differences lie only in slight adjustments of packaging text color and placement; (b) shows the same brand’s different capacity packages, where size difference is hard to judge from a single image. These examples highlight the need for fine-grained representation to address “extremely similar but not identical” recognition requirements in practical retail environments.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=challenge-4-insufficient-system-flexibility>Challenge 4: Insufficient System Flexibility<a href=#challenge-4-insufficient-system-flexibility class=hash-link aria-label="Direct link to Challenge 4: Insufficient System Flexibility" title="Direct link to Challenge 4: Insufficient System Flexibility">​</a></h3>
<p>Retail products are frequently updated, with new items introduced and packaging regularly redesigned. For product image recognition systems, retraining the entire model whenever a new product appears is time-consuming and impractical.</p>
<p>An ideal system should have:</p>
<ul>
<li><strong>Rapid scalability:</strong> Ability to incorporate new classes with very few samples (few-shot or zero-shot learning).</li>
<li><strong>Continual learning capability:</strong> Learning new products without forgetting old classes (continual or lifelong learning).</li>
</ul>
<p>However, CNN architectures commonly suffer from the “catastrophic forgetting” problem: fine-tuning on new categories causes significant degradation in recognition performance on previously learned classes.</p>
<p>For example:</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="new classes" src=/en/assets/images/img7-80b439ddf15c1a3e526f133e23b72cae.jpg width=1224 height=712 class=img_ev3q></figure></div>
<p>The model originally recognizes “orange” but loses this ability after training only on the “banana” category.</p>
<p>Currently, mainstream approaches still rely on retraining the entire model with complete data, which incurs high deployment costs and efficiency bottlenecks. Future recognition architectures supporting the following features would greatly benefit industrial applications:</p>
<ul>
<li>Models with long-term memory capacity;</li>
<li>Support for incremental class training;</li>
<li>Integration of sample replay or regularization techniques to mitigate forgetting.</li>
</ul>
<p>The <strong>flexibility</strong> of product recognition systems will determine their usability and lifecycle in rapidly changing markets.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=technical-overview>Technical Overview<a href=#technical-overview class=hash-link aria-label="Direct link to Technical Overview" title="Direct link to Technical Overview">​</a></h2>
<p>This chapter summarizes existing techniques proposed in the literature to address the four major challenges previously outlined. The focus is on recognition architectures centered on deep learning, supplemented by auxiliary methods that can be integrated with them.</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=techniques src=/en/assets/images/img8-dc52ef5fa133476b5ebd7a7362376b68.jpg width=1164 height=832 class=img_ev3q></figure></div>
<p>Through this categorized summary, readers can more quickly grasp the solution landscape and research trends in product recognition tasks.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p>In the following notes, references will be denoted by 【xx】corresponding to citation numbers in the original papers; readers are encouraged to look up the original sources using these identifiers for detailed information.</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=cnn-based-approaches>CNN-Based Approaches<a href=#cnn-based-approaches class=hash-link aria-label="Direct link to CNN-Based Approaches" title="Direct link to CNN-Based Approaches">​</a></h3>
<p>One of the core challenges in retail product classification is handling the massive number of categories. Within this context, CNN models are widely used for image feature extraction, generating recognizable embedding vectors as feature descriptors for classification or similarity retrieval.</p>
<p>Early handcrafted features like SIFT and SURF, although invariant to rotation and scale, lack semantic-level representations and cannot support large-scale category recognition demands, thus gradually being replaced by CNNs.</p>
<p>Below are several classic works; interested readers can refer to the original papers by their citation numbers.</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=cnn-based src=/en/assets/images/img9-343d8c002016ade55b458913b2b4b2fc.jpg width=1208 height=424 class=img_ev3q></figure></div>
<p>While most methods support hundreds to a thousand product categories, medium-to-large supermarkets typically exceed this scale, leaving room for improvement. Two recent representative works further tackle classification tasks with over a thousand categories:</p>
<ol>
<li><strong>Tonioni et al.【20】</strong>: Used VGG backbone with MAC (Maximum Activations of Convolutions) features to build whole-image embeddings, handling 3,288 product categories with Precision = 57.07% and mAP = 36.02%.</li>
<li><strong>Karlinsky et al.【21】</strong>: Employed fine-tuned VGG-F (fixing layers 2–15), recognizing 3,235 product categories and achieving mAP = 52.16%.</li>
</ol>
<p>These studies demonstrate that CNNs have potential to scale to thousands of categories, but practical recognition performance—especially in recall and fine-grained discrimination—still has significant room for improvement.</p>
<p>Additionally, YOLO9000 proposes a detection framework recognizing up to 9,000 classes using an improved Darknet implementation. Its key innovations include multi-dataset joint training and semantic embeddings (e.g., WordTree). However, its training requires millions of labeled images, making it difficult to apply in retail scenarios where product data is costly and annotation is challenging.</p>
<p>In summary, CNNs provide a viable foundation for large-scale product classification, but enhancing their scalability, data efficiency, and recognition accuracy under "category explosion" remains a key technical challenge.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=data-augmentation>Data Augmentation<a href=#data-augmentation class=hash-link aria-label="Direct link to Data Augmentation" title="Direct link to Data Augmentation">​</a></h3>
<p>Deep learning methods rely heavily on training data, but acquiring large annotated datasets in retail product recognition is time-consuming and expensive. Thus, data augmentation becomes a crucial strategy to mitigate data scarcity.</p>
<p>Common data augmentation techniques fall into two main categories, as summarized below:</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=cnn-based src=/en/assets/images/img10-756f3e37d3ed8f7b07a1d04405667da7.jpg width=1212 height=272 class=img_ev3q></figure></div>
<ul>
<li><strong>Traditional image transformation methods (common synthesis methods)</strong></li>
<li><strong>Generative models</strong></li>
</ul>
<p>Traditional synthesis methods mainly expand original images through geometric and photometric transformations, as illustrated below:</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="common synthesis methods" src=/en/assets/images/img11-1656609ca0ec0bd52883afbbc48b5c84.jpg width=1224 height=808 class=img_ev3q></figure></div>
<p>Popular augmentation tools like <a href=https://github.com/albumentations-team/albumentations target=_blank rel="noopener noreferrer"><strong>Albumentations</strong></a> provide comprehensive APIs supporting translation, rotation, scaling, flipping, random occlusion, noise addition, color enhancement, brightness, and contrast adjustment. These are widely applied in product detection tasks.</p>
<p>Although easy to implement, traditional augmentation struggles to simulate complex real-world conditions such as lighting changes, background clutter, and natural occlusions. Therefore, researchers have turned to generative models to improve data realism.</p>
<p>Generative models offer more realistic image synthesis and mainly include two architectures:</p>
<ul>
<li><strong>VAE (Variational Autoencoder)</strong>
Utilizes an encoder-decoder framework to generate samples, suitable for feature learning and attribute control, though currently limited in image-to-image translation tasks.</li>
<li><strong>GAN (Generative Adversarial Networks)</strong>
Employs adversarial training between generator and discriminator to produce visually realistic samples, supporting style transfer and image synthesis.</li>
</ul>
<p>Relevant literature is summarized below:</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=gan-based src=/en/assets/images/img12-de359f41f8084e0dea3e3fdebe1e41ec.jpg width=1204 height=640 class=img_ev3q></figure></div>
<p>VAEs consist of encoder and decoder components that learn latent space distributions to generate samples resembling original data. Although no specific application of VAEs in product recognition exists yet, research in other domains shows promise.</p>
<p>For example, in face and bird image generation tasks, VAEs can control attributes to produce samples achieving cosine similarity of 0.9057 and reasonable mean squared errors on Wild and CUB datasets【101】. Other studies use conditional VAEs for zero-shot learning, attaining strong performance on AwA, CUB, SUN, and ImageNet datasets【112】.</p>
<p>These successes suggest VAEs have potential to enhance data diversity and attribute control, and may be extended to product recognition tasks in the future.</p>
<p>In contrast, GANs have made more substantial breakthroughs in image generation recently.</p>
<p>Since their introduction in 2014, adversarial training between generator and discriminator enables the synthesis of large amounts of realistic images. Researchers have applied GANs to various data augmentation tasks, including nighttime vehicle detection【115】, semi-supervised semantic hashing【116】, and license plate image generation【122】, all significantly improving model performance.</p>
<p>Applications such as PixelCNN combined with one-hot class encoding for generating category-specific images【119】, or CycleGAN for style transfer and simulating real-world scenario variations, demonstrate strong generalization and transfer capabilities.</p>
<p>Though few GAN applications exist specifically for product recognition, several studies have preliminarily validated their feasibility.</p>
<p>For example, Wei et al.【7】combine background synthesis with CycleGAN style transfer to generate product images suited for checkout counter settings (as shown below), training an FPN detector with 96.57% mAP.</p>
<p><img decoding=async loading=lazy alt=cyclegan src=/en/assets/images/img13-5ebc0a09bcf2533ca4bbf6b29d8fa80b.jpg width=1818 height=494 class=img_ev3q></p>
<p>Subsequently, Li et al.【78】propose DPNet to select reliable images from synthetic data, boosting checkout accuracy to 80.51%. Another study【71】attempts to combine GANs with adversarial encoder training to generate visual samples usable for product recognition.</p>
<p>A limitation is that current methods mostly generate flat backgrounds, not yet simulating complex real checkout or shelf scenarios involving background textures, product overlap, and occlusions. Hence, generating more realistic product images remains a promising direction.</p>
<p>Future developments may involve enhancing semantic control, integrating 3D modeling and physical rendering engines, or combining domain adaptation and cross-domain augmentation strategies to further reduce the gap between synthetic data and real environments.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=fine-grained-classification>Fine-Grained Classification<a href=#fine-grained-classification class=hash-link aria-label="Direct link to Fine-Grained Classification" title="Direct link to Fine-Grained Classification">​</a></h3>
<p>Fine-grained classification is one of the representative challenging tasks in computer vision, aiming to distinguish subcategories under the same higher-level category, such as different flower species, car models, or animal breeds.</p>
<p>When applied to product recognition, the challenge intensifies due to high visual similarity among products combined with multiple interference factors like motion blur, lighting variations, deformation, viewpoint, and placement.</p>
<p>Based on existing literature, methods for fine-grained product recognition can be broadly categorized into two groups:</p>
<ol>
<li><strong>Fine Feature Representation</strong></li>
<li><strong>Context Awareness</strong></li>
</ol>
<p>The core of fine-grained classification is to extract discriminative subtle features from visually similar objects. According to the strength of supervision signals, approaches are divided into “strongly supervised” and “weakly supervised” methods:</p>
<ul>
<li>
<p><strong>Strongly Supervised Methods</strong></p>
<p>These methods require additional annotations such as bounding boxes and part information, enabling precise alignment of local regions and allowing the model to focus on key differences.</p>
<div style=white-space:nowrap;overflow-x:auto;font-size:1rem;line-height:0.8;justify-content:center;display:flex><table><thead><tr><th>Study<th>Method<th>Highlight<th>Application<tbody><tr><td>Part-based R-CNN【127】<td>R-CNN based, extracts global and local features<td>Achieved SOTA on bird dataset<td>Inspired local feature fusion in product recognition<tr><td>Pose-normalized CNN【137】<td>Uses DPM for localization and part extraction, followed by SVM classification<td>75.7% accuracy<td>Suitable for products with significant pose variation<tr><td>DiffNet【139】<td>Compares differences between two similar product images to auto-generate difference annotations<td>No annotation required for common products<td>Product recognition mAP = 95.56%</table></div>
</li>
<li>
<p><strong>Weakly Supervised Methods</strong></p>
<p>Weakly supervised methods do not require additional annotations; models learn to automatically discover local regions, suitable for scenarios where annotation costs are high.</p>
<div style=white-space:nowrap;overflow-x:auto;font-size:1rem;line-height:0.8;justify-content:center;display:flex><table><thead><tr><th>Study<th>Method<th>Concept<th>Performance<tbody><tr><td>Two-Level Attention【126】<td>Extracts both global and local attention features<td>Learns discriminative regions without part annotations<td>Effective for fine-grained classification<tr><td>Bilinear CNN【141】<td>Dual-branch CNN collaborating to extract regional features<td>One branch detects regions, the other classifies features<td>84.1% accuracy on Caltech-UCSD birds dataset<tr><td>Attention Map【74】<td>Uses attention to guide model focus on details<td>Applied on CAPG-GP dataset<td>Significantly outperforms baseline<tr><td>Discriminative Patch + SVM【143】<td>Extracts key middle-layer patches from packaging for classification<td>Suitable for visually similar products<td>Good results on supermarket shelf classification<tr><td>Self-Attention Module【144】<td>Uses activation maps to identify key image locations<td>Improves cross-domain classification<td>Enhances model generalization</table></div>
</li>
</ul>
<p>Another approach is <strong>Context Awareness</strong>.</p>
<p>When product appearance alone is insufficient for effective classification, “contextual information” can serve as an important auxiliary clue, especially since product placement on shelves often follows regular patterns, implying semantic relationships among neighboring items.</p>
<div style=white-space:nowrap;overflow-x:auto;font-size:1rem;line-height:0.8;justify-content:center;display:flex><table><thead><tr><th>Study<th>Method<th>Content<th>Results<tbody><tr><td>CRF + CNN【53】<td>Combines CNN features with visual similarity of adjacent products<td>Incorporates neighboring context to learn product embeddings<td>91% accuracy, 87% recall<tr><td>SIFT + Context【64】<td>Traditional features combined with arrangement relations for hybrid classification<td>11.4% improvement over no-context methods<td>Practical but non-deep learning<tr><td>Graph-based Consistency Check【148】<td>Models product arrangement as subgraph isomorphism problem<td>Detects out-of-stock and misplaced products<td>Emphasizes spatial consistency reasoning</table></div>
<p>Overall, context awareness techniques are still in early exploratory stages with limited application cases. Future integration with Transformer architectures, spatial modeling, or graph neural networks may further improve classification performance and adaptability in real scenarios.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=few-shot-learning>Few-Shot Learning<a href=#few-shot-learning class=hash-link aria-label="Direct link to Few-Shot Learning" title="Direct link to Few-Shot Learning">​</a></h3>
<p>In practical applications, retail product varieties continuously change, with frequent new product launches and packaging updates. Retraining the entire model every time categories change incurs high time and labor costs.</p>
<p><strong>One-Shot Learning (Few-Shot Learning)</strong> is thus proposed as a solution, with the core goal:</p>
<blockquote>
<p><strong>To recognize new categories using only a very small number of samples (even a single image) without retraining the entire classifier.</strong></p>
</blockquote>
<p>This technique originates from distance metric learning【149】, where deep models map images into a feature space and classification is performed by nearest neighbor search based on the distance between a query sample and class centers.</p>
<p>As illustrated below:</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="metric learning" src=/en/assets/images/img14-68e0e3837efb303e28d84ac8f4a7ec9d.jpg width=1112 height=676 class=img_ev3q></figure></div>
<p>Input image X is classified to the category whose feature center (e.g., C1, C2, C3) is closest in feature space.</p>
<p>The key characteristics and application potentials of this method are:</p>
<ul>
<li><strong>Supports dynamic category expansion:</strong> New products can be added to the feature database without retraining the model;</li>
<li><strong>Greatly reduces training data requirements:</strong> Particularly suitable for long-tail categories and data-scarce scenarios;</li>
<li><strong>Can integrate with CNN feature extraction modules</strong> to maintain semantic embedding quality and classification stability.</li>
</ul>
<p>Representative applications in image classification and object detection include:</p>
<div style=white-space:nowrap;overflow-x:auto;font-size:1rem;line-height:0.8;justify-content:center;display:flex><table><thead><tr><th>Task<th>Method<th>Key Concept<th>Summary of Results<tbody><tr><td>Image Classification【152】<td>Combines CNN embeddings with color information in metric learning<td>Addresses embedding distortion due to lighting and color variance<td>Improves person re-identification performance<tr><td>Image Classification【150】<td>Matching Networks based on neural metric learning<td>Enables fast recognition of new classes on ImageNet<td>One-shot accuracy improved from 87.6% to 93.2%<tr><td>Object Detection【155】<td>Combined with R-CNN for animal detection<td>Few-shot animal recognition<td>Successfully applied in scenarios with very limited training samples<tr><td>Video Segmentation【154】<td>Requires only a single labeled frame to track specific objects<td>Fine-tunes CNN embeddings on target<td>Enhances one-shot recognition and cross-frame tracking</table></div>
<p>Several works also apply these methods to product recognition:</p>
<ol>
<li>
<p><strong>Geng et al.【74】</strong></p>
<ul>
<li>Propose a coarse-to-fine framework combining feature matching with a one-shot classifier, allowing new product categories to be added without retraining.</li>
<li>Evaluated on datasets GroZi-3.2k (mAP = 73.93%), GP-20 (65.55%), and GP181 (85.79%).</li>
</ul>
</li>
<li>
<p><strong>Tonioni et al.【20】</strong></p>
<ul>
<li>Use similarity matching strategy comparing query images to product samples via CNN features.</li>
<li>Classifies using just a single sample, seamlessly handling packaging changes and new category introduction.</li>
</ul>
</li>
</ol>
<p>One-shot learning offers product recognition systems a more flexible and scalable design paradigm. Current mainstream approaches focus on combining CNN features with metric learning classification; future directions may integrate few-shot classification, meta-learning, and cross-domain adaptation to better handle real-world variations.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=public-datasets>Public Datasets<a href=#public-datasets class=hash-link aria-label="Direct link to Public Datasets" title="Direct link to Public Datasets">​</a></h2>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=dataset src=/en/assets/images/img15-ce427782aa8efc15610e4001a3f2d247.jpg width=1778 height=646 class=img_ev3q></figure></div>
<p>The performance of deep models depends heavily on data quality and scale, but manual annotation of product images is often costly.</p>
<p>To facilitate method comparison and rapid prototyping, the research community has released several public datasets, which can be categorized by application scenarios into:</p>
<ul>
<li><strong>On-Shelf Images</strong>: Products statically arranged on shelves, simulating restocking, arrangement inspection, and shopping guidance scenarios.</li>
<li><strong>Checkout Images</strong>: Checkout viewpoints handling crowded occlusion, multiple mixed items, and product counting challenges.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=on-shelf-image-datasets>On-Shelf Image Datasets<a href=#on-shelf-image-datasets class=hash-link aria-label="Direct link to On-Shelf Image Datasets" title="Direct link to On-Shelf Image Datasets">​</a></h3>
<ul>
<li>
<p><strong>GroZi‑120</strong> is one of the earliest widely cited retail product recognition datasets, containing 120 product categories. The training set has 676 white-background single product images taken under ideal conditions, suitable for one-shot model design; the test set includes 4,973 real shelf images and 30 minutes of video clips with varied lighting and angles, specifically for evaluating domain adaptation capability.</p>
</li>
<li>
<p><strong>GroZi‑3.2k</strong> expands categories and samples, covering 80 major product categories with over 8,000 web-crawled training images; test images come from five physical stores, captured by smartphones, containing 680 images with manual annotations. This dataset is especially suitable for fine-grained classification tasks and domain shift evaluation.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=GroZi‑120 src=/en/assets/images/img16-a03a1357e781167a24e701deac4208c3.jpg width=932 height=428 class=img_ev3q></figure></div>
</li>
<li>
<p><strong>Freiburg Grocery</strong> collected by a German research team contains 25 daily product categories. The training set consists of about 5,000 smartphone images resized to 256×256; the test set includes 74 high-resolution images captured by Kinect v2, featuring occlusions and noise. This dataset effectively tests multi-scale robustness.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="Freiburg Grocery" src=/en/assets/images/img17-c1276f665f70b2e2e2713aee150854e4.jpg width=1128 height=424 class=img_ev3q></figure></div>
</li>
<li>
<p><strong>Cigarette Dataset</strong> focuses on cigarette recognition with 10 classes. The training set has 3,600 single product images; the test set consists of 354 shelf images from 40 retail stores, containing about 13,000 object annotations. The dataset’s products are visually very similar and densely arranged, making it suitable for testing fine-grained recognition under occlusion.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="Cigarette Dataset" src=/en/assets/images/img18-dfafbe22a87c9204ee84494861ef745f.jpg width=1200 height=480 class=img_ev3q></figure></div>
</li>
<li>
<p><strong>Grocery Store Dataset</strong> contains 81 product categories with 5,125 images from 18 stores. It uniquely provides both “iconic” images (from product web pages) and “natural” images (in-store photos), making it suitable for cross-domain learning and retrieval tasks.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="Grocery Store Dataset" src=/en/assets/images/img19-d5d2b0e1facc10b3bb6c253d508aa9eb.jpg width=1076 height=616 class=img_ev3q></figure></div>
</li>
<li>
<p><strong>GP181</strong> is a subset of GroZi‑3.2k with only 183 training and 73 test images, each precisely annotated with bounding boxes. Its small scale and high quality make it ideal for rapid prototyping, few-shot learning, and combined dataset experiments.</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=GP181 src=/en/assets/images/img20-a44b4c0efda9a522013070bf02a603c0.jpg width=1664 height=362 class=img_ev3q></figure></div>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=checkout-image-datasets>Checkout Image Datasets<a href=#checkout-image-datasets class=hash-link aria-label="Direct link to Checkout Image Datasets" title="Direct link to Checkout Image Datasets">​</a></h3>
<p>In self-checkout scenarios, image conditions are more challenging due to overlapping products, fixed viewpoints, and variable quantities.</p>
<p>Two representative datasets supporting related research are:</p>
<ul>
<li>
<p><strong>D2S (Dataset to Shop)</strong> is the first checkout dataset providing instance-level masks, containing 21,000 high-resolution images covering 60 common retail products (e.g., bottled drinks, cereals, fruits, vegetables). Training images are all single product shots; test images contain mixed product arrangements with 1 to 15 objects, including some synthetic images. The dataset emphasizes diversity in lighting, background, and angles, ideal for testing model generalization and fine segmentation accuracy. Original papers show even strong models like Mask R-CNN or RetinaNet suffer accuracy drops at IoU = 0.75, reflecting scenario complexity.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=D2S src=/en/assets/images/img21-e10d56942162b338e7a8046d4578686b.jpg width=1196 height=568 class=img_ev3q></figure></div>
</li>
<li>
<p><strong>RPC (Retail Product Checkout)</strong> is currently the largest and most practical checkout dataset, comprising 83,739 images covering 200 product categories, organized into 17 mid-level classes. Training data are multi-view single product images (captured by four cameras); testing involves top-down crowded checkout images. The dataset provides bounding boxes and category annotations and defines <strong>Checkout Accuracy (cAcc)</strong>: a sample is successful only if all products in a single image are correctly recognized and counted. Baseline results show original models (FPN) reach only 56.7% cAcc; with DPNet filtering reliable synthetic samples, cAcc improves to 80.5%. This highlights the critical role of data augmentation quality in checkout tasks.</p>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=RPC src=/en/assets/images/img22-53b67b30d16f9b0afc42972e1ceb3c46.jpg width=1224 height=548 class=img_ev3q></figure></div>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=practical-recommendations>Practical Recommendations<a href=#practical-recommendations class=hash-link aria-label="Direct link to Practical Recommendations" title="Direct link to Practical Recommendations">​</a></h3>
<ul>
<li>For <strong>product retrieval, fine-grained recognition, or domain adaptation</strong> tasks, GroZi‑3.2k, Grocery Store, or GP181 are recommended.</li>
<li>For <strong>detection and segmentation under occlusion</strong>, Cigarette and Freiburg Grocery datasets are preferred.</li>
<li>For research focused on <strong>checkout tasks and counting accuracy</strong>, RPC is the best current benchmark; for testing occlusion generalization and segmentation, D2S is ideal.</li>
<li>Most datasets provide both ideal background and in-the-wild images, suitable for testing data augmentation, style transfer, and cross-domain learning strategies.</li>
</ul>
<p>Future research will require larger, more diverse, and longitudinal benchmarks. Researchers may also consider developing shared annotation tools and semi-automatic labeling methods to reduce data acquisition and maintenance costs.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=future-research-directions>Future Research Directions<a href=#future-research-directions class=hash-link aria-label="Direct link to Future Research Directions" title="Direct link to Future Research Directions">​</a></h2>
<p>The following six research directions are identified by the authors as key avenues for advancing the field:</p>
<ol>
<li>
<p><strong>Generating Product Images Using Deep Neural Networks</strong>
The largest publicly available dataset currently covers only 200 product categories, far below the thousands of SKUs in real supermarkets. Frequent packaging updates make comprehensive image collection impractical. Generative models such as DCGAN and CycleGAN have demonstrated the ability to synthesize realistic images. Developing generators that can simulate shelf viewpoints, occlusion structures, and lighting variations would greatly enhance training data diversity and adaptability.</p>
</li>
<li>
<p><strong>Incorporating Graph Neural Networks for Shelf Arrangement Inspection</strong>
Product placement follows spatial structures and contextual patterns that traditional convolutional architectures struggle to capture due to their inability to model non-Euclidean relationships between objects. GNNs can model connections (adjacency, category similarity) between nodes (products) and have been used in recommendation systems and knowledge graph construction. In planogram tasks, GNNs can learn discrepancies between observed and ideal shelf graphs to aid out-of-stock and misplacement detection.</p>
</li>
<li>
<p><strong>Leveraging Transfer Learning for Cross-Store Recognition</strong>
Most existing object detection models assume consistent data distribution between training and testing. However, real stores differ greatly in lighting, background, and display styles, often requiring retraining. Transfer learning using pretrained models (e.g., ImageNet) can facilitate rapid adaptation to new environments. Strategies like unsupervised domain adaptation and few-shot fine-tuning can reduce data requirements.</p>
</li>
<li>
<p><strong>Multimodal Feature Learning Combining Packaging Text and Images</strong>
Fine-grained products often look visually similar, but packaging text (e.g., flavor, volume) provides key distinguishing information. Humans frequently rely on reading packaging text when shopping. Joint learning of image features and OCR-extracted textual information can compensate for pure visual limitations. Future work may explore multimodal pretraining using vision-language models such as BLIP and CLIP.</p>
</li>
<li>
<p><strong>Model Update Mechanisms Supporting Incremental Learning</strong>
Deep models suffer from catastrophic forgetting: learning new classes degrades recognition of existing ones. Incremental learning enables models to add new product classes without full retraining. Some studies propose dual-network architectures where an old network preserves historical knowledge, and a new network trains on new classes, combined via feature realignment or distillation. This direction is highly valuable for practical deployment.</p>
</li>
<li>
<p><strong>Improving Accuracy of Regression-Based Detection Methods</strong>
Regression-based detectors like YOLO and SSD offer real-time performance suitable for edge devices and self-checkout systems but lag behind two-stage methods (e.g., Faster R-CNN) in accuracy. Future research should focus on improving localization and classification performance without sacrificing inference speed, through approaches such as anchor-free architectures and lightweight attention mechanisms.</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This review focuses on recent deep learning developments in retail product recognition, systematically organizing and analyzing state-of-the-art strategies from task fundamentals and application challenges. It is structured around four main technical challenges:</p>
<ol>
<li><strong>Large-scale Classification</strong></li>
<li><strong>Data Limitation</strong></li>
<li><strong>Intraclass Variation</strong></li>
<li><strong>System Flexibility and Rapid Update Capability</strong></li>
</ol>
<p>In addition to summarizing leading methods, the paper introduces representative datasets and experimental benchmarks to help new researchers quickly understand the technical landscape, lower entry barriers, and focus on promising research breakthroughs.</p>
<p>Facing increasing product diversity and complex retail scenarios, the authors hope future researchers can deepen model design and system deployment strategies on this foundation, advancing intelligent retail perception technologies toward higher-level practical applications.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-06-30T14:30:24.000Z itemprop=dateModified>Jun 30, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/retail-product/dpsnet/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[20.11] DPSNet</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/category/segmentation-1><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Segmentation (1)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#retail-product-recognition class="table-of-contents__link toc-highlight">Retail Product Recognition</a><li><a href=#problem-definition class="table-of-contents__link toc-highlight">Problem Definition</a><li><a href=#traditional-methods class="table-of-contents__link toc-highlight">Traditional Methods</a><li><a href=#deep-learning-methods class="table-of-contents__link toc-highlight">Deep Learning Methods</a><li><a href=#challenges-in-product-recognition class="table-of-contents__link toc-highlight">Challenges in Product Recognition</a><ul><li><a href=#challenge-1-large-number-of-categories class="table-of-contents__link toc-highlight">Challenge 1: Large Number of Categories</a><li><a href=#challenge-2-domain-gap-in-data-distribution class="table-of-contents__link toc-highlight">Challenge 2: Domain Gap in Data Distribution</a><li><a href=#challenge-3-high-intraclass-variation class="table-of-contents__link toc-highlight">Challenge 3: High Intraclass Variation</a><li><a href=#challenge-4-insufficient-system-flexibility class="table-of-contents__link toc-highlight">Challenge 4: Insufficient System Flexibility</a></ul><li><a href=#technical-overview class="table-of-contents__link toc-highlight">Technical Overview</a><ul><li><a href=#cnn-based-approaches class="table-of-contents__link toc-highlight">CNN-Based Approaches</a><li><a href=#data-augmentation class="table-of-contents__link toc-highlight">Data Augmentation</a><li><a href=#fine-grained-classification class="table-of-contents__link toc-highlight">Fine-Grained Classification</a><li><a href=#few-shot-learning class="table-of-contents__link toc-highlight">Few-Shot Learning</a></ul><li><a href=#public-datasets class="table-of-contents__link toc-highlight">Public Datasets</a><ul><li><a href=#on-shelf-image-datasets class="table-of-contents__link toc-highlight">On-Shelf Image Datasets</a><li><a href=#checkout-image-datasets class="table-of-contents__link toc-highlight">Checkout Image Datasets</a><li><a href=#practical-recommendations class="table-of-contents__link toc-highlight">Practical Recommendations</a></ul><li><a href=#future-research-directions class="table-of-contents__link toc-highlight">Future Research Directions</a><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>