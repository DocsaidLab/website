<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-text-recognition/union14m/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>[23.07] Union14M | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/text-recognition/union14m/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[23.07] Union14M | DOCSAID"><meta data-rh=true name=description content="Chess Pieces Falling Like Stars"><meta data-rh=true property=og:description content="Chess Pieces Falling Like Stars"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/text-recognition/union14m/><link data-rh=true rel=alternate href=https://docsaid.org/papers/text-recognition/union14m/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/text-recognition/union14m/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/text-recognition/union14m/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/text-recognition/union14m/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin><link rel=stylesheet href=/en/assets/css/styles.cb52ca60.css><script src=/en/assets/js/main.1a703068.js defer></script><script src=/en/assets/js/runtime~main.94e41c1f.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/text-recognition/union14m/ rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/text-recognition/union14m/ rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/text-recognition/union14m/ rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><a href=https://github.com/DocsaidLab target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a><a href=https://buymeacoffee.com/docsaid target=_blank rel="noopener noreferrer" class="navbar__item navbar__link">Support Us<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Research Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-1>Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-7>Feature Fusion (7)</a><button aria-label="Expand sidebar category 'Feature Fusion (7)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-2>Mamba (2)</a><button aria-label="Expand sidebar category 'Mamba (2)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-23>Multimodality (23)</a><button aria-label="Expand sidebar category 'Multimodality (23)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="Expand sidebar category 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-7>Reparameterization (7)</a><button aria-label="Expand sidebar category 'Reparameterization (7)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Collapse sidebar category 'Text Recognition (20)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/crnn/>[15.07] CRNN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/rare/>[16.03] RARE</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/cafcn/>[18.09] CA-FCN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/sar/>[18.11] SAR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/wwwstr/>[19.04] WWWSTR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/satrn/>[19.10] SATRN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/dan/>[19.12] DAN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/seed/>[20.05] SEED</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/abinet/>[21.03] ABINet</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/vitstr/>[21.05] ViTSTR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/yatr/>[21.07] YATR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/trocr/>[21.09] TrOCR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/safl/>[22.01] SAFL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/siga/>[22.03] SIGA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/parseq/>[22.07] PARSeq</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/clip4str/>[23.05] CLIP4STR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/diffusionstr/>[23.06] DiffusionSTR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/text-recognition/union14m/>[23.07] Union14M</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/dtrocr/>[23.08] DTrOCR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/ocr-scaling-law/>[24.01] OCR Scaling Law</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="Expand sidebar category 'Vision Transformers (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 150 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/en/papers/category/text-recognition-20><span itemprop=name>Text Recognition (20)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[23.07] Union14M</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[23.07] Union14M</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt=Zephyr class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Zephyr</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chess-pieces-falling-like-stars>Chess Pieces Falling Like Stars<a href=#chess-pieces-falling-like-stars class=hash-link aria-label="Direct link to Chess Pieces Falling Like Stars" title="Direct link to Chess Pieces Falling Like Stars">​</a></h2>
<p><a href=https://arxiv.org/abs/2307.08723 target=_blank rel="noopener noreferrer"><strong>Revisiting Scene Text Recognition: A Data Perspective</strong></a></p>
<hr>
<p>We’ve explored dozens of papers on scene text recognition (STR), and the validation datasets in use tend to revolve around the same few: ICDAR, IIIT5K, SVT, SVTP, and CUTE80.</p>
<p>Truth be told, performance on these datasets is already near saturation.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=defining-the-problem>Defining the Problem<a href=#defining-the-problem class=hash-link aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem">​</a></h2>
<p><img decoding=async loading=lazy alt="model performance" src=/en/assets/images/img1-940308a62a9159aa05c7e3b82a2649f1.jpg width=1224 height=572 class=img_ev3q></p>
<p>As shown above, this graph represents the average performance across six widely-used STR datasets.</p>
<p>Starting with familiar models like CRNN, advancing through SAR, SATRN, and ABINet, the field achieved more than <strong>92% accuracy by 2022.</strong> However, progress has since plateaued, leaving little room for further improvements.</p>
<p>Does this imply that STR is a <strong>“solved” problem</strong>? Or is it possible that the validation datasets we rely on are too limited, preventing us from exposing deeper challenges within the field?</p>
<p>If existing benchmarks cannot help us move forward, perhaps it’s time to overturn the chessboard—upending the status quo to create new possibilities. And with the pieces now falling freely, it’s time to reassemble them into something entirely new.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-solution>Problem Solution<a href=#problem-solution class=hash-link aria-label="Direct link to Problem Solution" title="Direct link to Problem Solution">​</a></h2>
<p>Since current datasets no longer capture the full scope of real-world challenges in STR, the authors propose <strong>Union14M</strong>, a large-scale, unified dataset.</p>
<p>Union14M brings together <strong>4 million labeled images (Union14M-L)</strong> and <strong>10 million unlabeled images (Union14M-U)</strong>, consolidated from <strong>17 publicly available datasets.</strong> The structure is shown below:</p>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=union14m src=/en/assets/images/img2-4a09d592cb8313973fec2572c2da5e3f.jpg width=1116 height=782 class=img_ev3q></figure></div>
<p>By aggregating existing datasets and categorizing diverse challenges, Union14M aims to better represent the variability of real-world text, offering new insights into the limitations of current STR models and pushing the field forward.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=union14m-l>Union14M-L<a href=#union14m-l class=hash-link aria-label="Direct link to Union14M-L" title="Direct link to Union14M-L">​</a></h3>
<p>Union14M-L consists of 4 million labeled images gathered from <strong>14 public datasets</strong> with the goal of representing diverse real-world scenarios. Some of these datasets include:</p>
<ul>
<li><strong>ArT</strong>: Focuses on curved text images.</li>
<li><strong>ReCTS, RCTW, LSVT, KAIST, NEOCR, and IIIT-ILST</strong>: Provide street-view text from various countries.</li>
<li><strong>MTWI</strong>: Extracts text images from web pages.</li>
<li><strong>COCOTextV2</strong>: Includes low-resolution and vertical text images.</li>
<li><strong>IntelOCR, TextOCR, and HierText</strong>: Collected from OpenImages, covering around 9 million images of various real-world scenarios.</li>
</ul>
<p>Simply merging these datasets, however, would not suffice. Challenges such as inconsistent annotations, duplicate samples, non-Latin characters, or corrupted images must be addressed. The authors applied several optimization strategies to ensure high-quality consolidation:</p>
<ul>
<li>
<p><strong>Cropping Text Instances</strong>: Instead of using polygon annotations, the authors applied <strong>minimum rotated rectangles</strong> to crop text regions. This method introduces more background noise, making the model more robust against distractions and reducing dependency on precise detectors. This setup also helps analyze the pure recognition performance of models.</p>
</li>
<li>
<p><strong>Removing Duplicates</strong>: To eliminate redundancy, the authors first filtered out overlaps between Union14M-L and other common benchmarks. They also removed duplicate samples within the 14 datasets. For example, <strong>HierText, TextOCR, and IntelOCR</strong> overlap, as they all derive from OpenImages. The authors kept HierText as the main reference, removing the redundant samples from the other two datasets.</p>
</li>
<li>
<p><strong>Excluding Non-Latin Characters and Ignored Samples</strong>: To focus on Latin-based text, only samples containing letters, numbers, or symbols were kept, while images marked as "ignored" were excluded.</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=union14m-u>Union14M-U<a href=#union14m-u class=hash-link aria-label="Direct link to Union14M-U" title="Direct link to Union14M-U">​</a></h3>
<p>Union14M-U comprises <strong>10 million unlabeled images</strong>, leveraging self-supervised learning—a growing trend in computer vision and STR. Since manually labeling text images is both time-consuming and requires linguistic expertise, the authors explored how unlabeled data can enhance STR models.</p>
<p>They sourced these unlabeled images from <strong>Book32, OpenImages, and Conceptual Captions (CC)</strong>. To select high-quality text instances, the authors employed <strong>three different text detectors</strong> and adopted an <strong>IoU voting mechanism</strong> to ensure consistency. Additionally, they removed duplicate samples from OpenImages to avoid redundancy with the labeled portion of Union14M.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=diverse-text-styles>Diverse Text Styles<a href=#diverse-text-styles class=hash-link aria-label="Direct link to Diverse Text Styles" title="Direct link to Diverse Text Styles">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="text style" src=/en/assets/images/img3-b5b4f8356e86599c2bbab2f148773f86.jpg width=1080 height=860 class=img_ev3q></figure></div>
<p>As depicted in the above image, Union14M includes a wide variety of text styles. These styles range from <strong>curved, slanted, and vertical text</strong> to challenging scenarios with <strong>blur, complex backgrounds, and occlusion.</strong> The dataset also captures text from diverse real-world environments, such as <strong>street signs and brand logos.</strong></p>
<p>One important aspect is the large presence of <strong>vertical text</strong>, which is quite common in real-world scenarios but often underrepresented in synthetic datasets.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=rich-vocabulary>Rich Vocabulary<a href=#rich-vocabulary class=hash-link aria-label="Direct link to Rich Vocabulary" title="Direct link to Rich Vocabulary">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=vocabulary src=/en/assets/images/img4-ddcf7ba385f82e48e003d75c6603360c.jpg width=1104 height=234 class=img_ev3q></figure></div>
<p>The vocabulary used in synthetic datasets is often derived from common corpora, but real-world text can vary significantly, including combinations not typically found in standard datasets—such as <strong>license plate numbers</strong> or <strong>mixed Chinese-English pinyin.</strong> As shown above, the vocabulary size of Union14M-L is nearly double that of synthetic datasets, providing a richer and more representative collection of real-world scenarios.</p>
<p>This expanded vocabulary enhances the depth and breadth of analysis available for STR models.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-architecture>Model Architecture<a href=#model-architecture class=hash-link aria-label="Direct link to Model Architecture" title="Direct link to Model Architecture">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="model architecture" src=/en/assets/images/img8-042c9bcd41a558cb0a876ccaea4fbb33.jpg width=1152 height=500 class=img_ev3q></figure></div>
<p>After constructing the Union14M dataset, the authors proposed a <strong>self-supervised learning-based solution: MAERec</strong>.</p>
<p>This model takes advantage of <strong>self-supervised pretraining</strong> to leverage the <strong>10 million unlabeled images</strong> from Union14M-U.</p>
<p>The core of MAERec is based on <strong>Vision Transformers (ViT)</strong>, which excel at <strong>masked image modeling</strong>.</p>
<ul>
<li><a href=/en/papers/vision-transformers/mae/><strong>[21.11] MAE: A Quarter of the Clue</strong></a></li>
</ul>
<p>The input image is divided into <strong>4 × 4 image patches</strong> and passed through the <strong>ViT backbone network</strong>. The output sequence is then processed by an <strong>autoregressive decoder</strong> (using the <strong>Transformer decoder</strong> from SATRN) to generate the final text prediction.</p>
<ul>
<li><a href=/en/papers/text-recognition/satrn/><strong>[19.10] SATRN: Transformer Reaches the Battlefield</strong></a></li>
</ul>
<p>To ensure fair comparisons, the <strong>number of character classes</strong> is standardized to <strong>91 classes</strong> (including numbers, uppercase and lowercase letters, symbols, and spaces), while other hyperparameters remain consistent with the original model configurations.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=pretraining-and-fine-tuning>Pretraining and Fine-Tuning<a href=#pretraining-and-fine-tuning class=hash-link aria-label="Direct link to Pretraining and Fine-Tuning" title="Direct link to Pretraining and Fine-Tuning">​</a></h3>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt="model pretrained" src=/en/assets/images/img9-192761ec704b18ad056b7340b3c9fd29.jpg width=1614 height=528 class=img_ev3q></figure></div>
<p>To fully utilize the large number of unlabeled images from Union14M-U, the authors adopted the <strong>MAE framework</strong> for pretraining, with some modifications.</p>
<p>As shown above, <strong>even with a 75% masking ratio</strong>, the <strong>ViT backbone</strong> can still generate high-quality reconstructed text images. This demonstrates that <strong>MAERec</strong> effectively learns both the structural and semantic representations of text, capturing useful features even from highly incomplete images.</p>
<p>After pretraining, the pretrained ViT weights are used to <strong>initialize MAERec</strong>, followed by <strong>fine-tuning on Union14M-L</strong>.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=the-harsh-reality>The Harsh Reality<a href=#the-harsh-reality class=hash-link aria-label="Direct link to The Harsh Reality" title="Direct link to The Harsh Reality">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt="model performance" src=/en/assets/images/img5-1381eb388f69abb912c66afa5fb8f478.jpg width=1104 height=710 class=img_ev3q></figure></div>
<p>The authors evaluated <strong>13 representative STR models</strong>, all trained on synthetic datasets, and tested them on Union14M-L. As shown in the table above, these models <strong>suffered a significant performance drop</strong> on Union14M-L, with the average accuracy falling by:</p>
<ul>
<li><strong>20.50%!</strong></li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>Who still dares to claim that STR is a solved problem?</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=uncovering-the-challenges>Uncovering the Challenges<a href=#uncovering-the-challenges class=hash-link aria-label="Direct link to Uncovering the Challenges" title="Direct link to Uncovering the Challenges">​</a></h3>
<p><img decoding=async loading=lazy alt=challenges src=/en/assets/images/img6-a21225140133b9a0856ba68f2e8c1a4c.jpg width=2232 height=1024 class=img_ev3q></p>
<p>To identify the types of errors common to these 13 models, the authors assigned a <strong>difficulty score</strong> to each sample in Union14M-L, focusing on those that most models failed to recognize correctly. From these, they identified <strong>four unresolved challenges</strong> and <strong>three additional real-world scenarios</strong> that have been underexplored in previous research.</p>
<ul>
<li>
<p><strong>Curve Text</strong></p>
<p>As shown in image (a), recognizing <strong>curved text</strong> has been a focus in recent years, with two main approaches:</p>
<ol>
<li><strong>Correction-based models</strong></li>
<li><strong>Models leveraging 2D attention mechanisms</strong></li>
</ol>
<p>While these methods have performed well on datasets like <strong>CUTE</strong>, the proportion of curved text in these datasets is small, and the curvature tends to be moderate. When faced with highly curved text, existing models still struggle.</p>
<hr>
</li>
<li>
<p><strong>Multi-Oriented Text</strong></p>
<p>In image (b), text appears in <strong>vertical, slanted, or mirrored orientations</strong> on various surfaces, such as vertical text on signboards or slanted text due to camera angles. Most STR models assume near-horizontal text alignment, neglecting multi-oriented text challenges.</p>
<p>These models often resize images to a fixed height (e.g., 32 pixels) while maintaining the aspect ratio. However, such resizing can <strong>compress vertical or slanted text</strong>, making it harder to recognize.</p>
<hr>
</li>
<li>
<p><strong>Artistic Text</strong></p>
<p>Image (e) showcases <strong>artistic text</strong>, which is crafted with unique fonts, effects, and layouts, often embedded within complex backgrounds. Each instance of artistic text can be unique, making it a <strong>zero-shot or few-shot problem</strong>. Models require specialized networks to handle this. Due to the lack of artistic text samples in synthetic datasets, current models struggle to maintain robustness when encountering these texts in real-world scenarios.</p>
<hr>
</li>
<li>
<p><strong>Contextless Text</strong></p>
<p>As shown in image (f), <strong>contextless text</strong> refers to text that carries no inherent meaning or is not found in dictionaries, such as abbreviations or random combinations of letters, numbers, and symbols. Even with clear backgrounds and minimal distortion, models may misinterpret these texts due to over-reliance on semantic knowledge from training corpora.</p>
<p>For example, a model might mistakenly recognize “YQJ” as “you,” which could be dangerous in applications like license plate recognition, invoice processing, or ID verification, where <strong>misrecognition can lead to security risks or financial losses</strong>.</p>
<hr>
</li>
<li>
<p><strong>Salient Text</strong></p>
<p>Image (c) depicts <strong>salient text</strong>, where irrelevant characters coexist with the primary text. When multiple texts of different sizes are adjacent or overlapping, these distractors may be included in the recognition, reducing model accuracy.</p>
<p>In the detection phase, <strong>ROI masking strategies</strong> (as proposed by Liao et al.) can help remove such distracting characters. However, when detection models fail to locate text regions accurately, it becomes crucial for recognition models to <strong>quickly identify key regions</strong> for robust performance.</p>
<hr>
</li>
<li>
<p><strong>Multi-Words Text</strong></p>
<p>As seen in image (d), <strong>multi-word text</strong> appears in contexts like <strong>logos or phrases</strong>, where a single word is insufficient to convey complete meaning. Most STR models are trained on synthetic datasets with individual word-level annotations, making them prone to <strong>ignoring spaces between words</strong>.</p>
<p>The authors observed that models often concatenate multiple words into one or modify some visible characters to match grammatical rules. For instance, <strong>"Live to Evolve"</strong> might be incorrectly recognized as <strong>"liveroee"</strong>, as the model interprets it as a single word.</p>
<hr>
</li>
<li>
<p><strong>Incomplete Text</strong></p>
<p>As shown in image (g), text can become incomplete due to <strong>occlusion or inaccurate detection bounding boxes</strong>. When the beginning or end of a word is cropped, models might automatically fill in the missing parts to produce a complete prediction, even though the missing characters are not visible.</p>
<p>This behavior is especially prominent in <strong>systems relying on language models</strong>, where predictions are heavily influenced by prior linguistic knowledge. However, this <strong>auto-completion feature</strong> can reduce reliability in certain applications. For example, if an image only shows “ight,” the model might complete it as <strong>"might"</strong> or <strong>"light"</strong>, but the ideal output would be just <strong>"ight,"</strong> leaving the decision to the downstream system for further anomaly detection.</p>
<p>Therefore, it is essential to thoroughly evaluate the auto-completion feature and carefully assess its potential impact on downstream applications.</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=the-benchmark-dataset>The Benchmark Dataset<a href=#the-benchmark-dataset class=hash-link aria-label="Direct link to The Benchmark Dataset" title="Direct link to The Benchmark Dataset">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=union14m src=/en/assets/images/img7-a9a400b4ebca55f5512661a1d4ed5e51.jpg width=856 height=548 class=img_ev3q></figure></div>
<p>To thoroughly evaluate the performance of STR models in real-world scenarios and facilitate further research on the <strong>seven key challenges</strong>, the authors developed a <strong>challenge-oriented benchmark dataset</strong>:</p>
<ul>
<li><strong>Union14M-Benchmark</strong></li>
</ul>
<p>This benchmark is divided into <strong>eight subsets</strong>, containing a total of <strong>409,393 images</strong>, representing a wide range of textual complexities and diversities.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=experimental-results>Experimental Results<a href=#experimental-results class=hash-link aria-label="Direct link to Experimental Results" title="Direct link to Experimental Results">​</a></h3>
<p>To comprehensively compare the models, the authors first reported results using models trained on <strong>synthetic datasets (MJ + ST)</strong>:</p>
<p><img decoding=async loading=lazy alt="synthetic data" src=/en/assets/images/img11-9b5086cf6a9ac4b1151f52a0401a85ea.jpg width=2876 height=890 class=img_ev3q></p>
<p>Next, they re-trained all models using <strong>Union14M</strong>:</p>
<p><img decoding=async loading=lazy alt="union14m data" src=/en/assets/images/img10-108322397cf6a73c91d0efc257935beb.jpg width=2962 height=1138 class=img_ev3q></p>
<hr>
<p>Compared to traditional benchmark datasets, models trained on synthetic data showed a significant 48.5% drop in average accuracy on the Union14M-Benchmark. However, when models were trained on Union14M-L, the accuracy drop was mitigated to 33.0%, indicating that real-world text images are far more complex than those in the six common benchmarks.</p>
<p>Moreover, models trained on Union14M-L achieved accuracy improvements of 3.9% on standard benchmarks and 19.6% on the Union14M-Benchmark. This demonstrates the inadequacy of synthetic data for addressing real-world complexities and highlights the generalization benefits of training with real-world datasets.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>The relatively smaller improvement on traditional benchmarks suggests that these benchmarks have reached a saturation point.</div></div>
<p>When models were trained only on Union14M-L, the highest average accuracy achieved on the Union14M-Benchmark was 74.6%, reinforcing that STR remains an unsolved problem. While large-scale real-world data provides some performance gains, more research is needed to overcome existing challenges.</p>
<p>Performance on the incomplete text subset showed a significant decline for all models trained with synthetic data. The drop was particularly severe for language models:</p>
<ul>
<li><strong>Language models</strong>: 10.2% drop</li>
<li><strong>CTC models</strong>: 5.6% drop</li>
<li><strong>Attention models</strong>: 5.9% drop</li>
</ul>
<p>The authors attribute this drop to the <strong>error-correction behavior</strong> of language models, where the models <strong>attempt to auto-complete text</strong> perceived as missing characters. This issue was somewhat alleviated when models were trained on Union14M-L, possibly due to the larger vocabulary, which reduced overfitting to training corpora. However, this auto-completion problem persists and requires further investigation.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This paper presents a wealth of insights, addressing both <strong>current model limitations</strong> and <strong>dataset challenges</strong>. The creation of Union14M was a significant undertaking, providing a more comprehensive real-world text recognition dataset that allows for more thorough model evaluation.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>We highly recommend reading the original paper and visiting their GitHub repository.<p>Project link: <a href=https://github.com/Mountchicken/Union14M target=_blank rel="noopener noreferrer"><strong>Union14M GitHub</strong></a></div></div></header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2024-12-10T14:04:39.000Z itemprop=dateModified>Dec 10, 2024</time></b> by <b>zephyr-sh</b></span></div></div><div style=margin-top:3rem> </div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/text-recognition/diffusionstr/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[23.06] DiffusionSTR</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/text-recognition/dtrocr/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[23.08] DTrOCR</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#chess-pieces-falling-like-stars class="table-of-contents__link toc-highlight">Chess Pieces Falling Like Stars</a><li><a href=#defining-the-problem class="table-of-contents__link toc-highlight">Defining the Problem</a><li><a href=#problem-solution class="table-of-contents__link toc-highlight">Problem Solution</a><ul><li><a href=#union14m-l class="table-of-contents__link toc-highlight">Union14M-L</a><li><a href=#union14m-u class="table-of-contents__link toc-highlight">Union14M-U</a><li><a href=#diverse-text-styles class="table-of-contents__link toc-highlight">Diverse Text Styles</a><li><a href=#rich-vocabulary class="table-of-contents__link toc-highlight">Rich Vocabulary</a><li><a href=#model-architecture class="table-of-contents__link toc-highlight">Model Architecture</a><li><a href=#pretraining-and-fine-tuning class="table-of-contents__link toc-highlight">Pretraining and Fine-Tuning</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#the-harsh-reality class="table-of-contents__link toc-highlight">The Harsh Reality</a><li><a href=#uncovering-the-challenges class="table-of-contents__link toc-highlight">Uncovering the Challenges</a><li><a href=#the-benchmark-dataset class="table-of-contents__link toc-highlight">The Benchmark Dataset</a><li><a href=#experimental-results class="table-of-contents__link toc-highlight">Experimental Results</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>成為作者</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a><span class=footer__link-separator>·</span><a href=https://buymeacoffee.com/docsaid target=_blank rel="noopener noreferrer" class=footer__link-item>Support Us<svg width=13.5 height=13.5 aria-hidden=true viewBox="0 0 24 24" class=iconExternalLink_nPIU><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></svg></a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>