<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-text-recognition/wwwstr/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">[19.04] WWWSTR | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/text-recognition/wwwstr/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[19.04] WWWSTR | DOCSAID"><meta data-rh="true" name="description" content="Data and Model Analysis"><meta data-rh="true" property="og:description" content="Data and Model Analysis"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/text-recognition/wwwstr/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/text-recognition/wwwstr/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/text-recognition/wwwstr/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/text-recognition/wwwstr/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.51ef4fe1.css">
<script src="/en/assets/js/runtime~main.4de21c6b.js" defer="defer"></script>
<script src="/en/assets/js/main.8a25b67d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a><a class="navbar__item navbar__link" href="/en/playground/intro">Playground</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/text-recognition/wwwstr/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/text-recognition/wwwstr/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="Expand sidebar category &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="Expand sidebar category &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="Expand sidebar category &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="Expand sidebar category &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/llm-tuning-5">LLM Tuning (5)</a><button aria-label="Expand sidebar category &#x27;LLM Tuning (5)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/multimodality-20">Multimodality (20)</a><button aria-label="Expand sidebar category &#x27;Multimodality (20)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization-1">Normalization (1)</a><button aria-label="Expand sidebar category &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection-8">Object Detection (8)</a><button aria-label="Expand sidebar category &#x27;Object Detection (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="Expand sidebar category &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="Expand sidebar category &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="Expand sidebar category &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/text-recognition-16">Text Recognition (16)</a><button aria-label="Collapse sidebar category &#x27;Text Recognition (16)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/crnn/">[15.07] CRNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/rare/">[16.03] RARE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/cafcn/">[18.09] CA-FCN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/sar/">[18.11] SAR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/text-recognition/wwwstr/">[19.04] WWWSTR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/satrn/">[19.10] SATRN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/dan/">[19.12] DAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/seed/">[20.05] SEED</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/abinet/">[21.03] ABINet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/vitstr/">[21.05] ViTSTR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/yatr/">[21.07] YATR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/safl/">[22.01] SAFL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/siga/">[22.03] SIGA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/parseq/">[22.07] PARSeq</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/diffusionstr/">[23.06] DiffusionSTR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/text-recognition/union14m/">[23.07] Union14M</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-spotting-4">Text Spotting (4)</a><button aria-label="Expand sidebar category &#x27;Text Spotting (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/transformers-15">Transformers (15)</a><button aria-label="Expand sidebar category &#x27;Transformers (15)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="Expand sidebar category &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">All Notes: 131 entries</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/text-recognition-16"><span itemprop="name">Text Recognition (16)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[19.04] WWWSTR</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[19.04] WWWSTR</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-and-model-analysis">Data and Model Analysis<a href="#data-and-model-analysis" class="hash-link" aria-label="Direct link to Data and Model Analysis" title="Direct link to Data and Model Analysis">​</a></h2>
<p><a href="https://arxiv.org/abs/1904.01906" target="_blank" rel="noopener noreferrer"><strong>What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</strong></a></p>
<hr>
<p>This paper, published by Clova AI, does not propose new techniques but instead presents a comprehensive analysis of existing algorithms and datasets, as well as an exploration of the best combinations of different modules to find the optimal solution.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dataset-analysis">Dataset Analysis<a href="#dataset-analysis" class="hash-link" aria-label="Direct link to Dataset Analysis" title="Direct link to Dataset Analysis">​</a></h2>
<p>Different studies have used various dataset combinations for training, which may result in unclear performance improvements. It becomes difficult to determine if the improvement is due to the proposed new model or the usage of a better or larger dataset.</p>
<p>This inconsistency in datasets affects the fairness of results.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="training-datasets">Training Datasets<a href="#training-datasets" class="hash-link" aria-label="Direct link to Training Datasets" title="Direct link to Training Datasets">​</a></h3>
<p>Due to the high cost of annotating scene text images, it&#x27;s challenging to acquire sufficient labeled data, and most STR (Scene Text Recognition) models opt for synthetic datasets for training.</p>
<p>Below are two widely used synthetic datasets in STR research:</p>
<ul>
<li>
<p><strong>MJSynth (MJ)</strong></p>
<ul>
<li>
<p><a href="https://www.robots.ox.ac.uk/~vgg/data/text/" target="_blank" rel="noopener noreferrer"><strong>Synthetic Word Dataset</strong></a></p>
<div align="left"><figure style="width:60%"><p><img decoding="async" loading="lazy" alt="MJSynth" src="/en/assets/images/img2-0f35900d2381dfb775886a6c83dafd2c.jpg" width="628" height="496" class="img_ev3q"></p></figure></div>
<p>MJSynth is a synthetic dataset specifically designed for STR, containing 8.9 million word-box images. The generation process involves font rendering, border and shadow rendering, background coloring, merging fonts and backgrounds, applying perspective distortions, mixing with real images, and adding noise.</p>
</li>
</ul>
</li>
<li>
<p><strong>SynthText (ST)</strong></p>
<ul>
<li>
<p><a href="https://www.robots.ox.ac.uk/~vgg/data/scenetext/" target="_blank" rel="noopener noreferrer"><strong>SynthText in the Wild Dataset</strong></a></p>
<div align="left"><figure style="width:60%"><p><img decoding="async" loading="lazy" alt="SynthText（ST）" src="/en/assets/images/img3-7752dfa9ee5593b40ad9ab6a9ef3f2f2.jpg" width="592" height="508" class="img_ev3q"></p></figure></div>
<p>SynthText was originally created for scene text detection but has also been used in STR research. Researchers cropped word boxes from this dataset for STR training. After cropping, the SynthText dataset contains about 5.5 million training samples.</p>
</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="testing-datasets">Testing Datasets<a href="#testing-datasets" class="hash-link" aria-label="Direct link to Testing Datasets" title="Direct link to Testing Datasets">​</a></h3>
<p>Different versions of benchmark datasets have certain differences.</p>
<p>For example, in IC03, a difference of 7 samples can lead to a 0.8% performance gap, which is a significant difference when comparing prior research results.</p>
<p>The sample number differences in IC13 and IC15 are even larger than those in IC03.</p>
<p>Seven real-scene STR datasets are widely used for post-training model evaluation. These datasets can be categorized into &quot;regular datasets&quot; and &quot;irregular datasets&quot; based on the difficulty of the text and its geometric layout:</p>
<ul>
<li>
<p><strong>Regular Datasets</strong>: These contain text images where characters are evenly spaced and arranged horizontally, making them relatively easier to process:</p>
<div align="center"><figure style="width:60%"><p><img decoding="async" loading="lazy" alt="regular" src="/en/assets/images/img4-8e9b6e395d7cf54cb195821c5bc05745.jpg" width="652" height="344" class="img_ev3q"></p></figure></div>
<ul>
<li><strong>IIIT5K-Words (IIIT)</strong>: A dataset from Google image search containing 2,000 training images and 3,000 evaluation images.</li>
<li><strong>Street View Text (SVT)</strong>: Collected from Google Street View, this dataset contains outdoor street scenes, with some images affected by noise, blur, or low resolution. It includes 257 training images and 647 evaluation images.</li>
<li><strong>ICDAR2003 (IC03)</strong>: Used in the ICDAR 2003 Robust Reading competition, it contains 1,156 training images and 1,110 evaluation images. There are two versions (860 and 867 images) due to the exclusion of words that are too short or contain non-alphanumeric characters.</li>
<li><strong>ICDAR2013 (IC13)</strong>: This dataset inherits most of the images from IC03 and was created for the ICDAR 2013 Robust Reading competition. It contains 848 training images and 1,095 evaluation images. Two versions are available for evaluation, one with 857 images and the other with 1,015.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>Irregular Datasets</strong>: These contain more challenging scene text, such as curved, rotated, or distorted text:</p>
<div align="center"><figure style="width:60%"><p><img decoding="async" loading="lazy" alt="irregular" src="/en/assets/images/img5-f5bb08005f432285a4c4cabbb0e0d444.jpg" width="496" height="344" class="img_ev3q"></p></figure></div>
<ul>
<li><strong>ICDAR2015 (IC15)</strong>: This dataset, used in the ICDAR 2015 Robust Reading competition, contains 4,468 training images and 2,077 evaluation images. The images are captured from a natural movement perspective using Google Glass, resulting in many images with noise, blur, and rotation. Evaluation versions include 1,811 and 2,077 images.</li>
<li><strong>SVT Perspective (SP)</strong>: Collected from Google Street View, it includes 645 evaluation images, many with perspective distortions due to non-frontal views.</li>
<li><strong>CUTE80 (CT)</strong>: A dataset collected from natural scenes, containing 288 cropped evaluation images, many of which feature curved text.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="version-differences">Version Differences<a href="#version-differences" class="hash-link" aria-label="Direct link to Version Differences" title="Direct link to Version Differences">​</a></h3>
<p><img decoding="async" loading="lazy" alt="different-versions" src="/en/assets/images/img1-b6fee3f37b9d379ef249184dc7ac9da9.jpg" width="1468" height="746" class="img_ev3q"></p>
<p>Based on the table above, different studies have used different versions of benchmark datasets for model evaluation, particularly in the IC03, IC13, and IC15 datasets.</p>
<p>For example, in IC03, a difference of 7 samples can lead to a 0.8% performance gap, which is significant when comparing results. The differences in sample sizes between IC13 and IC15 are even larger. These dataset version differences can lead to significant errors when evaluating model performance, which must be carefully considered when comparing different models.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Version errors can lead to major issues—be cautious!</p></div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="architecture-analysis">Architecture Analysis<a href="#architecture-analysis" class="hash-link" aria-label="Direct link to Architecture Analysis" title="Direct link to Architecture Analysis">​</a></h2>
<p><img decoding="async" loading="lazy" alt="architecture" src="/en/assets/images/img6-153ec9b98a6d0dad378fdb020d7d330a.jpg" width="1980" height="220" class="img_ev3q"></p>
<p>STR (Scene Text Recognition) is similar to &quot;object detection&quot; tasks and &quot;sequence prediction&quot; tasks, benefiting from both Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).</p>
<p>The earliest model to combine CNN and RNN for STR was CRNN, which used CNNs for feature extraction and RNNs for sequence prediction. Many variants have since been proposed to improve performance, introducing different modules to handle complex features like font styles and backgrounds.</p>
<p>Some methods even omitted the RNN stage to reduce inference time. Subsequent research has introduced attention-based decoders to improve character sequence prediction.</p>
<p>The STR architecture typically follows these four stages:</p>
<ol>
<li><strong>Transformation</strong>: Spatial transformation networks (STN) are used to normalize input text images, reducing the burden on subsequent stages.</li>
<li><strong>Feature Extraction</strong>: This stage converts input images into representations focused on character recognition while suppressing features irrelevant to fonts, colors, sizes, and backgrounds.</li>
<li><strong>Sequence Modeling</strong>: This stage captures contextual information in the character sequence, improving the accuracy of each character&#x27;s prediction.</li>
<li><strong>Prediction</strong>: The final character sequence is predicted based on the extracted features.</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="transformation-stage">Transformation Stage<a href="#transformation-stage" class="hash-link" aria-label="Direct link to Transformation Stage" title="Direct link to Transformation Stage">​</a></h3>
<p>This stage transforms input images X into normalized images X&#x27;.</p>
<p>Since text images in natural scenes vary in shape (such as curved or slanted text), if passed directly to later stages, the feature extraction stage would need to learn representations invariant to these geometric distortions.</p>
<p>To alleviate this burden, some research has employed Thin-Plate Spline (TPS) transformation, a variant of spatial transformation networks (STN). TPS normalizes text regions by performing smooth spline interpolation between a set of control points.</p>
<p>TPS is flexible in handling varying scales of text images and standardizes text regions into predetermined rectangles.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="feature-extraction-stage">Feature Extraction Stage<a href="#feature-extraction-stage" class="hash-link" aria-label="Direct link to Feature Extraction Stage" title="Direct link to Feature Extraction Stage">​</a></h3>
<p>In this stage, CNNs abstract the input image (X or X&#x27;) into visual feature maps.</p>
<p>Each column of the feature map corresponds to a distinguishable receptive field in the horizontal direction of the input image, and these features are used to predict the character within each receptive field.</p>
<p>Three common architectures are studied: VGG, RCNN, and ResNet:</p>
<ul>
<li><strong>VGG</strong>: A simple architecture composed of multiple convolutional layers and a few fully connected layers.</li>
<li><strong>RCNN</strong>: A variant of CNN that recursively adjusts the receptive fields, adapting to character shapes.</li>
<li><strong>ResNet</strong>: Incorporates residual connections to alleviate the difficulties of training deep CNNs.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sequence-modeling-stage">Sequence Modeling Stage<a href="#sequence-modeling-stage" class="hash-link" aria-label="Direct link to Sequence Modeling Stage" title="Direct link to Sequence Modeling Stage">​</a></h3>
<p>The output from the feature extraction stage is reshaped into a feature sequence V, where each column <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">v_i \in V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span> is treated as one frame in the sequence.</p>
<p>However, this sequence may lack contextual information, so previous studies have used Bidirectional Long Short-Term Memory (BiLSTM) networks to enhance the sequence <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mi>S</mi><mi>e</mi><mi>q</mi><mi mathvariant="normal">.</mi><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H = Seq.(V)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mord">.</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mclose">)</span></span></span></span>, capturing contextual relationships in the character sequence.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="prediction-stage">Prediction Stage<a href="#prediction-stage" class="hash-link" aria-label="Direct link to Prediction Stage" title="Direct link to Prediction Stage">​</a></h3>
<p>In this stage, the character sequence <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo></mrow><annotation encoding="application/x-tex">Y = y_1, y_2, …</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span></span></span></span> is predicted from sequence <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>.</p>
<p>Based on previous research, the authors provide two prediction options:</p>
<ol>
<li><strong>Connectionist Temporal Classification (CTC)</strong>: This method predicts sequences of variable length with a fixed number of features. By removing repeated characters and blanks, the final character sequence is produced.</li>
<li><strong>Attention-based Sequence Prediction (Attn)</strong>: This method automatically captures the information flow in the input sequence, learning a character-level language model that represents the dependencies between output classes.</li>
</ol>
<p>These modules can be selected or adjusted as needed to suit different STR applications.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="experimental-setup">Experimental Setup<a href="#experimental-setup" class="hash-link" aria-label="Direct link to Experimental Setup" title="Direct link to Experimental Setup">​</a></h2>
<p>As previously mentioned, the dataset significantly affects STR model performance. To ensure a fair comparison, the authors standardized the training, validation, and evaluation datasets used.</p>
<ul>
<li><strong>Training Dataset</strong>: A combined dataset of MJSynth (8.9 million samples) and SynthText (5.5 million samples), totaling 14.4 million samples.</li>
<li><strong>Validation Dataset</strong>: A combination of the training sets from IC13, IC15, IIIT, and SVT, used as the validation dataset.</li>
<li><strong>Training Parameters</strong>: AdaDelta optimizer, decay rate ρ=0.95, batch size 192, total iterations 300,000, gradient clipping at 5, He initialization method, and model validation every 2,000 steps.</li>
<li><strong>Duplicate Data Handling</strong>: Part of the IC03 training data was excluded due to 34 scene images (containing 215 word boxes) overlapping with the IC13 evaluation dataset.</li>
<li><strong>Evaluation Metrics</strong>:<!-- -->
<ul>
<li><strong>Accuracy</strong>: Success rate of word prediction for the nine evaluation datasets.</li>
<li><strong>Speed</strong>: Average processing time per image (in milliseconds).</li>
<li><strong>Memory</strong>: The number of trainable floating-point parameters in the entire STR model.</li>
</ul>
</li>
<li><strong>Experimental Environment</strong>: Intel Xeon(R) E5-2630 v4 2.20GHz CPU, NVIDIA TESLA P40 GPU, 252GB RAM.</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p><img decoding="async" loading="lazy" alt="discussion" src="/en/assets/images/img7-7eb0f4a4b882cc8ecddcedc5f74911f9.jpg" width="1678" height="990" class="img_ev3q"></p>
<p>The authors analyzed the accuracy-speed and accuracy-memory trade-offs for different module combinations.</p>
<p>The graph below shows the trade-off curves for all module combinations, including six previously proposed STR models (marked with asterisks).</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="accuracy-time-trade-off-analysis">Accuracy-Time Trade-off Analysis<a href="#accuracy-time-trade-off-analysis" class="hash-link" aria-label="Direct link to Accuracy-Time Trade-off Analysis" title="Direct link to Accuracy-Time Trade-off Analysis">​</a></h3>
<div align="center"><figure style="width:80%"><p><img decoding="async" loading="lazy" alt="accuracy-time" src="/en/assets/images/img8-896efaa39d3072851748eaa82af58ee1.jpg" width="1224" height="472" class="img_ev3q"></p></figure></div>
<p>According to chart (a), T1 is the fastest model as it does not include any transformation or sequence modules.</p>
<p>From T1 to T5, the modules are progressively added (in <strong>bold</strong>): <strong>ResNet</strong>, <strong>BiLSTM</strong>, <strong>TPS</strong> (Thin-Plate Spline), and <strong>Attn</strong> (Attention mechanism).</p>
<p>Through these changes from T1 to T5, only one module is altered each time, providing a smooth transition between performance and computational efficiency, allowing minimal trade-offs between performance and efficiency depending on the application scenario.</p>
<p><strong>ResNet, BiLSTM, and TPS</strong> significantly improve accuracy (69.5%→82.9%) with a moderate speed drop overall (1.3 ms → 10.9 ms).</p>
<p><strong>Attn</strong> further increases accuracy by 1.1%, but efficiency is drastically reduced (27.6 ms), showing a high efficiency cost.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="accuracy-memory-trade-off-analysis">Accuracy-Memory Trade-off Analysis<a href="#accuracy-memory-trade-off-analysis" class="hash-link" aria-label="Direct link to Accuracy-Memory Trade-off Analysis" title="Direct link to Accuracy-Memory Trade-off Analysis">​</a></h3>
<div align="center"><figure style="width:80%"><p><img decoding="async" loading="lazy" alt="accuracy-memory" src="/en/assets/images/img9-3da481f6010ccad62790170a14d92e82.jpg" width="1224" height="456" class="img_ev3q"></p></figure></div>
<p>According to chart (b), P1 is the model with the least memory consumption. As modules are progressively added from P1 to P5, accuracy improves while memory increases.</p>
<p>In terms of accuracy-memory trade-offs, one module is also altered at a time, and accuracy gradually improves with modules added in this order: <strong>Attn</strong>, <strong>TPS</strong>, <strong>BiLSTM</strong>, and <strong>ResNet</strong>.</p>
<p>Compared to VGG used in T1, <strong>RCNN</strong> is more lightweight in P1-P4, offering good accuracy-memory trade-offs. RCNN uses fewer standalone CNN layers, which are recursively applied, providing lightweight and efficient performance.</p>
<p>Transformation, sequence, and prediction modules have minimal impact on memory consumption (1.9M→7.2M parameters) but can significantly boost accuracy (75.4%→82.3%).</p>
<p><strong>ResNet</strong> is introduced in the final stage, improving accuracy by 1.7% but substantially increasing memory consumption to 49.6M floating-point parameters. Therefore, memory-sensitive applications can freely choose transformation, sequence, and prediction modules but should avoid high-load feature extractors like ResNet.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="module-analysis">Module Analysis<a href="#module-analysis" class="hash-link" aria-label="Direct link to Module Analysis" title="Direct link to Module Analysis">​</a></h3>
<p><img decoding="async" loading="lazy" alt="module-analysis" src="/en/assets/images/img11-72b5ebee7f9f3dbf028858bd37b02093.jpg" width="1400" height="664" class="img_ev3q"></p>
<p>The authors analyzed the performance of each module in terms of accuracy, speed, and memory requirements.</p>
<ul>
<li><strong>Accuracy Improvement</strong>: Compared to regular benchmark datasets, performance improvements on irregular datasets are approximately double. When comparing accuracy improvement and time usage, the optimal module upgrade order is: <strong>ResNet, BiLSTM, TPS, Attn</strong>. This is the most effective order for upgrading from the baseline combination (None-VGG-None-CTC) and aligns with the accuracy-time trade-off upgrade sequence (T1→T5).</li>
<li><strong>Accuracy-Memory Perspective</strong>: From a memory consumption standpoint, the most effective module upgrade order is: <strong>RCNN, Attn, TPS, BiLSTM, ResNet</strong>. This matches the accuracy-memory trade-off upgrade sequence (P1→P5). However, the most efficient module upgrade orders for time and memory are exactly opposite.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="failure-case-analysis">Failure Case Analysis<a href="#failure-case-analysis" class="hash-link" aria-label="Direct link to Failure Case Analysis" title="Direct link to Failure Case Analysis">​</a></h3>
<p><img decoding="async" loading="lazy" alt="failure-case" src="/en/assets/images/img12-6664cd07ad0357a4b4f63cfd0e92f8fb.jpg" width="1952" height="346" class="img_ev3q"></p>
<p>The authors also analyzed all failure cases.</p>
<p>The chart shows six common types of failures. Of the 8,539 examples in the benchmark datasets, 644 images (7.5%) were not correctly recognized by any model.</p>
<ol>
<li>
<p><strong>Calligraphy Fonts</strong>:</p>
<ul>
<li>Special fonts for brands (e.g., &quot;Coca Cola&quot;) or street store names (e.g., &quot;Cafe&quot;) remain a challenge. These diverse font styles require a new feature extractor to provide more adaptable visual features.</li>
<li><strong>Future Research</strong>: Regularization may help prevent the model from overfitting to the font styles in the training dataset.</li>
</ul>
</li>
<li>
<p><strong>Vertical Text</strong>:</p>
<ul>
<li>Most current STR models assume that the text images are oriented horizontally, making it difficult to handle vertical text. Some models have attempted to utilize vertical information, but handling vertical text remains underdeveloped.</li>
<li><strong>Future Research</strong>: Further research may need to focus on recognizing vertical text more effectively.</li>
</ul>
</li>
<li>
<p><strong>Special Characters</strong>:</p>
<ul>
<li>Since current benchmark tests do not evaluate special characters, existing research excludes them during training, causing models to misclassify these characters as alphabetic or numeric.</li>
<li><strong>Future Research</strong>: Including special characters in training could improve the accuracy of the IIIT dataset from 87.9% to 90.3%.</li>
</ul>
</li>
<li>
<p><strong>Severe Occlusion</strong>:</p>
<ul>
<li>Current methods do not fully utilize contextual information to overcome occlusion issues.</li>
<li><strong>Future Research</strong>: Using stronger language models to maximize the use of contextual information could help.</li>
</ul>
</li>
<li>
<p><strong>Low Resolution</strong>:</p>
<ul>
<li>Existing models do not explicitly address low-resolution cases.</li>
<li><strong>Future Research</strong>: Using image pyramids or super-resolution modules may improve performance.</li>
</ul>
</li>
<li>
<p><strong>Label Noise</strong>:</p>
<ul>
<li>Some failure cases are due to mislabeled data in the datasets. Upon examination, all benchmark datasets contain noisy labels.</li>
<li><strong>Label Noise Statistics</strong>:<!-- -->
<ul>
<li>The proportion of incorrect labels without considering special characters is 1.3%.</li>
<li>The proportion of incorrect labels considering special characters is 6.1%.</li>
<li>The proportion of incorrect labels considering case sensitivity is 24.1%.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This paper provides an in-depth and comprehensive analysis of the scene text recognition field, offering a unified framework for fairly comparing the performance of different models.</p>
<p>If you don’t have time to read other papers, at least read this one!</p>
<p>It will help you quickly understand the state of the text recognition field as of 2019 and provide clear directions for your research, avoiding many potential pitfalls.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-11-04T07:54:36.000Z" itemprop="dateModified">Nov 4, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/text-recognition/sar/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[18.11] SAR</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/text-recognition/satrn/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[19.10] SATRN</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#data-and-model-analysis" class="table-of-contents__link toc-highlight">Data and Model Analysis</a></li><li><a href="#dataset-analysis" class="table-of-contents__link toc-highlight">Dataset Analysis</a><ul><li><a href="#training-datasets" class="table-of-contents__link toc-highlight">Training Datasets</a></li><li><a href="#testing-datasets" class="table-of-contents__link toc-highlight">Testing Datasets</a></li><li><a href="#version-differences" class="table-of-contents__link toc-highlight">Version Differences</a></li></ul></li><li><a href="#architecture-analysis" class="table-of-contents__link toc-highlight">Architecture Analysis</a><ul><li><a href="#transformation-stage" class="table-of-contents__link toc-highlight">Transformation Stage</a></li><li><a href="#feature-extraction-stage" class="table-of-contents__link toc-highlight">Feature Extraction Stage</a></li><li><a href="#sequence-modeling-stage" class="table-of-contents__link toc-highlight">Sequence Modeling Stage</a></li><li><a href="#prediction-stage" class="table-of-contents__link toc-highlight">Prediction Stage</a></li></ul></li><li><a href="#experimental-setup" class="table-of-contents__link toc-highlight">Experimental Setup</a></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#accuracy-time-trade-off-analysis" class="table-of-contents__link toc-highlight">Accuracy-Time Trade-off Analysis</a></li><li><a href="#accuracy-memory-trade-off-analysis" class="table-of-contents__link toc-highlight">Accuracy-Memory Trade-off Analysis</a></li><li><a href="#module-analysis" class="table-of-contents__link toc-highlight">Module Analysis</a></li><li><a href="#failure-case-analysis" class="table-of-contents__link toc-highlight">Failure Case Analysis</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Projects</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/terms-of-service">TermsOfUse</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/privacy-policy">Privacy Policy</a><span class="footer__link-separator">·</span><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="footer__link-item">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>