<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-text-recognition/wwwstr/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>[19.04] WWWSTR | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/text-recognition/wwwstr/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[19.04] WWWSTR | DOCSAID"><meta data-rh=true name=description content="Data and Model Analysis"><meta data-rh=true property=og:description content="Data and Model Analysis"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/text-recognition/wwwstr/><link data-rh=true rel=alternate href=https://docsaid.org/papers/text-recognition/wwwstr/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/text-recognition/wwwstr/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/text-recognition/wwwstr/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/text-recognition/wwwstr/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.8b5c2e41.css><script src=/en/assets/js/runtime~main.832a0d74.js defer></script><script src=/en/assets/js/main.da9e96bd.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/docsaid_logo.png><link rel=preload as=image href=/en/img/docsaid_logo_white.png><link rel=preload as=image href=https://github.com/zephyr-sh.png><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/text-recognition/wwwstr/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/text-recognition/wwwstr/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/text-recognition/wwwstr/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-7ny38l ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="Expand sidebar category 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-anti-spoofing-1>Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="Expand sidebar category 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Collapse sidebar category 'Text Recognition (20)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/crnn/>[15.07] CRNN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/rare/>[16.03] RARE</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/cafcn/>[18.09] CA-FCN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/sar/>[18.11] SAR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/text-recognition/wwwstr/>[19.04] WWWSTR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/satrn/>[19.10] SATRN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/dan/>[19.12] DAN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/seed/>[20.05] SEED</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/abinet/>[21.03] ABINet</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/vitstr/>[21.05] ViTSTR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/yatr/>[21.07] YATR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/trocr/>[21.09] TrOCR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/safl/>[22.01] SAFL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/siga/>[22.03] SIGA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/parseq/>[22.07] PARSeq</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/clip4str/>[23.05] CLIP4STR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/diffusionstr/>[23.06] DiffusionSTR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/union14m/>[23.07] Union14M</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/dtrocr/>[23.08] DTrOCR</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/text-recognition/ocr-scaling-law/>[24.01] OCR Scaling Law</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="Expand sidebar category 'Vision Transformers (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 175 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/en/papers/category/text-recognition-20><span itemprop=name>Text Recognition (20)</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>[19.04] WWWSTR</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[19.04] WWWSTR</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=data-and-model-analysis>Data and Model Analysis<a href=#data-and-model-analysis class=hash-link aria-label="Direct link to Data and Model Analysis" title="Direct link to Data and Model Analysis">​</a></h2>
<p><a href=https://arxiv.org/abs/1904.01906 target=_blank rel="noopener noreferrer"><strong>What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis</strong></a></p>
<hr>
<p>This paper, published by Clova AI, does not propose new techniques but instead presents a comprehensive analysis of existing algorithms and datasets, as well as an exploration of the best combinations of different modules to find the optimal solution.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=dataset-analysis>Dataset Analysis<a href=#dataset-analysis class=hash-link aria-label="Direct link to Dataset Analysis" title="Direct link to Dataset Analysis">​</a></h2>
<p>Different studies have used various dataset combinations for training, which may result in unclear performance improvements. It becomes difficult to determine if the improvement is due to the proposed new model or the usage of a better or larger dataset.</p>
<p>This inconsistency in datasets affects the fairness of results.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=training-datasets>Training Datasets<a href=#training-datasets class=hash-link aria-label="Direct link to Training Datasets" title="Direct link to Training Datasets">​</a></h3>
<p>Due to the high cost of annotating scene text images, it's challenging to acquire sufficient labeled data, and most STR (Scene Text Recognition) models opt for synthetic datasets for training.</p>
<p>Below are two widely used synthetic datasets in STR research:</p>
<ul>
<li>
<p><strong>MJSynth (MJ)</strong></p>
<ul>
<li>
<p><a href=https://www.robots.ox.ac.uk/~vgg/data/text/ target=_blank rel="noopener noreferrer"><strong>Synthetic Word Dataset</strong></a></p>
<div align=left><figure style=width:60%><p><img decoding=async loading=lazy alt=MJSynth src=/en/assets/images/img2-0f35900d2381dfb775886a6c83dafd2c.jpg width=628 height=496 class=img_ev3q></figure></div>
<p>MJSynth is a synthetic dataset specifically designed for STR, containing 8.9 million word-box images. The generation process involves font rendering, border and shadow rendering, background coloring, merging fonts and backgrounds, applying perspective distortions, mixing with real images, and adding noise.</p>
</li>
</ul>
</li>
<li>
<p><strong>SynthText (ST)</strong></p>
<ul>
<li>
<p><a href=https://www.robots.ox.ac.uk/~vgg/data/scenetext/ target=_blank rel="noopener noreferrer"><strong>SynthText in the Wild Dataset</strong></a></p>
<div align=left><figure style=width:60%><p><img decoding=async loading=lazy alt=SynthText（ST） src=/en/assets/images/img3-7752dfa9ee5593b40ad9ab6a9ef3f2f2.jpg width=592 height=508 class=img_ev3q></figure></div>
<p>SynthText was originally created for scene text detection but has also been used in STR research. Researchers cropped word boxes from this dataset for STR training. After cropping, the SynthText dataset contains about 5.5 million training samples.</p>
</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=testing-datasets>Testing Datasets<a href=#testing-datasets class=hash-link aria-label="Direct link to Testing Datasets" title="Direct link to Testing Datasets">​</a></h3>
<p>Different versions of benchmark datasets have certain differences.</p>
<p>For example, in IC03, a difference of 7 samples can lead to a 0.8% performance gap, which is a significant difference when comparing prior research results.</p>
<p>The sample number differences in IC13 and IC15 are even larger than those in IC03.</p>
<p>Seven real-scene STR datasets are widely used for post-training model evaluation. These datasets can be categorized into "regular datasets" and "irregular datasets" based on the difficulty of the text and its geometric layout:</p>
<ul>
<li>
<p><strong>Regular Datasets</strong>: These contain text images where characters are evenly spaced and arranged horizontally, making them relatively easier to process:</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=regular src=/en/assets/images/img4-8e9b6e395d7cf54cb195821c5bc05745.jpg width=652 height=344 class=img_ev3q></figure></div>
<ul>
<li><strong>IIIT5K-Words (IIIT)</strong>: A dataset from Google image search containing 2,000 training images and 3,000 evaluation images.</li>
<li><strong>Street View Text (SVT)</strong>: Collected from Google Street View, this dataset contains outdoor street scenes, with some images affected by noise, blur, or low resolution. It includes 257 training images and 647 evaluation images.</li>
<li><strong>ICDAR2003 (IC03)</strong>: Used in the ICDAR 2003 Robust Reading competition, it contains 1,156 training images and 1,110 evaluation images. There are two versions (860 and 867 images) due to the exclusion of words that are too short or contain non-alphanumeric characters.</li>
<li><strong>ICDAR2013 (IC13)</strong>: This dataset inherits most of the images from IC03 and was created for the ICDAR 2013 Robust Reading competition. It contains 848 training images and 1,095 evaluation images. Two versions are available for evaluation, one with 857 images and the other with 1,015.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>Irregular Datasets</strong>: These contain more challenging scene text, such as curved, rotated, or distorted text:</p>
<div align=center><figure style=width:60%><p><img decoding=async loading=lazy alt=irregular src=/en/assets/images/img5-f5bb08005f432285a4c4cabbb0e0d444.jpg width=496 height=344 class=img_ev3q></figure></div>
<ul>
<li><strong>ICDAR2015 (IC15)</strong>: This dataset, used in the ICDAR 2015 Robust Reading competition, contains 4,468 training images and 2,077 evaluation images. The images are captured from a natural movement perspective using Google Glass, resulting in many images with noise, blur, and rotation. Evaluation versions include 1,811 and 2,077 images.</li>
<li><strong>SVT Perspective (SP)</strong>: Collected from Google Street View, it includes 645 evaluation images, many with perspective distortions due to non-frontal views.</li>
<li><strong>CUTE80 (CT)</strong>: A dataset collected from natural scenes, containing 288 cropped evaluation images, many of which feature curved text.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=version-differences>Version Differences<a href=#version-differences class=hash-link aria-label="Direct link to Version Differences" title="Direct link to Version Differences">​</a></h3>
<p><img decoding=async loading=lazy alt=different-versions src=/en/assets/images/img1-b6fee3f37b9d379ef249184dc7ac9da9.jpg width=1468 height=746 class=img_ev3q></p>
<p>Based on the table above, different studies have used different versions of benchmark datasets for model evaluation, particularly in the IC03, IC13, and IC15 datasets.</p>
<p>For example, in IC03, a difference of 7 samples can lead to a 0.8% performance gap, which is significant when comparing results. The differences in sample sizes between IC13 and IC15 are even larger. These dataset version differences can lead to significant errors when evaluating model performance, which must be carefully considered when comparing different models.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>Version errors can lead to major issues—be cautious!</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=architecture-analysis>Architecture Analysis<a href=#architecture-analysis class=hash-link aria-label="Direct link to Architecture Analysis" title="Direct link to Architecture Analysis">​</a></h2>
<p><img decoding=async loading=lazy alt=architecture src=/en/assets/images/img6-153ec9b98a6d0dad378fdb020d7d330a.jpg width=1980 height=220 class=img_ev3q></p>
<p>STR (Scene Text Recognition) is similar to "object detection" tasks and "sequence prediction" tasks, benefiting from both Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).</p>
<p>The earliest model to combine CNN and RNN for STR was CRNN, which used CNNs for feature extraction and RNNs for sequence prediction. Many variants have since been proposed to improve performance, introducing different modules to handle complex features like font styles and backgrounds.</p>
<p>Some methods even omitted the RNN stage to reduce inference time. Subsequent research has introduced attention-based decoders to improve character sequence prediction.</p>
<p>The STR architecture typically follows these four stages:</p>
<ol>
<li><strong>Transformation</strong>: Spatial transformation networks (STN) are used to normalize input text images, reducing the burden on subsequent stages.</li>
<li><strong>Feature Extraction</strong>: This stage converts input images into representations focused on character recognition while suppressing features irrelevant to fonts, colors, sizes, and backgrounds.</li>
<li><strong>Sequence Modeling</strong>: This stage captures contextual information in the character sequence, improving the accuracy of each character's prediction.</li>
<li><strong>Prediction</strong>: The final character sequence is predicted based on the extracted features.</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=transformation-stage>Transformation Stage<a href=#transformation-stage class=hash-link aria-label="Direct link to Transformation Stage" title="Direct link to Transformation Stage">​</a></h3>
<p>This stage transforms input images X into normalized images X'.</p>
<p>Since text images in natural scenes vary in shape (such as curved or slanted text), if passed directly to later stages, the feature extraction stage would need to learn representations invariant to these geometric distortions.</p>
<p>To alleviate this burden, some research has employed Thin-Plate Spline (TPS) transformation, a variant of spatial transformation networks (STN). TPS normalizes text regions by performing smooth spline interpolation between a set of control points.</p>
<p>TPS is flexible in handling varying scales of text images and standardizes text regions into predetermined rectangles.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=feature-extraction-stage>Feature Extraction Stage<a href=#feature-extraction-stage class=hash-link aria-label="Direct link to Feature Extraction Stage" title="Direct link to Feature Extraction Stage">​</a></h3>
<p>In this stage, CNNs abstract the input image (X or X') into visual feature maps.</p>
<p>Each column of the feature map corresponds to a distinguishable receptive field in the horizontal direction of the input image, and these features are used to predict the character within each receptive field.</p>
<p>Three common architectures are studied: VGG, RCNN, and ResNet:</p>
<ul>
<li><strong>VGG</strong>: A simple architecture composed of multiple convolutional layers and a few fully connected layers.</li>
<li><strong>RCNN</strong>: A variant of CNN that recursively adjusts the receptive fields, adapting to character shapes.</li>
<li><strong>ResNet</strong>: Incorporates residual connections to alleviate the difficulties of training deep CNNs.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=sequence-modeling-stage>Sequence Modeling Stage<a href=#sequence-modeling-stage class=hash-link aria-label="Direct link to Sequence Modeling Stage" title="Direct link to Sequence Modeling Stage">​</a></h3>
<p>The output from the feature extraction stage is reshaped into a feature sequence V, where each column <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow><annotation encoding=application/x-tex>v_i \in V</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6891em;vertical-align:-0.15em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>∈</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.22222em>V</span></span></span></span> is treated as one frame in the sequence.</p>
<p>However, this sequence may lack contextual information, so previous studies have used Bidirectional Long Short-Term Memory (BiLSTM) networks to enhance the sequence <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>H</mi><mo>=</mo><mi>S</mi><mi>e</mi><mi>q</mi><mi mathvariant=normal>.</mi><mo stretchy=false>(</mo><mi>V</mi><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>H = Seq.(V)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.08125em>H</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.05764em>S</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style=margin-right:0.03588em>q</span><span class=mord>.</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.22222em>V</span><span class=mclose>)</span></span></span></span>, capturing contextual relationships in the character sequence.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=prediction-stage>Prediction Stage<a href=#prediction-stage class=hash-link aria-label="Direct link to Prediction Stage" title="Direct link to Prediction Stage">​</a></h3>
<p>In this stage, the character sequence <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>Y</mi><mo>=</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator=true>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator=true>,</mo><mo>…</mo></mrow><annotation encoding=application/x-tex>Y = y_1, y_2, …</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.22222em>Y</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em>y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class=minner>…</span></span></span></span> is predicted from sequence <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>H</mi></mrow><annotation encoding=application/x-tex>H</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.08125em>H</span></span></span></span>.</p>
<p>Based on previous research, the authors provide two prediction options:</p>
<ol>
<li><strong>Connectionist Temporal Classification (CTC)</strong>: This method predicts sequences of variable length with a fixed number of features. By removing repeated characters and blanks, the final character sequence is produced.</li>
<li><strong>Attention-based Sequence Prediction (Attn)</strong>: This method automatically captures the information flow in the input sequence, learning a character-level language model that represents the dependencies between output classes.</li>
</ol>
<p>These modules can be selected or adjusted as needed to suit different STR applications.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=experimental-setup>Experimental Setup<a href=#experimental-setup class=hash-link aria-label="Direct link to Experimental Setup" title="Direct link to Experimental Setup">​</a></h2>
<p>As previously mentioned, the dataset significantly affects STR model performance. To ensure a fair comparison, the authors standardized the training, validation, and evaluation datasets used.</p>
<ul>
<li><strong>Training Dataset</strong>: A combined dataset of MJSynth (8.9 million samples) and SynthText (5.5 million samples), totaling 14.4 million samples.</li>
<li><strong>Validation Dataset</strong>: A combination of the training sets from IC13, IC15, IIIT, and SVT, used as the validation dataset.</li>
<li><strong>Training Parameters</strong>: AdaDelta optimizer, decay rate ρ=0.95, batch size 192, total iterations 300,000, gradient clipping at 5, He initialization method, and model validation every 2,000 steps.</li>
<li><strong>Duplicate Data Handling</strong>: Part of the IC03 training data was excluded due to 34 scene images (containing 215 word boxes) overlapping with the IC13 evaluation dataset.</li>
<li><strong>Evaluation Metrics</strong>:<!-- -->
<ul>
<li><strong>Accuracy</strong>: Success rate of word prediction for the nine evaluation datasets.</li>
<li><strong>Speed</strong>: Average processing time per image (in milliseconds).</li>
<li><strong>Memory</strong>: The number of trainable floating-point parameters in the entire STR model.</li>
</ul>
</li>
<li><strong>Experimental Environment</strong>: Intel Xeon(R) E5-2630 v4 2.20GHz CPU, NVIDIA TESLA P40 GPU, 252GB RAM.</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p><img decoding=async loading=lazy alt=discussion src=/en/assets/images/img7-7eb0f4a4b882cc8ecddcedc5f74911f9.jpg width=1678 height=990 class=img_ev3q></p>
<p>The authors analyzed the accuracy-speed and accuracy-memory trade-offs for different module combinations.</p>
<p>The graph below shows the trade-off curves for all module combinations, including six previously proposed STR models (marked with asterisks).</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=accuracy-time-trade-off-analysis>Accuracy-Time Trade-off Analysis<a href=#accuracy-time-trade-off-analysis class=hash-link aria-label="Direct link to Accuracy-Time Trade-off Analysis" title="Direct link to Accuracy-Time Trade-off Analysis">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=accuracy-time src=/en/assets/images/img8-896efaa39d3072851748eaa82af58ee1.jpg width=1224 height=472 class=img_ev3q></figure></div>
<p>According to chart (a), T1 is the fastest model as it does not include any transformation or sequence modules.</p>
<p>From T1 to T5, the modules are progressively added (in <strong>bold</strong>): <strong>ResNet</strong>, <strong>BiLSTM</strong>, <strong>TPS</strong> (Thin-Plate Spline), and <strong>Attn</strong> (Attention mechanism).</p>
<p>Through these changes from T1 to T5, only one module is altered each time, providing a smooth transition between performance and computational efficiency, allowing minimal trade-offs between performance and efficiency depending on the application scenario.</p>
<p><strong>ResNet, BiLSTM, and TPS</strong> significantly improve accuracy (69.5%→82.9%) with a moderate speed drop overall (1.3 ms → 10.9 ms).</p>
<p><strong>Attn</strong> further increases accuracy by 1.1%, but efficiency is drastically reduced (27.6 ms), showing a high efficiency cost.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=accuracy-memory-trade-off-analysis>Accuracy-Memory Trade-off Analysis<a href=#accuracy-memory-trade-off-analysis class=hash-link aria-label="Direct link to Accuracy-Memory Trade-off Analysis" title="Direct link to Accuracy-Memory Trade-off Analysis">​</a></h3>
<div align=center><figure style=width:80%><p><img decoding=async loading=lazy alt=accuracy-memory src=/en/assets/images/img9-3da481f6010ccad62790170a14d92e82.jpg width=1224 height=456 class=img_ev3q></figure></div>
<p>According to chart (b), P1 is the model with the least memory consumption. As modules are progressively added from P1 to P5, accuracy improves while memory increases.</p>
<p>In terms of accuracy-memory trade-offs, one module is also altered at a time, and accuracy gradually improves with modules added in this order: <strong>Attn</strong>, <strong>TPS</strong>, <strong>BiLSTM</strong>, and <strong>ResNet</strong>.</p>
<p>Compared to VGG used in T1, <strong>RCNN</strong> is more lightweight in P1-P4, offering good accuracy-memory trade-offs. RCNN uses fewer standalone CNN layers, which are recursively applied, providing lightweight and efficient performance.</p>
<p>Transformation, sequence, and prediction modules have minimal impact on memory consumption (1.9M→7.2M parameters) but can significantly boost accuracy (75.4%→82.3%).</p>
<p><strong>ResNet</strong> is introduced in the final stage, improving accuracy by 1.7% but substantially increasing memory consumption to 49.6M floating-point parameters. Therefore, memory-sensitive applications can freely choose transformation, sequence, and prediction modules but should avoid high-load feature extractors like ResNet.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=module-analysis>Module Analysis<a href=#module-analysis class=hash-link aria-label="Direct link to Module Analysis" title="Direct link to Module Analysis">​</a></h3>
<p><img decoding=async loading=lazy alt=module-analysis src=/en/assets/images/img11-72b5ebee7f9f3dbf028858bd37b02093.jpg width=1400 height=664 class=img_ev3q></p>
<p>The authors analyzed the performance of each module in terms of accuracy, speed, and memory requirements.</p>
<ul>
<li><strong>Accuracy Improvement</strong>: Compared to regular benchmark datasets, performance improvements on irregular datasets are approximately double. When comparing accuracy improvement and time usage, the optimal module upgrade order is: <strong>ResNet, BiLSTM, TPS, Attn</strong>. This is the most effective order for upgrading from the baseline combination (None-VGG-None-CTC) and aligns with the accuracy-time trade-off upgrade sequence (T1→T5).</li>
<li><strong>Accuracy-Memory Perspective</strong>: From a memory consumption standpoint, the most effective module upgrade order is: <strong>RCNN, Attn, TPS, BiLSTM, ResNet</strong>. This matches the accuracy-memory trade-off upgrade sequence (P1→P5). However, the most efficient module upgrade orders for time and memory are exactly opposite.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=failure-case-analysis>Failure Case Analysis<a href=#failure-case-analysis class=hash-link aria-label="Direct link to Failure Case Analysis" title="Direct link to Failure Case Analysis">​</a></h3>
<p><img decoding=async loading=lazy alt=failure-case src=/en/assets/images/img12-6664cd07ad0357a4b4f63cfd0e92f8fb.jpg width=1952 height=346 class=img_ev3q></p>
<p>The authors also analyzed all failure cases.</p>
<p>The chart shows six common types of failures. Of the 8,539 examples in the benchmark datasets, 644 images (7.5%) were not correctly recognized by any model.</p>
<ol>
<li>
<p><strong>Calligraphy Fonts</strong>:</p>
<ul>
<li>Special fonts for brands (e.g., "Coca Cola") or street store names (e.g., "Cafe") remain a challenge. These diverse font styles require a new feature extractor to provide more adaptable visual features.</li>
<li><strong>Future Research</strong>: Regularization may help prevent the model from overfitting to the font styles in the training dataset.</li>
</ul>
</li>
<li>
<p><strong>Vertical Text</strong>:</p>
<ul>
<li>Most current STR models assume that the text images are oriented horizontally, making it difficult to handle vertical text. Some models have attempted to utilize vertical information, but handling vertical text remains underdeveloped.</li>
<li><strong>Future Research</strong>: Further research may need to focus on recognizing vertical text more effectively.</li>
</ul>
</li>
<li>
<p><strong>Special Characters</strong>:</p>
<ul>
<li>Since current benchmark tests do not evaluate special characters, existing research excludes them during training, causing models to misclassify these characters as alphabetic or numeric.</li>
<li><strong>Future Research</strong>: Including special characters in training could improve the accuracy of the IIIT dataset from 87.9% to 90.3%.</li>
</ul>
</li>
<li>
<p><strong>Severe Occlusion</strong>:</p>
<ul>
<li>Current methods do not fully utilize contextual information to overcome occlusion issues.</li>
<li><strong>Future Research</strong>: Using stronger language models to maximize the use of contextual information could help.</li>
</ul>
</li>
<li>
<p><strong>Low Resolution</strong>:</p>
<ul>
<li>Existing models do not explicitly address low-resolution cases.</li>
<li><strong>Future Research</strong>: Using image pyramids or super-resolution modules may improve performance.</li>
</ul>
</li>
<li>
<p><strong>Label Noise</strong>:</p>
<ul>
<li>Some failure cases are due to mislabeled data in the datasets. Upon examination, all benchmark datasets contain noisy labels.</li>
<li><strong>Label Noise Statistics</strong>:<!-- -->
<ul>
<li>The proportion of incorrect labels without considering special characters is 1.3%.</li>
<li>The proportion of incorrect labels considering special characters is 6.1%.</li>
<li>The proportion of incorrect labels considering case sensitivity is 24.1%.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This paper provides an in-depth and comprehensive analysis of the scene text recognition field, offering a unified framework for fairly comparing the performance of different models.</p>
<p>If you don’t have time to read other papers, at least read this one!</p>
<p>It will help you quickly understand the state of the text recognition field as of 2019 and provide clear directions for your research, avoiding many potential pitfalls.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-02-11T02:49:16.000Z itemprop=dateModified>Feb 11, 2025</time></b> by <b>zephyr-sh</b></span></div></div><div style=margin-top:3rem> </div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/text-recognition/sar/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[18.11] SAR</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/text-recognition/satrn/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[19.10] SATRN</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#data-and-model-analysis class="table-of-contents__link toc-highlight">Data and Model Analysis</a><li><a href=#dataset-analysis class="table-of-contents__link toc-highlight">Dataset Analysis</a><ul><li><a href=#training-datasets class="table-of-contents__link toc-highlight">Training Datasets</a><li><a href=#testing-datasets class="table-of-contents__link toc-highlight">Testing Datasets</a><li><a href=#version-differences class="table-of-contents__link toc-highlight">Version Differences</a></ul><li><a href=#architecture-analysis class="table-of-contents__link toc-highlight">Architecture Analysis</a><ul><li><a href=#transformation-stage class="table-of-contents__link toc-highlight">Transformation Stage</a><li><a href=#feature-extraction-stage class="table-of-contents__link toc-highlight">Feature Extraction Stage</a><li><a href=#sequence-modeling-stage class="table-of-contents__link toc-highlight">Sequence Modeling Stage</a><li><a href=#prediction-stage class="table-of-contents__link toc-highlight">Prediction Stage</a></ul><li><a href=#experimental-setup class="table-of-contents__link toc-highlight">Experimental Setup</a><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#accuracy-time-trade-off-analysis class="table-of-contents__link toc-highlight">Accuracy-Time Trade-off Analysis</a><li><a href=#accuracy-memory-trade-off-analysis class="table-of-contents__link toc-highlight">Accuracy-Memory Trade-off Analysis</a><li><a href=#module-analysis class="table-of-contents__link toc-highlight">Module Analysis</a><li><a href=#failure-case-analysis class="table-of-contents__link toc-highlight">Failure Case Analysis</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>