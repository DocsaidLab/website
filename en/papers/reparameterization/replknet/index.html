<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-reparameterization/replknet/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">[22.03] RepLKNet | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/reparameterization/replknet/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[22.03] RepLKNet | DOCSAID"><meta data-rh="true" name="description" content="Giant Convolutional Kernels"><meta data-rh="true" property="og:description" content="Giant Convolutional Kernels"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/reparameterization/replknet/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/reparameterization/replknet/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/reparameterization/replknet/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/reparameterization/replknet/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.51ef4fe1.css">
<script src="/en/assets/js/runtime~main.98a28c2a.js" defer="defer"></script>
<script src="/en/assets/js/main.f058e02f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a><a class="navbar__item navbar__link" href="/en/playground/intro">Playground</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/reparameterization/replknet/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/reparameterization/replknet/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="Expand sidebar category &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="Expand sidebar category &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="Expand sidebar category &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="Expand sidebar category &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/llm-tuning-5">LLM Tuning (5)</a><button aria-label="Expand sidebar category &#x27;LLM Tuning (5)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/multimodality-20">Multimodality (20)</a><button aria-label="Expand sidebar category &#x27;Multimodality (20)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization-1">Normalization (1)</a><button aria-label="Expand sidebar category &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection-8">Object Detection (8)</a><button aria-label="Expand sidebar category &#x27;Object Detection (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="Collapse sidebar category &#x27;Reparameterization (7)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/reparameterization/repvgg/">[21.01] RepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/reparameterization/replknet/">[22.03] RepLKNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/reparameterization/mobileone/">[22.06] MobileOne</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/reparameterization/qarepvgg/">[22.12] QARepVGG</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/reparameterization/fastvit/">[23.03] FastViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/reparameterization/vanillanet/">[23.05] VanillaNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/reparameterization/repvit/">[23.07] RepViT</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="Expand sidebar category &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="Expand sidebar category &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-recognition-18">Text Recognition (18)</a><button aria-label="Expand sidebar category &#x27;Text Recognition (18)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-spotting-4">Text Spotting (4)</a><button aria-label="Expand sidebar category &#x27;Text Spotting (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/transformers-15">Transformers (15)</a><button aria-label="Expand sidebar category &#x27;Transformers (15)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="Expand sidebar category &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">All Notes: 133 entries</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/reparameterization-7"><span itemprop="name">Reparameterization (7)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[22.03] RepLKNet</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[22.03] RepLKNet</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="giant-convolutional-kernels">Giant Convolutional Kernels<a href="#giant-convolutional-kernels" class="hash-link" aria-label="Direct link to Giant Convolutional Kernels" title="Direct link to Giant Convolutional Kernels">​</a></h2>
<p><a href="https://arxiv.org/abs/2203.06717" target="_blank" rel="noopener noreferrer"><strong>Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs</strong></a></p>
<hr>
<p>In recent years, CNN architectures have faced challenges from ViT, causing their dominance to waver.</p>
<p>Some argue that ViT&#x27;s strength lies in its multi-head attention mechanism, which offers greater flexibility, reduced inductive bias, and robustness to distortions.</p>
<p>However, another perspective attributes ViT&#x27;s performance to its architecture, even when the multi-head attention is removed, resulting in similar performance.</p>
<p>This paper differs by focusing on a single aspect: the design of large convolutional kernels.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-definition">Problem Definition<a href="#problem-definition" class="hash-link" aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>Large kernel models have not been popular since VGG, except for some older models like Inceptions.</p>
<p>VGG demonstrated that stacking small kernels can achieve the same receptive field, leading to a decline in interest in large kernel designs.</p>
<p>Recent studies on large kernels have not addressed a key question:</p>
<ul>
<li><strong>Why do traditional CNNs underperform compared to ViTs? Is achieving a large receptive field crucial for closing the performance gap between CNNs and ViTs?</strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-guide-to-using-large-kernels-in-convolution">A Guide to Using Large Kernels in Convolution<a href="#a-guide-to-using-large-kernels-in-convolution" class="hash-link" aria-label="Direct link to A Guide to Using Large Kernels in Convolution" title="Direct link to A Guide to Using Large Kernels in Convolution">​</a></h2>
<p>To answer this question, the authors systematically explored the design of large kernels in CNNs and summarized five guidelines.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deep-large-kernels-remain-efficient">Deep Large Kernels Remain Efficient<a href="#deep-large-kernels-remain-efficient" class="hash-link" aria-label="Direct link to Deep Large Kernels Remain Efficient" title="Direct link to Deep Large Kernels Remain Efficient">​</a></h3>
<p>Large kernels are often considered computationally expensive due to increased FLOPs.</p>
<p>However, using depthwise convolutions can significantly mitigate this issue.</p>
<p>On the other hand, modern GPUs and parallel computing devices have limited support for depthwise convolutions, increasing memory access costs.</p>
<p>Existing deep learning tools also lack adequate support for depthwise convolutions, resulting in high inference latency with PyTorch implementations, as shown below.</p>
<p><img decoding="async" loading="lazy" alt="img1" src="/en/assets/images/img1-222d061bc955a73aa4af9101cc77b839.jpg" width="1460" height="322" class="img_ev3q"></p>
<p>To address this, the authors released an efficient PyTorch implementation, reducing depthwise convolution latency from 49.5% to 12.3%, roughly proportional to FLOPs:</p>
<ul>
<li><a href="https://github.com/DingXiaoH/RepLKNet-pytorch" target="_blank" rel="noopener noreferrer"><strong>RepLKNet-pytorch</strong></a></li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="large-kernels-heavily-rely-on-residual-connections">Large Kernels Heavily Rely on Residual Connections<a href="#large-kernels-heavily-rely-on-residual-connections" class="hash-link" aria-label="Direct link to Large Kernels Heavily Rely on Residual Connections" title="Direct link to Large Kernels Heavily Rely on Residual Connections">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img2" src="/en/assets/images/img2-28c80f7f6e53734a93305913f115564a.jpg" width="1200" height="344" class="img_ev3q"></p>
<p>Using MobileNet-V2 for benchmarking, the authors replaced all DW 3×3 layers with 13×13 layers.</p>
<p>The table shows that large kernels improve the accuracy of MobileNet-V2 with shortcuts by 0.77%. Without shortcuts, large kernels reduce accuracy to only 53.98%.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>As seen in previous studies, shortcuts create implicit ensembles of models with different receptive fields, benefiting from a larger maximum receptive field without losing the ability to capture small-scale patterns.</p></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="large-kernels-require-reparameterization">Large Kernels Require Reparameterization<a href="#large-kernels-require-reparameterization" class="hash-link" aria-label="Direct link to Large Kernels Require Reparameterization" title="Direct link to Large Kernels Require Reparameterization">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img3" src="/en/assets/images/img3-7577b785adb3ce0e46040fbbe83e3056.jpg" width="1788" height="282" class="img_ev3q"></p>
<p>The table shows that directly increasing kernel size from 9 to 13 reduces accuracy, but reparameterization resolves this issue.</p>
<p>It is known that ViT faces optimization problems with small datasets. A common solution is to introduce convolutional priors, such as adding a DW 3×3 convolution to each self-attention block, similar to this study. These strategies introduce additional translation invariance and locality, making optimization easier on small datasets without losing generality.</p>
<p>Similarly, the authors found that reparameterization can be omitted without performance degradation when the pre-training dataset is increased to 73 million images.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="large-kernels-significantly-enhance-downstream-task-performance">Large Kernels Significantly Enhance Downstream Task Performance<a href="#large-kernels-significantly-enhance-downstream-task-performance" class="hash-link" aria-label="Direct link to Large Kernels Significantly Enhance Downstream Task Performance" title="Direct link to Large Kernels Significantly Enhance Downstream Task Performance">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img4" src="/en/assets/images/img4-7b24555c5e3d38eff197e8090f5a1f4f.jpg" width="1116" height="426" class="img_ev3q"></p>
<p>The table shows that increasing the kernel size from 3×3 to 9×9 in MobileNet V2 improves ImageNet accuracy by 1.33% and Cityscapes mIoU by 3.99%.</p>
<p><img decoding="async" loading="lazy" alt="img5" src="/en/assets/images/img5-9d9cb49c653b1ea12b4d0c5c9dc4323b.jpg" width="1536" height="528" class="img_ev3q"></p>
<p>Another experiment shows a similar trend: as kernel size increases from [3, 3, 3, 3] to [31, 29, 27, 13], ImageNet accuracy improves by only 0.96%, while ADE20K mIoU increases by 3.12%. This indicates that models with similar ImageNet scores can have very different capabilities in downstream tasks.</p>
<p>The authors attribute this to two reasons:</p>
<ol>
<li>Large kernels significantly increase the effective receptive field (ERF). Numerous studies have shown that &quot;context&quot; information (large ERF) is crucial for many downstream tasks, such as object detection and semantic segmentation.</li>
<li>Large kernels introduce more shape bias to the network. ImageNet images can be correctly classified based on texture or shape, but humans primarily rely on shape cues to recognize objects. Thus, models with stronger shape bias may perform better in downstream tasks.</li>
</ol>
<p>Recent studies also indicate that ViT has a strong shape bias, partly explaining why ViT excels in transfer tasks. In contrast, traditional CNNs trained on ImageNet tend to have a texture bias.</p>
<ul>
<li><a href="https://arxiv.org/abs/2105.07197" target="_blank" rel="noopener noreferrer"><strong>[21.05] Are convolutional neural networks or transformers more like human vision?</strong></a></li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="large-kernels-are-effective-even-with-small-feature-maps">Large Kernels Are Effective Even with Small Feature Maps<a href="#large-kernels-are-effective-even-with-small-feature-maps" class="hash-link" aria-label="Direct link to Large Kernels Are Effective Even with Small Feature Maps" title="Direct link to Large Kernels Are Effective Even with Small Feature Maps">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img6" src="/en/assets/images/img6-c9b037aa23e2c85cfeedfff047fdb9ee.jpg" width="1548" height="366" class="img_ev3q"></p>
<p>The authors enlarged the DW convolution in the final stage of MobileNet V2 to 7×7 or 13×13, making the kernel size equal to or larger than the feature map size (default 7×7). The table shows that while large kernels do not improve ImageNet performance, they significantly boost downstream tasks, increasing Cityscapes mIoU to 74.62%.</p>
<p>The authors believe that as kernel size increases, the translational invariance of CNNs does not strictly hold. Adjacent spatial positions&#x27; outputs share only part of the kernel weights, transforming through different mappings, as illustrated below. This mechanism might explain the effectiveness of large kernels on small feature maps.</p>
<p><img decoding="async" loading="lazy" alt="img7" src="/en/assets/images/img7-9f8a761150efe82a433cb51afa5d93ca.jpg" width="1318" height="428" class="img_ev3q"></p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-problem">Solving the Problem<a href="#solving-the-problem" class="hash-link" aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="network-architecture">Network Architecture<a href="#network-architecture" class="hash-link" aria-label="Direct link to Network Architecture" title="Direct link to Network Architecture">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img8" src="/en/assets/images/img8-13c0ba4be7a10e75433f67ebbe642d30.jpg" width="1140" height="972" class="img_ev3q"></p>
<p>Based on the above insights, the authors proposed a new network architecture, RepLKNet.</p>
<p>Firstly, RepLKNet&#x27;s design focuses on capturing more details in the initial stages through multiple convolution layers. After the initial 3×3 convolution and downsampling, a 3×3 depthwise separable convolution (DW) layer captures low-level patterns, followed by a 1×1 convolution layer and another DW 3×3 layer for downsampling. This ensures thorough extraction of detail information in the initial stages.</p>
<p>Secondly, each stage (1-4) of RepLKNet contains multiple RepLK blocks. These blocks leverage shortcuts and DW large kernels to enhance performance. Each DW convolution layer uses a 5×5 kernel for reparameterization to capture more details.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Although not shown in the diagram above, this part is crucial for performance improvement. These blocks enable efficient feature extraction and processing at different stages.</p></div></div>
<p>Additionally, to enhance nonlinearity and cross-channel communication, RepLKNet introduces ConvFFN blocks. These blocks consist of shortcuts, Batch Normalization (BN), two 1×1 convolution layers, and GELU activation, similar to feedforward networks (FFN) in Transformers and MLPs.</p>
<p>Compared to the classic FFN using Layer Normalization, BN integrates better with convolution operations, improving inference efficiency. This design is inspired by Transformers and MLP networks and has been successfully applied in CNNs.</p>
<p>Between different stages, RepLKNet uses Transition Blocks to adjust channel dimensions and perform downsampling. These blocks first increase channel dimensions with a 1×1 convolution, then downsample using a DW 3×3 convolution, ensuring comprehensive feature extraction and transformation. These transition blocks allow smooth transitions between stages, maintaining information consistency and integrity.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluation-of-large-kernels">Evaluation of Large Kernels<a href="#evaluation-of-large-kernels" class="hash-link" aria-label="Direct link to Evaluation of Large Kernels" title="Direct link to Evaluation of Large Kernels">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img9" src="/en/assets/images/img9-4c86c7455c6b73f91609117353d26cd8.jpg" width="1326" height="442" class="img_ev3q"></p>
<p>To evaluate the impact of large kernels on RepLKNet, the authors changed kernel sizes while keeping some hyperparameters fixed, observing their performance on classification and semantic segmentation tasks.</p>
<p>The results show that increasing kernel size from 3 to 13 improves accuracy on ImageNet, but further enlarging the kernel does not bring significant improvement. However, on ADE20K, expanding the kernel from 13 to 31 results in higher mIoU, highlighting the advantages of large kernels in downstream tasks.</p>
<p>This indicates that while large kernels have limited benefits for certain tasks, they are valuable in specific applications.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="performance-on-imagenet">Performance on ImageNet<a href="#performance-on-imagenet" class="hash-link" aria-label="Direct link to Performance on ImageNet" title="Direct link to Performance on ImageNet">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img10" src="/en/assets/images/img10-f10a0f047adff656d27d98343a13eec3.jpg" width="1392" height="870" class="img_ev3q"></p>
<p>Given the similarities in architecture between RepLKNet and Swin, the authors conducted a detailed comparison.</p>
<p>On ImageNet-1K, the authors extended the training schedule of RepLKNet-31B to 300 epochs to ensure a fair comparison. Then, they fine-tuned for 30 epochs with 384×384 input resolution, at a training cost significantly lower than training Swin-B from scratch with 384×384 resolution.</p>
<p>Additionally, the authors pre-trained RepLKNet-B and RepLKNet-L on ImageNet-22K and fine-tuned them on ImageNet-1K. RepLKNet-XL was pre-trained on the authors&#x27; private semi-supervised dataset MegData73M.</p>
<p>The authors also tested the models&#x27; throughput on the same 2080Ti GPU with a batch size of 64.</p>
<hr>
<p>The results show that although very large kernels are not suitable for ImageNet classification, RepLKNet models achieve a good balance between accuracy and efficiency. For instance, RepLKNet-31B achieved 84.8% accuracy with only ImageNet-1K training, 0.3% higher than Swin-B, and ran 43% faster. Despite RepLKNet-XL having higher FLOPs than Swin-L, it ran faster, emphasizing the efficiency of large kernel designs.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="large-kernel-cnns-have-larger-erf-than-deep-small-kernel-models">Large Kernel CNNs Have Larger ERF Than Deep Small Kernel Models<a href="#large-kernel-cnns-have-larger-erf-than-deep-small-kernel-models" class="hash-link" aria-label="Direct link to Large Kernel CNNs Have Larger ERF Than Deep Small Kernel Models" title="Direct link to Large Kernel CNNs Have Larger ERF Than Deep Small Kernel Models">​</a></h3>
<p>The authors demonstrated that large kernel designs significantly improve CNN performance.</p>
<p>Notably, although large kernels can be represented by a series of small convolutions, such as decomposing a 7×7 convolution into three stacked 3×3 kernels, a question arises:</p>
<ul>
<li><strong>Why do traditional CNNs with numerous small convolution layers (e.g., ResNets) still underperform compared to large kernel networks?</strong></li>
</ul>
<p>The authors argue that a single large kernel is more effective in achieving a large receptive field (ERF) than many small kernels. According to ERF theory, ERF is proportional to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>K</mi><msqrt><mi>L</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(K\sqrt{L})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1767em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord mathnormal">L</span></span></span><span style="top:-2.8867em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em"><span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, where K is the kernel size, and L is the depth (number of layers).</p>
<p>In other words, <strong>ERF is mainly influenced by kernel size, followed by depth</strong>.</p>
<hr>
<p>Moreover, increasing depth brings optimization challenges.</p>
<p>Although ResNets are thought to overcome this issue, they do not entirely. Studies show that ResNets behave like ensembles of shallow networks, with ERF remaining limited despite increased depth.</p>
<p>To support this, the authors conducted an experiment:</p>
<p><img decoding="async" loading="lazy" alt="img11" src="/en/assets/images/img11-d5c2db9c884456b97a2ae5b9e253a464.jpg" width="1544" height="494" class="img_ev3q"></p>
<p>To visualize ERF, the authors used a simple yet effective method.</p>
<p>They generated an aggregated contribution score matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false">(</mo><mn>1024</mn><mo>×</mo><mn>1024</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A (1024×1024)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">A</span><span class="mopen">(</span><span class="mord">1024</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1024</span><span class="mclose">)</span></span></span></span>, measuring the contribution of corresponding pixels in the input image to the center point of the final layer&#x27;s feature map.</p>
<p>In simpler terms: <strong>Which pixels in the input image contribute to the center point of the feature map?</strong></p>
<p>The results show that in ResNet, ERF is very limited, even with increased depth. In contrast, RepLKNet exhibits more uniformly distributed high-contribution pixels, resulting in a larger ERF.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="shape-bias">Shape Bias<a href="#shape-bias" class="hash-link" aria-label="Direct link to Shape Bias" title="Direct link to Shape Bias">​</a></h3>
<p><img decoding="async" loading="lazy" alt="img12" src="/en/assets/images/img12-2b8671d93a2ef1c0c860a7796a1c85bd.jpg" width="996" height="1028" class="img_ev3q"></p>
<p>The authors note that large kernel designs introduce a stronger shape bias, crucial for downstream tasks.</p>
<p>They measured the shape bias of RepLKNet-31B and Swin-B, trained on ImageNet-1K or 22K, and two small kernel baselines, RepLKNet-3 and ResNet-152.</p>
<p>The results show that RepLKNet-31B has a significantly higher shape bias than Swin Transformer and small kernel CNNs. The authors believe that shape bias is closely related to ERF rather than self-attention, explaining:</p>
<ol>
<li>ViT&#x27;s high shape bias due to global attention</li>
<li>Swin Transformer&#x27;s low shape bias due to local window attention</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="limitations">Limitations<a href="#limitations" class="hash-link" aria-label="Direct link to Limitations" title="Direct link to Limitations">​</a></h3>
<p>While large kernel designs significantly enhance CNN performance on ImageNet and downstream tasks, RepLKNet begins to lag behind Swin Transformers as the data and model scale increase.</p>
<p>The authors are unsure whether this gap is due to suboptimal hyperparameter tuning or fundamental flaws in CNNs with larger data and model scales.</p>
<p>This remains an open area for future research.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In this study, the authors revisited the long-neglected design of large convolutional kernels in CNN architecture.</p>
<p>Through experiments, they demonstrated that using a few large kernels instead of many small ones can more effectively achieve a larger effective receptive field, significantly improving CNN performance. This finding helps narrow the performance gap between CNNs and ViTs.</p>
<p>For the CNN community, the results suggest that special attention should be given to ERF, which may be key to achieving high performance. For the ViT community, the study shows that large convolutional kernels can replace multi-head self-attention mechanisms with similar behavior, potentially aiding in understanding the intrinsic mechanisms of self-attention.</p>
<p>The authors hope this work fosters mutual understanding and progress between the two communities and inspires further research to explore and optimize deep learning model architecture design.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-09-03T01:48:11.000Z" itemprop="dateModified">Sep 3, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/reparameterization/repvgg/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[21.01] RepVGG</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/reparameterization/mobileone/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[22.06] MobileOne</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#giant-convolutional-kernels" class="table-of-contents__link toc-highlight">Giant Convolutional Kernels</a></li><li><a href="#problem-definition" class="table-of-contents__link toc-highlight">Problem Definition</a></li><li><a href="#a-guide-to-using-large-kernels-in-convolution" class="table-of-contents__link toc-highlight">A Guide to Using Large Kernels in Convolution</a><ul><li><a href="#deep-large-kernels-remain-efficient" class="table-of-contents__link toc-highlight">Deep Large Kernels Remain Efficient</a></li><li><a href="#large-kernels-heavily-rely-on-residual-connections" class="table-of-contents__link toc-highlight">Large Kernels Heavily Rely on Residual Connections</a></li><li><a href="#large-kernels-require-reparameterization" class="table-of-contents__link toc-highlight">Large Kernels Require Reparameterization</a></li><li><a href="#large-kernels-significantly-enhance-downstream-task-performance" class="table-of-contents__link toc-highlight">Large Kernels Significantly Enhance Downstream Task Performance</a></li><li><a href="#large-kernels-are-effective-even-with-small-feature-maps" class="table-of-contents__link toc-highlight">Large Kernels Are Effective Even with Small Feature Maps</a></li></ul></li><li><a href="#solving-the-problem" class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href="#network-architecture" class="table-of-contents__link toc-highlight">Network Architecture</a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#evaluation-of-large-kernels" class="table-of-contents__link toc-highlight">Evaluation of Large Kernels</a></li><li><a href="#performance-on-imagenet" class="table-of-contents__link toc-highlight">Performance on ImageNet</a></li><li><a href="#large-kernel-cnns-have-larger-erf-than-deep-small-kernel-models" class="table-of-contents__link toc-highlight">Large Kernel CNNs Have Larger ERF Than Deep Small Kernel Models</a></li><li><a href="#shape-bias" class="table-of-contents__link toc-highlight">Shape Bias</a></li><li><a href="#limitations" class="table-of-contents__link toc-highlight">Limitations</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Projects</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/terms-of-service">TermsOfUse</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/privacy-policy">Privacy Policy</a><span class="footer__link-separator">·</span><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="footer__link-item">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>