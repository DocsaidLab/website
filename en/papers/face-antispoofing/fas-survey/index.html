<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-face-antispoofing/fas-survey/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[22.10] FAS Survey | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/face-antispoofing/fas-survey/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[22.10] FAS Survey | DOCSAID"><meta data-rh=true name=description content="A Chronicle of Attacks and Defenses"><meta data-rh=true property=og:description content="A Chronicle of Attacks and Defenses"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/face-antispoofing/fas-survey/><link data-rh=true rel=alternate href=https://docsaid.org/papers/face-antispoofing/fas-survey/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/face-antispoofing/fas-survey/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/face-antispoofing/fas-survey/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/face-antispoofing/fas-survey/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/face-anti-spoofing-40","name":"Face Anti-Spoofing (40)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/face-antispoofing/fas-survey/","name":"[22.10] FAS Survey","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.106e88b1.js defer></script><script src=/en/assets/js/main.351309db.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/face-antispoofing/fas-survey/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/face-antispoofing/fas-survey/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/face-antispoofing/fas-survey/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="Expand sidebar category 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/face-anti-spoofing-40>Face Anti-Spoofing (40)</a><button aria-label="Collapse sidebar category 'Face Anti-Spoofing (40)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/slrbd/>[10.09] SLRBD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/lbp/>[12.09] LBP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/three-d-mad/>[14.05] 3DMAD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/rppg/>[16.12] rPPG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/oulu-npu/>[17.06] OULU-NPU</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cfrppg/>[18.09] CFrPPG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/vafas/>[19.05] VA-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/wmca/>[19.09] WMCA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cdcn/>[20.03] CDCN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cefa/>[20.03] CeFA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/ssdg/>[20.04] SSDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/celeba-spoof/>[20.07] CelebA-Spoof</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/stdn/>[20.07] STDN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/disentangle-fas/>[20.08] Disentangle-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/d2am/>[21.05] D²AM</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/dualstage/>[21.10] DualStage</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/dsdg/>[21.12] DSDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/personalized-fas/>[22.01] Personalized-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/ssan/>[22.03] SSAN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/echo-fas/>[22.08] Echo-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/face-antispoofing/fas-survey/>[22.10] FAS Survey</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cdftn/>[22.12] CDFTN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/divt/>[23.01] DiVT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/m2a2e/>[23.02] M²A²E</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/sa-fas/>[23.03] SA-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/iadg/>[23.04] IADG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/ma-vit/>[23.04] MA-ViT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/flip/>[23.09] FLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/s-adapter/>[23.09] S-Adapter</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/udg-fas/>[23.10] UDG-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/three-a-tta/>[23.11] 3A-TTA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/mmdg/>[24.02] MMDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/shield/>[24.02] SHIELD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/aface/>[24.03] AFace</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cfpl-fas/>[24.03] CFPL-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/fas-challenge/>[24.04] FAS-Challenge</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/pd-fas/>[24.04] PD-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/la-softmoe/>[24.08] La-SoftMoE</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/fm-clip/>[24.10] FM-CLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/i-fas/>[25.01] I-FAS</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="Expand sidebar category 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-13>Vision Transformers (13)</a><button aria-label="Expand sidebar category 'Vision Transformers (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 216 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/face-anti-spoofing-40><span>Face Anti-Spoofing (40)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[22.10] FAS Survey</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[22.10] FAS Survey</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=a-chronicle-of-attacks-and-defenses>A Chronicle of Attacks and Defenses<a href=#a-chronicle-of-attacks-and-defenses class=hash-link aria-label="Direct link to A Chronicle of Attacks and Defenses" title="Direct link to A Chronicle of Attacks and Defenses">​</a></h2>
<p><a href=https://ieeexplore.ieee.org/abstract/document/9925105 target=_blank rel="noopener noreferrer"><strong>Deep Learning for Face Anti-Spoofing: A Survey</strong></a></p>
<hr>
<p>After reviewing a few papers, we have a general understanding of the research background and development context of FAS.</p>
<p>Now, let's directly review the various methods developed over the past decade or so.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>This survey paper includes around two hundred references, so interested readers can look it up for further reading.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=fas-framework>FAS Framework<a href=#fas-framework class=hash-link aria-label="Direct link to FAS Framework" title="Direct link to FAS Framework">​</a></h2>
<p>The first step of all attacks is to deceive the sensor.</p>
<p>Therefore, we can roughly divide attacks into two categories:</p>
<ul>
<li><strong>Digital Attacks (Digital Manipulation)</strong>: Such as deepfakes or image processing, where modifications are made directly in the virtual domain.</li>
<li><strong>Physical Presentation Attacks</strong>: The focus of this paper, where the goal is to deceive the camera through physical media in the real world.</li>
</ul>
<p>Common attack types and the construction methods of FAS systems are shown in the diagram below:</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=model_arch src=/en/assets/images/img1-c392f98720b419bc92f8bb5bab9a71cf.jpg width=1224 height=836 class=img_ev3q></figure></div>
<p>From physical presentation attacks, we can further categorize them based on intent:</p>
<ul>
<li><strong>Impersonation</strong>: Imitating others, such as holding a photo or wearing a 3D mask to make the system believe you are a specific person.</li>
<li><strong>Obfuscation</strong>: Concealing oneself, such as with makeup, sunglasses, or wigs, preventing the system from recognizing who you are.</li>
</ul>
<p>Based on geometric structure, attacks can also be divided into:</p>
<ul>
<li><strong>2D Attacks</strong>: For example, flat photos, screen replay videos, cutout photos, etc.</li>
<li><strong>3D Attacks</strong>: Includes masks made from various materials such as paper, plaster, silicone, or resin. These types of attacks are often more realistic and challenging.</li>
</ul>
<p>Interestingly, besides full-face "direct attacks," there are also many "side attacks," which are <strong>local attacks</strong> targeting specific areas, such as wearing funny glasses or sticking stickers on the cheek, deceiving only partial regions and adding difficulty to defense.</p>
<p>With the development of deep learning methods, the scale and complexity of datasets have also gradually increased.</p>
<p>This paper summarizes three major trends:</p>
<ol>
<li><strong>Scaling Up</strong>: For example, CelebA-Spoof, HiFiMask, etc., with the number of images and videos reaching hundreds of thousands, providing saturated training data.</li>
<li><strong>Enhanced Diversity</strong>: New datasets no longer just contain common print and replay attacks, but introduce more refined types of 3D attacks, lighting variations, and cross-scene recordings, such as SiW-M, which covers up to 13 types of attack.</li>
<li><strong>Sensor Upgrades</strong>: Expanding from a single RGB camera to advanced sensors like Depth, Near Infrared (NIR), Thermal, Shortwave Infrared (SWIR), and even Light Field cameras for recording.</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=overview-of-deep-learning-methods>Overview of Deep Learning Methods<a href=#overview-of-deep-learning-methods class=hash-link aria-label="Direct link to Overview of Deep Learning Methods" title="Direct link to Overview of Deep Learning Methods">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=method_tree src=/en/assets/images/img2-87fe8dfeb05f28fa7046756b309f1a1c.jpg width=1224 height=504 class=img_ev3q></figure></div>
<p>The image above shows a <strong>FAS method classification tree</strong>, starting from the sensor level and extending to different model designs and generalization strategies.</p>
<p>Starting with the classification at the sensor level:</p>
<ul>
<li><strong>Commercial RGB Camera</strong>: Currently the most common type of device, found in most smartphones or laptops. FAS methods developed for this type of device focus on detecting forgeries in the visible light modality.</li>
<li><strong>Advanced Sensor</strong>: Examples include NIR, Depth, Thermal, SWIR, and Light Field cameras, which provide richer image features. Models applied to these devices can combine cross-modal information to enhance recognition capabilities, but also increase development and deployment costs.</li>
</ul>
<hr>
<p>Since in most cases, only an RGB camera is available, we will focus on FAS methods for this modality.</p>
<ul>
<li>
<p><strong>Common Deep Learning Method</strong></p>
<p>This involves traditional binary classification supervised learning (such as Binary Cross-Entropy), or further pixel-level supervision (Pixel-wise Supervision), emphasizing the model's ability to learn local spoof cues.</p>
<p>Common auxiliary designs include:</p>
<ul>
<li><strong>Auxiliary Supervision</strong>: Introducing intermediate feature supervision or deep feature assistance for judgment.</li>
<li><strong>Generative Model</strong>: Using GANs or reconstruction methods to enhance recognition of anomalous samples.</li>
</ul>
</li>
<li>
<p><strong>Hybrid Method</strong></p>
<p>This combines different supervision structures (e.g., BCE + reconstruction loss), balancing classification and regional reconstruction abilities to improve the model's ability to capture spoof cues.</p>
</li>
<li>
<p><strong>Generalized Deep Learning Method</strong></p>
<p>The core goal of this approach is <strong>Domain Generalization</strong>, not only to perform well in the training scenario but also to handle unseen data distributions.
The main technical branches include:</p>
<ul>
<li><strong>Domain Adaptation</strong>: Fine-tuning the model when there is a small amount of target data.</li>
<li><strong>Domain Generalization</strong>: Ensuring generalization ability without any target data.</li>
<li><strong>Federated Learning</strong>: Integrating learning across multiple devices to enhance generalization performance under privacy constraints.</li>
</ul>
</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>The Vision Language Model (VLM) approach, which is not popular yet, will be discussed in later sections.</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=evaluation-metrics>Evaluation Metrics<a href=#evaluation-metrics class=hash-link aria-label="Direct link to Evaluation Metrics" title="Direct link to Evaluation Metrics">​</a></h3>
<p>The evaluation of FAS tasks focuses on two core aspects:</p>
<ul>
<li><strong>FAR (False Acceptance Rate)</strong>: The rate at which attack samples are misclassified as legitimate users.</li>
<li><strong>FRR (False Rejection Rate)</strong>: The rate at which legitimate users are misclassified as attack samples.</li>
</ul>
<p>To balance these two, common composite metrics include:</p>
<ul>
<li>
<p><strong>HTER (Half Total Error Rate)</strong>: The average of FAR and FRR</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mtext>HTER</mtext><mo>=</mo><mfrac><mrow><mtext>FAR</mtext><mo>+</mo><mtext>FRR</mtext></mrow><mn>2</mn></mfrac></mrow><annotation encoding=application/x-tex>\text{HTER} = \frac{\text{FAR} + \text{FRR}}{2}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord text"><span class=mord>HTER</span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.0463em;vertical-align:-0.686em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3603em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>2</span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord text"><span class=mord>FAR</span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span><span class="mord text"><span class=mord>FRR</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.686em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
</li>
<li>
<p><strong>EER (Equal Error Rate)</strong>: The error rate when FAR and FRR are equal (i.e., the value of HTER at the balance point), commonly used as a comprehensive performance evaluation metric for models.</p>
</li>
<li>
<p><strong>AUC (Area Under Curve)</strong>: The area under the ROC curve, reflecting the model's ability to distinguish between bonafide and spoof attack samples. The closer to 1, the better.</p>
</li>
</ul>
<p>In addition, based on the <strong>ISO/IEC 30107-3</strong> standard, three more detailed error metrics have been gradually introduced:</p>
<ul>
<li>
<p><strong>APCER (Attack Presentation Classification Error Rate)</strong>: The rate at which attack samples are misclassified as bonafide.</p>
</li>
<li>
<p><strong>BPCER (Bonafide Presentation Classification Error Rate)</strong>: The rate at which bonafide samples are misclassified as attacks.</p>
</li>
<li>
<p><strong>ACER (Average Classification Error Rate)</strong>: The average of APCER and BPCER</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mtext>ACER</mtext><mo>=</mo><mfrac><mrow><mtext>APCER</mtext><mo>+</mo><mtext>BPCER</mtext></mrow><mn>2</mn></mfrac></mrow><annotation encoding=application/x-tex>\text{ACER} = \frac{\text{APCER} + \text{BPCER}}{2}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord text"><span class=mord>ACER</span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.0463em;vertical-align:-0.686em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3603em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>2</span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class="mord text"><span class=mord>APCER</span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span><span class="mord text"><span class=mord>BPCER</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.686em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
</li>
</ul>
<p>These metrics provide specific quantitative measures of model performance in various misclassification scenarios, particularly suitable for fine-grained evaluation in real-world deployment environments.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>HTER and ACER are essentially the same thing, just with different naming conventions.</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=evaluation-protocols>Evaluation Protocols<a href=#evaluation-protocols class=hash-link aria-label="Direct link to Evaluation Protocols" title="Direct link to Evaluation Protocols">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=evaluation_protocol src=/en/assets/images/img3-32c7164bf0e1c9a06cae881610959663.jpg width=1224 height=880 class=img_ev3q></figure><figcaption>Comparison of deep learning FAS method performance under four mainstream testing protocols.</figcaption></div>
<hr>
<p>Different protocol settings reflect the <strong>level of challenge faced by models in real-world deployment scenarios</strong>. We can think of these settings as growth stages for models, from "familiar" to "unknown," and they can be divided into four common test protocols:</p>
<ol>
<li>
<p><strong>Intra-Dataset Intra-Type</strong></p>
<p>Training and test data come from the <strong>same dataset</strong> and <strong>the same type of attack</strong>. This is the most basic evaluation method, where the model uses data from the same distribution in both training and testing phases.</p>
<p>This is common in sub-protocols of datasets like OULU-NPU and SiW. In this setting, the domain gap is minimal, making it the easiest for deep models to achieve high accuracy.</p>
<p>As shown in the figure (a), most methods perform well in the OULU-NPU Protocol-4 test, with ACER typically below 5%.</p>
<hr>
</li>
<li>
<p><strong>Cross-Dataset Intra-Type</strong></p>
<p>Training and test data come from <strong>different datasets</strong>, but the attack type is the same, mainly testing the model's domain generalization ability.</p>
<p>This simulates the challenge of having "development environments" and "real-world application scenarios" that are inconsistent. The results shown in figure (b) display testing on Replay-Attack training and CASIA-MFSD test. When only a single dataset is used for training (green bars), HTER is generally higher. However, when combining multiple datasets (purple bars), domain adaptation methods like SSDG and SSAN significantly improve generalization performance.</p>
<hr>
</li>
<li>
<p><strong>Intra-Dataset Cross-Type</strong></p>
<p>A leave-one-type-out setup, meaning a specific attack type appears <strong>only in the test phase</strong>, not included in the training data.</p>
<p>This protocol is designed to test how well the model can handle "unknown attack types." As shown in figure (c) for SiW-M testing, which includes up to 13 attack types with varying difficulty, resulting in an average EER of about 10%, though with a large standard deviation.</p>
<p>By using pre-training and transfer learning, models like ViTranZFAS can reduce the EER to 6.7%.</p>
<hr>
</li>
<li>
<p><strong>Cross-Dataset Cross-Type</strong></p>
<p>This is the most challenging setup, where both the <strong>data source and attack type</strong> are changed.</p>
<p>Training only uses OULU-NPU and SiW (mainly 2D attacks), while testing uses datasets like HKBU-MARs or CASIA-SURF 3DMask, which involve 3D masks. The results shown in figure (d) indicate that current methods like NAS-FAS and DTN can only detect certain low-realism 3D masks, with high-realism materials still difficult to differentiate.</p>
<p>This protocol is the most realistic, addressing unknown scenarios and composite attack challenges that are likely to be encountered in real-world deployments, making it an important direction for future research.</p>
</li>
</ol>
<hr>
<p>Each protocol's design is a step-by-step elevation of the model's ability: From the ideal "closed training and testing" to the open "unknown real-world scenarios," how an FAS system grows and learns to identify previously unseen attacks is the core challenge of this counter-forgery fight.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=rgb-based-fas>RGB-based FAS<a href=#rgb-based-fas class=hash-link aria-label="Direct link to RGB-based FAS" title="Direct link to RGB-based FAS">​</a></h2>
<p><img decoding=async loading=lazy alt=models src=/en/assets/images/img4-0f7a381abf330dd154090802dda936e1.jpg width=1224 height=296 class=img_ev3q></p>
<p>The image above summarizes the evolution of FAS technology based on RGB cameras.</p>
<p>Starting from the era reliant on handcrafted features, it has progressed toward deep model designs that emphasize multimodal fusion and generalization capabilities.</p>
<p>Since RGB cameras are standard on almost all devices, the anti-spoofing systems developed for these devices have become the main battlefield for the application of FAS technology.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=hybrid-methods>Hybrid Methods<a href=#hybrid-methods class=hash-link aria-label="Direct link to Hybrid Methods" title="Direct link to Hybrid Methods">​</a></h2>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=hybrid_method src=/en/assets/images/img5-6f5e4782e288e6dc94b71209347f45b3.jpg width=1076 height=1056 class=img_ev3q></figure></div>
<p>Although deep learning has dominated classification and detection tasks, the practical reality of FAS tasks is quite different:</p>
<blockquote>
<p><strong>The dataset is generally small, and the sample distribution is often imbalanced, which leads to model training being prone to overfitting.</strong></p>
</blockquote>
<p>At this point, some research has turned to combining traditional handcrafted features with deep models, hoping to fill the "blind spots" in model perception of non-textural cues, such as lighting changes, heartbeat rhythms, and abnormal motions.</p>
<p>These methods can generally be divided into three approaches.</p>
<p>The first is "<strong>feature pre-fusion</strong>," where traditional methods are used to extract static or dynamic features before feeding them into a deep model for classification. Common techniques include using multi-scale LBP or Weber descriptors to capture detailed textures, or combining LBP with CNN to retain low-level information such as edges and intensity.</p>
<p>If the goal is to capture dynamic cues, dense optical flow is used to analyze motion trajectories, rPPG is turned into time-series images and fed to Vision Transformer, and some studies even use histograms of brightness changes to counter replay attacks.</p>
<blockquote>
<p><strong>The core logic of these methods is: The model learns from you, but I will prepare the clues you need to see.</strong></p>
</blockquote>
<hr>
<p>The second method is "<strong>feature post-fusion</strong>," which follows the opposite process: deep models first extract features, and then some handcrafted descriptors are added for reinforcement. For instance, PCA might be used to clean unnecessary information from VGG-face, or color LBP might be directly extracted from convolution layers to reinforce statistical signals. There are also cases where LBP-TOP or optical flow is stacked onto temporal features to increase the resolution of time dynamics.</p>
<blockquote>
<p><strong>The biggest challenge with this approach is that CNN features change semantically as the layer depth increases, so selecting the appropriate fusion layer often requires experience and experimentation, with no standard answer.</strong></p>
</blockquote>
<hr>
<p>The third approach is "<strong>dual-stream fusion</strong>," where handcrafted features and deep models each run independently and are then integrated at either the feature layer or score layer.</p>
<p>Examples of this approach are more varied: some directly combine the predictions of LBP and VGG16, while others use features like HOG and LBP to guide the learning direction of the CNN's lower layers. There are also studies that combine the brightness and blur clues extracted by a 1D CNN, specifically targeting replay-type attacks.</p>
<blockquote>
<p><strong>This approach involves placing different models together, letting each utilize its strengths, and then synthesizing the results.</strong></p>
</blockquote>
<hr>
<p>Overall, the biggest advantage of hybrid methods is that they can compensate for the parts where pure deep models are weak, especially those subtle but meaningful non-textural features, such as facial micro-reflections, heartbeat cycles, and camera motion blur. In scenarios with insufficient data and large scene variations, this approach can indeed offer additional stability and flexibility.</p>
<p>However, it also has obvious limitations, such as the non-learnability of handcrafted features, the need for expert parameter tuning, and limited generalization capabilities. Furthermore, due to the semantic inconsistency between handcrafted and deep features, fusion can sometimes lead to information conflicts and model confusion.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=traditional-deep-learning-methods>Traditional Deep Learning Methods<a href=#traditional-deep-learning-methods class=hash-link aria-label="Direct link to Traditional Deep Learning Methods" title="Direct link to Traditional Deep Learning Methods">​</a></h2>
<p>Hybrid methods existed because deep models were still in their developmental stage, and there wasn't enough data, so handcrafted features were used to hold the fort.</p>
<p>However, as CNN architectures have matured and large anti-spoofing datasets have been released, the FAS community has started embracing an "end-to-end learning" approach, focusing on the idea of "more data, thicker networks, and harsher supervision."</p>
<p>This approach emphasizes learning the <strong>difference features between live and spoofed images directly from the image</strong>, discarding all non-learnable add-ons and using end-to-end learning from input to output, becoming the mainstream route for commercial RGB FAS.</p>
<p>Although the forms are varied, these methods can generally be divided into two categories:</p>
<ul>
<li>Treating FAS as a binary classification problem.</li>
<li>Using pixel-level supervision along with generative designs to teach the model more detailed spoof patterns.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=binary-classification-supervision>Binary Classification Supervision<a href=#binary-classification-supervision class=hash-link aria-label="Direct link to Binary Classification Supervision" title="Direct link to Binary Classification Supervision">​</a></h3>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=binary src=/en/assets/images/img6a-25b3280c857e428134cd48b89983ded8.jpg width=922 height=276 class=img_ev3q></figure></div>
<p>The most intuitive approach is to treat FAS as a Bonafide vs. Spoof binary classification problem, using BCE Loss as the loss function to directly predict the label from the input image.</p>
<p>Early networks like 8-layer CNN, VGG16, and ResNet18 were commonly used, and later, lightweight architectures based on MobileNet appeared. There were even studies that combined CNNs with LSTMs, allowing the model to analyze "multiple-frame" subtle movements, such as blinking and slight head motions, in an attempt to incorporate temporal signals into the decision-making process.</p>
<p>To prevent the model from being misled, the loss function has also evolved:</p>
<ul>
<li><strong>Changed to multi-class classification</strong>: Some researchers added attack type labels (multi-class CE) to allow the model to distinguish between different spoof techniques like replay, printing, and 3D masks, turning the binary classification problem into a multi-class classification problem.</li>
<li><strong>Contrastive learning</strong>: Some researchers used Triplet or Contrastive Loss to make intra-class features compact and inter-class features separable, strengthening the model’s representation capability.</li>
</ul>
<p>Even designs like Focal Loss and asymmetric Softmax have been introduced to address issues like sample imbalance and spoof distribution skew.</p>
<p>The advantage of this design is that it is easy to implement and converges quickly, but there are potential pitfalls. The biggest risk is:</p>
<blockquote>
<p><strong>The model may easily learn things it shouldn't.</strong></p>
</blockquote>
<p>For instance, "hints" like screen edges, black borders, or lighting anomalies may lead to high accuracy in the short term, but the model is essentially memorizing the dataset, with poor generalization performance. If not careful, it can become a bunch of electronic trash.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=pixel-level-supervision>Pixel-level Supervision<a href=#pixel-level-supervision class=hash-link aria-label="Direct link to Pixel-level Supervision" title="Direct link to Pixel-level Supervision">​</a></h3>
<p>To address the issue of the model overly relying on hints, some research has introduced pixel-level supervision signals, allowing the model to focus more on the physical and material features of the face itself, rather than background or device-induced noise.</p>
<hr>
<ul>
<li>
<p><strong>(a) Auxiliary Task Supervision</strong></p>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=pixel-level src=/en/assets/images/img6b-fa238d38ca8166c71c1e06e1ae9d05a7.jpg width=1038 height=302 class=img_ev3q></figure></div>
<p>This approach uses one or more auxiliary labels, such as:</p>
<ul>
<li>Pseudo depth maps to distinguish between flat attacks and real faces.</li>
<li>Binary masks to indicate spoof areas, or more advanced labels like ternary maps, rPPG, light reflection maps, etc.</li>
</ul>
<p>These methods allow the model to learn both identification and interpretation. (The output images can be used as interpretative results.)</p>
<p>Examples include DepthNet and CDCN, which were among the earliest architectures to use depth maps for supervision. FAS-SGTD also adds short-term and long-term motion estimations, trying to establish a rhythm of "even if you pretend well, the flaw will still show."</p>
<p>Mask-based methods were initially promoted by George & Marcel, and later studies added attention modules to address the issue of model attention shifting. Some studies also introduced ternary masks to exclude background signals and help the model focus on key areas.</p>
<p>The advantages of these methods are obvious:</p>
<blockquote>
<p><strong>Strong interpretability, spatial semantics, and support for multi-task learning, but they heavily depend on the quality of the data.</strong></p>
</blockquote>
<p>Most pixel labels are constructed either manually or using external models, and poor-quality labels can actually mislead the model.</p>
</li>
</ul>
<hr>
<ul>
<li>
<p><strong>(b) Generative Supervision</strong></p>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=generative src=/en/assets/images/img6c-cf4d0d7273ba278ec0cc293f9c77cfb6.jpg width=1224 height=298 class=img_ev3q></figure></div>
<p>In addition to direct labeling, another line of research has emerged, which focuses on "letting the model discover anomalies on its own."</p>
<p>These methods often use autoencoders or encoder-decoder structures, allowing the model to attempt to reconstruct bonafide images and then assess the spoof level based on reconstruction errors.</p>
<p>Some studies define spoofing as a form of "noise added to the input," trying to uncover flaws through noise estimation. A more advanced approach is the concept of meta-teachers, where a teacher model generates pixel-level supervision, and the student model is responsible for learning spoof features, essentially forming a built-in QA system.</p>
<p>The advantages of these methods are:</p>
<blockquote>
<p><strong>Strong visualization, interpretability, and particularly well-suited for data-driven generalization learning.</strong></p>
</blockquote>
<p>However, there are notable downsides: training can be unstable, convergence is slow, and sometimes the model may learn sensor-specific noise, increasing the risk of overfitting.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>For example, the model may learn the imaging characteristics of an iPhone and then classify images from other brands of phones as spoof.</div></div>
</li>
</ul>
<hr>
<p>From this wave of traditional deep learning methods, it’s evident that the FAS community is no longer satisfied with just classification accuracy. There is now increasing attention on the model's interpretability, generalization ability, and scene adaptability.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=generalization-oriented-deep-learning-methods>Generalization-Oriented Deep Learning Methods<a href=#generalization-oriented-deep-learning-methods class=hash-link aria-label="Direct link to Generalization-Oriented Deep Learning Methods" title="Direct link to Generalization-Oriented Deep Learning Methods">​</a></h2>
<p>If the challenge of traditional deep learning methods is insufficient data and the model's over-reliance on incorrect hints, then the challenges of generalization-oriented methods are even more practical:</p>
<blockquote>
<p><strong>The model only performs well in the scenes it has "seen," but as soon as the camera changes, the lighting changes, or even just the face changes, it immediately breaks down.</strong></p>
</blockquote>
<p>This "good training, poor real-world performance" issue is especially severe in the FAS domain.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>We had great accuracy in the office, but when we deployed it at a convenience store entrance, the model failed miserably.<p>This situation is clearly unacceptable in applications such as access control, payments, and remote verification, which is why more and more research is focusing on the <strong>generalization ability</strong> of FAS models.</div></div>
<p>Generalization challenges can be roughly divided into two types:</p>
<ul>
<li>One comes from environmental variations (unseen domains), such as lighting, sensors, background noise, etc.</li>
<li>The other comes from the unknown nature of attack types (unknown attacks), such as 3D masks or deformable cover-ups that the model has never seen before.</li>
</ul>
<p>These two problems are fundamentally different, and thus, their solutions differ. Let’s take a closer look at each.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=facing-unknown-environments>Facing Unknown Environments<a href=#facing-unknown-environments class=hash-link aria-label="Direct link to Facing Unknown Environments" title="Direct link to Facing Unknown Environments">​</a></h3>
<div align=center><figure style=width:85%><p><img decoding=async loading=lazy alt=unseen src=/en/assets/images/img7-5eec04eae7884907e889feccc9db8954.jpg width=940 height=440 class=img_ev3q></figure></div>
<p>When data distribution shifts due to environmental variations, even the best-trained models cannot guarantee stable outputs.</p>
<p>In such scenarios, generalization strategies are mainly divided into two approaches:</p>
<ul>
<li><strong>Domain Adaptation</strong></li>
<li><strong>Domain Generalization</strong></li>
</ul>
<p>The difference between the two lies in whether you can obtain data from the target environment.</p>
<hr>
<ul>
<li>
<p><strong>Domain Adaptation: I know where you're going, so I'll adjust for you</strong></p>
<p>This approach assumes that you already know where the model will be deployed, and may even have some (labeled or unlabeled) data from the target domain. Since the destination is known, it’s possible to adjust the direction in advance.</p>
<p>Some methods perform unsupervised alignment, such as Li et al. minimizing the maximum mean discrepancy (MMD) to reduce the distribution difference between source and target features, or using adversarial learning to make it hard for the model to distinguish between the source domain, forcing it to learn universal features.</p>
<p>Semi-supervised methods further assume a small amount of target domain samples (e.g., 3 samples per class), which can significantly improve model performance. However, there’s a cost: if you only have bonafide samples and no spoof samples, the model may become biased.</p>
<p>More advanced designs, like multi-layer MMD, domain-specific normalization, or knowledge distillation, try to adjust the network architecture itself. Some studies even perform filter pruning, slimming the model while transferring it.</p>
<blockquote>
<p><strong>The downside is obvious: in real-world scenarios, you don’t know what spoof samples will look like, making it difficult to obtain valid data.</strong></p>
</blockquote>
<hr>
</li>
<li>
<p><strong>Domain Generalization: I don't know where you're going, but I hope you can go anywhere</strong></p>
<p>In contrast, Domain Generalization assumes a more extreme scenario: there is no data from the target domain, so the model must be trained using multiple source domains to create a model that can "handle all future situations." This sounds like an AI adventurer training camp, which is why it has become a hot research topic in recent years.</p>
<p>Methods can generally be divided into several categories: the most basic is adversarial learning, where a domain discriminator is designed to force features to not carry domain-specific information; others use feature disentanglement to separate interference factors like identity and camera noise, leaving behind spoof representations that can survive across different domains.</p>
<p>Meta-learning methods are more "role-playing": training the model on different source domains as different tasks, enabling it to quickly adapt to new environments. Some studies don’t even need domain labels and instead use clustering to dynamically generate pseudo-domains, essentially turning generalization into a built-in skill.</p>
<p>Additionally, there are simple but effective designs from normalization, such as Liu’s BIAN (Batch + Instance Adaptive Normalization), which combines multiple normalization strategies to remove domain bias with unexpectedly good results.</p>
<blockquote>
<p><strong>The downside is that it's hard to train and can be heavily influenced by outlier samples, reducing overall performance.</strong></p>
</blockquote>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=facing-unknown-attacks>Facing Unknown Attacks<a href=#facing-unknown-attacks class=hash-link aria-label="Direct link to Facing Unknown Attacks" title="Direct link to Facing Unknown Attacks">​</a></h3>
<p>Another challenge comes from the diversity of attack methods:</p>
<blockquote>
<p><strong>You can prepare for different spoof types, but you can’t guarantee that attackers won’t come up with a method you haven’t seen.</strong></p>
</blockquote>
<p>In this case, models trained to only recognize known attacks will have a blind spot, leading to incorrect classifications.</p>
<p>Therefore, the approach to counter unknown attacks has gradually shifted from a closed-set to an open-set mindset, primarily manifesting in two directions:</p>
<ol>
<li><strong>Zero / Few-Shot Learning</strong></li>
<li><strong>Anomaly Detection</strong></li>
</ol>
<hr>
<ul>
<li>
<p><strong>Zero / Few-Shot Learning: I haven't seen it, but I have a hunch</strong></p>
<p>The concept of zero-shot learning is: abstract features are learned from known spoof types, so when encountering an unseen attack, the model can "guess" based on semantic knowledge. For example, Liu et al. proposed the Deep Tree Network (DTN), categorizing spoof types and building a tree-like semantic structure, then using this structure to identify unknown attack types.</p>
<p>Few-shot learning is a more practical approach, allowing the model to see a very small number of new samples (e.g., five samples) and quickly updating model parameters using meta-learning to adapt quickly. Qin’s method combines the advantages of zero- and few-shot learning, adding a dynamic learning rate mechanism, while Perez-Cabo designed a continual few-shot learning system, allowing the model to update as data grows and preventing forgetting of old tasks.</p>
<p>The common advantage of these methods is their fast response and scalability, but when the number of samples is extremely small or even zero, confusion may still occur. Especially when spoof methods are highly convincing, distinguishing them becomes much more difficult.</p>
<hr>
</li>
<li>
<p><strong>Anomaly Detection: I don't know you, so I don't trust you</strong></p>
<p>The anomaly detection approach is simpler and more straightforward: I only learn bonafide, and anything that doesn’t resemble bonafide is suspicious.</p>
<p>These methods often adopt one-class training strategies, such as using GMM, One-Class SVM, or Autoencoder reconstruction error to define the "normal region."</p>
<p>Further designs converge bonafide features in the feature space into a hypersphere, and any point outside the sphere could be considered a spoof. There’s also George’s One-Class Contrastive Loss (OCCL), combining contrastive learning with posterior scoring to enhance the model's ability to distinguish anomalous points.</p>
<p>The advantage of these methods is that they are open-world friendly and do not require attack samples. However, the downside is clear:</p>
<blockquote>
<p><strong>If spoof samples look too much like bonafide, they can easily be confused, and the classification boundaries are difficult to define.</strong></p>
</blockquote>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=multi-sensor-deep-learning-methods>Multi-Sensor Deep Learning Methods<a href=#multi-sensor-deep-learning-methods class=hash-link aria-label="Direct link to Multi-Sensor Deep Learning Methods" title="Direct link to Multi-Sensor Deep Learning Methods">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p>This paper also discusses research methods that use advanced sensors, such as near-infrared, thermal imaging, and 3D depth sensors. These methods have the advantage of capturing more biological features and environmental changes, thus improving the model's accuracy and stability.<p>However, in practical life, RGB cameras remain the mainstream. Therefore, we will skip this section.<p>Interested readers can refer to the relevant chapters in the original paper.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p>Thanks to the rapid progress of deep learning technologies, FAS models have made significant strides in recent years.</p>
<p>According to most benchmark tests (e.g., OULU, SiW, CASIA-SURF), current models have achieved stable performance under major protocols, with ACER &lt; 5%, EER &lt; 10%, and HTER &lt; 15%. Particularly, with the help of sophisticated architectures (such as NAS-FAS, FAS-SGTD) and detailed pixel-level supervision (e.g., pseudo depth, reflection map), the recognition performance for 2D and some 3D attacks has been quite good.</p>
<p>However, these numbers are just part of the picture. When models step out of the laboratory and into the real world, many challenges remain:</p>
<ul>
<li>They collapse as soon as lighting changes.</li>
<li>They crash when the attack type changes.</li>
<li>They confidently make wrong predictions when data is sparse.</li>
</ul>
<p>Even though generalization-oriented designs (such as SSDG, SSAN, FGHV) have shown potential for cross-domain adaptation, overall, FAS remains an unsolved task.</p>
<p>The authors of this paper summarize the current challenges and potential future breakthroughs in the following five aspects.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=architecture-design-and-model-interpretability>Architecture Design, and Model Interpretability<a href=#architecture-design-and-model-interpretability class=hash-link aria-label="Direct link to Architecture Design, and Model Interpretability" title="Direct link to Architecture Design, and Model Interpretability">​</a></h3>
<p>Current mainstream methods still heavily rely on classic CNN architectures (such as ResNet, MobileNet) and manually designed supervision signals (e.g., depth maps, masks). While these configurations are stable, they often struggle in real-world scenarios with high data diversity.</p>
<p>In the future, we can consider introducing automation mechanisms, such as:</p>
<ul>
<li>Using <strong>AutoML</strong> to search for the best model architectures under different modalities and temporal conditions;</li>
<li>Automatically designing reasonable fusion strategies for multi-modal inputs (e.g., RGB, Depth, NIR), replacing manual stacking;</li>
<li>Designing lightweight models so that FAS is not limited to servers but can also be deployed on smartphones and IoT devices;</li>
<li>Strengthening interpretability: from Grad-CAM to spoof maps and even natural language generation for prediction explanations, making the model’s decision process less of a black box.</li>
</ul>
<p>Enabling the model to explain why it recognizes a face as fake will be a key step toward trusted FAS systems in the future.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=representation-learning>Representation Learning<a href=#representation-learning class=hash-link aria-label="Direct link to Representation Learning" title="Direct link to Representation Learning">​</a></h3>
<p>The essence of FAS is actually an advanced application of representation learning. The problem is not about capturing "differences," but about whether the differences captured are truly related to spoofing.</p>
<p>Future focus can be directed toward several areas:</p>
<ul>
<li>Introducing <strong>transfer learning</strong> and large pre-trained models (e.g., ViT, SAM) to transfer general visual knowledge;</li>
<li>Achieving <strong>feature disentanglement</strong>: separating spoof-specific signals from identity, lighting, and image quality;</li>
<li>Combining <strong>metric learning</strong> (e.g., Triplet, Hypersphere Loss) to separate the feature distributions of bonafide and spoof;</li>
<li>Using <strong>self-supervised learning</strong> to learn regional contrasts from unlabeled patches and build detail recognition capabilities;</li>
<li>More advanced adversarial data augmentation techniques, such as synthetic reflections, micro-movement distortions, and extreme materials, to allow the model to make judgments even in the face of unfamiliar attacks.</li>
</ul>
<p>In simple terms, the goal is to make the model learn: "I not only know you don’t look alive, I know exactly <em>where</em> you don’t look like one."</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=real-world-open-set-testing>Real-World Open-Set Testing<a href=#real-world-open-set-testing class=hash-link aria-label="Direct link to Real-World Open-Set Testing" title="Direct link to Real-World Open-Set Testing">​</a></h3>
<p>Current FAS test protocols are mostly "small-scale, single-factor, closed-set," for example, only testing lighting variations or a single spoof type. However, real-world scenarios involve more than one changing variable, and even the attackers themselves can vary.</p>
<p>Therefore, <strong>establishing a practical open-set test framework</strong> will be a necessary condition for evaluating the next generation of models.</p>
<p>Future directions could include:</p>
<ul>
<li>GrandTest or cross-protocol testing: exposing the model to unseen domains + spoof;</li>
<li>Multi-modal training: if the model is trained using only RGB, can it generalize when exposed to RGB + Depth? Can it predict pseudo-modalities?</li>
<li>Mixed multi-protocol testing: for example, random combinations of RGB-NIR-D to simulate inconsistent sensor scenarios.</li>
</ul>
<p>If past tests were designed to make the model pass, future tests should aim to make the model fail, because only when it fails can we truly understand what it has learned.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=universal-and-integrated-attack-detection>Universal and Integrated Attack Detection<a href=#universal-and-integrated-attack-detection class=hash-link aria-label="Direct link to Universal and Integrated Attack Detection" title="Direct link to Universal and Integrated Attack Detection">​</a></h3>
<p>With the rise of digital forgeries like Deepfake, morphing, and GAN-generated attacks, the scope of FAS is gradually expanding. Future anti-spoofing tasks will no longer be just about whether a face is real, but rather:</p>
<blockquote>
<p><strong>Is the origin, integrity, and authenticity of this image trustworthy?</strong></p>
</blockquote>
<p>This means FAS models must begin to possess:</p>
<ul>
<li><strong>Multi-task learning capabilities</strong>, learning common spoof patterns from multiple domains, such as documents, products, faces, etc.;</li>
<li><strong>Cross-domain knowledge integration</strong>, modeling both physical spoofing (e.g., glasses, makeup) and digital spoofing (e.g., StyleGAN, FaceSwap);</li>
<li><strong>Adversarial attack defense</strong>, including the recognition and resistance of physical spoofing techniques like glasses, stickers, and special patterns.</li>
</ul>
<p>Universal PAD (Presentation Attack Detection) will be the next critical turning point, marking the final step toward the commercialization of FAS applications.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=privacy-preserving-training-methods>Privacy-Preserving Training Methods<a href=#privacy-preserving-training-methods class=hash-link aria-label="Direct link to Privacy-Preserving Training Methods" title="Direct link to Privacy-Preserving Training Methods">​</a></h3>
<p>With the introduction of privacy laws such as GDPR, the practice of "centralizing data for training" is becoming more challenging. Facing the reality that data cannot be shared or labeled, model training must shift to a "data stays put, model moves" strategy.</p>
<p>Promising directions include:</p>
<ul>
<li><strong>Federated Learning</strong>: Each client trains locally, then aggregates model parameters, balancing privacy and learning efficiency;</li>
<li><strong>Source-Free Learning</strong>: The model is publicly available but data is sealed, continuing to learn autonomously post-deployment using pseudo-labels;</li>
<li><strong>Risk control in private data learning</strong>: How to prevent the model from memorizing user-specific features and how to achieve reliable de-identification will be the new battlefield combining FAS and AI security.</li>
</ul>
<p>Being able to "determine whether you're real, without knowing who you are" will be the new challenge for FAS in the privacy era.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Past FAS systems were designed to block paper and mobile spoofing; today’s FAS is facing server-side generation, cross-domain attacks, and camouflage-based interferences. On this evolutionary path, we’ve transitioned from classification models to generative models, from closed-set to open-set, and from single supervision to multi-task learning and self-supervised optimization.</p>
<p>But it's still not enough.</p>
<p>FAS is the first line of defense against attackers, yet it remains the most vulnerable part of AI systems. The future of FAS requires integrated systems capable of cross-modal understanding, cross-data learning, and cross-scenario survival. It should be able to assess trustworthiness, adjust strategies, identify risks, and adapt in the face of unknown attacks.</p>
<p>The exploration of this field is still ongoing, and the road ahead remains long.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-04-30T01:16:24.000Z itemprop=dateModified>Apr 30, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/face-antispoofing/echo-fas/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[22.08] Echo-FAS</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/face-antispoofing/cdftn/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[22.12] CDFTN</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#a-chronicle-of-attacks-and-defenses class="table-of-contents__link toc-highlight">A Chronicle of Attacks and Defenses</a><li><a href=#fas-framework class="table-of-contents__link toc-highlight">FAS Framework</a><ul><li><a href=#overview-of-deep-learning-methods class="table-of-contents__link toc-highlight">Overview of Deep Learning Methods</a><li><a href=#evaluation-metrics class="table-of-contents__link toc-highlight">Evaluation Metrics</a><li><a href=#evaluation-protocols class="table-of-contents__link toc-highlight">Evaluation Protocols</a></ul><li><a href=#rgb-based-fas class="table-of-contents__link toc-highlight">RGB-based FAS</a><li><a href=#hybrid-methods class="table-of-contents__link toc-highlight">Hybrid Methods</a><li><a href=#traditional-deep-learning-methods class="table-of-contents__link toc-highlight">Traditional Deep Learning Methods</a><ul><li><a href=#binary-classification-supervision class="table-of-contents__link toc-highlight">Binary Classification Supervision</a><li><a href=#pixel-level-supervision class="table-of-contents__link toc-highlight">Pixel-level Supervision</a></ul><li><a href=#generalization-oriented-deep-learning-methods class="table-of-contents__link toc-highlight">Generalization-Oriented Deep Learning Methods</a><ul><li><a href=#facing-unknown-environments class="table-of-contents__link toc-highlight">Facing Unknown Environments</a><li><a href=#facing-unknown-attacks class="table-of-contents__link toc-highlight">Facing Unknown Attacks</a></ul><li><a href=#multi-sensor-deep-learning-methods class="table-of-contents__link toc-highlight">Multi-Sensor Deep Learning Methods</a><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#architecture-design-and-model-interpretability class="table-of-contents__link toc-highlight">Architecture Design, and Model Interpretability</a><li><a href=#representation-learning class="table-of-contents__link toc-highlight">Representation Learning</a><li><a href=#real-world-open-set-testing class="table-of-contents__link toc-highlight">Real-World Open-Set Testing</a><li><a href=#universal-and-integrated-attack-detection class="table-of-contents__link toc-highlight">Universal and Integrated Attack Detection</a><li><a href=#privacy-preserving-training-methods class="table-of-contents__link toc-highlight">Privacy-Preserving Training Methods</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>