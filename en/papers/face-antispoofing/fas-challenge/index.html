<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-face-antispoofing/fas-challenge/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.0"><title data-rh=true>[24.04] FAS-Challenge | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/face-antispoofing/fas-challenge/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[24.04] FAS-Challenge | DOCSAID"><meta data-rh=true name=description content=Arsenal><meta data-rh=true property=og:description content=Arsenal><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/face-antispoofing/fas-challenge/><link data-rh=true rel=alternate href=https://docsaid.org/papers/face-antispoofing/fas-challenge/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/face-antispoofing/fas-challenge/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/face-antispoofing/fas-challenge/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/face-antispoofing/fas-challenge/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/face-anti-spoofing-40","name":"Face Anti-Spoofing (40)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/face-antispoofing/fas-challenge/","name":"[24.04] FAS-Challenge","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.72575d1c.js defer></script><script src=/en/assets/js/main.ddf711aa.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/face-antispoofing/fas-challenge/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/face-antispoofing/fas-challenge/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/face-antispoofing/fas-challenge/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-5uvb3z ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns-11>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/contrastive-learning-13>Contrastive Learning (13)</a><button aria-label="Expand sidebar category 'Contrastive Learning (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek-5>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/face-anti-spoofing-40>Face Anti-Spoofing (40)</a><button aria-label="Collapse sidebar category 'Face Anti-Spoofing (40)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/slrbd/>[10.09] SLRBD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/lbp/>[12.09] LBP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/three-d-mad/>[14.05] 3DMAD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/rppg/>[16.12] rPPG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/oulu-npu/>[17.06] OULU-NPU</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cfrppg/>[18.09] CFrPPG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/vafas/>[19.05] VA-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/wmca/>[19.09] WMCA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cdcn/>[20.03] CDCN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cefa/>[20.03] CeFA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/ssdg/>[20.04] SSDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/celeba-spoof/>[20.07] CelebA-Spoof</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/stdn/>[20.07] STDN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/disentangle-fas/>[20.08] Disentangle-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/d2am/>[21.05] D²AM</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/dualstage/>[21.10] DualStage</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/dsdg/>[21.12] DSDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/personalized-fas/>[22.01] Personalized-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/ssan/>[22.03] SSAN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/echo-fas/>[22.08] Echo-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/fas-survey/>[22.10] FAS Survey</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cdftn/>[22.12] CDFTN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/divt/>[23.01] DiVT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/m2a2e/>[23.02] M²A²E</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/sa-fas/>[23.03] SA-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/iadg/>[23.04] IADG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/ma-vit/>[23.04] MA-ViT</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/flip/>[23.09] FLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/s-adapter/>[23.09] S-Adapter</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/udg-fas/>[23.10] UDG-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/three-a-tta/>[23.11] 3A-TTA</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/mmdg/>[24.02] MMDG</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/shield/>[24.02] SHIELD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/aface/>[24.03] AFace</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/cfpl-fas/>[24.03] CFPL-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/face-antispoofing/fas-challenge/>[24.04] FAS-Challenge</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/pd-fas/>[24.04] PD-FAS</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/la-softmoe/>[24.08] La-SoftMoE</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/fm-clip/>[24.10] FM-CLIP</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/face-antispoofing/i-fas/>[25.01] I-FAS</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition-4>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion-10>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation-1>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight-10>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba-4>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning-8>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality-24>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization-1>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection-8>Object Detection (8)</a><button aria-label="Expand sidebar category 'Object Detection (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization-8>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation-1>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection-14>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition-20>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting-4>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers-17>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers-12>Vision Transformers (12)</a><button aria-label="Expand sidebar category 'Vision Transformers (12)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 215 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/face-anti-spoofing-40><span>Face Anti-Spoofing (40)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[24.04] FAS-Challenge</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[24.04] FAS-Challenge</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=arsenal>Arsenal<a href=#arsenal class=hash-link aria-label="Direct link to Arsenal" title="Direct link to Arsenal">​</a></h2>
<p><a href=https://arxiv.org/abs/2404.06211 target=_blank rel="noopener noreferrer"><strong>Unified Physical-Digital Attack Detection Challenge</strong></a></p>
<hr>
<p>This is the FAS competition held at CVPR2024, officially titled:</p>
<p><a href=https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024 target=_blank rel="noopener noreferrer"><strong>5th Chalearn Face Anti-spoofing Workshop and Challenge@CVPR2024</strong></a></p>
<p>It’s a lively event that everyone working in the FAS field can check out.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-definition>Problem Definition<a href=#problem-definition class=hash-link aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>The essence of the Face Anti-Spoofing (FAS) task is to mine the "syntax" of liveness from images.</p>
<p>However, in reality, attack methods have evolved into two parallel technical branches:</p>
<ul>
<li><strong>Physical Attacks (PA):</strong> The impersonator presents the face through physical media such as paper, screens, or silicone masks. These interferences mostly occur at the sensor level and have direct interaction with the real world.</li>
<li><strong>Digital Attacks (DA):</strong> Such as Deepfake, face swapping, or adversarial examples, which manipulate from the data generation source or feature level. Although visually realistic, their essence is synthetic.</li>
</ul>
<p>Traditional methods mostly model one of these categories, thus model performance is limited to their training distribution and cannot generalize broadly.</p>
<p>Although both physical and digital attacks belong to the “fake” class in final classification, their image statistical features and variation directions are highly heterogeneous. This causes intra-class feature distances far larger than expected, leading to a generalization bottleneck.</p>
<p>Currently, the core reasons why it is difficult to build a "unified model" are twofold:</p>
<ol>
<li><strong>Lack of large-scale unified datasets:</strong> Past works typically splice separately collected PA and DA datasets, without covering complete attack types for the same ID.</li>
<li><strong>Absence of public evaluation benchmarks:</strong> Physical and digital attacks use different metrics and protocols, making consistent comparison across domains impossible.</li>
</ol>
<p>This is the background for initiating the <strong>Unified Physical-Digital Attack Detection Challenge</strong>. Through new datasets, standard protocols, and open competitions, it attempts to define a new problem setting:</p>
<blockquote>
<p><strong>Can a single model handle two heterogeneous spoofing types simultaneously while maintaining discriminative power in unseen domains?</strong></p>
</blockquote>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=unified-dataset-uniattackdata>Unified Dataset: UniAttackData<a href=#unified-dataset-uniattackdata class=hash-link aria-label="Direct link to Unified Dataset: UniAttackData" title="Direct link to Unified Dataset: UniAttackData">​</a></h2>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=uniattackdata src=/en/assets/images/img1-8a5cd34f0ce53328e21f3125ac8ac38b.jpg width=996 height=532 class=img_ev3q></figure></div>
<p><strong>UniAttackData</strong> is currently the largest and most comprehensively designed unified attack dataset, covering 1,800 subjects and totaling 28,706 facial video clips, consisting of the following three sample types:</p>
<ul>
<li><strong>Live:</strong> 1,800 genuine video clips</li>
<li><strong>Physical Attacks (PA):</strong> 5,400 clips, including print, screen, 3D masks, etc.</li>
<li><strong>Digital Attacks (DA):</strong> 21,506 clips, including Deepfake, Face Swap, adversarial examples, etc.</li>
</ul>
<p>The dataset’s key feature is that <strong>each ID has complete corresponding attack samples</strong>, ensuring the model’s learning process does not skew due to unbalanced attack type distribution. This design avoids models over-relying on irrelevant secondary features such as identity, ethnicity, or lighting, refocusing training on detecting the “spoof” itself.</p>
<p>To prevent models from prematurely exploiting dataset artifacts, the research team performed meticulous preprocessing: face regions were cropped and name steganography applied, ensuring no extraneous clues remain at the pixel level in any image.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=evaluation-protocols-and-generalization-design>Evaluation Protocols and Generalization Design<a href=#evaluation-protocols-and-generalization-design class=hash-link aria-label="Direct link to Evaluation Protocols and Generalization Design" title="Direct link to Evaluation Protocols and Generalization Design">​</a></h3>
<p>The challenge includes two main protocols simulating different real-world deployment scenarios:</p>
<ul>
<li>
<p><strong>Protocol 1: Unified Attack Detection</strong>
Simulates scenarios where the model must simultaneously recognize both PA and DA, testing the model’s integration and classification capability for mixed attack types.</p>
</li>
<li>
<p><strong>Protocol 2: Generalization to Unseen Attacks</strong>
Zero-shot testing for “unseen attack types,” further divided into:</p>
<ul>
<li><strong>Protocol 2.1: Unseen Digital Attacks</strong></li>
<li><strong>Protocol 2.2: Unseen Physical Attacks</strong></li>
</ul>
</li>
</ul>
<p>This protocol adopts a leave-one-type-out strategy, meaning the model completely excludes one attack type during training, forcing it to learn more semantically generalizable discriminative logic.</p>
<hr>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=competition-process-and-rules>Competition Process and Rules<a href=#competition-process-and-rules class=hash-link aria-label="Direct link to Competition Process and Rules" title="Direct link to Competition Process and Rules">​</a></h3>
<p>The overall competition runs in two phases on the CodaLab platform:</p>
<ul>
<li>
<p><strong>Development Phase (2/1–2/22)</strong>
Provides labeled training data and unlabeled development set; participants can repeatedly submit predictions to the leaderboard for iterative model tuning.</p>
</li>
<li>
<p><strong>Final Phase (2/23–3/3)</strong>
Reveals development set labels and releases an unlabeled test set. Teams must submit final predictions without access to true test labels. The last submission counts as the official score, and teams must publicly release their code and fact sheets to qualify for awards.</p>
</li>
</ul>
<hr>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=evaluation-metrics>Evaluation Metrics<a href=#evaluation-metrics class=hash-link aria-label="Direct link to Evaluation Metrics" title="Direct link to Evaluation Metrics">​</a></h3>
<p>The challenge uses the ISO/IEC 30107-3 international standard metrics to quantify model spoof detection ability, including:</p>
<ul>
<li><strong>APCER</strong> (Attack Presentation Classification Error Rate)</li>
<li><strong>BPCER</strong> (Bona Fide Presentation Classification Error Rate)</li>
<li><strong>ACER</strong> (Average Classification Error Rate)</li>
<li><strong>AUC</strong> (Area Under the ROC Curve)</li>
</ul>
<p>The main ranking metric is ACER, with AUC as a secondary indicator. To maintain evaluation consistency, the final ACER is computed using a threshold calibrated by the Equal Error Rate (EER) on the development set.</p>
<hr>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=arsenal-1>Arsenal<a href=#arsenal-1 class=hash-link aria-label="Direct link to Arsenal" title="Direct link to Arsenal">​</a></h2>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=participants src=/en/assets/images/img2-1523068332cc59f1541c54bc01e76275.jpg width=984 height=960 class=img_ev3q></figure></div>
<p>Next is a technical overview of the top 13 participating teams. Let’s take a closer look at what they actually did.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=1-mtface>1. MTFace<a href=#1-mtface class=hash-link aria-label="Direct link to 1. MTFace" title="Direct link to 1. MTFace">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=mtface src=/en/assets/images/img3-c251938668e977ee1c78a4a9b56cdd56.jpg width=1224 height=664 class=img_ev3q></figure></div>
<p>In this challenge spanning both digital and physical attacks, the <strong>MTFace</strong> team proposed the ultimate winning solution.</p>
<p>The MTFace architecture can be briefly described as:</p>
<blockquote>
<p><strong>Optimized Data Augmentation for Comprehensive Face Attack Detection Across Physical and Digital Domains</strong></p>
</blockquote>
<p>The name sounds long but captures the core issue: how to expose the model during training to "sufficiently diverse and realistic" spoof appearances.</p>
<p>The core of MTFace lies in the joint design of data augmentation and loss balancing.</p>
<p>Its data preprocessing steps are as follows:</p>
<ol>
<li><strong>Face detection and cropping:</strong> Detect faces in all images, with an additional 20-pixel margin to retain peripheral features.</li>
<li><strong>Face mask extraction:</strong> Pre-extract masks of live data faces for subsequent augmentation use.</li>
</ol>
<p>Then, for the generalization requirements of different protocols, two "semantically aligned augmentation strategies" were designed:</p>
<ul>
<li>
<p><strong>Moire simulation augmentation</strong> (for Protocols 1 and 2.1)
Screen replay often causes moire patterns, which become important visual cues for attack recognition.
MTFace simulates this interference by injecting moire patterns into original images, enabling the model to anticipate this physical phenomenon’s potential impact.</p>
</li>
<li>
<p><strong>Self-fusion augmentation</strong> (for Protocols 1 and 2.2)
Inspired by previous literature, MTFace uses live data as a base and injects surface features of digital attacks.
Through color distortion, spatial deformation, and mask folding, it creates "hybrid samples of digital spoofs," improving the model’s discrimination against Deepfake-type attacks.</p>
</li>
</ul>
<p>The ratio between genuine and attack samples varies greatly across protocols.</p>
<p>Without adjustment, the model tends to bias toward the more numerous class, losing the ability to recognize rare types.</p>
<p>To address this, MTFace adjusts cross-entropy weight configurations per protocol:</p>
<ul>
<li><strong>Protocol 1: live : fake = 1 : 1</strong>
Maintain balanced loss to ensure unbiased learning.</li>
<li><strong>Protocol 2.1 (Unseen DA): live : fake = 5 : 1</strong>
Strengthen learning of live features to boost defense against digital attacks.</li>
<li><strong>Protocol 2.2 (Unseen PA): live : fake = 2 : 1</strong>
Control the physical attack sample proportion to avoid over-reliance on moire patterns and mask contours.</li>
</ul>
<p>Such meticulous tuning allows the model to precisely focus on key features under different protocols while maintaining generalization resilience.</p>
<p>MTFace ultimately adopts <strong>ResNet-50 as backbone</strong>, loading <strong>ImageNet pretrained weights</strong>.</p>
<p>No ViT, no custom large model—just strategy and data, clinching first place in this competition.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=2-searecluse>2. SeaRecluse<a href=#2-searecluse class=hash-link aria-label="Direct link to 2. SeaRecluse" title="Direct link to 2. SeaRecluse">​</a></h3>
<p><strong>SeaRecluse</strong> chose to build defenses through data boundaries and proportion configurations.</p>
<p>Their solution is named:</p>
<blockquote>
<p><strong>Cross-domain Face Anti-spoofing in Unified Physical-Digital Attack Dataset</strong></p>
</blockquote>
<p>This architecture does not emphasize extreme transformations or style transfer. Instead, it focuses on <strong>data usage ratios and cropping strategies</strong> to optimize model stability and generalization in cross-domain tasks, closer to real deployment conditions.</p>
<p>SeaRecluse uses <strong>SCRFD</strong> to detect and crop faces from uncropped images in the training set.</p>
<p>Unlike other teams, they distinguish and supplement between loose and tight crops as one form of data augmentation.</p>
<p>Additionally, for different task protocols, <strong>data split ratios and augmentation strategies differ completely</strong>:</p>
<ul>
<li>
<p><strong>Protocol 1: No additional augmentation</strong>
Simulate baseline recognition performance, training on 80% training data mixed with validation set.</p>
</li>
<li>
<p><strong>Protocol 2.1 (Unseen DA):</strong>
Downsample and pad live data edges, expanding real face data to <strong>3×</strong> the original to balance real and fake samples.</p>
</li>
<li>
<p><strong>Protocol 2.2 (Unseen PA):</strong>
To enhance model perception of fake samples, downsample fake faces by <strong>4× and 8×</strong>, totaling <strong>7×</strong> the original amount.</p>
</li>
</ul>
<p>The team also corrected some images with abnormal aspect ratios to restore reasonable proportions, preventing the model from learning erroneous biases due to visual distortion.</p>
<p>For image augmentation, all tasks applied standard operations (flipping, random cropping, etc.), with Protocol 2.1 additionally using <strong>Gaussian Blur</strong> to simulate photographic blur and distant-view defocus.</p>
<p>The backbone network is <strong>ConvNeXt V2</strong>; chosen as a trade-off between performance and computational cost considering resource efficiency and challenge demands.</p>
<p>To further strengthen generalization, the team applied two training techniques:</p>
<ul>
<li><strong>Image CutMix:</strong> Mixes two images and labels to improve adaptation to visual boundaries and spatial variations.</li>
<li><strong>Label Smoothing:</strong> Converts hard labels to soft labels to reduce overfitting risk.</li>
</ul>
<p>These techniques help the model focus on semantic-level features and mitigate overfitting caused by data imbalance.</p>
<p>SeaRecluse’s approach is not drastic but rather wall-building—starting from task-specific data ratios and cropping scales to confine spoofing within the model’s field of vision.</p>
<p>It is a patient solution and a deployment-oriented mindset.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=3-duileduile>3. duileduile<a href=#3-duileduile class=hash-link aria-label="Direct link to 3. duileduile" title="Direct link to 3. duileduile">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=duileduile src=/en/assets/images/img4-e8bd4c8d043ae908fb86df3aeeb387ab.jpg width=1064 height=688 class=img_ev3q></figure></div>
<p>The <strong>duileduile</strong> team designed a modular two-stage learning pipeline that combats the fluid boundary between real and spoof through abstract visual representations. The core of their architecture is:</p>
<blockquote>
<p><strong>Swin Transformer + Masked Image Modeling + Physical and Digital Attack Corresponding Augmentation</strong></p>
</blockquote>
<p>The backbone model is <strong>Swin-Large Transformer</strong>, extracting a 1536-dimensional feature vector, with strong regional awareness and hierarchical abstraction capabilities.</p>
<p>In the <strong>pretraining stage</strong>, duileduile employs the <strong>simMIM (Simple Masked Image Modeling)</strong> strategy,
which partitions the image into non-overlapping patches and randomly masks some parts, forcing the model to learn “reconstructing the whole from incomplete inputs.”</p>
<p>This self-supervised method effectively enhances the model’s robustness in scenarios with feature loss or occlusion attacks,
especially benefiting zero-shot tests on unseen attack types in Protocol 2.</p>
<p>After training the visual syntax, the process moves to the <strong>fine-tuning stage</strong>. At this point, the strategy shifts from large-scale data stacking to precise pattern fitting per attack type, with corresponding augmentation procedures designed as follows:</p>
<ul>
<li><strong>Gaussian Noise:</strong> Simulates pixel-level noise and compression artifacts caused by digital attacks</li>
<li><strong>ColorJitter + Moire Pattern + Gamma Correction:</strong> Reconstructs light and shadow variations and display biases of physical attacks</li>
</ul>
<p>These augmentations are not uniformly applied to all data but probabilistically applied according to different training samples and protocol tasks, exposing the model to varied disturbances and spoofs each time it learns.</p>
<p>Compared to the first two teams focusing on data ratios and semantic augmentation, duileduile’s method is closer to a platform-like anti-spoof strategy. It applies consistent settings across protocols, possessing high transfer potential and structural consistency.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=4-bsp-idiap>4. BSP-Idiap<a href=#4-bsp-idiap class=hash-link aria-label="Direct link to 4. BSP-Idiap" title="Direct link to 4. BSP-Idiap">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=bsp-idiap src=/en/assets/images/img5-3934b23d1af1d92f25595e9655148b29.jpg width=1224 height=492 class=img_ev3q></figure></div>
<p>The <strong>BSP-Idiap</strong> team took a different path: <strong>returning to the signal’s intrinsic texture rhythm</strong> by reconstructing attack signals in the frequency domain.</p>
<p>Their method is called: <strong>DBPixBiS (Dual-Branch Pixel-wise Binary Supervision)</strong></p>
<p>Building upon their prior design philosophy, they expanded it into a dual-branch architecture.</p>
<p><strong>DBPixBiS</strong> employs a dual-branch neural network structure:</p>
<ol>
<li><strong>RGB branch:</strong> Uses <strong>Central Difference Convolution (CDC)</strong> instead of traditional convolution, emphasizing local texture changes and enhancing the model’s perception of abnormal edges and subtle variation areas.</li>
<li><strong>Fourier branch:</strong> Applies Fourier transform to input images and feeds them into a separate feature path, capturing spoofing textures in the frequency domain, such as repetitiveness, interference, and compression residues.</li>
</ol>
<p>This design enables the model to simultaneously perceive “visual anomalies” in the image and “signal-level spoofing” within the data.</p>
<p>To mitigate overfitting and the subtlety of adversarial examples, BSP-Idiap adopts highly targeted training designs:</p>
<ul>
<li><strong>Pixel-wise Binary Supervision:</strong> Supervises binary classification at each pixel on the feature map instead of the entire image, improving recognition of localized spoofing.</li>
<li><strong>Attentional Angular Margin Loss:</strong> Adds an angular margin penalty during training, guiding the model to distinctly separate live and spoof feature vectors, reinforcing stable class boundaries.</li>
</ul>
<p>During testing, the model performs global average pooling on the feature map and uses the <strong>mean activation</strong> as the final spoof probability score.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=5-vai-face>5. VAI-Face<a href=#5-vai-face class=hash-link aria-label="Direct link to 5. VAI-Face" title="Direct link to 5. VAI-Face">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=vaiface src=/en/assets/images/img6-9908645fe3b714d593151a33d5ebacad.jpg width=1064 height=520 class=img_ev3q></figure></div>
<p>In this unified recognition battle, some teams build mechanisms, some restore syntax, and the <strong>VAI-Face</strong> team chose a third way: <strong>create anomalies to strengthen recognition.</strong></p>
<p>Their solution centers on the <strong>Dinov2 Vision Transformer (ViT-Large)</strong> architecture, aiming to <strong>expose flaws in distorted faces through visual attention.</strong></p>
<p>Their key data augmentation strategy is the <strong>deliberate asymmetric treatment of live and fake images.</strong></p>
<p>They treat the two as semantically heterogeneous sources and thus apply drastically different augmentation pipelines:</p>
<ul>
<li><strong>Live images:</strong> Only undergo <strong>RandomResizedCrop</strong> and <strong>HorizontalFlip</strong>, preserving natural distribution and geometric stability.</li>
<li><strong>Fake images:</strong> Subject to heavy asymmetric perturbations such as blur, distortion, and custom cutout occlusions, simulating various unnatural traces and structural breakages common in spoof images.</li>
</ul>
<p>This strategy trains the ViT model to identify abnormal geometry and texture.</p>
<p>Beyond data strategies, VAI-Face exhibits high engineering sensitivity in training configuration:</p>
<ul>
<li><strong>OneCycleLR:</strong> Precisely controls learning rate ramp-up and decay to improve convergence efficiency and generalization.</li>
<li><strong>Label Smoothing:</strong> Prevents overconfidence on specific patterns, reducing overfitting.</li>
<li><strong>Mixup Augmentation:</strong> Combines two images and labels to improve robustness at sample space boundaries.</li>
<li><strong>Optimizer:</strong> Uses ADAN, a novel optimizer combining adaptive gradients with momentum, providing more stable gradient dynamics in this challenge.</li>
</ul>
<p>ViT-Large inherently possesses strong regional relational modeling ability. When processing spoof images, its global attention mechanism captures subtle inconsistencies, amplifying flaws hard to conceal. They do not add extra branches or generative modules but maximize the backbone’s recognition potential through finely tuned data perturbations and learning schedules, squeezing out generalization from a simple architecture.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=6-llw>6. L&L&W<a href=#6-llw class=hash-link aria-label="Direct link to 6. L&L&W" title="Direct link to 6. L&L&W">​</a></h3>
<p>While most models choose holistic input and full-image classification, the <strong>L&L&W</strong> team took the opposite approach, starting from local details: slicing a face into countless fragments and piecing together subtle clues of authenticity or spoof.</p>
<p>Their core strategy revolves around:</p>
<blockquote>
<p><strong>Patch-based Feature Learning + Frequency-guided Sampling + Local Attention Guidance</strong></p>
</blockquote>
<p>The process begins with image patch extraction, splitting each input image into multiple small regions for independent feature learning. On this foundation, they introduce the <strong>Centre Difference Attention (CDA)</strong> mechanism.</p>
<p>CDA is an attention method enhancing subtle texture differences, focusing on regions that "should be consistent but show minor variations," such as edge halos, misaligned reassembly, and low-frequency fusion failures.</p>
<p>Beyond spatial details, L&L&W also leverage hidden frequency-domain clues. They designed the <strong>High-Frequency Wavelet Sampler (HFWS)</strong> module to concentrate on high-frequency bands in images, aiming to detect compression artifacts, fusion distortions, and unnatural texture discontinuities left during forgery.</p>
<p>This dual-domain fused feature strategy incorporates spatial attention and frequency intensity into the recognition process, allowing the model not only to see "where it looks odd" but also to hear "at which frequencies the anomaly lies."</p>
<p>To improve prediction stability and multi-angle coverage, the team generates <strong>36 different cropped versions</strong> per image during testing. Each patch is independently evaluated by the model, and results are averaged to form the final score.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=7-sarm>7. SARM<a href=#7-sarm class=hash-link aria-label="Direct link to 7. SARM" title="Direct link to 7. SARM">​</a></h3>
<p>The <strong>SARM</strong> team’s design focuses not on complex structures or data augmentation but on a repeated “training the trainer” process.</p>
<p>Their method is called:</p>
<blockquote>
<p><strong>Multi-Attention Training (MAT) + Label Flip Augmentation</strong></p>
</blockquote>
<p>This is a staged visual refinement approach: first, train a model to understand the task, then use this model to finely tune and adapt to different attack types.</p>
<p>SARM splits the process into two phases:</p>
<ol>
<li>
<p><strong>Phase One:</strong> Train dedicated pre-detectors for each protocol (P1, P2.1, P2.2) using <strong>Supervised Contrastive Learning</strong> to maximize semantic distance between live and fake representations.</p>
</li>
<li>
<p><strong>Phase Two:</strong> Fine-tune the actual anti-spoofing model starting from phase one’s representations as initial weights.</p>
</li>
</ol>
<p>This design strengthens “expected object understanding” per task, reducing misleading effects common in direct end-to-end training, especially effective for cross-domain generalization tasks (P2.1 and P2.2).</p>
<p>In data augmentation, SARM innovates by <strong>transforming live faces into fake ones and labeling them as fake.</strong></p>
<p>This is not ordinary synthetic spoofing but a weak spoof simulation using <strong>OpenCV style transformations</strong>.</p>
<p>For training data in P2.1 and P2.2, they apply style variations including hue shifts, lighting changes, gamma adjustments, and camouflage filters.</p>
<p>These processed live images are labeled as <strong>spoof</strong>, creating a <strong>label-flip augmentation</strong> set to generate more diverse, distribution-closer “weak attack samples,” thereby narrowing the domain gap.</p>
<p>The optimizer is <strong>Adam</strong>, combined with cross-entropy and contrastive losses for stable training. Protocol P1 uses standard strategies for stable convergence, while P2.1/P2.2 introduces enhanced augmentation.</p>
<p>This method mainly relies on "prior task intention understanding" to drive model convergence.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=8-m2-purdue>8. M2-Purdue<a href=#8-m2-purdue class=hash-link aria-label="Direct link to 8. M2-Purdue" title="Direct link to 8. M2-Purdue">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=m2-purdue src=/en/assets/images/img7-b8999c9986b76d02b625d8ce0c0bd160.jpg width=1064 height=368 class=img_ev3q></figure></div>
<p>The <strong>M2-Purdue</strong> team offers a different strategy.</p>
<p>They abandoned complex architecture design and heavy data augmentation, opting to use CLIP’s semantic representations combined with an “extreme risk–oriented” loss, named:</p>
<blockquote>
<p><strong>Robust Face Attack Detection with CLIP + MLP + CVAR–AUC Loss Fusion</strong></p>
</blockquote>
<p>The pipeline starts with standard image preprocessing, resizing all inputs to <strong>224×224</strong> to ensure scale consistency.</p>
<p>Next, they extract semantic features via <strong>CLIP’s image encoder</strong>, converting visual information into deep embedding representations.</p>
<p>The key here is not creating new features but “leveraging a model pretrained on massive data,” allowing CLIP to output generalized semantic features. They then attach a three-layer <strong>MLP classifier</strong> as the task-specific decision module — the only part requiring fine-tuning — minimalist yet precise, fitting modern lightweight deployment needs.</p>
<p>The most distinctive aspect is the dual loss design, integrating two supervisory signals:</p>
<ul>
<li><strong>CVAR (Conditional Value at Risk) Loss:</strong> Common in financial risk control, focusing on tail risk—the hardest-to-classify, highest-risk sample regions.</li>
<li><strong>AUC (Area Under Curve) Loss:</strong> Emphasizes overall discriminative ability, optimizing the model’s perception of correct ranking.</li>
</ul>
<p>The overall loss is defined as:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi mathvariant=script>L</mi><mo>=</mo><mi>λ</mi><mo>⋅</mo><msub><mi mathvariant=script>L</mi><mrow><mi mathvariant=normal>C</mi><mi mathvariant=normal>V</mi><mi mathvariant=normal>A</mi><mi mathvariant=normal>R</mi></mrow></msub><mo>+</mo><mo stretchy=false>(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo stretchy=false>)</mo><mo>⋅</mo><msub><mi mathvariant=script>L</mi><mrow><mi mathvariant=normal>A</mi><mi mathvariant=normal>U</mi><mi mathvariant=normal>C</mi></mrow></msub></mrow><annotation encoding=application/x-tex>\mathcal{L} = \lambda \cdot \mathcal{L}_{\mathrm{CVAR}} + (1 - \lambda) \cdot \mathcal{L}_{\mathrm{AUC}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathcal">L</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">λ</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathcal">L</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">CVAR</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mopen>(</span><span class=mord>1</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal">λ</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathcal">L</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">AUC</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span></span>
<p>where parameter <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>λ</mi></mrow><annotation encoding=application/x-tex>\lambda</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">λ</span></span></span></span> balances “extreme risk sensitivity” and “overall generalization.”</p>
<p>The underlying logic is: not striving for all-round correctness, but minimizing errors in the “most error-prone zones.”</p>
<p>Training uses <strong>Adam</strong> for parameter updates without excessive hyperparameter tuning. Built on CLIP features and risk-aware objectives, this simple architecture achieves stable convergence and decent cross-domain recognition.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=9-cloud-recesses>9. Cloud Recesses<a href=#9-cloud-recesses class=hash-link aria-label="Direct link to 9. Cloud Recesses" title="Direct link to 9. Cloud Recesses">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=cloud-recesses src=/en/assets/images/img8-a95385ae759abf761b195b11da68c59b.jpg width=1224 height=202 class=img_ev3q></figure></div>
<p>If the essence of anti-spoofing is finding flaws on the face, then the <strong>Cloud Recesses</strong> team’s strategy provides a creative twist: <strong>masking the most recognizable parts, forcing the model to understand authenticity from subtler signals.</strong></p>
<p>Their method is called:</p>
<blockquote>
<p><strong>Random Masking for Face Anti-Spoofing Detection</strong></p>
</blockquote>
<p>This is a data-level adversarial training approach that directly obscures key facial regions, <strong>forcing the model to learn liveness recognition even without clear views of eyes and mouth.</strong></p>
<p>The overall process breaks down into three steps:</p>
<ol>
<li><strong>Face Detection:</strong> Use <strong>RetinaFace</strong> to crop faces from original images, standardizing them to 256×256 size.</li>
<li><strong>Keypoint Annotation:</strong> Use <strong>dlib</strong> to detect 68 facial landmarks, precisely outlining eyes, nose tip, lips, etc.</li>
<li><strong>Random Masking:</strong> For each training sample, randomly mask three to five key regions, depriving the model of shortcut cues relying on facial features.</li>
</ol>
<p>This design intentionally disrupts features the model might overly depend on (such as eye sclera texture and mouth contours), forcing it to learn more abstract, stable liveness cues like skin granularity, facial contour continuity, and local motion blur.</p>
<p>The backbone is <strong>EfficientNet</strong>, balancing accuracy and efficiency, suitable for lightweight recognition on masked images.</p>
<p>Training is straightforward without complex hyperparameter tuning, treating the masking strategy as a core data-level regularizer that imposes “visual stress,” compelling the model to reason with incomplete information.</p>
<p>Cloud Recesses’ solution involves no extra modules, no dual branches, no feature fusion — just removing key visual cues from the face to test if the model can recognize truth from darkness.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=10-image-lab>10. Image Lab<a href=#10-image-lab class=hash-link aria-label="Direct link to 10. Image Lab" title="Direct link to 10. Image Lab">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=image_lab src=/en/assets/images/img9-45f8e28df018874a2b203fa2a595e56d.jpg width=1224 height=592 class=img_ev3q></figure></div>
<p>In anti-spoofing detection tasks, we usually want the model to "see clearly," but the <strong>Image Lab</strong> team chose to have the model "see many times." Their architecture is called:</p>
<blockquote>
<p><strong>Multiattention-Net: A Deep Visual Recognition Network Composed of Multiple Attention Layers</strong></p>
</blockquote>
<p>This design systematically integrates multi-stage spatial information and multiple attentions, extracting possible anomaly cues from different dimensions of the image.</p>
<p>The network starts with a <strong>7×7 convolutional layer</strong> capturing local texture details, followed by ten layers of <strong>modified squeezed residual blocks</strong>, each paired with <strong>max pooling</strong>, progressively abstracting input information into increasingly global spatial semantics.</p>
<p>Throughout this process, spatial information is continuously extracted at each layer, finally followed by a <strong>Dual Attention Block</strong> that strengthens the recognition weights of key regions. This enables the model to discover spoofing traces both in fine details and structure.</p>
<p>Finally, <strong>Global Average Pooling (GAP)</strong> reduces dimensions, and all layer outputs are concatenated before entering a fully connected layer for classification.</p>
<p>During training, Image Lab uses <strong>Binary Focal Cross Entropy Loss</strong>, a loss function designed to penalize <strong>minority classes</strong> and <strong>high-confidence errors</strong> more strongly, with the formula:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi mathvariant=script>L</mi><mo stretchy=false>(</mo><mi>y</mi><mo separator=true>,</mo><mover accent=true><mi>y</mi><mo>^</mo></mover><mo stretchy=false>)</mo><mo>=</mo><mo>−</mo><mi>α</mi><mo>⋅</mo><mo stretchy=false>(</mo><mn>1</mn><mo>−</mo><mover accent=true><mi>y</mi><mo>^</mo></mover><msup><mo stretchy=false>)</mo><mi>γ</mi></msup><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mo stretchy=false>(</mo><mover accent=true><mi>y</mi><mo>^</mo></mover><mo stretchy=false>)</mo><mo>−</mo><mo stretchy=false>(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy=false>)</mo><mo>⋅</mo><msup><mover accent=true><mi>y</mi><mo>^</mo></mover><mi>γ</mi></msup><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mo stretchy=false>(</mo><mn>1</mn><mo>−</mo><mover accent=true><mi>y</mi><mo>^</mo></mover><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\mathcal{L}(y, \hat{y}) = -\alpha \cdot (1 - \hat{y})^\gamma \cdot \log(\hat{y}) - (1 - \alpha) \cdot \hat{y}^\gamma \cdot \log(1 - \hat{y})</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathcal">L</span><span class=mopen>(</span><span class="mord mathnormal" style=margin-right:0.03588em>y</span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6944em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.03588em>y</span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1944em><span class=mord>^</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1944em><span></span></span></span></span></span><span class=mclose>)</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6667em;vertical-align:-0.0833em></span><span class=mord>−</span><span class="mord mathnormal" style=margin-right:0.0037em>α</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mopen>(</span><span class=mord>1</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6944em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.03588em>y</span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1944em><span class=mord>^</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1944em><span></span></span></span></span></span><span class=mclose><span class=mclose>)</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.05556em>γ</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mop>lo<span style=margin-right:0.01389em>g</span></span><span class=mopen>(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6944em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.03588em>y</span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1944em><span class=mord>^</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1944em><span></span></span></span></span></span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mopen>(</span><span class=mord>1</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal" style=margin-right:0.0037em>α</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.9088em;vertical-align:-0.1944em></span><span class=mord><span class="mord accent"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6944em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.03588em>y</span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1944em><span class=mord>^</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1944em><span></span></span></span></span></span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.7144em><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.05556em>γ</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mop>lo<span style=margin-right:0.01389em>g</span></span><span class=mopen>(</span><span class=mord>1</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6944em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.03588em>y</span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1944em><span class=mord>^</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1944em><span></span></span></span></span></span><span class=mclose>)</span></span></span></span></span>
<p>where:</p>
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mover accent=true><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=application/x-tex>\hat{y}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8889em;vertical-align:-0.1944em></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.6944em><span style=top:-3em><span class=pstrut style=height:3em></span><span class="mord mathnormal" style=margin-right:0.03588em>y</span></span><span style=top:-3em><span class=pstrut style=height:3em></span><span class=accent-body style=left:-0.1944em><span class=mord>^</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1944em><span></span></span></span></span></span></span></span></span> is the predicted probability, <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>y</mi></mrow><annotation encoding=application/x-tex>y</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em></span><span class="mord mathnormal" style=margin-right:0.03588em>y</span></span></span></span> is the ground truth;</li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.25</mn></mrow><annotation encoding=application/x-tex>\alpha = 0.25</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em></span><span class="mord mathnormal" style=margin-right:0.0037em>α</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>0.25</span></span></span></span> (to handle class imbalance);</li>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>γ</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=application/x-tex>\gamma = 3</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em></span><span class="mord mathnormal" style=margin-right:0.05556em>γ</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.6444em></span><span class=mord>3</span></span></span></span> (to emphasize hard samples).</li>
</ul>
<p>Though Multiattention-Net is deeper than many other participants’ models, its modular design and residual stability ensure smooth training, and combined with meticulous loss weighting, it demonstrates strong generalization and convergence efficiency.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=11-bovifocr-ufpr>11. BOVIFOCR-UFPR<a href=#11-bovifocr-ufpr class=hash-link aria-label="Direct link to 11. BOVIFOCR-UFPR" title="Direct link to 11. BOVIFOCR-UFPR">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=bovifocr-ufpr src=/en/assets/images/img10-b475a1dfa8c62536b27fe66be5d8eb40.jpg width=1224 height=548 class=img_ev3q></figure></div>
<p>In this battle focused on "recognizing spoofs," <strong>BOVIFOCR-UFPR</strong> is among the few teams extending analysis beyond the 2D plane. Rather than only analyzing image colors and textures, they attempt to <strong>reconstruct the 3D authenticity of the entire face</strong>, thereby detecting spatial inconsistencies that spoofs cannot reproduce.</p>
<p>Their core solution is:</p>
<blockquote>
<p><strong>3D Reconstruction + ArcFace + Chamfer Loss</strong></p>
</blockquote>
<p>Inspired by <strong>3DPC-Net</strong>, the framework centers on an <strong>Encoder-Decoder architecture</strong>:</p>
<ol>
<li><strong>Preprocessing:</strong> Uses high-quality alignment and cropping to ensure consistent image scale and unified facial regions.</li>
<li><strong>Encoder:</strong> Employs <strong>ResNet-50</strong> backbone to extract high-level features.</li>
<li><strong>Decoder:</strong> Transforms features into corresponding <strong>3D point cloud representations</strong>, simulating the face's geometric shape in space as a basis for subsequent discrimination.</li>
</ol>
<p>With this structure, the model learns not only "how similar it looks" but "how plausible it is," verifying spatial consistency.</p>
<p>During training, the team combines two feature-guided losses:</p>
<ul>
<li><strong>ArcFace Loss:</strong> Enhances class separation, ensuring feature vectors from different identities are angularly separable.</li>
<li><strong>Chamfer Distance Loss:</strong> Measures spatial distance between predicted and ground-truth 3D point clouds to constrain geometric accuracy.</li>
</ul>
<p>The model must classify live/fake and simultaneously output a 3D geometry consistent with reality.</p>
<p>This method is currently the only team using "3D point cloud reconstruction," demonstrating the potential of geometry-driven anti-spoofing.</p>
<p>It is a cross-dimensional recognition approach, relying on geometric plausibility rather than pixel details to build defenses.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=12-inria-cenatav-tec>12. Inria-CENATAV-Tec<a href=#12-inria-cenatav-tec class=hash-link aria-label="Direct link to 12. Inria-CENATAV-Tec" title="Direct link to 12. Inria-CENATAV-Tec">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=inria-cenatav-tec src=/en/assets/images/img11-f9acab71992cd26ce0f352cec1c5f5ca.jpg width=1224 height=356 class=img_ev3q></figure></div>
<p>The <strong>Inria-CENATAV-Tec</strong> team returned to a classic question:</p>
<blockquote>
<p><strong>Can stable anti-spoofing be achieved under limited computational resources?</strong></p>
</blockquote>
<p>Their answer is: <strong>MobileNetV3-spoof with hyperparameter tuning</strong></p>
<p>This is a dynamic balance experiment between model complexity and recognition accuracy.</p>
<p>The pipeline reflects their systematic and conservative strategy:</p>
<ol>
<li><strong>Facial landmark detection:</strong> Uses <strong>ResNet-50</strong> for landmark detection.</li>
<li><strong>Alignment and fallback:</strong> If landmarks are detected, use <strong>InsightFace template</strong> for precise alignment; otherwise, keep original image resized.</li>
</ol>
<p>This approach balances rigor (precise alignment when landmarks are available) and fault tolerance (does not discard data without landmarks), showing practical understanding of real-world scenarios.</p>
<p>Choosing <strong>MobileNetV3-large1.25</strong> as backbone is key.</p>
<p>This architecture is optimized for edge AI and low-power devices, introducing <strong>SE attention blocks</strong> and <strong>h-swish nonlinear activation</strong> to balance parameters and recognition power.</p>
<p>With <strong>SGD optimizer + Multi-step Learning Rate scheduler</strong>, the model converges steadily in stages rather than through a one-shot global descent.</p>
<p>Besides basic augmentations (cropping, flipping), the team normalizes mean and standard deviation per protocol dataset to prevent feature shifts caused by domain heterogeneity.</p>
<p>This per-protocol preprocessing design, though engineering-level, significantly impacts generalization stability and is a major reason this lightweight model sustains recognition ability across multitask scenarios.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=13-vicognit>13. Vicognit<a href=#13-vicognit class=hash-link aria-label="Direct link to 13. Vicognit" title="Direct link to 13. Vicognit">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=vicognit src=/en/assets/images/img12-050b94ce8aab2e913e31313c4e054670.jpg width=1224 height=472 class=img_ev3q></figure></div>
<p>Among all participating teams, <strong>Vicognit</strong> is one of the few that explicitly focused on Vision Transformers (ViT).</p>
<p>Their solution is named:</p>
<blockquote>
<p><strong>FASTormer: Leveraging Vision Transformers for Face Anti-Spoofing</strong></p>
</blockquote>
<p>This is a design with pure intent and clear strategy, avoiding data manipulation, additional modules, or multi-branch paths. They rely fully on the Transformer architecture’s ability to encode relationships and capture sequential structure, allowing the model to autonomously establish facial liveness syntax from spatial information.</p>
<p>The core of Vicognit’s approach is feeding the ViT model with original-resolution inputs without unnecessary downsampling or compression, preserving spatial detail and enabling self-attention to naturally leverage global relationships.</p>
<p>Though not novel per se, this strategy is challenging in face anti-spoofing tasks.</p>
<p>Since differences between real and fake faces often lie not in geometry but in texture, material, and subtle perturbations, ViT’s global relational modeling fits well to learn these fine and irregular semantic gaps.</p>
<p>Their training strategy includes:</p>
<ul>
<li>Carefully tuning learning rate and weight decay for stable, precise convergence;</li>
<li>Using adaptive training methods to avoid early overfitting common in Transformer architectures;</li>
<li>Omitting extra augmentation pipelines, keeping the architecture simple and focusing learning pressure on sequence pattern construction itself.</li>
</ul>
<p>This approach allows FASTormer to effectively capture semantic key points without extra guidance, maintaining good generalization and flexible recognition of unseen patterns.</p>
<p>Vicognit’s contribution lies in empirically demonstrating the feasibility of pure Transformer architectures for FAS.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt=result src=/en/assets/images/img13-0f39a57e34a5766d4b9a7a521f62d8f9.jpg width=1224 height=636 class=img_ev3q></figure></div>
<p>According to official final statistics, the top five teams exhibit three key characteristics:</p>
<ol>
<li><strong>The top three teams clearly outperform others on ACER</strong>, demonstrating high generalization stability.</li>
<li><strong>The first-place team leads in ACER, AUC, and BPCER,</strong> but the best APCER was claimed by the fifth-place team, showing different models have selective advantages on different error types.</li>
<li><strong>All top five teams come from industry,</strong> reflecting the practical design strategies’ substantial impact on UAD effectiveness.</li>
<li><strong>There is a large ACER variance across teams,</strong> indicating UAD is still in early technical exploration without stable consensus or definitive architectures.</li>
</ol>
<p>This challenge ultimately reveals not only who won but which model design philosophies hold up.</p>
<p>From an overall perspective, the distilled generalization design principles in current UAD tasks can be summarized into three paths:</p>
<ul>
<li><strong>Path One: Build global awareness using large models (e.g., ViT and CLIP).</strong></li>
<li><strong>Path Two: Build robustness through data perturbations (e.g., masking, style transfer, self-fusion augmentations).</strong></li>
<li><strong>Path Three: Use semantic alignment as feature regularization (e.g., supervised contrastive learning, ArcFace, dual-branch alignment).</strong></li>
</ul>
<p>Almost all effective solutions integrate subsets of these logics in some form.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Based on observations and organizers’ summaries of this competition, future UAD development still requires breakthroughs in three major directions:</p>
<ol>
<li>
<p><strong>More comprehensive dataset construction:</strong>
Although UniAttackData has laid the foundation for unified cross-attack datasets, improvements remain needed in attack type diversity, subject variability, and image quality. Especially with emerging adversarial attacks and style-transferred deepfakes, current sample sizes are insufficient for systematic generalization validation.</p>
</li>
<li>
<p><strong>Visual Language Model (VLM)-guided generalization strategies:</strong>
Introduction of VLMs like CLIP and DINOv2 offers a semantic-level generalization pressure mechanism. More effective leveraging of these multimodal pretrained knowledge for UAD may reduce dependence on labeled spoof data.</p>
</li>
<li>
<p><strong>Task protocol and standard reconstruction:</strong>
Existing protocols, though representative, struggle to cover mixed attacks, multimodal scenarios, and mobile deployment. Developing higher-level task definitions and layered evaluation mechanisms (e.g., distinguishing high-risk from tolerable errors) is necessary to enhance real-world reliability.</p>
</li>
</ol>
<p>Currently, we still walk within the mist of recognizing spoofs.</p>
<p>Different teams have illuminated various algorithmic paths—some piercing anomalies via attention, some building robustness through masking, some disentangling spatial truth via 3D reconstruction.</p>
<p>But the ultimate question remains unchanged:</p>
<blockquote>
<p><strong>Do we really understand how a fake face is composed?</strong></p>
</blockquote>
<p>Perhaps the answer is yet to form, but after this open technical contest, we have stepped one step closer.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-06-02T15:13:46.000Z itemprop=dateModified>Jun 2, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-5uvb3z" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-5uvb3z"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-5uvb3z" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-5uvb3z">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/face-antispoofing/cfpl-fas/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[24.03] CFPL-FAS</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/face-antispoofing/pd-fas/><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>[24.04] PD-FAS</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#arsenal class="table-of-contents__link toc-highlight">Arsenal</a><li><a href=#problem-definition class="table-of-contents__link toc-highlight">Problem Definition</a><li><a href=#unified-dataset-uniattackdata class="table-of-contents__link toc-highlight">Unified Dataset: UniAttackData</a><ul><li><a href=#evaluation-protocols-and-generalization-design class="table-of-contents__link toc-highlight">Evaluation Protocols and Generalization Design</a><li><a href=#competition-process-and-rules class="table-of-contents__link toc-highlight">Competition Process and Rules</a><li><a href=#evaluation-metrics class="table-of-contents__link toc-highlight">Evaluation Metrics</a></ul><li><a href=#arsenal-1 class="table-of-contents__link toc-highlight">Arsenal</a><ul><li><a href=#1-mtface class="table-of-contents__link toc-highlight">1. MTFace</a><li><a href=#2-searecluse class="table-of-contents__link toc-highlight">2. SeaRecluse</a><li><a href=#3-duileduile class="table-of-contents__link toc-highlight">3. duileduile</a><li><a href=#4-bsp-idiap class="table-of-contents__link toc-highlight">4. BSP-Idiap</a><li><a href=#5-vai-face class="table-of-contents__link toc-highlight">5. VAI-Face</a><li><a href=#6-llw class="table-of-contents__link toc-highlight">6. L&L&W</a><li><a href=#7-sarm class="table-of-contents__link toc-highlight">7. SARM</a><li><a href=#8-m2-purdue class="table-of-contents__link toc-highlight">8. M2-Purdue</a><li><a href=#9-cloud-recesses class="table-of-contents__link toc-highlight">9. Cloud Recesses</a><li><a href=#10-image-lab class="table-of-contents__link toc-highlight">10. Image Lab</a><li><a href=#11-bovifocr-ufpr class="table-of-contents__link toc-highlight">11. BOVIFOCR-UFPR</a><li><a href=#12-inria-cenatav-tec class="table-of-contents__link toc-highlight">12. Inria-CENATAV-Tec</a><li><a href=#13-vicognit class="table-of-contents__link toc-highlight">13. Vicognit</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>