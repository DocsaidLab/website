<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-contrastive-learning/dinov2/index" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>[23.04] DINOv2 | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/papers/contrastive-learning/dinov2/><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-papers-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-papers-current><meta data-rh=true property=og:title content="[23.04] DINOv2 | DOCSAID"><meta data-rh=true name=description content="Universal Feature Forging Method"><meta data-rh=true property=og:description content="Universal Feature Forging Method"><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/papers/contrastive-learning/dinov2/><link data-rh=true rel=alternate href=https://docsaid.org/papers/contrastive-learning/dinov2/ hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/papers/contrastive-learning/dinov2/ hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/papers/contrastive-learning/dinov2/ hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/papers/contrastive-learning/dinov2/ hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/papers/category/contrastive-learning","name":"Contrastive Learning (14)","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/papers/contrastive-learning/dinov2/","name":"[23.04] DINOv2","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.ef02043f.css><script src=/en/assets/js/runtime~main.43d6c5ca.js defer></script><script src=/en/assets/js/main.2f583823.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/papers/contrastive-learning/dinov2/ target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/papers/contrastive-learning/dinov2/ target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/papers/contrastive-learning/dinov2/ target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>Paper Notes</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/classic-cnns>Classic CNNs (11)</a><button aria-label="Expand sidebar category 'Classic CNNs (11)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/papers/category/contrastive-learning>Contrastive Learning (14)</a><button aria-label="Collapse sidebar category 'Contrastive Learning (14)'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/examplar-cnn/>[14.06] Exemplar CNN</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/instdisc/>[18.05] InstDisc</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/cpc/>[18.07] CPC</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/invaspread/>[19.04] InvaSpread</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/moco-v1/>[19.11] MoCo v1</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/simclr-v1/>[20.02] SimCLR v1</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/moco-v2/>[20.03] MoCo v2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/byol/>[20.06] BYOL</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/simclr-v2/>[20.06] SimCLR v2</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/swav/>[20.06] SwAV</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/simsiam/>[20.11] SimSiam</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/dino/>[21.04] DINO</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/papers/contrastive-learning/moco-v3/>[21.04] MoCo v3</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/papers/contrastive-learning/dinov2/>[23.04] DINOv2</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/deepseek>DeepSeek (5)</a><button aria-label="Expand sidebar category 'DeepSeek (5)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-antispoofing>Face Anti-Spoofing (43)</a><button aria-label="Expand sidebar category 'Face Anti-Spoofing (43)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/face-recognition>Face Recognition (4)</a><button aria-label="Expand sidebar category 'Face Recognition (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/feature-fusion>Feature Fusion (10)</a><button aria-label="Expand sidebar category 'Feature Fusion (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/image-generation>Image Generation (1)</a><button aria-label="Expand sidebar category 'Image Generation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/lightweight>Lightweight (10)</a><button aria-label="Expand sidebar category 'Lightweight (10)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/mamba>Mamba (4)</a><button aria-label="Expand sidebar category 'Mamba (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/model-tuning>Model Tuning (8)</a><button aria-label="Expand sidebar category 'Model Tuning (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/multimodality>Multimodality (24)</a><button aria-label="Expand sidebar category 'Multimodality (24)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/normalization>Normalization (1)</a><button aria-label="Expand sidebar category 'Normalization (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/object-detection>Object Detection (16)</a><button aria-label="Expand sidebar category 'Object Detection (16)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/reparameterization>Reparameterization (8)</a><button aria-label="Expand sidebar category 'Reparameterization (8)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/retail-product>Retail Product (6)</a><button aria-label="Expand sidebar category 'Retail Product (6)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/segmentation>Segmentation (1)</a><button aria-label="Expand sidebar category 'Segmentation (1)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-detection>Text Detection (14)</a><button aria-label="Expand sidebar category 'Text Detection (14)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-recognition>Text Recognition (20)</a><button aria-label="Expand sidebar category 'Text Recognition (20)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/text-spotting>Text Spotting (4)</a><button aria-label="Expand sidebar category 'Text Spotting (4)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/transformers>Transformers (17)</a><button aria-label="Expand sidebar category 'Transformers (17)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/papers/category/vision-transformers>Vision Transformers (13)</a><button aria-label="Expand sidebar category 'Vision Transformers (13)'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/papers/intro>All Notes: 235 entries</a></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/papers/category/contrastive-learning><span>Contrastive Learning (14)</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>[23.04] DINOv2</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[23.04] DINOv2</h1><div class="undefined margin-bottom--md"><div class=docAuthor_y7l7><img src=https://github.com/zephyr-sh.png alt="Z. Yuan" class=docAuthorImg_vlJe><div><div class=docAuthorName_aSh4><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer">Z. Yuan</a></div><div class=docAuthorTitle_Yp5_>Dosaid maintainer, Full-Stack AI Engineer</div><div class=docAuthorSocials_M3ba><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 496 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a><a href=https://www.linkedin.com/in/ze-yuan-sh7/ target=_blank rel="noopener noreferrer" class=docAuthorSocialLink_trj9><svg stroke=currentColor fill=currentColor stroke-width=0 viewBox="0 0 448 512" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a></div></div></div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=universal-feature-forging-method>Universal Feature Forging Method<a href=#universal-feature-forging-method class=hash-link aria-label="Direct link to Universal Feature Forging Method" title="Direct link to Universal Feature Forging Method">​</a></h2>
<p><a href=https://arxiv.org/pdf/2304.07193 target=_blank rel="noopener noreferrer"><strong>DINOv2: Learning Robust Visual Features without Supervision</strong></a></p>
<hr>
<p>We have already reviewed the first generation of DINO.</p>
<p>After two years, let’s see what improvements Meta AI Research has made on this architecture.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>This paper is nearly 20 pages long—perfect for passing the time.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=problem-definition>Problem Definition<a href=#problem-definition class=hash-link aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>In recent years, the mainstream development trend of visual models has become polarized:</p>
<p>On one hand, methods like CLIP and ALIGN learn through image-text alignment, using language as a semantic supervisory axis, successfully training foundational models with semantic understanding capabilities. These methods show strong performance in classification, retrieval, and multimodal tasks.</p>
<p>On the other hand, another route chooses not to rely on language, even rejecting any external annotation. This is a purer self-supervised perspective, letting images “speak for themselves,” enabling models to learn recognition, memory, abstraction, and generalization silently.</p>
<p>This route attempts to answer a more fundamental question:</p>
<blockquote>
<p><strong>In a world without language, can images still understand themselves?</strong></p>
</blockquote>
<p>This is the core pursuit of Self-Supervised Learning (SSL) techniques.</p>
<p>These methods can be roughly divided into two categories:</p>
<ol>
<li>
<p><strong>Intra-image pretext tasks</strong>, such as predicting masked patches, reconstructing original images, color restoration, or patch order prediction. These are similar in form to “fill-in-the-blank” tasks in language models, with typical examples including MAE and BEiT.</p>
</li>
<li>
<p><strong>Discriminative self-supervision</strong>, emphasizing distinctive training across different images, such as contrastive learning, instance classification, or clustering. Representative methods include SimCLR, MoCo, BYOL, and DINO.</p>
</li>
</ol>
<p>These methods have been validated on standard datasets like ImageNet-1k to learn frozen features with generalization ability, even comparable to supervised training. However, when scaling up model size, extending training time, or using uncurated datasets, these methods often face challenges such as performance collapse, unstable training, and feature degradation.</p>
<p>Perhaps the quality and diversity of data are even more critical factors?</p>
<p>Although self-supervised learning does not rely on manual annotation, it depends more heavily on the breadth and structural balance of the input data itself. When datasets suffer from topic shifts, long-tail imbalance, or excessive repetition, models tend to learn partial and hard-to-transfer representations.</p>
<p>Despite accumulating many impressive academic and industrial results, there remain several key unresolved issues for pushing these methods toward stable and usable visual foundational models:</p>
<ul>
<li><strong>Can existing self-supervised techniques maintain sufficient stability and scalability when scaled up?</strong></li>
<li><strong>Can truly language- and annotation-free learning still produce universal features suitable for direct downstream applications?</strong></li>
<li><strong>How can we effectively and automatically perform data filtering and distribution balancing when facing massive, uncurated image collections?</strong></li>
</ul>
<p>These questions remain open.</p>
<p>DINOv2 starts from the first generation’s foundation, further adjusting data strategies, enhancing architectural stability, and reconsidering systematic design for self-supervised learning. If the data is better and the architecture more stable, perhaps we can go further—enabling language-free models to become the universal basis for modern visual systems.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=data-collection>Data Collection<a href=#data-collection class=hash-link aria-label="Direct link to Data Collection" title="Direct link to Data Collection">​</a></h2>
<p>To enable the model to learn solid, generalizable visual features, data quality is the first priority.</p>
<p>When the goal is “fully self-supervised without annotation,” the responsibility of data selection no longer lies on labels but on <strong>the diversity and representativeness of the images themselves</strong>.</p>
<p>The data processing pipeline planned by the authors can be roughly divided into three steps, as illustrated below:</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="data arch" src=/en/assets/images/img3-bba6611a3c05fd6b4680acbde47f621e.jpg width=1224 height=368 class=img_ev3q></figure></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=1-preparing-the-data-pool>1. Preparing the Data Pool<a href=#1-preparing-the-data-pool class=hash-link aria-label="Direct link to 1. Preparing the Data Pool" title="Direct link to 1. Preparing the Data Pool">​</a></h3>
<p>The chosen target dataset LVD-142M consists of two parts:</p>
<ul>
<li><strong>Curated portion</strong>: based on ImageNet-1k/22k, Google Landmarks, and multiple fine-grained datasets, serving as sources of “representative samples”;</li>
<li><strong>Uncurated portion</strong>: raw images collected from public web crawler databases, filtered for safety (removing illegal URLs, NSFW content, blurred faces, etc.), totaling 1.2 billion images.</li>
</ul>
<p>These uncurated images are unlabeled, undocumented, and unfiltered, representing the most “wild web” data state.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p>The paper uses the term “curated” extensively, so a brief reminder for readers:<ul>
<li><strong>Curated data</strong>: data that has been manually selected, cleaned, and organized into high-quality datasets;</li>
<li><strong>Uncurated data</strong>: data crawled from the web or other sources without filtering or cleaning.</li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=2-removing-duplicates>2. Removing Duplicates<a href=#2-removing-duplicates class=hash-link aria-label="Direct link to 2. Removing Duplicates" title="Direct link to 2. Removing Duplicates">​</a></h3>
<p>To avoid bias caused by duplicate training data, the authors introduced a specialized copy detection process to remove highly similar images, not only within the uncurated set but also excluding samples similar to test sets to prevent data leakage.</p>
<p>The implementation details of this step come from the algorithm by Pizzi et al. (2022), ensuring a diverse and clean dataset.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>Reference: Pizzi et al. <a href=https://arxiv.org/abs/2202.10261 target=_blank rel="noopener noreferrer"><strong>[22.02] A self-supervised descriptor for image copy detection</strong></a></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=3-self-supervised-retrieval>3. Self-Supervised Retrieval<a href=#3-self-supervised-retrieval class=hash-link aria-label="Direct link to 3. Self-Supervised Retrieval" title="Direct link to 3. Self-Supervised Retrieval">​</a></h3>
<p>Here lies the truly ingenious design.</p>
<p>The authors first use a ViT-H/16 model previously self-supervised trained on ImageNet-22k to compute embedding representations for all images. Using cosine similarity as the distance metric, they retrieve uncurated images that “look like curated data.”</p>
<p>This retrieval is neither manual annotation nor keyword filtering but purely based on visual similarity of image features.</p>
<p>Next, the uncurated images are clustered by K-means. Depending on the scenario (large or small query datasets), for each base image, the top N nearest neighbors (N=4) or M images from the corresponding cluster are selected.</p>
<p>This choice balances data overlap and retrieval diversity.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=engineering-scale-and-efficiency>Engineering Scale and Efficiency<a href=#engineering-scale-and-efficiency class=hash-link aria-label="Direct link to Engineering Scale and Efficiency" title="Direct link to Engineering Scale and Efficiency">​</a></h3>
<p>Such large-scale data processing and retrieval require powerful computational resources.</p>
<p>The entire pipeline is built on the FAISS framework, using GPU-accelerated inverted indexing and quantization coding, distributed across 20 computing nodes each equipped with 8 V100 GPUs. It took less than two days to complete the cleaning and organization of 142 million high-quality training images.</p>
<p>Overall, DINOv2’s data preparation cleverly combines “existing high-quality datasets” with “visually semantically similar web images,” using self-supervision to automatically select training samples, laying a foundation for subsequent model learning.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=solution>Solution<a href=#solution class=hash-link aria-label="Direct link to Solution" title="Direct link to Solution">​</a></h2>
<p>The training strategy adopted by DINOv2 belongs to <strong>Discriminative Self-Supervised Learning</strong>. Overall, it can be seen as a hybrid design combining DINO and iBOT with SwAV’s centering mechanism, augmented by regularization terms and a high-resolution training strategy.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>Wait—this sudden flood of terms might catch readers off guard.<p>For those unfamiliar with the above terms, you can refer to the following papers:<ul>
<li><a href=/en/papers/contrastive-learning/swav/><strong>[20.06] SwAV: Swapping Cluster Predictions</strong></a></li>
<li><a href=/en/papers/contrastive-learning/dino/><strong>[21.04] DINO: Self-Distillation With No Labels</strong></a></li>
<li><a href=https://arxiv.org/abs/2111.07832 target=_blank rel="noopener noreferrer"><strong>[21.11] iBOT: Image BERT Pre-Training with Online Tokenizer</strong></a></li>
</ul></div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=image-level-feature-alignment>Image-Level Feature Alignment<a href=#image-level-feature-alignment class=hash-link aria-label="Direct link to Image-Level Feature Alignment" title="Direct link to Image-Level Feature Alignment">​</a></h3>
<div align=center><figure style=width:50%><p><img decoding=async loading=lazy alt="dino arch" src=/en/assets/images/img0-b584cd08fc9fac0edf98106d84e41600.jpg width=664 height=642 class=img_ev3q></figure></div>
<p>This part is based on the core idea of the DINO method, as illustrated above.</p>
<p>Using the DINO loss function, the goal is to ensure that the semantic representations of images under different views (augmentations), processed by the student and teacher networks respectively, remain consistent.</p>
<p>The specific procedure is as follows:</p>
<ul>
<li>
<p>Take two different crops (called views) from the same image and input them separately into the <strong>student and teacher</strong> models;</p>
</li>
<li>
<p>Both models are Vision Transformers, and their class tokens are taken as the overall semantic vector representing the image;</p>
</li>
<li>
<p>The class tokens are then passed through separate MLP heads to produce vectors called “prototype scores,” which are each normalized by a softmax:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi mathvariant=bold>p</mi><mi>s</mi></msub><mo>=</mo><mrow><mi mathvariant=normal>s</mi><mi mathvariant=normal>o</mi><mi mathvariant=normal>f</mi><mi mathvariant=normal>t</mi><mi mathvariant=normal>m</mi><mi mathvariant=normal>a</mi><mi mathvariant=normal>x</mi></mrow><mo stretchy=false>(</mo><msub><mi>h</mi><mi>s</mi></msub><mo stretchy=false>(</mo><msub><mi>z</mi><mi>s</mi></msub><mo stretchy=false>)</mo><mo stretchy=false>)</mo><mo separator=true>,</mo><mspace width=1em /><msub><mi mathvariant=bold>p</mi><mi>t</mi></msub><mo>=</mo><mrow><mi mathvariant=normal>s</mi><mi mathvariant=normal>o</mi><mi mathvariant=normal>f</mi><mi mathvariant=normal>t</mi><mi mathvariant=normal>m</mi><mi mathvariant=normal>a</mi><mi mathvariant=normal>x</mi></mrow><mo stretchy=false>(</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy=false>(</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy=false>)</mo><mo stretchy=false>)</mo></mrow><annotation encoding=application/x-tex>\mathbf{p}_s = \mathrm{softmax}(h_s(z_s)), \quad \mathbf{p}_t = \mathrm{softmax}(h_t(z_t))</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6389em;vertical-align:-0.1944em></span><span class=mord><span class="mord mathbf">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mord><span class="mord mathrm">softmax</span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">h</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.1514em><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mclose>))</span><span class=mpunct>,</span><span class=mspace style=margin-right:1em></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathbf">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mord><span class="mord mathrm">softmax</span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">h</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:0.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.2806em><span style=top:-2.55em;margin-left:-0.044em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mclose>))</span></span></span></span></span>
</li>
<li>
<p>To avoid overly sharp predictions from the teacher, the teacher’s softmax outputs are further smoothed by a process called <strong>centering</strong>, commonly implemented via moving average or the Sinkhorn-Knopp algorithm;</p>
</li>
<li>
<p>The final loss is the cross-entropy between the two distributions:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi mathvariant=script>L</mi><mtext>DINO</mtext></msub><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>k</mi></munder><msubsup><mi>p</mi><mi>t</mi><mrow><mo stretchy=false>(</mo><mi>k</mi><mo stretchy=false>)</mo></mrow></msubsup><mi>log</mi><mo>⁡</mo><msubsup><mi>p</mi><mi>s</mi><mrow><mo stretchy=false>(</mo><mi>k</mi><mo stretchy=false>)</mo></mrow></msubsup></mrow><annotation encoding=application/x-tex>\mathcal{L}_{\text{DINO}} = - \sum_{k} p_t^{(k)} \log p_s^{(k)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathcal">L</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">DINO</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.3521em;vertical-align:-1.3021em></span><span class=mord>−</span><span class=mspace style=margin-right:0.1667em></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.05em><span style=top:-1.8479em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em>k</span></span></span></span><span style=top:-3.05em><span class=pstrut style=height:3.05em></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.3021em><span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.0448em><span style=top:-2.4542em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style=top:-3.2198em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style=margin-right:0.03148em>k</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2458em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mop>lo<span style=margin-right:0.01389em>g</span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.938em><span style=top:-2.453em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style=margin-right:0.03148em>k</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span></span></span></span></span>
</li>
</ul>
<p>Here, the student is the training target, while the teacher is an exponential moving average (EMA) of the student weights and does not participate in backpropagation.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=patch-level-prediction-task>Patch-Level Prediction Task<a href=#patch-level-prediction-task class=hash-link aria-label="Direct link to Patch-Level Prediction Task" title="Direct link to Patch-Level Prediction Task">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="ibot arch" src=/en/assets/images/img00-5c8984a976bb739b6e39ffa26ffeea94.jpg width=1224 height=392 class=img_ev3q></figure></div>
<p>To encourage the model to learn fine details and local semantics, the authors incorporate iBOT’s “pixel-level prediction task,” structured as follows:</p>
<ul>
<li>
<p>Randomly mask certain patches in the student input, forming masked tokens;</p>
</li>
<li>
<p>These patches remain fully visible in the teacher input;</p>
</li>
<li>
<p>For each masked patch, the student’s predicted distribution <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msubsup><mi mathvariant=bold>p</mi><mi>s</mi><mrow><mo stretchy=false>(</mo><mi>i</mi><mo stretchy=false>)</mo></mrow></msubsup></mrow><annotation encoding=application/x-tex>\mathbf{p}_{s}^{(i)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.2392em;vertical-align:-0.1944em></span><span class=mord><span class="mord mathbf">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.0448em><span style=top:-2.5834em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span style=top:-3.2198em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.1166em><span></span></span></span></span></span></span></span></span></span> and the teacher’s target distribution <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msubsup><mi mathvariant=bold>p</mi><mi>t</mi><mrow><mo stretchy=false>(</mo><mi>i</mi><mo stretchy=false>)</mo></mrow></msubsup></mrow><annotation encoding=application/x-tex>\mathbf{p}_{t}^{(i)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.2906em;vertical-align:-0.2458em></span><span class=mord><span class="mord mathbf">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.0448em><span style=top:-2.4542em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span style=top:-3.2198em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2458em><span></span></span></span></span></span></span></span></span></span> are computed, both followed by softmax and centering:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi mathvariant=script>L</mi><mtext>iBOT</mtext></msub><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi mathvariant=script>M</mi></mrow></munder><munder><mo>∑</mo><mi>k</mi></munder><msubsup><mi>p</mi><mi>t</mi><mrow><mo stretchy=false>(</mo><mi>i</mi><mo separator=true>,</mo><mi>k</mi><mo stretchy=false>)</mo></mrow></msubsup><mi>log</mi><mo>⁡</mo><msubsup><mi>p</mi><mi>s</mi><mrow><mo stretchy=false>(</mo><mi>i</mi><mo separator=true>,</mo><mi>k</mi><mo stretchy=false>)</mo></mrow></msubsup></mrow><annotation encoding=application/x-tex>\mathcal{L}_{\text{iBOT}} = - \sum_{i \in \mathcal{M}} \sum_{k} p_t^{(i, k)} \log p_s^{(i, k)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathcal">L</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">iBOT</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.3717em;vertical-align:-1.3217em></span><span class=mord>−</span><span class=mspace style=margin-right:0.1667em></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.05em><span style=top:-1.8557em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight">M</span></span></span></span><span style=top:-3.05em><span class=pstrut style=height:3.05em></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.3217em><span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.05em><span style=top:-1.8479em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.03148em>k</span></span></span></span><span style=top:-3.05em><span class=pstrut style=height:3.05em></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.3021em><span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.0448em><span style=top:-2.4542em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style=top:-3.2198em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style=margin-right:0.03148em>k</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2458em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mop>lo<span style=margin-right:0.01389em>g</span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">p</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.938em><span style=top:-2.453em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style=top:-3.113em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style=margin-right:0.03148em>k</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.247em><span></span></span></span></span></span></span></span></span></span></span>
</li>
</ul>
<p>where <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi mathvariant=script>M</mi></mrow><annotation encoding=application/x-tex>\mathcal{M}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathcal">M</span></span></span></span> is the index set of all masked patches. This task provides finer spatial-level supervision, aiding downstream pixel-level tasks such as segmentation.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=head-design>Head Design<a href=#head-design class=hash-link aria-label="Direct link to Head Design" title="Direct link to Head Design">​</a></h3>
<p>Both DINOv2 and iBOT require additional MLP projection heads to map transformer outputs into logits.</p>
<p>Although the original iBOT paper reports better results sharing the heads, DINOv2’s large-scale experiments found <strong>using two separate heads performs better</strong>, thus the two are fully decoupled.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=sinkhorn-knopp-centering>Sinkhorn-Knopp Centering<a href=#sinkhorn-knopp-centering class=hash-link aria-label="Direct link to Sinkhorn-Knopp Centering" title="Direct link to Sinkhorn-Knopp Centering">​</a></h3>
<p>To stabilize the teacher’s predicted distributions, DINOv2 adopts the <strong>Sinkhorn-Knopp normalization (SK)</strong> from SwAV to balance batch distributions.</p>
<p>This method iteratively normalizes rows and columns, approximating a doubly stochastic matrix. In practice, three iterations suffice.</p>
<p>Using SK reduces the risk of model collapse by preventing the output from concentrating on a single prototype.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=koleo-regularization>KoLeo Regularization<a href=#koleo-regularization class=hash-link aria-label="Direct link to KoLeo Regularization" title="Direct link to KoLeo Regularization">​</a></h3>
<p>DINOv2 introduces a rarely seen regularizer in this domain: <strong>KoLeo Regularizer (Kozachenko-Leonenko Regularizer)</strong>, which encourages feature embeddings to be evenly and widely distributed in embedding space.</p>
<p>This term originates from differential entropy estimation and is formulated as:</p>
<span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><msub><mi mathvariant=script>L</mi><mtext>KoLeo</mtext></msub><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>log</mi><mo>⁡</mo><msub><mi>d</mi><mrow><mi>n</mi><mo separator=true>,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding=application/x-tex>\mathcal{L}_{\text{KoLeo}} = -\frac{1}{n} \sum_{i=1}^{n} \log d_{n,i}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em></span><span class=mord><span class="mord mathcal">L</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">KoLeo</span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:2.9291em;vertical-align:-1.2777em></span><span class=mord>−</span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.3214em><span style=top:-2.314em><span class=pstrut style=height:3em></span><span class=mord><span class="mord mathnormal">n</span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:0.04em></span></span><span style=top:-3.677em><span class=pstrut style=height:3em></span><span class=mord><span class=mord>1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.686em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace style=margin-right:0.1667em></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.6514em><span style=top:-1.8723em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style=top:-3.05em><span class=pstrut style=height:3.05em></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style=top:-4.3em;margin-left:0em><span class=pstrut style=height:3.05em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:1.2777em><span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mop>lo<span style=margin-right:0.01389em>g</span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span></span></span></span></span>
<p>where:</p>
<ul>
<li><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>d</mi><mrow><mi>n</mi><mo separator=true>,</mo><mi>i</mi></mrow></msub><mo>=</mo><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>j</mi><mo mathvariant=normal>≠</mo><mi>i</mi></mrow></msub><mi mathvariant=normal>∣</mi><msub><mi mathvariant=bold>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi mathvariant=bold>x</mi><mi>j</mi></msub><mi mathvariant=normal>∣</mi></mrow><annotation encoding=application/x-tex>d_{n,i} = \min_{j \ne i} | \mathbf{x}_i - \mathbf{x}_j |</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.9805em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:1.0361em;vertical-align:-0.2861em></span><span class=mop><span class=mop>min</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3361em><span style=top:-2.55em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style=margin-right:0.05724em>j</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord vbox mtight"><span class="thinbox mtight"><span class="rlap mtight"><span class=strut style=height:0.8889em;vertical-align:-0.1944em></span><span class=inner><span class="mord mtight"><span class="mrel mtight"></span></span></span><span class=fix></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.1667em></span><span class=mord>∣</span><span class=mord><span class="mord mathbf">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1.0361em;vertical-align:-0.2861em></span><span class=mord><span class="mord mathbf">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style=margin-right:0.05724em>j</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.2861em><span></span></span></span></span></span></span><span class=mord>∣</span></span></span></span> is the nearest neighbor distance for the <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6595em></span><span class="mord mathnormal">i</span></span></span></span>-th vector;</li>
<li>All vectors <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi mathvariant=bold>x</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>\mathbf{x}_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.5944em;vertical-align:-0.15em></span><span class=mord><span class="mord mathbf">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> are <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi mathvariant=normal>ℓ</mi><mn>2</mn></msub></mrow><annotation encoding=application/x-tex>\ell_2</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8444em;vertical-align:-0.15em></span><span class=mord><span class=mord>ℓ</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em><span style=top:-2.55em;margin-left:0em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:0.15em><span></span></span></span></span></span></span></span></span></span> normalized beforehand.</li>
</ul>
<p>This regularization prevents features from over-concentrating during training, promoting coverage and diversity in feature representation.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=high-resolution-training-phase>High-Resolution Training Phase<a href=#high-resolution-training-phase class=hash-link aria-label="Direct link to High-Resolution Training Phase" title="Direct link to High-Resolution Training Phase">​</a></h3>
<p>Although high-resolution images are critical for tasks like segmentation and detection, directly using high resolution during pretraining is computationally expensive. Therefore, the authors adopt a compromise: <strong>briefly switch to 518×518 high-resolution images in the later pretraining phase</strong> to enhance detail signals.</p>
<p>This approach is similar to the resolution ramp-up in methods like UniViT and FlexiViT, and experiments show it effectively improves pixel-level performance with minimal extra training time.</p>
<p>Overall, DINOv2’s training objective integrates semantic consistency, patch prediction, distribution balancing, and spatial regularization, achieving a stable balance between efficiency and generalization.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=engineering-implementation>Engineering Implementation<a href=#engineering-implementation class=hash-link aria-label="Direct link to Engineering Implementation" title="Direct link to Engineering Implementation">​</a></h2>
<p>The authors dedicate a chapter to introducing how to accelerate the training pipeline and reduce memory usage. Compared to iBOT, with the same hardware, DINOv2 achieves roughly 2× speedup and only 1/3 memory usage.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=flashattention>FlashAttention<a href=#flashattention class=hash-link aria-label="Direct link to FlashAttention" title="Direct link to FlashAttention">​</a></h3>
<p>Self-attention layers are the biggest computational bottleneck in ViT architectures.</p>
<p>The team developed an optimized FlashAttention implementation that outperforms the original in both memory and speed, and supports more hardware platforms and architectural variants.</p>
<p>They found that aligning embedding dimensions with GPU-friendly sizes (e.g., 64 per head, total embedding multiple of 256) further improves efficiency.</p>
<p>Therefore, ViT-g uses 1536 dimensions and 24 heads (64 dims per head), replacing the commonly used 1408 dims / 16 heads design.</p>
<p>Experiments show comparable accuracy with better efficiency.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=sequence-packing>Sequence Packing<a href=#sequence-packing class=hash-link aria-label="Direct link to Sequence Packing" title="Direct link to Sequence Packing">​</a></h3>
<p>ViT inputs are patch tokens, and DINO-style methods process two crops simultaneously:</p>
<ul>
<li>Large crops (resolution 224) produce long sequences;</li>
<li>Small crops (resolution 98) produce short sequences.</li>
</ul>
<p>Previous approaches forwarded these separately, causing batch size inconsistency and wasted computation.</p>
<p>DINOv2 introduces a technique from NLP: <strong>Sequence Packing</strong>.</p>
<p>Multiple token sequences of varying lengths are concatenated into one long sequence, and a block-diagonal attention mask prevents cross-attention between different sequences. This yields the same effect as separate forwards but significantly improves computational efficiency.</p>
<p>This design relies on low-level support and is implemented on top of the <code>xFormers</code> library.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=stochastic-depth>Stochastic Depth<a href=#stochastic-depth class=hash-link aria-label="Direct link to Stochastic Depth" title="Direct link to Stochastic Depth">​</a></h3>
<p>Stochastic Depth randomly drops residual blocks during training as a form of regularization.</p>
<p>Traditionally, the output is zeroed out after computation; DINOv2 instead skips computation for dropped blocks entirely, saving proportional memory and compute.</p>
<p>This requires a special kernel that randomly permutes samples in the batch dimension and only computes the first <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo stretchy=false>(</mo><mn>1</mn><mo>−</mo><mi>d</mi><mo stretchy=false>)</mo><mo>⋅</mo><mi>B</mi></mrow><annotation encoding=application/x-tex>(1-d) \cdot B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class=mopen>(</span><span class=mord>1</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>−</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-0.25em></span><span class="mord mathnormal">d</span><span class=mclose>)</span><span class=mspace style=margin-right:0.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:0.2222em></span></span><span class=base><span class=strut style=height:0.6833em></span><span class="mord mathnormal" style=margin-right:0.05017em>B</span></span></span></span> samples per layer. With the paper’s drop rate <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>d</mi><mo>=</mo><mn>40</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>d=40\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em></span><span class="mord mathnormal">d</span><span class=mspace style=margin-right:0.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em></span></span><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em></span><span class=mord>40%</span></span></span></span>, about 40% computation is saved per layer, greatly reducing overhead.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=distributed-training>Distributed Training<a href=#distributed-training class=hash-link aria-label="Direct link to Distributed Training" title="Direct link to Distributed Training">​</a></h3>
<p>Training billion-scale models (e.g., ViT-g with ~1.1B parameters) is bottlenecked by memory.</p>
<p>DINOv2 employs <strong>Fully-Sharded Data Parallel (FSDP)</strong> to shard model replicas across GPUs, including:</p>
<ul>
<li>Student network,</li>
<li>Teacher network,</li>
<li>Optimizer first- and second-moment parameters (AdamW).</li>
</ul>
<p>Together, these float32 parameters occupy ~16 GB memory. FSDP shards these tensors across GPUs, freeing single-card limits and enabling larger models.</p>
<p>Additionally, model weights and gradients are communicated in float16 (except MLP heads) to reduce GPU communication overhead by at least 50% compared to float32 all-reduce in DDP.</p>
<p>Empirically, PyTorch FSDP + mixed precision significantly outperforms DDP + autocast.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-distillation>Model Distillation<a href=#model-distillation class=hash-link aria-label="Direct link to Model Distillation" title="Direct link to Model Distillation">​</a></h3>
<p>After training large models like ViT-g, DINOv2 does not directly train smaller models like ViT-B or ViT-L from scratch. Instead, knowledge distillation is used to let smaller models mimic the large one’s outputs.</p>
<p>The training strategy reuses the teacher-student framework with these modifications:</p>
<ul>
<li>The pretrained ViT-g is frozen as teacher;</li>
<li>Masking and stochastic depth are disabled to simplify training;</li>
<li>The iBOT loss is applied to two global crops;</li>
<li>The student’s EMA is kept as the final model output.</li>
</ul>
<p>This design is similar to Duval et al.’s method but without changing the loss function structure, reusing the original pipeline for simplicity and practicality.</p>
<p>Experiments show that even ViT-L distilled this way outperforms training from scratch significantly.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=ablation-experiments>Ablation Experiments<a href=#ablation-experiments class=hash-link aria-label="Direct link to Ablation Experiments" title="Direct link to Ablation Experiments">​</a></h2>
<p>To verify the effectiveness of each component design, the authors conducted several experiments covering training strategies, data sources, model sizes, loss components, and resolutions.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-training-process>Model Training Process<a href=#model-training-process class=hash-link aria-label="Direct link to Model Training Process" title="Direct link to Model Training Process">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt=training src=/en/assets/images/img4-03bbabed209f9bd8dedb004f6d926fbc.jpg width=1220 height=728 class=img_ev3q></figure></div>
<p>Using iBOT as the baseline, the authors sequentially added various DINOv2 techniques (such as dual heads, Sinkhorn centering, KoLeo regularization, etc.) and observed performance on ImageNet-1k (including k-NN and linear probe evaluations).</p>
<p>Results show that <strong>most technical components individually improve at least one evaluation metric</strong>. In some cases (e.g., LayerScale and Stochastic Depth), linear probing performance decreases but training stability improves significantly, which is a more important practical indicator.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=pretraining-data>Pretraining Data<a href=#pretraining-data class=hash-link aria-label="Direct link to Pretraining Data" title="Direct link to Pretraining Data">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="pretrain data" src=/en/assets/images/img5-f2a6fd74bb990aa49fe3f64cebf8970a.jpg width=1224 height=216 class=img_ev3q></figure></div>
<p>To test the impact of data quality on features, the authors compared three datasets:</p>
<ul>
<li>LVD-142M (curated set selected via visual retrieval)</li>
<li>ImageNet-22k (traditional large-scale labeled dataset)</li>
<li>Uncurated-142M (random samples from the same source web data)</li>
</ul>
<p>All trained with ViT-g/14 under identical conditions.</p>
<p>Experiments show as above:</p>
<ul>
<li><strong>Curated data significantly outperforms uncurated</strong>, even at equal quantities;</li>
<li>LVD-142M surpasses ImageNet-22k on most benchmarks (except ImageNet-1k);</li>
<li>On domains without data filtering reference (e.g., INaturalist, Places205), LVD-142M also demonstrates better transferability.</li>
</ul>
<p>This confirms that “data curation” remains a crucial design factor in self-supervised learning.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=importance-of-loss-components>Importance of Loss Components<a href=#importance-of-loss-components class=hash-link aria-label="Direct link to Importance of Loss Components" title="Direct link to Importance of Loss Components">​</a></h3>
<p><img decoding=async loading=lazy alt="loss comp" src=/en/assets/images/img6-9f07fa7d76a4a14634d7d587dc7f2747.jpg width=1734 height=262 class=img_ev3q></p>
<p>Next, the authors evaluated the actual contribution of each loss term via ablation on:</p>
<ul>
<li>
<p><strong>KoLeo Regularization</strong></p>
<p>On the Oxford-M image retrieval task, <strong>enabling KoLeo brings about 8% accuracy improvement</strong>; meanwhile, it causes no significant side effects on ImageNet classification or ADE segmentation.</p>
<p>This shows the regularizer successfully encourages feature dispersion, especially benefiting retrieval tasks requiring detailed feature discrimination.</p>
</li>
<li>
<p><strong>iBOT-style Masked Image Modeling</strong></p>
<p>Removing this loss causes about <strong>3% accuracy drop on ADE-20k segmentation</strong>. This highlights the critical role of patch-level supervision in pixel-level tasks.</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=knowledge-distillation-effect>Knowledge Distillation Effect<a href=#knowledge-distillation-effect class=hash-link aria-label="Direct link to Knowledge Distillation Effect" title="Direct link to Knowledge Distillation Effect">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="knowledge distillation" src=/en/assets/images/img7-847fb7f99fefa44d7f701bc493b7365e.jpg width=1224 height=492 class=img_ev3q></figure></div>
<p>To reduce training costs of smaller models, DINOv2 uses ViT-g as a frozen teacher to distill ViT-L/14 and compares it against training from scratch.</p>
<p>Results show <strong>distilled ViT-L outperforms the from-scratch model across 12 benchmarks</strong>, simplifying training for smaller models and proving DINOv2’s framework has strong “teach big to small” transfer capability.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=high-resolution-training>High-Resolution Training<a href=#high-resolution-training class=hash-link aria-label="Direct link to High-Resolution Training" title="Direct link to High-Resolution Training">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="role of resolution" src=/en/assets/images/img8-e84efeaf945888f211c47934df76ad46.jpg width=1224 height=468 class=img_ev3q></figure></div>
<p>Finally, the authors tested whether high-resolution training from scratch is necessary:</p>
<ul>
<li>Compared fixed resolutions of 224, 416, and a setting that switches to 416 resolution only during the final 10k iterations;</li>
<li>Evaluation metrics included ImageNet classification and ADE segmentation;</li>
<li><strong>Training entirely at 416 resolution achieves best results but incurs triple the computational cost of 224</strong>; switching to high resolution only at the last 10k iterations yields nearly the same performance but with only a small additional computation overhead.</li>
</ul>
<p>Therefore, DINOv2 adopts the latter as a compromise, retaining the benefits of higher resolution without the cost of prolonged high-res training.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=discussion>Discussion<a href=#discussion class=hash-link aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=imagenet-classification>ImageNet Classification<a href=#imagenet-classification class=hash-link aria-label="Direct link to ImageNet Classification" title="Direct link to ImageNet Classification">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="ImageNet Classification" src=/en/assets/images/img9-039df403c9d93bf79153f965156ac2c2.jpg width=1224 height=900 class=img_ev3q></figure></div>
<p>In the ImageNet-1k linear classification task, DINOv2 demonstrates extremely strong feature representation capability.</p>
<p>Even with only a frozen backbone paired with a simple linear classifier, its accuracy surpasses all existing self-supervised methods and, for the first time under the same evaluation setup, matches or even surpasses several weakly supervised models, leading in some architectural conditions.</p>
<p>Moreover, without relying on large-scale language-text alignment or extra supervision, the learned features still show excellent linear separability and robust category generalization. Its performance in k-NN evaluation is equally stable, indicating good discriminability and neighborhood consistency.</p>
<p>Examining results across different model architectures and resolutions reveals that DINOv2 maintains considerable scalability while keeping training resolution fixed. It shows stable performance across mainstream architectures, from medium-sized ViTs to larger G-scale models.</p>
<p>Overall, the experiments clearly indicate that DINOv2 has pushed past previous self-supervised performance boundaries toward the practical threshold of weakly supervised feature learning. This makes frozen features more useful and flexible in real tasks and proves the method’s sufficient transfer potential without fine-tuning.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=video-classification>Video Classification<a href=#video-classification class=hash-link aria-label="Direct link to Video Classification" title="Direct link to Video Classification">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="Video Classification" src=/en/assets/images/img10-c3f89b5c6a433ee170d401208b8ca7be.jpg width=1224 height=448 class=img_ev3q></figure></div>
<p>In broader downstream image and video classification tasks, DINOv2’s learned representations show strong cross-modal and cross-domain generalization potential. Whether on natural fine-grained datasets (e.g., iNaturalist series), scene recognition (Places205), or varied classification benchmarks from SimCLR, DINOv2 delivers very competitive performance.</p>
<p>DINOv2 significantly outperforms weakly supervised methods in natural category recognition, demonstrating robust discrimination of fine-grained visual variations. Even in specific domains like indoor scenes (Places205), where language-aligned models slightly outperform, its overall performance remains highly usable without text supervision.</p>
<p>Extending to video classification, despite no video data seen during pretraining, DINOv2’s frozen features provide meaningful foundations for action recognition. On datasets demanding strong temporal information, it even surpasses similarly frozen CLIP-like models, showing spatial features retain rich semantics after temporal integration.</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="fine-grained benchmarks" src=/en/assets/images/img11-6da672227f0530a543b8ad27102d758b.jpg width=1500 height=432 class=img_ev3q></figure></div>
<p>Finally, across 12 image classification tasks including objects, scenes, and materials, DINOv2 maintains a clear advantage over self-supervised models and competes closely with weakly supervised methods on most datasets. It only shows slight disadvantage on text-prior-biased datasets (e.g., SUN). Especially in fine-grained vehicle and airplane classification tasks, DINOv2 performs exceptionally well, reflecting that its learned features have internalized many key discriminative cues without language guidance.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=instance-recognition>Instance Recognition<a href=#instance-recognition class=hash-link aria-label="Direct link to Instance Recognition" title="Direct link to Instance Recognition">​</a></h3>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="Instance Recognition" src=/en/assets/images/img12-1ac4a7fbde57048ed5c65b8b73edfb4b.jpg width=1498 height=490 class=img_ev3q></figure></div>
<p>In instance-level recognition tasks, DINOv2 also exhibits strong discriminative ability.</p>
<p>Using non-parametric retrieval ranking by cosine similarity between query and database images, the comparison includes both self- and weakly supervised models.</p>
<p>Results show that across domains such as landmark recognition (Oxford, Paris), artwork retrieval (Met), and historical street view matching (AmsterTime), DINOv2 significantly outperforms existing baselines. Its average precision improvements are particularly notable in challenging settings, indicating exceptional sensitivity to subtle differences and fine details.</p>
<p>Notably, DINOv2’s features excel not only at category-level classification but also at instance-level recognition, demonstrating consistent and effective representation across task granularities.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=semantic-segmentation>Semantic Segmentation<a href=#semantic-segmentation class=hash-link aria-label="Direct link to Semantic Segmentation" title="Direct link to Semantic Segmentation">​</a></h3>
<div align=center><figure style=width:70%><p><img decoding=async loading=lazy alt="Semantic segmentation" src=/en/assets/images/img13-4dad3195d3e7361d1f9b2872ca3077cb.jpg width=1224 height=596 class=img_ev3q></figure></div>
<p>In dense-level tasks such as semantic segmentation and monocular depth estimation, DINOv2 shows strong patch-level representation ability, enabling practical frozen feature predictions.</p>
<p>For semantic segmentation, even with a simple linear predictor and low-resolution upsampling, DINOv2 achieves stable performance across multiple datasets. Enhanced by simple multi-layer feature concatenation and multi-scale inference (+ms setting), its results approach fully fine-tuned MAE models and rival current state-of-the-art methods on specific datasets—despite using a much simpler prediction architecture.</p>
<p>Moreover, integrating the frozen backbone into state-of-the-art segmentation pipelines (ViT-Adapter + Mask2Former) without end-to-end training still achieves comparable accuracy, demonstrating the backbone’s high transferability and reusability.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=qualitative-results>Qualitative Results<a href=#qualitative-results class=hash-link aria-label="Direct link to Qualitative Results" title="Direct link to Qualitative Results">​</a></h3>
<p>Beyond quantitative evaluation, the authors performed qualitative analyses verifying DINOv2 features’ semantic sensitivity, structural consistency, and cross-domain transferability.</p>
<p>First, visual comparisons of depth estimation:</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="depth estimation" src=/en/assets/images/img14-3a3f2e12511a5f2ff66cb89e7407da8b.jpg width=1542 height=820 class=img_ev3q></figure></div>
<p>Semantic segmentation and depth estimation visuals show that even using only a frozen backbone with a linear predictor, DINOv2 produces semantically clear, boundary-coherent segmentation maps far superior to OpenCLIP under the same setup.</p>
<p>Depth predictions also follow a similar trend: DINOv2 outputs smoother, spatially consistent depth maps with better shape restoration, especially accurately recognizing fine objects indoors (e.g., chairs).</p>
<p>Next, evaluation on out-of-domain images tests generalization to unseen data:</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="out-of-distribution examples" src=/en/assets/images/img15-cecbe9fecbb5af4df373f075e92098d5.jpg width=1224 height=488 class=img_ev3q></figure></div>
<p>Generalization experiments on anomalous data show that even for unseen modalities (animals, paintings), DINOv2’s features enable segmentation and depth modules to produce reasonable predictions. This indicates the learned visual representations do not overfit to a single domain, possessing genuine transfer potential.</p>
<p>Finally, analysis of feature space structure is presented:</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="out-of-distribution examples" src=/en/assets/images/img16-9cd3c398bc6ed3b244655375a31749d8.jpg width=1224 height=672 class=img_ev3q></figure></div>
<p>PCA projections reveal that patch-level features naturally separate foreground and background, and even under unsupervised conditions capture internal semantic regions (e.g., structural zones, part continuity).</p>
<p>Components belonging to the same semantic class across different images (e.g., animal heads, wings) exhibit strong alignment, suggesting the model has learned a spatially consistent visual decomposition internally.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>From an engineering practitioner’s perspective, DINOv2’s greatest value lies in delivering “high-quality, versatile visual features that can be directly applied to multiple tasks without fine-tuning.”</p>
<p>This means we can solve problems originally requiring fine-tuning or extra supervision using fewer resources and simpler downstream modules. Whether classification, segmentation, depth estimation, or instance retrieval, DINOv2 provides immediately deployable frozen features, paired with minimal linear heads—significantly reducing deployment and maintenance complexity.</p>
<p>Furthermore, DINOv2 shows robustness in cross-domain performance and detailed structural recognition, making it an excellent backbone choice for tasks with scarce data, large variability, yet demanding precise semantic discrimination.</p>
<p>More importantly, it is not a black box.</p>
<p>Through PCA and patch matching analyses, we can concretely understand the semantic structures and object part segmentation encoded in the features, greatly aiding interpretability and debugging.</p>
<p>If you plan to work on medical imaging, satellite imagery, retail product recognition, or even autonomous vehicle perception modules, DINOv2 is a versatile foundational model worthy of consideration.</header></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-07-01T13:02:21.000Z itemprop=dateModified>Jul 1, 2025</time></b> by <b>zephyr-sh</b></span></div></div><section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/papers/contrastive-learning/moco-v3/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>[21.04] MoCo v3</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/papers/category/deepseek><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>DeepSeek (5)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#universal-feature-forging-method class="table-of-contents__link toc-highlight">Universal Feature Forging Method</a><li><a href=#problem-definition class="table-of-contents__link toc-highlight">Problem Definition</a><li><a href=#data-collection class="table-of-contents__link toc-highlight">Data Collection</a><ul><li><a href=#1-preparing-the-data-pool class="table-of-contents__link toc-highlight">1. Preparing the Data Pool</a><li><a href=#2-removing-duplicates class="table-of-contents__link toc-highlight">2. Removing Duplicates</a><li><a href=#3-self-supervised-retrieval class="table-of-contents__link toc-highlight">3. Self-Supervised Retrieval</a><li><a href=#engineering-scale-and-efficiency class="table-of-contents__link toc-highlight">Engineering Scale and Efficiency</a></ul><li><a href=#solution class="table-of-contents__link toc-highlight">Solution</a><ul><li><a href=#image-level-feature-alignment class="table-of-contents__link toc-highlight">Image-Level Feature Alignment</a><li><a href=#patch-level-prediction-task class="table-of-contents__link toc-highlight">Patch-Level Prediction Task</a><li><a href=#head-design class="table-of-contents__link toc-highlight">Head Design</a><li><a href=#sinkhorn-knopp-centering class="table-of-contents__link toc-highlight">Sinkhorn-Knopp Centering</a><li><a href=#koleo-regularization class="table-of-contents__link toc-highlight">KoLeo Regularization</a><li><a href=#high-resolution-training-phase class="table-of-contents__link toc-highlight">High-Resolution Training Phase</a></ul><li><a href=#engineering-implementation class="table-of-contents__link toc-highlight">Engineering Implementation</a><ul><li><a href=#flashattention class="table-of-contents__link toc-highlight">FlashAttention</a><li><a href=#sequence-packing class="table-of-contents__link toc-highlight">Sequence Packing</a><li><a href=#stochastic-depth class="table-of-contents__link toc-highlight">Stochastic Depth</a><li><a href=#distributed-training class="table-of-contents__link toc-highlight">Distributed Training</a><li><a href=#model-distillation class="table-of-contents__link toc-highlight">Model Distillation</a></ul><li><a href=#ablation-experiments class="table-of-contents__link toc-highlight">Ablation Experiments</a><ul><li><a href=#model-training-process class="table-of-contents__link toc-highlight">Model Training Process</a><li><a href=#pretraining-data class="table-of-contents__link toc-highlight">Pretraining Data</a><li><a href=#importance-of-loss-components class="table-of-contents__link toc-highlight">Importance of Loss Components</a><li><a href=#knowledge-distillation-effect class="table-of-contents__link toc-highlight">Knowledge Distillation Effect</a><li><a href=#high-resolution-training class="table-of-contents__link toc-highlight">High-Resolution Training</a></ul><li><a href=#discussion class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href=#imagenet-classification class="table-of-contents__link toc-highlight">ImageNet Classification</a><li><a href=#video-classification class="table-of-contents__link toc-highlight">Video Classification</a><li><a href=#instance-recognition class="table-of-contents__link toc-highlight">Instance Recognition</a><li><a href=#semantic-segmentation class="table-of-contents__link toc-highlight">Semantic Segmentation</a><li><a href=#qualitative-results class="table-of-contents__link toc-highlight">Qualitative Results</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>