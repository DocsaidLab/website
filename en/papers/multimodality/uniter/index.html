<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multimodality/uniter/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[19.09] UNITER | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/multimodality/uniter/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[19.09] UNITER | DOCSAID"><meta data-rh="true" name="description" content="The Song of the Unifiers"><meta data-rh="true" property="og:description" content="The Song of the Unifiers"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/multimodality/uniter/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/uniter/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/multimodality/uniter/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/uniter/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.3057f3b6.css">
<script src="/en/assets/js/runtime~main.9e79312f.js" defer="defer"></script>
<script src="/en/assets/js/main.613cb89a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/multimodality/uniter/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/multimodality/uniter/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns">Classic CNNs</a><button aria-label="Expand sidebar category &#x27;Classic CNNs&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing">Face Anti-Spoofing</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition">Face Recognition</a><button aria-label="Expand sidebar category &#x27;Face Recognition&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion">Feature Fusion</a><button aria-label="Expand sidebar category &#x27;Feature Fusion&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight">Lightweight</a><button aria-label="Expand sidebar category &#x27;Lightweight&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/multimodality">Multimodality</a><button aria-label="Collapse sidebar category &#x27;Multimodality&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/multimodality/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/meter/">[21.11] METER</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization">Normalization</a><button aria-label="Expand sidebar category &#x27;Normalization&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection">Object Detection</a><button aria-label="Expand sidebar category &#x27;Object Detection&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/reparameterization">Reparameterization</a><button aria-label="Expand sidebar category &#x27;Reparameterization&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation">Segmentation</a><button aria-label="Expand sidebar category &#x27;Segmentation&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection">Text Detection</a><button aria-label="Expand sidebar category &#x27;Text Detection&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/transformers">Transformers</a><button aria-label="Expand sidebar category &#x27;Transformers&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/vision-transformers">Vision Transformers</a><button aria-label="Expand sidebar category &#x27;Vision Transformers&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/multimodality"><span itemprop="name">Multimodality</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[19.09] UNITER</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[19.09] UNITER</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-song-of-the-unifiers">The Song of the Unifiers<a href="#the-song-of-the-unifiers" class="hash-link" aria-label="Direct link to The Song of the Unifiers" title="Direct link to The Song of the Unifiers">​</a></h2>
<p><a href="https://arxiv.org/abs/1909.11740" target="_blank" rel="noopener noreferrer"><strong>UNITER: UNiversal Image-TExt Representation Learning</strong></a></p>
<hr>
<p>We&#x27;ve seen previous papers evolve from &quot;single-stream models&quot; to &quot;dual-tower models&quot; and &quot;dual-stream models.&quot;</p>
<p>Essentially, you can either combine multimodal information in a single computation or process them separately.</p>
<p>If you process them separately, someone will later try to combine them and surpass you. If you combine them, someone will eventually separate them and surpass you.</p>
<p>We just wanted to casually read some papers, but we&#x27;ve stumbled upon some life lessons.</p>
<p>Now, let&#x27;s get back to UNITER.</p>
<p>In the field of vision and language (V+L) research, joint multimodal encoding has been widely used to bridge the semantic gap between images and text. However, these multimodal encoding methods and representations are often tailored for specific tasks. This means that methods designed for particular V+L tasks might struggle to adapt to other related tasks. This phenomenon is common in multimodal research, making it challenging for the research community to find a universal solution.</p>
<p>Previous studies, such as MCB, BAN, DFAF, SCAN, and MAttNet, have all proposed advanced methods within their specific domains. However, due to the diversity of these models&#x27; architectures and the highly task-specific nature of the learned representations, their application across various V+L tasks has been limited.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-definition">Problem Definition<a href="#problem-definition" class="hash-link" aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>The authors believe that current research, although successful in specific tasks, still faces many issues and challenges in learning generalized multimodal representations, effective masking strategies, and optimized alignment techniques:</p>
<ol>
<li>
<p><strong>Task-Specific Multimodal Encoding:</strong> Existing visual and linguistic multimodal encoding methods are often designed for specific tasks, limiting the model&#x27;s generalization ability to other V+L tasks.</p>
</li>
<li>
<p><strong>Diversity of Model Architectures and Representations:</strong> Previous studies like MCB and BAN have their unique architectures and representation methods within specific domains. Due to this diversity, these models struggle to adapt to a wide range of V+L tasks.</p>
</li>
<li>
<p><strong>Direct Application of NLP Strategies:</strong> Although Transformers and BERT have achieved great success in NLP, directly applying these strategies to the V+L field may not be the optimal solution. Specific strategies and pre-training tasks are needed to ensure success in V+L tasks.</p>
</li>
<li>
<p><strong>Limitations of Masking Strategies:</strong> Existing multimodal pre-training methods face challenges in masking strategies. Different masking strategies may affect the learned representations and further impact the performance on downstream tasks.</p>
</li>
<li>
<p><strong>Challenges in Semantic Alignment:</strong> Ensuring semantic alignment between images and text is a core challenge in multimodal research. Although previous studies have proposed some methods, fine-grained alignment remains difficult.</p>
</li>
</ol>
<p>The authors raise a central question:</p>
<ul>
<li><strong>Can we learn a truly universal image and text representation for all V+L tasks?</strong></li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solution">Solution<a href="#solution" class="hash-link" aria-label="Direct link to Solution" title="Direct link to Solution">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="uniter-model-design">UNITER Model Design<a href="#uniter-model-design" class="hash-link" aria-label="Direct link to UNITER Model Design" title="Direct link to UNITER Model Design">​</a></h3>
<p><img decoding="async" loading="lazy" alt="UNITER Model Architecture" src="/en/assets/images/arch_uniter-73c71ccee17ff17462647e07e34a49cb.jpg" width="1224" height="492" class="img_ev3q"></p>
<p>As a solution specifically designed for vision and language tasks, the primary goal of the UNITER model is to effectively bridge the semantic gap between images and text. Below is an in-depth discussion of the main components and ideas of the model&#x27;s design.</p>
<ol>
<li>
<p><strong>Model Architecture</strong></p>
<ul>
<li>The basic structure of UNITER consists of an image encoder, a text encoder, and multiple layers of Transformers. This ensures that the model can extract complex features from both images and text, which are then deeply interacted and integrated through the Transformers.</li>
</ul>
</li>
<li>
<p><strong>Encoding Strategies</strong></p>
<ul>
<li>For images, UNITER utilizes Faster R-CNN to extract visual features and encodes each region&#x27;s positional features through a 7-dimensional vector. These two features are merged via a fully connected layer, ensuring both positional and visual information are considered.</li>
<li>For text, UNITER follows BERT&#x27;s strategy, using WordPieces for tokenization and then encoding each token. This ensures effective encoding of word meanings and contextual information.</li>
</ul>
</li>
<li>
<p><strong>Self-Attention Strategy</strong></p>
<ul>
<li>UNITER&#x27;s Transformers leverage self-attention mechanisms to learn contextual encodings, allowing the model to establish deep associations between different modalities. By explicitly encoding tokens and regions&#x27; positions, the model captures more detailed contextual information.</li>
</ul>
</li>
<li>
<p><strong>Pre-Training Tasks</strong></p>
<ul>
<li>A highlight of UNITER is its diverse pre-training tasks, including masked language modeling (MLM), masked region modeling (MRM), image-text matching (ITM), and word-region alignment (WRA). These tasks are designed to enhance the model&#x27;s cross-modal learning capabilities from multiple angles.</li>
</ul>
</li>
<li>
<p><strong>Conditional Masking Strategy</strong></p>
<ul>
<li>Unlike other pre-training methods, UNITER chooses to mask only one modality while keeping the other intact. This strategy aims to prevent potential modality misalignment during the learning process, ensuring the model more accurately aligns images and text.</li>
</ul>
</li>
<li>
<p><strong>Word-Region Alignment Strategy</strong></p>
<ul>
<li>By introducing a pre-training task based on optimal transport for word-region alignment, UNITER explicitly encourages fine-grained alignment between words and image regions. This strategy ensures the model effectively optimizes cross-modal semantic alignment.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pre-training-strategy">Pre-Training Strategy<a href="#pre-training-strategy" class="hash-link" aria-label="Direct link to Pre-Training Strategy" title="Direct link to Pre-Training Strategy">​</a></h3>
<p>The pre-training strategy of the UNITER model is core to its success, designed to address the unique challenges of cross-modal learning. These strategies aim to optimize the model&#x27;s ability to deeply understand and integrate the interrelationships between images and text.</p>
<p>Let&#x27;s examine these pre-training strategies in detail:</p>
<ul>
<li>
<p><strong>Masked Language Modeling (MLM)</strong></p>
<p>Masked language modeling (MLM) is a pre-training strategy designed to enhance a model&#x27;s language understanding and cross-modal learning capabilities. Specifically, MLM works by randomly selecting and masking certain words (typically 15% of the words) in the input text and then using the context and any associated auxiliary information (such as images) to predict the masked words.</p>
<p>Imagine a scenario where we have the sentence &quot;The puppy is playing in the [MASK],&quot; accompanied by an image showing a puppy happily playing on a green lawn. In this scenario, the model needs to use the visual information from the image—namely, the puppy on the lawn—to correctly predict the word in the [MASK] position, which is &quot;lawn.&quot;</p>
<p>This approach is not merely a fill-in-the-blank game. The underlying idea is to force the model to understand not only the textual context but also the visual content of the image and its association with the text. When the model is trained to optimize this prediction task, it simultaneously learns to understand both the text and the image more deeply and intricately.</p>
</li>
<li>
<p><strong>Image-Text Matching (ITM)</strong></p>
<p>Image-text matching (ITM) is a strategy used to evaluate how well a model can match a textual description to an image. This task is not just about finding matching items; it involves assessing the deep semantic relationship between the text and the image.</p>
<p>To perform this task, the model uses a special token in the input, known as [CLS]. The purpose of this token is to generate a fused representation of the image and the text for the model. This fused representation provides a single perspective from which the model can judge whether the image and the text match.</p>
<p>For example, consider a text description &quot;A beach at sunset&quot; and a photo showing a beach with a sunset. When these are input into the model, the model will generate a fused representation with the help of the [CLS] token. This representation is then passed through a fully connected layer (FC layer) and a sigmoid function to produce a score between 0 and 1. This score indicates how well the text and the image match.</p>
<p>In the training process, along with correctly matched image-text pairs, there are so-called &quot;negative pairs,&quot; which are mismatched combinations of images and texts. For instance, the description &quot;A beach at sunset&quot; might be paired with a mountain image. These negative pairs are created by randomly selecting an image or text from other samples and pairing it with the original sample.</p>
<p>The model&#x27;s goal is to minimize its prediction errors for both the correct and negative pairs. This is usually achieved using binary cross-entropy loss, a common loss function for evaluating a model&#x27;s binary classification performance.</p>
</li>
<li>
<p><strong>Word-Region Alignment (WRA)</strong></p>
<p>Word-region alignment (WRA) is an advanced strategy that uses optimal transport (OT) to refine the association between textual elements (such as words or phrases) and image regions. The primary goal of this strategy is to ensure that the model can accurately map textual descriptions to their corresponding parts in the image.</p>
<p>For example, given the description &quot;red apple&quot; and an image containing apples of various colors, WRA aims to make the model align &quot;red apple&quot; precisely with the red apple in the image, rather than with the green or yellow apples.</p>
<p>OT provides a robust mathematical framework for achieving this goal, featuring the following characteristics:</p>
<ul>
<li><strong>Normalization:</strong> This ensures that the sum of all transport values equals 1, regularizing the alignment between the data.</li>
<li><strong>Sparsity:</strong> OT offers a sparse solution for alignment, considering only the most relevant matches, making the alignment more precise and interpretable.</li>
<li><strong>Efficiency:</strong> Although traditional OT methods can be computationally intensive, there are strategies to efficiently solve large-scale problems, which is particularly useful for large model pre-training.</li>
</ul>
<p>OT works by evaluating the distance between two distributions and optimizing a &quot;transport plan&quot; to describe how to move from one distribution to another. In this paper&#x27;s context, these two distributions are the text and the image. Once this transport plan is obtained, it can be used as a loss function to update the model parameters, improving the alignment between text and images.</p>
</li>
<li>
<p><strong>Masked Region Modeling (MRM)</strong></p>
<p>Masked region modeling (MRM) is a critical strategy in the UNITER model, specifically targeting the visual features of image regions. Similar to MLM, this strategy randomly selects and masks the features of image regions with a probability of 15%. For instance, if we have an image of multiple birds flying in the sky, MRM might randomly select and mask the features of a few birds. The model&#x27;s primary task is to use the remaining visual information and relevant textual content to reconstruct or infer the features of the masked birds. This not only strengthens the model&#x27;s understanding of image regions but also enhances its reasoning capabilities with incomplete information.</p>
</li>
</ul>
<p>There are three main variations of this strategy:</p>
<ul>
<li>
<p><strong>Masked Region Feature Regression (MRFR):</strong></p>
<p>This is the most intuitive strategy, aiming to reconstruct the masked visual features. For example, if some birds&#x27; features are masked, MRFR will attempt to directly reconstruct the masked birds&#x27; features using other birds&#x27; features and relevant textual descriptions.</p>
</li>
<li>
<p><strong>Masked Region Classification (MRC):</strong></p>
<p>This strategy is more abstract, attempting to predict the possible classes or characteristics of the masked regions. For instance, if some birds&#x27; features are masked, MRC will try to predict which species or type the masked birds might be, based on other information in the image and relevant textual content.</p>
</li>
<li>
<p><strong>Masked Region Classification with KL Divergence (MRC-kl):</strong></p>
<p>This is an advanced version of MRC. Unlike MRC, it does not solely rely on hard labels or the most likely answer but considers multiple possible answers. It uses the original output of an object detector as a form of soft labels, providing probabilities for each object category, and the model then tries to match these distributions.</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pre-training-datasets">Pre-Training Datasets<a href="#pre-training-datasets" class="hash-link" aria-label="Direct link to Pre-Training Datasets" title="Direct link to Pre-Training Datasets">​</a></h3>
<p>In this paper, the authors meticulously designed a pre-training dataset drawn from four well-known V+L datasets: COCO, Visual Genome (VG), Conceptual Captions (CC), and SBU Captions. The combination of these four datasets ensures that the model is exposed to rich and diverse data during the pre-training phase, thus enhancing its performance on subsequent tasks. However, these datasets are not used indiscriminately. Recognizing that different datasets may have varying impacts on pre-training, the authors categorized them accordingly. Firstly, COCO image caption data and VG dense caption data were combined, labeled as &quot;in-domain&quot; data because many V+L tasks&#x27; base datasets are built on them.</p>
<p>When using these &quot;in-domain&quot; data, the authors implemented specific strategies to ensure fairness and uniqueness. For example, given that COCO and Flickr30K images are both crawled from Flickr and may overlap, these overlapping images were excluded. This process ultimately resulted in 5.6 million image-text pairs for training and 131K pairs for internal validation.</p>
<p>In addition to these &quot;in-domain&quot; data, the authors also utilized two extra datasets, Conceptual Captions and SBU Captions, as &quot;out-of-domain&quot; data for pre-training.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="performance-on-pre-training-tasks">Performance on Pre-Training Tasks<a href="#performance-on-pre-training-tasks" class="hash-link" aria-label="Direct link to Performance on Pre-Training Tasks" title="Direct link to Performance on Pre-Training Tasks">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Performance on Pre-Training Tasks" src="/en/assets/images/uniter_table1-abc49a92c61b9d42bda669a6e9ce548f.jpg" width="1024" height="545" class="img_ev3q"></p>
<p>In evaluating the effectiveness of the multimodal pre-training strategies proposed by the authors, they selected four representative V+L benchmarks: VQA, NLVR2, Flickr30K, and RefCOCO+. Additionally, they introduced a global indicator, Meta-Sum, which is the sum of all scores across all benchmarks, providing a comprehensive evaluation perspective.</p>
<ol>
<li>
<p><strong>Baseline Settings</strong></p>
<p>The authors first established two baselines. The first baseline (L1) involved no pre-training, while the second baseline (L2) used MLM weights pre-trained only on text. Results indicated that even text-only pre-training significantly improved the Meta-Sum score compared to L1.</p>
</li>
<li>
<p><strong>Effect of Single Pre-Training Tasks</strong></p>
<p>Next, they explored the effect of single pre-training tasks. Specifically, when the model was pre-trained only on ITM (L4) or MLM (L5), there were significant performance improvements across all tasks compared to baselines L1 and L2.</p>
</li>
<li>
<p><strong>Combination of Pre-Training Tasks</strong></p>
<p>The authors further discovered that combining different pre-training tasks, such as MLM and ITM (L6), resulted in better performance than using any single task alone. When MLM, ITM, and MRM were trained together, the model showed consistent performance gains across all benchmarks.</p>
</li>
<li>
<p><strong>Fine-Grained Alignment</strong></p>
<p>Adding the WRA pre-training task (as in L11) led to significant improvements, particularly in VQA and RefCOCO+. This strongly suggests that learning fine-grained alignments between words and regions during pre-training is highly beneficial for downstream tasks involving region-level recognition or reasoning.</p>
</li>
<li>
<p><strong>Conditional Masking Strategy</strong></p>
<p>By comparing different masking strategies, the authors found that the conditional masking strategy allowed the model to effectively learn better joint image-text representations.</p>
</li>
<li>
<p><strong>Impact of Pre-Training Datasets</strong></p>
<p>Finally, the authors explored the effect of different pre-training datasets. Results showed that even pre-training on out-of-domain data could improve model performance as long as the data were similar to the downstream tasks. When pre-training on both in-domain and out-of-domain data, the model&#x27;s performance further improved.</p>
</li>
</ol>
<p>Through this series of experiments, the authors provided valuable insights into the effects of different pre-training settings, helping us better understand the deep mechanisms of multimodal pre-training.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="performance-on-downstream-tasks">Performance on Downstream Tasks<a href="#performance-on-downstream-tasks" class="hash-link" aria-label="Direct link to Performance on Downstream Tasks" title="Direct link to Performance on Downstream Tasks">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Performance on Downstream Tasks" src="/en/assets/images/uniter_table2-84e7b0edbdeedbd2f8ae0d8a012305e8.jpg" width="774" height="1024" class="img_ev3q"></p>
<p>The authors conducted extensive tests on various downstream tasks to evaluate the performance of the UNITER model, revealing several key findings:</p>
<ol>
<li>
<p><strong>Overall Performance</strong></p>
<p>The UNITER model achieved outstanding results across all downstream tasks, with the UNITER-large model reaching state-of-the-art levels in all benchmarks. The model&#x27;s performance even surpassed current best techniques in some tasks.</p>
</li>
<li>
<p><strong>Comparison with Task-Specific Models</strong></p>
<p>Compared to some task-specific models like MCAN, MaxEnt, and B2T2, the UNITER-based model performed exceptionally well in most tasks. For example, in the VCR task, the UNITER model outperformed the current best technique by about 2.8%.</p>
</li>
<li>
<p><strong>Comparison with Other Pre-Training Models</strong></p>
<p>Compared to other multimodal pre-training models like ViLBERT and LXMERT, UNITER outperformed in most tasks. Specifically, it excelled in VQA, outperforming all other models pre-trained on image-text pairs, with an improvement of over 1.5%.</p>
</li>
<li>
<p><strong>Single-Stream vs. Dual-Stream Models</strong></p>
<p>While previous research like ViLBERT and LXMERT observed that dual-stream models outperformed single-stream models, the authors found that the single-stream UNITER model could also achieve state-of-the-art levels under pre-training settings and with fewer parameters.</p>
</li>
<li>
<p><strong>Two-Stage Pre-Training Method</strong></p>
<p>For tasks like VCR, the authors proposed a two-stage pre-training method, first pre-training on standard datasets, then pre-training on the downstream VCR dataset. This strategy proved effective for new downstream tasks.</p>
</li>
<li>
<p><strong>Adaptability to NLVR2</strong></p>
<p>For special tasks like NLVR2, the authors tried different settings to ensure the model&#x27;s adaptability. They found that bidirectional attention mechanisms could complement inter-image interactions, leading to better performance.</p>
</li>
</ol>
<p>These results further emphasized the powerful capabilities of the UNITER model in multimodal tasks. By combining state-of-the-art pre-training strategies, it successfully achieved excellent performance across various downstream tasks, demonstrating its leading position in the field of vision and language integration.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-did-the-model-learn">What Did the Model Learn?<a href="#what-did-the-model-learn" class="hash-link" aria-label="Direct link to What Did the Model Learn?" title="Direct link to What Did the Model Learn?">​</a></h3>
<p><img decoding="async" loading="lazy" alt="UNITER Model Learning Behavior" src="/en/assets/images/uniter_table3-d55627c1d734ad2e0c2f9946ec1d0af9.jpg" width="1024" height="592" class="img_ev3q"></p>
<p>In the UNITER model, the authors conducted an in-depth analysis of the model&#x27;s learning behavior through attention visualization.</p>
<p>This visualization revealed how the model understands and connects information from different modalities. Here are the main findings:</p>
<ul>
<li>
<p><strong>Vertical Pattern</strong></p>
<p>This pattern occurs when the model mainly focuses on specific tokens such as [CLS] or [SEP]. It indicates that the model is searching for overall context or summary information at those positions. Frequent appearance of this pattern might suggest that the model is overly reliant on these special tokens, possibly due to over-parameterization or insufficient training data.</p>
</li>
<li>
<p><strong>Diagonal Pattern</strong></p>
<p>This pattern appears when the model&#x27;s attention is concentrated on tokens or regions and their immediate surroundings. It indicates that the model is parsing local information within the current context, which is a normal expected pattern.</p>
</li>
<li>
<p><strong>Vertical + Diagonal Pattern</strong></p>
<p>This is a fusion of the first two patterns, indicating that the model is interpreting both overall information and local context simultaneously.</p>
</li>
<li>
<p><strong>Block Pattern</strong></p>
<p>In this pattern, the model&#x27;s attention is mainly concentrated within its modality, such as text or visual, rather than cross-modality. This might indicate that the model is conducting certain modality-specific reasoning at that moment.</p>
</li>
<li>
<p><strong>Heterogeneous Pattern</strong></p>
<p>This pattern shows a diverse distribution of the model&#x27;s attention, indicating that the model is understanding information from different perspectives based on the current input.</p>
</li>
<li>
<p><strong>Reversed Block Pattern</strong></p>
<p>In this pattern, the model&#x27;s attention is cross-modal, discerning the relationship between text and images. The existence of this pattern indicates that the model is closely integrating visual and linguistic information.</p>
</li>
</ul>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Looking back at this paper, we can see that a few years ago, researchers continued to show deep interest in the exploration of vision and language fusion. In this study, UNITER was proposed as a large-scale pre-training model designed to establish a powerful and universal image-text representation. Through a series of ablation studies, the researchers clearly evaluated the four proposed pre-training tasks.</p>
<p>By training on both in-domain and out-of-domain datasets, UNITER demonstrated relatively outstanding performance at the time, especially in multiple vision and language tasks. The insights provided by this research also pointed out several directions worth exploring, especially in the interaction between images and sentences and more efficient pre-training strategies.</p>
<p>This paper provides us with an opportunity to review and reflect, helping us better understand the historical progress and development of the field of vision and language integration.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-09-13T07:47:46.000Z" itemprop="dateModified">Sep 13, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/multimodality/vlbert/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[19.08] VL-BERT</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/multimodality/oscar/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[20.04] Oscar</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-song-of-the-unifiers" class="table-of-contents__link toc-highlight">The Song of the Unifiers</a></li><li><a href="#problem-definition" class="table-of-contents__link toc-highlight">Problem Definition</a></li><li><a href="#solution" class="table-of-contents__link toc-highlight">Solution</a><ul><li><a href="#uniter-model-design" class="table-of-contents__link toc-highlight">UNITER Model Design</a></li><li><a href="#pre-training-strategy" class="table-of-contents__link toc-highlight">Pre-Training Strategy</a></li><li><a href="#pre-training-datasets" class="table-of-contents__link toc-highlight">Pre-Training Datasets</a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#performance-on-pre-training-tasks" class="table-of-contents__link toc-highlight">Performance on Pre-Training Tasks</a></li><li><a href="#performance-on-downstream-tasks" class="table-of-contents__link toc-highlight">Performance on Downstream Tasks</a></li><li><a href="#what-did-the-model-learn" class="table-of-contents__link toc-highlight">What Did the Model Learn?</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>