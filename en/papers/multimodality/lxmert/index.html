<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multimodality/lxmert/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">[19.08] LXMERT | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/multimodality/lxmert/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[19.08] LXMERT | DOCSAID"><meta data-rh="true" name="description" content="More Pre-training"><meta data-rh="true" property="og:description" content="More Pre-training"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/multimodality/lxmert/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/lxmert/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/multimodality/lxmert/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/lxmert/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.64d77125.css">
<script src="/en/assets/js/runtime~main.0b972c6b.js" defer="defer"></script>
<script src="/en/assets/js/main.3f214792.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/multimodality/lxmert/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/multimodality/lxmert/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Sponsor<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="Expand sidebar category &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="Expand sidebar category &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="Expand sidebar category &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="Expand sidebar category &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/multimodality-19">Multimodality (19)</a><button aria-label="Collapse sidebar category &#x27;Multimodality (19)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/multimodality/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/meter/">[21.11] METER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/blip/">[22.01] BLIP</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization-1">Normalization (1)</a><button aria-label="Expand sidebar category &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection-7">Object Detection (7)</a><button aria-label="Expand sidebar category &#x27;Object Detection (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="Expand sidebar category &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="Expand sidebar category &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="Expand sidebar category &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/transformers-15">Transformers (15)</a><button aria-label="Expand sidebar category &#x27;Transformers (15)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/vision-transformers-10">Vision Transformers (10)</a><button aria-label="Expand sidebar category &#x27;Vision Transformers (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/multimodality-19"><span itemprop="name">Multimodality (19)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[19.08] LXMERT</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[19.08] LXMERT</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="more-pre-training">More Pre-training<a href="#more-pre-training" class="hash-link" aria-label="Direct link to More Pre-training" title="Direct link to More Pre-training">​</a></h2>
<p><a href="https://arxiv.org/abs/1908.07490" target="_blank" rel="noopener noreferrer"><strong>LXMERT: Learning Cross-Modality Encoder Representations from Transformers</strong></a></p>
<hr>
<p>In recent years, we have witnessed numerous efforts to merge vision and language. This field attracts relentless exploration because it involves two primary human perception modes: vision and language, which are often inseparable in daily life.</p>
<p>Reflecting on this journey, models like ViLBERT, VL-BERT, and VisualBERT have been milestones, each attempting to integrate these powerful information sources: images and text. However, like all pioneering research, they each had their limitations and characteristics.</p>
<p>LXMERT emerged during this peak of exploration, but instead of merely repeating previous achievements, it chose a different path. By adopting more attention mechanisms and more comprehensive pre-training strategies, LXMERT aimed to address some shortcomings of its predecessors, providing a relatively complete solution for vision and language at the time.</p>
<p>So, what exactly did LXMERT attempt at the time? How did it compare to other research, and what were its features and contributions?</p>
<p>Let&#x27;s revisit and review the research of that period and understand how this field has gradually evolved.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="defining-the-problem">Defining the Problem<a href="#defining-the-problem" class="hash-link" aria-label="Direct link to Defining the Problem" title="Direct link to Defining the Problem">​</a></h2>
<p>Although the authors provided an extensive review of past literature and research in their paper:</p>
<ol>
<li>
<p><strong>Application of Transformers in Cross-Modality</strong></p>
<p>While Transformers have succeeded in machine translation, how do they adapt to single-modality and cross-modality encoders?</p>
</li>
<li>
<p><strong>Object Feature Encoding</strong></p>
<p><a href="https://arxiv.org/abs/1707.07998" target="_blank" rel="noopener noreferrer"><strong>BUTD (Anderson et al., 2018)</strong></a> proposed using object RoI features to encode images, but how can we better combine object positions and relationships to express image semantics?</p>
</li>
<li>
<p><strong>Applicability of Large-Scale Pre-trained Models</strong></p>
<p>Models like ELMo, GPT, and BERT have shown the potential of language pre-training, but how can we use these models for more challenging cross-modality tasks?</p>
</li>
<li>
<p><strong>Recent Developments in Cross-Modality Pre-Training</strong></p>
<p>Recent research, such as ViLBERT and VisualBERT, attempted similar cross-modality pre-training. However, how can a model excel in various tasks, and which additional pre-training tasks can genuinely enhance model performance?</p>
</li>
</ol>
<p>But at its core, the problem is: <strong>How to design and train a model that effectively understands and represents visual and linguistic information, excelling in various cross-modality tasks?</strong></p>
<p>Models like ViLBERT and VisualBERT were already addressing this issue, but the authors believed they were not generalized enough.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-problem">Solving the Problem<a href="#solving-the-problem" class="hash-link" aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lxmert-model-design">LXMERT Model Design<a href="#lxmert-model-design" class="hash-link" aria-label="Direct link to LXMERT Model Design" title="Direct link to LXMERT Model Design">​</a></h3>
<p><img decoding="async" loading="lazy" alt="LXMERT Model Architecture" src="/en/assets/images/arch_lxmert-9fd1e01baec2801eac650d8cce30644e.jpg" width="1024" height="325" class="img_ev3q"></p>
<ul>
<li>
<p><strong>Overall Architecture</strong></p>
<p>Inspired by the Transformer structure, LXMERT integrates self-attention and cross-attention layers, allowing the model to handle both image and text inputs simultaneously. This design enables the model to encode visual and linguistic information independently while allowing these two types of information to interact and produce a cross-modality representation.</p>
</li>
<li>
<p><strong>Input Encoding</strong></p>
<p>For image and sentence inputs, LXMERT first converts them into corresponding encoded representations. Sentences are tokenized using WordPiece tokenizer and then encoded to obtain vector representations for each word and its position in the sentence. For images, the model uses an object-level encoding method, including object region features and spatial position information to enhance spatial awareness.</p>
</li>
<li>
<p><strong>Encoder Design</strong></p>
<p>LXMERT&#x27;s core includes language encoders, object relationship encoders, and cross-modality encoders. Using self-attention and cross-attention techniques, the model captures information from each modality and establishes strong associations between different modalities.</p>
</li>
<li>
<p><strong>Output Representation</strong></p>
<p>Finally, LXMERT provides outputs at three levels: language, visual, and cross-modality. Language and visual outputs come directly from the cross-modality encoder&#x27;s feature sequence. The cross-modality output uses a special [CLS] token, which combines language and visual information and can be used for various downstream tasks.</p>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pre-training-strategy">Pre-Training Strategy<a href="#pre-training-strategy" class="hash-link" aria-label="Direct link to Pre-Training Strategy" title="Direct link to Pre-Training Strategy">​</a></h3>
<p><img decoding="async" loading="lazy" alt="LXMERT Pre-Training Tasks" src="/en/assets/images/lxmert_pretrain-1f080ab82345b75b9a28db571cb48692.jpg" width="1226" height="380" class="img_ev3q"></p>
<p>To deeply understand the connections between vision and language, LXMERT is pre-trained on a large aggregate dataset:</p>
<ol>
<li>
<p><strong>Pre-Training Tasks</strong></p>
<ul>
<li><strong>Language Tasks - Masked Cross-Modality Language Model</strong>
<ul>
<li><strong>Description</strong>: Helps the model understand and generate masked parts of sentences.</li>
<li><strong>Method</strong>: Randomly masks 15% of words in sentences. Unlike BERT, LXMERT uses visual information to predict these masked words. For example, if &quot;apple&quot; is masked but the image shows an apple, LXMERT uses the image to fill in the word.</li>
</ul>
</li>
<li><strong>Visual Tasks - Masked Object Prediction</strong>
<ul>
<li><strong>Description</strong>: Strengthens the model&#x27;s understanding of objects in images.</li>
<li><strong>Method</strong>: Randomly masks object features in images, and the model predicts the attributes or identities of these masked objects using other parts of the image or related language context.</li>
</ul>
</li>
<li><strong>Cross-Modality Tasks</strong>
<ul>
<li><strong>Cross-Modality Matching</strong>
<ul>
<li><strong>Description</strong>: Enhances the model&#x27;s ability to understand the association between images and sentences.</li>
<li><strong>Method</strong>: The model receives an image and a descriptive sentence and decides whether the sentence correctly describes the image, similar to BERT&#x27;s &quot;next sentence prediction&quot; but focused on vision-language matching.</li>
</ul>
</li>
<li><strong>Image Question Answering</strong>
<ul>
<li><strong>Description</strong>: Enables the model to answer questions related to image content.</li>
<li><strong>Method</strong>: The model receives an image and a related question and must generate or select the correct answer, requiring good visual and language understanding and cross-modality integration.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Pre-Training Data</strong></p>
<ul>
<li>Data comes from five major vision and language datasets, sourced from MS COCO and Visual Genome.</li>
<li>The overall dataset includes 180,000 images with 9.18 million image-sentence pairs.</li>
</ul>
</li>
<li>
<p><strong>Pre-Training Procedure</strong></p>
<ul>
<li>Uses the WordPiece tokenizer for sentence tokenization.</li>
<li>Uses Faster R-CNN as a feature extractor with fixed parameters.</li>
<li>Model parameters are initialized from scratch, not using pre-trained BERT parameters.</li>
<li>Multiple pre-training tasks are combined into a single training process.</li>
<li>The entire pre-training runs for 10 days on 4 Titan Xp GPUs.</li>
</ul>
</li>
<li>
<p><strong>Fine-Tuning</strong></p>
<ul>
<li>Fine-tuning is done on specific tasks.</li>
<li>Adjusts the model as needed to fit specific tasks.</li>
<li>Uses a smaller learning rate to fine-tune the model based on pre-trained parameters.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="comparison-of-lxmert-with-other-models">Comparison of LXMERT with Other Models<a href="#comparison-of-lxmert-with-other-models" class="hash-link" aria-label="Direct link to Comparison of LXMERT with Other Models" title="Direct link to Comparison of LXMERT with Other Models">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Comparison of LXMERT with Other Models" src="/en/assets/images/lxmert_table1-0a80181d21e3b95f84fb39189af540d3.jpg" width="1024" height="315" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Before starting the discussion, it&#x27;s worth noting that the author only labeled the comparison target as &quot;State-of-the-Art&quot; (SoTA), requiring further investigation to determine what exactly is being referenced.</p><p>Alright, let&#x27;s continue.</p></div></div>
<p>According to the data in the table above, LXMERT demonstrates excellent performance across various tests.</p>
<ol>
<li>
<p><strong>VQA (Visual Question Answering)</strong></p>
<ul>
<li>The previous state-of-the-art result was achieved by Kim et al. in 2018 with <a href="https://arxiv.org/abs/1805.07932" target="_blank" rel="noopener noreferrer"><strong>BAN+Counter</strong></a>, outperforming other recent methods such as MFH, Pythia, DFAF, and Cycle-Consistency.</li>
<li>LXMERT improved overall accuracy by 2.1%, with a 2.4% improvement in the &quot;Binary&quot; and &quot;Other&quot; question subcategories. Notably, even without a specific counting module like BAN+Counter, LXMERT achieved comparable or better results on counting-related questions (such as &quot;Number&quot;).</li>
</ul>
</li>
<li>
<p><strong>GQA</strong></p>
<ul>
<li>The state-of-the-art result for GQA was based on BAN, reported by Kim et al. in 2018.</li>
<li>LXMERT improved accuracy on GQA by 3.2% compared to previous SoTA methods, surpassing the improvement on VQA. This significant improvement is attributed to GQA&#x27;s reliance on visual reasoning, where LXMERT&#x27;s novel encoders and cross-modality pre-training provided a 4.6% boost in open-domain questions.</li>
</ul>
</li>
<li>
<p><strong>NLVR2</strong></p>
<ul>
<li>NLVR2 is a challenging visual reasoning dataset, where some existing methods, such as those by Hu et al. in 2017 and Perez et al. in 2018, performed poorly. The current SoTA method was presented by Suhr et al. in 2019 with <a href="https://arxiv.org/abs/1811.00491" target="_blank" rel="noopener noreferrer"><strong>MaxEnt</strong></a>.</li>
<li>The failure of many methods (including LXMERT without pre-training) indicates that without large-scale pre-training, the connection between visual and language components might not be effectively learned in complex tasks.</li>
<li>LXMERT&#x27;s novel pre-training strategy significantly improved accuracy on the NLVR2 unseen test set by 22%, achieving 76.2%. Additionally, LXMERT achieved a notable improvement in the consistency evaluation metric, reaching 42.1%, a 3.5-fold increase over previous methods.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-pre-training-affect-model-performance">How Does Pre-Training Affect Model Performance?<a href="#how-does-pre-training-affect-model-performance" class="hash-link" aria-label="Direct link to How Does Pre-Training Affect Model Performance?" title="Direct link to How Does Pre-Training Affect Model Performance?">​</a></h3>
<ol>
<li>
<p><strong>Comparison of BERT and LXMERT</strong></p>
<ul>
<li><strong>Visual Language Challenge</strong>: When applying BERT to visual language tasks like NLVR2, relying solely on language pre-training is insufficient. NLVR2, combining language and visual complexity, requires the model to understand image content and match it with natural language descriptions. This explains the 22% performance drop when using only the BERT model.</li>
<li><strong>LXMERT&#x27;s Advantage</strong>: LXMERT offers a specialized pre-training strategy that considers both language and visual information, enabling better capture of complex associations between vision and language.</li>
</ul>
</li>
<li>
<p><strong>Importance of Image QA Pre-Training</strong></p>
<ul>
<li><strong>Pre-Training Benefits</strong>: Through image QA pre-training, the model learns not only language structures but also how to interpret and answer questions based on images. This enhances performance on tasks like NLVR2, even when such data was not seen during pre-training.</li>
<li><strong>Performance Boost</strong>: In experiments, the QA pre-training strategy improved performance on NLVR2 by 2.1%.</li>
</ul>
</li>
<li>
<p><strong>Pre-Training vs. Data Augmentation</strong></p>
<ul>
<li><strong>Common Data Augmentation Strategies</strong>: Data augmentation is widely used to expand training data, increasing its diversity and helping models generalize to real-world scenarios.</li>
<li><strong>LXMERT&#x27;s Unique Strategy</strong>: Instead of merely increasing data volume, LXMERT employs a strategy of pre-training on multiple QA datasets, proving more effective than single-dataset data augmentation.</li>
</ul>
</li>
<li>
<p><strong>Impact of Visual Pre-Training Tasks</strong></p>
<ul>
<li><strong>Need for Visual Pre-Training</strong>: Relying solely on language pre-training is inadequate for visual language tasks, necessitating specific visual pre-training tasks to capture visual information better.</li>
<li><strong>Performance Improvement</strong>: Combining two visual pre-training tasks, such as RoI-feature regression and detected-label classification, further improved model performance.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="which-model-design-is-most-useful">Which Model Design is Most Useful?<a href="#which-model-design-is-most-useful" class="hash-link" aria-label="Direct link to Which Model Design is Most Useful?" title="Direct link to Which Model Design is Most Useful?">​</a></h3>
<p>From LXMERT&#x27;s ablation studies, the following conclusions can be drawn:</p>
<ul>
<li>
<p><strong>Importance of Pre-Training</strong></p>
<p>Compared to BERT without LXMERT pre-training, LXMERT shows significant performance improvements in visual language tasks, highlighting the impact of pre-training strategies on model performance.</p>
</li>
<li>
<p><strong>Cross-Modality Learning</strong></p>
<p>Ablation studies reveal that adding more cross-modality layers can further improve performance, but gains diminish after a certain number of layers. This emphasizes the importance of cross-modality learning and its performance limits.</p>
</li>
<li>
<p><strong>Impact of Data Strategies</strong></p>
<p>Comparing pre-training and data augmentation strategies, LXMERT&#x27;s pre-training strategy outperforms simple data augmentation in visual language tasks.</p>
</li>
</ul>
<p>The authors did not focus much on model architecture design, indicating this was not their primary concern.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This study delves deeply into the intricate interplay between vision and language. LXMERT&#x27;s unique Transformer encoder and cross-modality encoders ensure that vision and language are closely integrated throughout the learning process.</p>
<p>This close interaction provides additional strength, especially during large-scale pre-training. By combining this pre-training strategy with various pre-training tasks, LXMERT achieves notable results across a range of visual language tasks. It sets new standards on major image QA datasets like VQA and GQA and demonstrates impressive generality on NLVR2.</p>
<p>LXMERT not only advances the frontier of visual language research but also lays a solid foundation for future cross-modality learning.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">Sep 11, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/category/multimodality-19"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Multimodality (19)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/multimodality/vilbert/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[19.08] ViLBERT</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#more-pre-training" class="table-of-contents__link toc-highlight">More Pre-training</a></li><li><a href="#defining-the-problem" class="table-of-contents__link toc-highlight">Defining the Problem</a></li><li><a href="#solving-the-problem" class="table-of-contents__link toc-highlight">Solving the Problem</a><ul><li><a href="#lxmert-model-design" class="table-of-contents__link toc-highlight">LXMERT Model Design</a></li><li><a href="#pre-training-strategy" class="table-of-contents__link toc-highlight">Pre-Training Strategy</a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#comparison-of-lxmert-with-other-models" class="table-of-contents__link toc-highlight">Comparison of LXMERT with Other Models</a></li><li><a href="#how-does-pre-training-affect-model-performance" class="table-of-contents__link toc-highlight">How Does Pre-Training Affect Model Performance?</a></li><li><a href="#which-model-design-is-most-useful" class="table-of-contents__link toc-highlight">Which Model Design is Most Useful?</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Docs</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/terms-of-service" target="_blank" rel="noopener noreferrer" class="footer__link-item">TermsOfUse<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><span class="footer__link-separator">·</span><a href="https://docsaid.org/blog/privacy-policy" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy Policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>