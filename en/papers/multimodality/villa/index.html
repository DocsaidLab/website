<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-papers docs-version-current docs-doc-page docs-doc-id-multimodality/villa/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.6.1">
<title data-rh="true">[20.06] VILLA | DOCSAID</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docsaid.org/en/img/docsaid-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docsaid.org/en/papers/multimodality/villa/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_hant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-papers-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-papers-current"><meta data-rh="true" property="og:title" content="[20.06] VILLA | DOCSAID"><meta data-rh="true" name="description" content="The Phantom in the Villa"><meta data-rh="true" property="og:description" content="The Phantom in the Villa"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docsaid.org/en/papers/multimodality/villa/"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/villa/" hreflang="zh-hant"><link data-rh="true" rel="alternate" href="https://docsaid.org/en/papers/multimodality/villa/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docsaid.org/papers/multimodality/villa/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S9NC0RYCHF-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/en/blog/rss.xml" title="DOCSAID RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/blog/atom.xml" title="DOCSAID Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="DOCSAID" href="/en/opensearch.xml">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.51ef4fe1.css">
<script src="/en/assets/js/runtime~main.d118cdf3.js" defer="defer"></script>
<script src="/en/assets/js/main.1375d374.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/en/docs/">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/papers/intro">Papers</a><a class="navbar__item navbar__link" href="/en/blog">Blog</a><a class="navbar__item navbar__link" href="/en/playground/intro">Playground</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/papers/multimodality/villa/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-hant">繁體中文</a></li><li><a href="/en/papers/multimodality/villa/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://github.com/DocsaidLab" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/en/"><img src="/en/img/docsaid_logo.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/docsaid_logo_white.png" alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">Research Paper Notes</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/classic-cnns-11">Classic CNNs (11)</a><button aria-label="Expand sidebar category &#x27;Classic CNNs (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-anti-spoofing-1">Face Anti-Spoofing (1)</a><button aria-label="Expand sidebar category &#x27;Face Anti-Spoofing (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/face-recognition-4">Face Recognition (4)</a><button aria-label="Expand sidebar category &#x27;Face Recognition (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/feature-fusion-7">Feature Fusion (7)</a><button aria-label="Expand sidebar category &#x27;Feature Fusion (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/lightweight-10">Lightweight (10)</a><button aria-label="Expand sidebar category &#x27;Lightweight (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/llm-tuning-5">LLM Tuning (5)</a><button aria-label="Expand sidebar category &#x27;LLM Tuning (5)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/mamba-1">Mamba (1)</a><button aria-label="Expand sidebar category &#x27;Mamba (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/en/papers/category/multimodality-21">Multimodality (21)</a><button aria-label="Collapse sidebar category &#x27;Multimodality (21)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/lxmert/">[19.08] LXMERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vilbert/">[19.08] ViLBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/visualbert/">[19.08] VisualBERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vlbert/">[19.08] VL-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/uniter/">[19.09] UNITER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/oscar/">[20.04] Oscar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/pixelbert/">[20.04] Pixel-BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/ernie-vil/">[20.06] ERNIE-ViL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/papers/multimodality/villa/">[20.06] VILLA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/unimo/">[20.12] UNIMO</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vinvl/">[21.01] VinVL</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vilt/">[21.02] ViLT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/vlt5/">[21.02] VL-T5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/clip/">[21.03] CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/mdetr/">[21.04] MDETR</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/albef/">[21.07] ALBEF</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/simvlm/">[21.08] SimVLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/meter/">[21.11] METER</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/blip/">[22.01] BLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/flip/">[22.12] FLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/papers/multimodality/blip2/">[23.01] BLIP-2</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/normalization-1">Normalization (1)</a><button aria-label="Expand sidebar category &#x27;Normalization (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/object-detection-8">Object Detection (8)</a><button aria-label="Expand sidebar category &#x27;Object Detection (8)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/reparameterization-7">Reparameterization (7)</a><button aria-label="Expand sidebar category &#x27;Reparameterization (7)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/segmentation-1">Segmentation (1)</a><button aria-label="Expand sidebar category &#x27;Segmentation (1)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-detection-10">Text Detection (10)</a><button aria-label="Expand sidebar category &#x27;Text Detection (10)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-recognition-20">Text Recognition (20)</a><button aria-label="Expand sidebar category &#x27;Text Recognition (20)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/text-spotting-4">Text Spotting (4)</a><button aria-label="Expand sidebar category &#x27;Text Spotting (4)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/transformers-15">Transformers (15)</a><button aria-label="Expand sidebar category &#x27;Transformers (15)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/en/papers/category/vision-transformers-11">Vision Transformers (11)</a><button aria-label="Expand sidebar category &#x27;Vision Transformers (11)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/papers/intro">All Notes: 137 entries</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/en/papers/category/multimodality-21"><span itemprop="name">Multimodality (21)</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">[20.06] VILLA</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>[20.06] VILLA</h1></header>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-phantom-in-the-villa">The Phantom in the Villa<a href="#the-phantom-in-the-villa" class="hash-link" aria-label="Direct link to The Phantom in the Villa" title="Direct link to The Phantom in the Villa">​</a></h2>
<p><strong><a href="https://arxiv.org/abs/2006.06195" target="_blank" rel="noopener noreferrer">Large-Scale Adversarial Training for Vision-and-Language Representation Learning</a></strong></p>
<hr>
<p>This time, we delve into an intriguing paper.</p>
<p>When trying to understand how to make machines better at understanding and describing objects in images, we face a challenge: how to ensure that the machine performs well not just in controlled settings but also in real-world scenarios with unpredictable problems?</p>
<p>Imagine you are training a boxer.</p>
<p>This boxer performs excellently on punching bags in the gym but struggles against real opponents who use various strategies and techniques to defeat him. Real opponents don&#x27;t stay still like punching bags.</p>
<p>To make this boxer perform better in matches, you need to simulate real adversarial situations, giving him sudden challenges to learn how to respond. For instance, you could create a &quot;high-tech punching bag&quot; that detects the boxer&#x27;s movements, grows limbs, and fights back.</p>
<p>You might say, &quot;What on earth?&quot;</p>
<p>Exactly, the model&#x27;s reaction would be the same!</p>
<p>This design of showing the model &quot;who knows what kind of weird things&quot; is the concept of adversarial training.</p>
<p>But doesn&#x27;t this sound like a denoising task?</p>
<p>These two can be easily confused, but they are different concepts.</p>
<p>Let&#x27;s compare them in terms of concepts and objectives:</p>
<ol>
<li>
<p><strong>Denoising</strong></p>
<ul>
<li><strong>Objective</strong>: The goal is to remove noise from images or data, restoring the original, undisturbed data.</li>
<li><strong>Concept</strong>: When you have a noisy image (e.g., due to low light or compression), the denoising process tries to eliminate the noise, making the image closer to its original, clean state.</li>
<li><strong>Method</strong>: Using specific algorithms or pre-trained models that have learned to identify and remove various types of noise.</li>
</ul>
</li>
<li>
<p><strong>Adversarial Training</strong></p>
<ul>
<li><strong>Objective</strong>: To make the model correctly predict even in the presence of &quot;adversarial attacks.&quot;</li>
<li><strong>Concept</strong>: During model training, not only standard data is provided, but also &quot;adversarial examples,&quot; which are deliberately modified inputs that look almost identical to normal data but lead the model to incorrect predictions. Training the model to recognize and resist these examples enhances its generalization.</li>
<li><strong>Method</strong>: Typically involves two steps: generating adversarial examples and using them to train the model.</li>
</ul>
</li>
</ol>
<p>Denoising focuses on restoring original, clean data, while adversarial training aims to enhance the model&#x27;s generalization, making it resistant to malicious, misleading inputs. Moreover, denoising typically involves known noise patterns, trying to remove them, whereas adversarial training involves creating new, potentially misleading inputs.</p>
<p>In training models, we not only show them standard, familiar images and texts but also introduce &quot;hypothetical opponents&quot; – inputs that are &quot;deliberately modified to confuse the model.&quot; These &quot;hypothetical opponents&quot; are like sudden challenges in training, forcing the model to learn to handle various situations.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="problem-definition">Problem Definition<a href="#problem-definition" class="hash-link" aria-label="Direct link to Problem Definition" title="Direct link to Problem Definition">​</a></h2>
<p>The authors focus on the following key issues in this paper:</p>
<ol>
<li>
<p>Overfitting in pre-trained models</p>
<p>Due to the large capacity of pre-trained models and the limited labeled data in downstream tasks, models tend to overfit.</p>
</li>
<li>
<p>Application of adversarial training</p>
<p>Although adversarial training has proven effective in other domains (such as images and text), how it applies to vision-and-language (V+L) problems and whether it can improve model performance remains unresolved.</p>
</li>
<li>
<p>Adversarial training in multimodal encoding</p>
<p>Traditional adversarial training methods focus on image pixels and text subword token levels. The authors propose that adversarial training at the multimodal encoding level might be more beneficial.</p>
</li>
<li>
<p>Enhancing performance on clean inputs</p>
<p>While the goal of adversarial training is to improve model resistance to adversarial attacks, can it also enhance performance on clean inputs?</p>
</li>
<li>
<p>Efficiency of adversarial training</p>
<p>Adversarial training is computationally expensive and time-consuming. How can we improve the efficiency of large-scale training?</p>
</li>
<li>
<p>Balancing robustness and generalization</p>
<p>How can we enhance model robustness (resistance to adversarial attacks) while maintaining or improving its generalization on clean data?</p>
</li>
<li>
<p>General applicability of adversarial training in V+L tasks</p>
<p>Considering that VILLA is a new adversarial training strategy, the authors need to explore its applicability and effectiveness across different V+L tasks.</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solution">Solution<a href="#solution" class="hash-link" aria-label="Direct link to Solution" title="Direct link to Solution">​</a></h2>
<p>This section is a bit technical, but let&#x27;s read through it together.</p>
<p>The authors structure the entire model into three parts.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="villa-model-design">VILLA Model Design<a href="#villa-model-design" class="hash-link" aria-label="Direct link to VILLA Model Design" title="Direct link to VILLA Model Design">​</a></h3>
<p><img decoding="async" loading="lazy" alt="VILLA Model Design" src="/en/assets/images/villa_1-63baf9b3eda3521b902813a6ab7f0de9.jpg" width="1224" height="332" class="img_ev3q"></p>
<p>The primary goal of this stage is to learn generalized image and text representations applicable to various downstream tasks.</p>
<ol>
<li>
<p><strong>Dataset</strong></p>
<p>The dataset for pre-training (denoted as Dp in the paper) consists of image-text pairs (X_img, X_txt).</p>
</li>
<li>
<p><strong>Feature Representation</strong></p>
<ul>
<li>Images (X_img) are transformed into feature vectors through a bottom-up feature extractor (g_bu(·)).</li>
<li>Text (X_txt) is converted into feature vectors through a learnable word embedding function (g_emb(·)).</li>
</ul>
</li>
<li>
<p><strong>Multimodal Fusion</strong></p>
<p>To fuse image and text features, a multi-layer Transformer structure is used, popular in natural language processing and other multimodal tasks. Here, the [CLS] token holds special significance, with its encoding serving as the joint representation for multimodal tasks.</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pre-training-tasks">Pre-Training Tasks<a href="#pre-training-tasks" class="hash-link" aria-label="Direct link to Pre-Training Tasks" title="Direct link to Pre-Training Tasks">​</a></h3>
<p>MLM and ITM have been discussed in previous papers, so we&#x27;ll briefly cover them and delve into the MRM strategy.</p>
<ul>
<li>
<p><strong>Masked Language Model (MLM)</strong></p>
<p>In masked language modeling, about 15% of the input tokens are randomly selected and masked. The model&#x27;s task is to predict these masked tokens based on other visible language tokens and provided visual tokens.</p>
</li>
<li>
<p><strong>Image-Text Matching (ITM)</strong></p>
<p>ITM is a complex strategy designed to evaluate the model&#x27;s ability to understand the deep semantic relationships between images and their corresponding textual descriptions.</p>
</li>
<li>
<p><strong>Masked Region Modeling (MRM)</strong></p>
<p>The main objective of MRM is to enable the model to learn how to infer the masked parts based on complete image information. This ability helps the model better understand and recognize various parts of the image and effectively combine them with language features in subsequent tasks. This method is similar to &quot;Masked Language Modeling&quot; (MLM) in natural language processing but applies to image regions instead of text tokens.</p>
<p>In image data X_img​, specific regions (randomly chosen or selected by a strategy) are masked by setting their features to zero or using other methods. These masked regions are treated as parts that the model needs to &quot;fill in.&quot;</p>
<p>Given other unmasked multimodal information (such as other parts of the image and related text descriptions), the model&#x27;s task is to predict the correct content of the masked regions. This prediction can be quantified through cross-entropy loss, KL divergence loss, or contrastive learning.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>This is akin to having a children&#x27;s coloring book where part of an apple&#x27;s image is erased or covered. Despite seeing only part of the apple, based on previous complete apples you&#x27;ve seen, you can guess how the missing part should look.</p></div></div>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="two-stage-adversarial-training">Two-Stage Adversarial Training<a href="#two-stage-adversarial-training" class="hash-link" aria-label="Direct link to Two-Stage Adversarial Training" title="Direct link to Two-Stage Adversarial Training">​</a></h3>
<p>This refers to both pre-training and fine-tuning stages.</p>
<p>This discussion explores how to conduct adversarial training during pre-training and fine-tuning stages and their relationship.</p>
<ol>
<li>
<p><strong>Connection Between Pre-Training and Fine-Tuning</strong></p>
<p>Pre-training and fine-tuning are two main stages in most deep learning workflows. Pre-training aims to gain a fundamental understanding and basic feature extraction capabilities, while fine-tuning specializes the model for specific tasks. These two stages are closely related, with pre-training providing the necessary foundation for fine-tuning.</p>
</li>
<li>
<p><strong>Importance of Cross-Modal Understanding</strong></p>
<p>The model needs to understand both image and text content in tasks like MLM or VQA. For example, if a picture contains a dog, the model needs to link this visual information with the word &quot;dog.&quot;</p>
</li>
<li>
<p><strong>Assumptions in Adversarial Training</strong></p>
<ul>
<li>The first assumption is that adversarial pre-training can enhance the model&#x27;s generalization ability, benefiting the fine-tuning stage. Generalization refers to the model&#x27;s performance on unseen data.</li>
<li>The second assumption is that during the fine-tuning stage, with specific task training data, adversarial training methods can be reused to further improve model performance.</li>
</ul>
</li>
<li>
<p><strong>Shared Mathematical Formulas</strong></p>
<p>Since pre-training and fine-tuning stages are mathematically similar, both can employ the same adversarial training methods.</p>
</li>
</ol>
<p>It is emphasized that both stages can undergo adversarial training to improve the model&#x27;s generalization ability and robustness against attacks.</p>
<p>Adversarial training in the pre-training stage helps enhance the model&#x27;s basic generalization ability, while in the fine-tuning stage, it can further improve performance for specific tasks.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="perturbations">Perturbations<a href="#perturbations" class="hash-link" aria-label="Direct link to Perturbations" title="Direct link to Perturbations">​</a></h3>
<p>This section explores how to add adversarial perturbations in the embedding space of image and text.</p>
<ol>
<li>
<p><strong>Perturbations in Image Modality</strong></p>
<p>In state-of-the-art V+L models, the input typically consists of image features obtained from pre-trained object detectors. Unlike traditional methods that add perturbations at the pixel level, here, perturbations are added directly in the feature encoding space. For example, instead of making minor adjustments to the pixels of a dog picture, perturbations are made to its feature representation in the model. This allows for more precise manipulation of the encoding, making the perturbations stricter.</p>
</li>
<li>
<p><strong>Perturbations in Text Modality</strong></p>
<p>Unlike the continuous values in image pixels, text tokens are discrete, making them harder to manipulate. Traditionally, creating adversarial examples that preserve the original semantics is challenging in text modality. However, this method chooses to add perturbations in the word embedding space instead of directly altering the words. This avoids changing the original text content while still affecting the model&#x27;s prediction.</p>
<p>For instance, consider the sentence &quot;The dog barks.&quot; Instead of replacing or modifying any word, slight adjustments are made to the embedding of the word &quot;dog,&quot; leading to a representation that is similar but slightly different in the model.</p>
</li>
<li>
<p><strong>Position Encoding</strong></p>
<p>In pre-trained V+L models, position encoding is used to encode the positions of image regions and subword tokens. In this adversarial training method, only image and word encodings are modified, keeping other features unchanged.</p>
</li>
<li>
<p><strong>Consideration for Simultaneous Perturbations</strong></p>
<p>The authors suggest adding perturbations to one modality (image or text) at a time. This is based on the unique characteristics and differences of images and text. After adding perturbations, the goal is for the model&#x27;s predictions to remain unchanged, preserving the original semantics.</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="multimodal-free-adversarial-training">Multimodal &quot;Free&quot; Adversarial Training<a href="#multimodal-free-adversarial-training" class="hash-link" aria-label="Direct link to Multimodal &quot;Free&quot; Adversarial Training" title="Direct link to Multimodal &quot;Free&quot; Adversarial Training">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Multimodal &amp;quot;Free&amp;quot; Adversarial Training" src="/en/assets/images/villa_2-5cc697e3e18b6b7d83ebffb737feae57.jpg" width="1024" height="616" class="img_ev3q"></p>
<p>This section elaborates on the multimodal &quot;free&quot; adversarial training method used in the VILLA model, involving multiple mathematical formulas and technical details. Don&#x27;t worry, let&#x27;s break it down:</p>
<ul>
<li>
<p><strong>Training Objective</strong></p>
<p>In the VILLA model, &quot;adversarial training&quot; is a core technique aimed at maintaining stable prediction performance even when the input data is slightly perturbed. This method enhances the model&#x27;s generalization, making it less susceptible to external noise or minor changes, ensuring reliable predictions in various scenarios. This means the model can perform well on both training data and new, unseen data.</p>
<p>The training process considers three main loss components:</p>
<ol>
<li>
<p><strong>Standard Cross-Entropy Loss (L-std)</strong></p>
<p>This is the basic loss function used in most classification problems, measuring the gap between the model&#x27;s predictions and the true labels. Smaller cross-entropy loss indicates the model&#x27;s predictions are closer to reality.</p>
</li>
<li>
<p><strong>Label-Preserving Adversarial Training Loss (R-at)</strong></p>
<p>This ensures that the model&#x27;s predictions remain the same when the input data undergoes slight perturbations. In other words, even with minor changes to the image or text, the model should produce the same prediction.</p>
</li>
<li>
<p><strong>Fine-Grained Adversarial Regularization Term (R-kl)</strong></p>
<p>This more complex loss component requires the model not only to maintain the same label with perturbed inputs but also to ensure that the prediction confidence or probability distribution remains similar. This ensures that the model&#x27;s generalization is preserved not only at the label level but also in the details of the prediction.</p>
</li>
</ol>
</li>
<li>
<p><strong>Multimodal Adversarial Perturbations</strong></p>
<p>In training the VILLA model, not only are the original image and text encodings considered, but adversarial perturbations are also added to these encodings. These perturbations are intentionally introduced minor changes to test and enhance the model&#x27;s generalization, ensuring it works correctly even with unknown, minor noise or variations.</p>
<p>However, these adversarial perturbations are not added arbitrarily. They have specific &quot;norm constraints,&quot; meaning the intensity or magnitude of these perturbations is controlled to ensure they don&#x27;t cause significant changes that make the model unrecognizable.</p>
<p>The model&#x27;s training involves two main optimization steps: outer minimization and inner maximization. Outer minimization aims to minimize the model&#x27;s prediction error (loss) during the overall training process, achievable through common gradient descent methods like SGD (Stochastic Gradient Descent). Inner maximization seeks to find adversarial perturbations that maximize the loss, identifying the most likely disturbances to the model. This optimization is done using a method called PGD (Projected Gradient Descent).</p>
<p>Specifically, for image modality, during each iteration of updating perturbations, PGD first calculates the current perturbation&#x27;s impact on the loss, i.e., the gradient of the loss with respect to the perturbation. Then, it takes a small step in this gradient direction to find a new perturbation that maximizes the loss. This process is repeated until the preset number of iterations or perturbation size limit is reached.</p>
<p>Imagine you are training a machine learning model to distinguish between dogs and cats in photos. The original image encoding represents the features or information extracted from the photo.</p>
<ol>
<li>
<p><strong>Adding Adversarial Perturbations</strong></p>
<p>Suppose someone makes minor pixel adjustments to the photos during training, making cats look more like dogs or vice versa. These adjustments are the &quot;adversarial perturbations.&quot;</p>
</li>
<li>
<p><strong>Norm Constraints</strong></p>
<p>However, these adjustments are not arbitrary. They have a limit to ensure the changes are not too significant. For example, the perturbations won&#x27;t deform the entire cat&#x27;s face, only minor adjustments enough to confuse the model.</p>
</li>
<li>
<p><strong>Optimization</strong></p>
<p>In each training iteration, the model tries to identify these adjustments and learn to ignore them, focusing on distinguishing features between dogs and cats. Using SGD, the model adjusts its parameters based on these adjustments&#x27; impact, reducing the error rate on these perturbed images. Using PGD, the model tries to find the most likely disturbances in each iteration, learning and adjusting for the worst-case scenarios.</p>
</li>
</ol>
</li>
<li>
<p><strong>Further Enhancement of Adversarial Regularization</strong></p>
<p>Adversarial regularization is a technique in machine learning aimed at improving the model&#x27;s generalization, ensuring it can make correct predictions even with adversarial perturbations. In some situations, adversarial perturbations can significantly alter the model&#x27;s prediction, and this regularization method seeks to limit such changes.</p>
<p>This enhanced adversarial regularization not only requires the model to maintain correct classifications with perturbations but also ensures that the prediction confidence or probability distribution remains similar before and after perturbations. This means the model must be confident in its predictions, even with minor disturbances.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Kullback-Leibler divergence is a tool for measuring the difference between two probability distributions. In this context, it compares the similarity of the model&#x27;s prediction distributions before and after perturbations. If the distributions are similar, the KL divergence will be close to 0; otherwise, the value will increase.</p></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>For example:</p><p>Consider a model that predicts a photo&#x27;s animal as a cat with 90% probability without perturbations. After adversarial perturbations, the model&#x27;s prediction confidence drops to 60%. This change indicates that the perturbations affected the model&#x27;s certainty. The difference between these confidence levels, captured by KL divergence, encourages the model to maintain similar prediction confidence with perturbations.</p></div></div>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="free-adversarial-training-strategy">Free Adversarial Training Strategy<a href="#free-adversarial-training-strategy" class="hash-link" aria-label="Direct link to Free Adversarial Training Strategy" title="Direct link to Free Adversarial Training Strategy">​</a></h3>
<p>Adversarial Training (AT) is a method to enhance model generalization, using perturbed data to improve resistance to adversarial attacks. Here, the authors mention a &quot;Free&quot; adversarial training strategy.</p>
<ul>
<li>
<p><strong>Computational Cost of K-step PGD</strong></p>
<p>PGD (Projected Gradient Descent) is commonly used in adversarial training. Using K-step PGD requires K forward and backward passes through the model, which is time-consuming. Moreover, only the final perturbation after K steps is used for model training, meaning all previous steps are just for generating this final perturbation.</p>
</li>
<li>
<p><strong>Solution: FreeLB</strong></p>
<p>To overcome the computational challenges and effectively execute large-scale training, the authors adopt a method called FreeLB. FreeLB performs multiple PGD iterations to create adversarial encodings and accumulates the &quot;free&quot; parameter gradients ∇θL during each iteration. Instead of updating model parameters after each iteration, it updates them once using the accumulated gradients after multiple iterations.</p>
<p>This strategy simulates a larger &quot;virtual&quot; mini-batch, effectively mimicking a K-times larger mini-batch, making each update richer and more diverse.</p>
</li>
</ul>
<p>To make it more relatable, let&#x27;s consider an example:</p>
<p>Imagine assembling a bicycle, where each step involves specific parts.</p>
<ul>
<li>
<p><strong>Traditional K-step PGD Method</strong></p>
<p>This is like testing the bicycle&#x27;s functionality at each assembly step. For instance, after installing the pedals, you take a short ride, then add the chain and ride again, testing at each step. While this ensures each part is correctly installed, it&#x27;s time-consuming.</p>
<p>However, you find that only the final ride test after assembling all parts is crucial, indicating whether the entire bicycle is correctly assembled.</p>
</li>
<li>
<p><strong>FreeLB Strategy</strong></p>
<p>Now, imagine a different assembly strategy.</p>
<p>Instead of testing at each step, you note potential issues or considerations during assembly (equivalent to accumulating gradients). After assembling all parts, you conduct a comprehensive test and adjustment based on accumulated issues and considerations.</p>
<p>This method allows more efficient assembly, focusing on comprehensive adjustments at the end rather than time-consuming tests at each step.</p>
</li>
</ul>
<p>Of course, all analogies have limitations and may not perfectly map to the target concept, but they help provide a feel.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2>
<p>The focus of this paper is mainly on the design of adversarial training.</p>
<p>But we still need to see if this design is effective. And how effective?</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="experimental-setup">Experimental Setup<a href="#experimental-setup" class="hash-link" aria-label="Direct link to Experimental Setup" title="Direct link to Experimental Setup">​</a></h3>
<p>To validate the functionality and effectiveness of VILLA, the authors conducted a series of experimental applications. These applications focused on V+L pre-trained models and comprehensively evaluated various downstream tasks, including Visual Question Answering (VQA), Visual Commonsense Reasoning (VCR), Refer Expression (RE) compression, Visual Entailment, Image-Text Retrieval, and NLVR2.</p>
<p>The validation process was divided into two stages: first, incorporating VILLA into the leading UNITER model for downstream task evaluation and ablation analysis. Second, to demonstrate VILLA&#x27;s broader applicability, the authors chose another V+L model, LXMERT, for more comprehensive testing.</p>
<p>If you haven&#x27;t read about these two models, refer to the following:</p>
<ul>
<li>
<p><strong>Links: <a href="/en/papers/multimodality/lxmert/">LXMERT</a>, <a href="/en/papers/multimodality/uniter/">UNITER</a></strong></p>
</li>
<li>
<p><strong>Main Model Configurations: UNITER and LXMERT</strong></p>
<ul>
<li>
<p><strong>UNITER</strong></p>
<ul>
<li>UNITER-base: A single-stream model with 12 layers, 768 hidden units per layer, and 12 attention heads.</li>
<li>UNITER-large: A larger version with 24 layers, 1024 hidden units per layer, and 16 attention heads.</li>
<li>Same structure as BERT, but with inputs combining both modalities (vision and language).</li>
</ul>
</li>
<li>
<p><strong>LXMERT</strong></p>
<ul>
<li>LXMERT is a dual-stream model with independent self-attention layers for each modality (9 layers for text, 5 layers for vision), followed by 5 layers for cross-attention and self-attention processing.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Setup Details</strong></p>
<ul>
<li>For UNITER experiments, the authors used four main large datasets for pre-training: COCO, Visual Genome (VG), Conceptual Captions, and SBU Captions.</li>
<li>VILLA also applies to MLM and ITM pre-training tasks, with different training steps provided for UNITER-base and UNITER-large to ensure fair comparison and training time consideration.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="villa-on-uniter-and-analysis">VILLA on UNITER and Analysis<a href="#villa-on-uniter-and-analysis" class="hash-link" aria-label="Direct link to VILLA on UNITER and Analysis" title="Direct link to VILLA on UNITER and Analysis">​</a></h3>
<p><img decoding="async" loading="lazy" alt="VILLA on UNITER and Analysis" src="/en/assets/images/villa_3-06ffbecd19ded8cece9bddc9e363514c.jpg" width="982" height="1024" class="img_ev3q"></p>
<ol>
<li>
<p><strong>VILLA Compared to Other Pre-Trained V+L Models</strong></p>
<ul>
<li>VILLA achieved state-of-the-art performance on all test benchmarks.</li>
<li>VILLA-base model improvements:</li>
</ul>
</li>
<li>
<p><strong>In VQA: +0.76 increase over UNITER-base</strong></p>
<ul>
<li>In VCR Q→AR: +2.4 increase over UNITER-base</li>
<li>In NLVR2: +1.45 increase over UNITER-base</li>
<li>In SNLI-VE: Outperforms UNITER-base</li>
<li>In Flickr30k image/text retrieval: +2.22/+0.70 (R@1) increase over UNITER-base</li>
<li>Average increase of +0.99 across three RE datasets.</li>
</ul>
</li>
<li>
<p><strong>VILLA-large model improvements</strong></p>
<ul>
<li>Similar performance improvement trends overall.</li>
<li>In VCR Q→AR: Absolute increase of +2.9 points, significant for understanding complex social dynamics implicitly encoded in images.</li>
<li>In VQA benchmark: From 74.02 to 74.87.</li>
<li>Ensemble strategy further improved VILLA-large performance to 75.85.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="in-depth-analysis-of-villa">In-Depth Analysis of VILLA<a href="#in-depth-analysis-of-villa" class="hash-link" aria-label="Direct link to In-Depth Analysis of VILLA" title="Direct link to In-Depth Analysis of VILLA">​</a></h3>
<ol>
<li>
<p><strong>Pre-Training vs. Fine-Tuning</strong></p>
<p><img decoding="async" loading="lazy" alt="Pre-Training vs. Fine-Tuning" src="/en/assets/images/villa_5-1b2428a2724103cc86a2b0f7941d64c3.jpg" width="1024" height="207" class="img_ev3q"></p>
<p>Using UNITER-base to study the effects of adversarial training in pre-training and fine-tuning stages.</p>
<ul>
<li>UNITER (reimp.): UNITER-base model re-implemented with standard training.</li>
<li>VILLA-pre vs. VILLA-fine: Models applying adversarial training only in pre-training or fine-tuning stages, respectively.</li>
<li>Results: VILLA-pre and VILLA-fine provided +0.51 and +0.82 average performance gains across six evaluation tasks. Combining both stages resulted in +1.15 gain.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="Training Curves" src="/en/assets/images/villa_4-66aaec9c13b31703ff12e2fd707176cc.jpg" width="1024" height="271" class="img_ev3q"></p>
<p>The training curves show that with increased training steps, the gap between adversarially strengthened models and the original UNITER widens.</p>
</li>
<li>
<p><strong>Image vs. Text Modality (Table 3a)</strong></p>
<p><img decoding="async" loading="lazy" alt="Image vs. Text Modality" src="/en/assets/images/villa_6-ef23c4f6a6c1600ca298110090a6dc44.jpg" width="1024" height="301" class="img_ev3q"></p>
<p>When discussing adversarial examples, these are deliberately designed inputs that look similar to original inputs to humans but lead models to incorrect predictions. Here, perturbations are added focusing on image and text modalities.</p>
<ul>
<li>Experiments and Results<!-- -->
<ul>
<li>Independent image perturbation: Adding perturbation only to image features significantly improves model performance. This contrasts with traditional belief where adversarial training often negatively impacts model accuracy on clean images.</li>
<li>Intuition vs. Actual Results: Initially, it might seem that adding perturbations to both modalities (image and text) simultaneously would be more beneficial due to increased adversarial example diversity. However, adding perturbations to a single modality already provides significant performance improvements.</li>
</ul>
</li>
</ul>
<p>The VCR (Visual Commonsense Reasoning) task is particularly challenging, requiring models to understand complex social dynamics in images and perform commonsense reasoning. Due to its complexity, performance improvements are more pronounced. This suggests that adversarial training may benefit more from challenging tasks.</p>
</li>
<li>
<p><strong>FreeLB vs. VILLA (Table 3b)</strong></p>
<p><img decoding="async" loading="lazy" alt="FreeLB vs. VILLA" src="/en/assets/images/villa_6-ef23c4f6a6c1600ca298110090a6dc44.jpg" width="1024" height="301" class="img_ev3q"></p>
<p>The authors compare FreeLB and VILLA on two representative and challenging V+L tasks, VQA and VCR. Due to additional fine-grained adversarial regularization, VILLA outperforms FreeLB in both benchmarks.</p>
<hr>
<p>But wait, didn&#x27;t we mention earlier that VILLA adopts the FreeLB strategy? Why compare it again?</p>
<p>Although VILLA employs FreeLB for adversarial training, it is not merely FreeLB. VILLA is a more comprehensive framework, incorporating additional strategies or regularizations like fine-grained adversarial regularization to optimize and enhance model performance.</p>
<p>The comparison aims to demonstrate that even though VILLA uses FreeLB as its core adversarial training method, its additional optimizations and enhancements make it perform better than just using FreeLB.</p>
</li>
<li>
<p><strong>Results on LXMERT</strong></p>
<p><img decoding="async" loading="lazy" alt="Results on LXMERT" src="/en/assets/images/villa_7-4ebdcfd8e2c0b314502858042fc5fb95.jpg" width="1024" height="247" class="img_ev3q"></p>
<p>To show VILLA&#x27;s effectiveness beyond UNITER, it was also applied to LXMERT, achieving significant improvements, even outperforming UNITER-base!</p>
<p>The authors demonstrate VILLA&#x27;s generalizability, providing +0.88 average performance improvement across three evaluation tasks.</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>In exploring deep learning model robustness, adversarial attacks are a significant area of interest. This study focuses on the adversarial robustness of vision-and-language models (V+L models). Despite being a nascent field, there is a relative lack of literature on strategies for adversarial attacks on these models. Challenges include backpropagating gradients from multimodal Transformer models to CNN backbones for image adversarial attacks and synthesizing text adversarial attacks consistent with visual contexts.</p>
<p>VILLA is a novel adversarial training framework designed to enhance vision-and-language representation learning. Its uniqueness lies in applying adversarial training during both pre-training and fine-tuning stages. Additionally, by adding adversarial perturbations in the embedding space, VILLA consistently improves performance across various evaluation benchmarks.</p>
<p>The authors view VILLA as a &quot;plug-in&quot; concept, meaning after developing a model, you don&#x27;t need to change its structure. Just incorporate VILLA&#x27;s adversarial training techniques at the end to enhance model performance by 1-3 percentage points.</p>
<p>Finally, the authors acknowledge that despite VILLA&#x27;s significant benefits, the time-consuming nature of adversarial training remains a challenge. Future work aims to find more efficient adversarial training methods to make large-scale pre-training more feasible in everyday practice.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-09-11T07:30:19.000Z" itemprop="dateModified">Sep 11, 2024</time></b> by <b>zephyr-sh</b></span></div></div></footer><div style="margin-top:3rem"> </div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/papers/multimodality/ernie-vil/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">[20.06] ERNIE-ViL</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/papers/multimodality/unimo/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">[20.12] UNIMO</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-phantom-in-the-villa" class="table-of-contents__link toc-highlight">The Phantom in the Villa</a></li><li><a href="#problem-definition" class="table-of-contents__link toc-highlight">Problem Definition</a></li><li><a href="#solution" class="table-of-contents__link toc-highlight">Solution</a><ul><li><a href="#villa-model-design" class="table-of-contents__link toc-highlight">VILLA Model Design</a></li><li><a href="#pre-training-tasks" class="table-of-contents__link toc-highlight">Pre-Training Tasks</a></li><li><a href="#two-stage-adversarial-training" class="table-of-contents__link toc-highlight">Two-Stage Adversarial Training</a></li><li><a href="#perturbations" class="table-of-contents__link toc-highlight">Perturbations</a></li><li><a href="#multimodal-free-adversarial-training" class="table-of-contents__link toc-highlight">Multimodal &quot;Free&quot; Adversarial Training</a></li><li><a href="#free-adversarial-training-strategy" class="table-of-contents__link toc-highlight">Free Adversarial Training Strategy</a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a><ul><li><a href="#experimental-setup" class="table-of-contents__link toc-highlight">Experimental Setup</a></li><li><a href="#villa-on-uniter-and-analysis" class="table-of-contents__link toc-highlight">VILLA on UNITER and Analysis</a></li><li><a href="#in-depth-analysis-of-villa" class="table-of-contents__link toc-highlight">In-Depth Analysis of VILLA</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class="footer__links"><a class="footer__link-item" href="/en/docs">Projects</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/papers/intro">Papers</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/blog">Blog</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/terms-of-service">TermsOfUse</a><span class="footer__link-separator">·</span><a class="footer__link-item" href="/en/privacy-policy">Privacy Policy</a><span class="footer__link-separator">·</span><a href="https://buymeacoffee.com/zephyr_docsaid" target="_blank" rel="noopener noreferrer" class="footer__link-item">Support Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 DOCSAID.</div></div></div></footer></div>
</body>
</html>