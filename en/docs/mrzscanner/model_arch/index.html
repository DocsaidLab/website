<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-mrzscanner/model_arch" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>Model Design | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/docs/mrzscanner/model_arch><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="Model Design | DOCSAID"><meta data-rh=true name=description content="The world is simple, yet we complicate it ourselves."><meta data-rh=true property=og:description content="The world is simple, yet we complicate it ourselves."><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/docs/mrzscanner/model_arch><link data-rh=true rel=alternate href=https://docsaid.org/docs/mrzscanner/model_arch hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/docs/mrzscanner/model_arch hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/docs/mrzscanner/model_arch hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/docs/mrzscanner/model_arch hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://docsaid.org/en/docs/mrzscanner/","name":"MRZScanner","position":1},{"@type":"ListItem","item":"https://docsaid.org/en/docs/mrzscanner/model_arch","name":"Model Design","position":2}]}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.e527be82.js defer></script><script src=/en/assets/js/main.a3555bd8.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/docs/>Projects</a><a class="navbar__item navbar__link" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/docs/mrzscanner/model_arch target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/docs/mrzscanner/model_arch target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/docs/mrzscanner/model_arch target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/docs/>Open Source Projects</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/model-training-guide/>Model Training Guide</a><button aria-label="Expand sidebar category 'Model Training Guide'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/autotraderx/>AutoTraderX</a><button aria-label="Expand sidebar category 'AutoTraderX'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/capybara/>Capybara</a><button aria-label="Expand sidebar category 'Capybara'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/docaligner/>DocAligner</a><button aria-label="Expand sidebar category 'DocAligner'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/docclassifier/>DocClassifier</a><button aria-label="Expand sidebar category 'DocClassifier'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/gmailsummary/>GmailSummary</a><button aria-label="Expand sidebar category 'GmailSummary'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/docs/mrzscanner/>MRZScanner</a><button aria-label="Collapse sidebar category 'MRZScanner'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/intro>Introduction</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/installation>Installation</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/quickstart>Quick Start</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/advance>Advanced Settings</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/docs/mrzscanner/model_arch>Model Design</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/benchmark>Evaluation</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/dataset>Dataset</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/summit_data>Submission</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/mrzscanner/reference>References</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/nginx-notes/>Nginx Notes</a><button aria-label="Expand sidebar category 'Nginx Notes'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/wordcanvas/>WordCanvas</a><button aria-label="Expand sidebar category 'WordCanvas'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li class=breadcrumbs__item><a class=breadcrumbs__link href=/en/docs/mrzscanner/><span>MRZScanner</span></a><li class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link>Model Design</span></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Model Design</h1></header>
<p>The world is simple, yet we complicate it ourselves.</p>
<p>Yes, we're the ones causing our own headaches—both annoyed and intrigued at the same time.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=two-stage-recognition-model>Two-Stage Recognition Model<a href=#two-stage-recognition-model class=hash-link aria-label="Direct link to Two-Stage Recognition Model" title="Direct link to Two-Stage Recognition Model">​</a></h2>
<p>A two-stage model refers to dividing MRZ recognition into two phases: localization and recognition.</p>
<p>Following this approach, we can start designing related models. First, let's look at the localization model.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=localization-model>Localization Model<a href=#localization-model class=hash-link aria-label="Direct link to Localization Model" title="Direct link to Localization Model">​</a></h3>
<p>Localization of the MRZ area can generally be divided into two directions:</p>
<ol>
<li>
<p><strong>Locate the corners of the MRZ area:</strong></p>
<div align=center><figure style=width:50%><p><img decoding=async loading=lazy alt=corners src=/en/assets/images/img2-4c688adc6a2092ef627a98e50b3f3751.jpg width=1237 height=1384 class=img_ev3q>
<figcaption>Image source: <a href=http://l3i-share.univ-lr.fr/MIDV2020/midv2020.html target=_blank rel="noopener noreferrer"><strong>MIDV-2020 Synthetic Dataset</strong></a></figcaption><p></figure></div>
<p>This is similar to the document localization projects we've done before, except that here we are localizing the MRZ area instead of a document.</p>
<p>The difference is that in document localization, the corner points exist "realistically" on the image and don't require the model to "imagine" a corner. In contrast, for the MRZ area, we need the model to "guess" these corner points.</p>
<p>It turns out that using this method results in an unstable model; just a slight movement of the passport can cause the predicted corner points to wander around the MRZ area.</p>
<hr>
</li>
<li>
<p><strong>Segmentation of the MRZ area:</strong></p>
<div align=center><figure style=width:50%><p><img decoding=async loading=lazy alt=area src=/en/assets/images/img3-1e876a97bdc10e5af658d51578f1ca29.jpg width=1237 height=1384 class=img_ev3q>
<figcaption>Image source: <a href=http://l3i-share.univ-lr.fr/MIDV2020/midv2020.html target=_blank rel="noopener noreferrer"><strong>MIDV-2020 Synthetic Dataset</strong></a></figcaption><p></figure></div>
<p>This approach is more stable because we can directly use a segmentation model to predict the MRZ area boundaries. The text in the MRZ area is realistically present in the image, so the model doesn't have to make unnecessary assumptions. This way, we can directly segment the MRZ area and avoid concerns about the corner points.</p>
</li>
</ol>
<hr>
<p>We adopted the segmentation approach.</p>
<p>In real-world scenarios, the passport held by the user is inevitably tilted, so we need to correct the MRZ area to transform it into a proper rectangle.</p>
<p>For the loss function, we referred to a review paper:</p>
<ul>
<li><a href=https://arxiv.org/abs/2006.14822 target=_blank rel="noopener noreferrer"><strong>[20.06] A survey of loss functions for semantic segmentation</strong></a></li>
</ul>
<p>This paper provides a unified comparison and introduction of various segmentation loss functions proposed in recent years, and presents a solution to existing problems, namely <strong>Log-Cosh Dice Loss</strong>.</p>
<p>Interested readers can refer to this paper; we won't go into further details here.</p>
<div align=center><figure style=width:90%><p><img decoding=async loading=lazy alt="Log-Cosh Dice Loss" src=/en/assets/images/img4-e16824cd8031318fd44188fabe8737b6.jpg width=1224 height=720 class=img_ev3q></figure></div>
<p>In our experiments, using <code>Log-Cosh Dice Loss</code> alone yielded suboptimal results, so we had to combine it with pixel classification loss <code>CrossEntropyLoss</code> and pixel regression loss <code>SmoothL1Loss</code> for training.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=recognition-model>Recognition Model<a href=#recognition-model class=hash-link aria-label="Direct link to Recognition Model" title="Direct link to Recognition Model">​</a></h3>
<p>The recognition model is simpler because we've already segmented the MRZ area, and we just need to input that area into a text recognition model to get the final result.</p>
<p>At this stage, there are several design directions we can consider:</p>
<ol>
<li>
<p><strong>Segment the string and recognize them one by one:</strong></p>
<p>Some MRZs have two lines of text, such as the TD2 and TD3 formats, while others have three lines, such as the TD1 format. We can split these texts and recognize them one by one.</p>
<p>The recognition model needs to convert a string of image text into text output. There are many possible methods, such as the earlier popular CRNN+CTC or the more recent CLIP4STR, among others.</p>
<p>This method has many drawbacks, such as the need for additional logic to handle two or three lines in the MRZ area, or difficulties in distinguishing characters due to narrow spacing between MRZ lines in some documents.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>If you're interested in the related papers, you can refer to the articles we've previously read:<ul>
<li><a href=https://docsaid.org/en/papers/text-recognition/crnn/ target=_blank rel="noopener noreferrer"><strong>[15.07] CRNN: I Want It All!</strong></a></li>
<li><a href=https://docsaid.org/en/papers/text-recognition/clip4str/ target=_blank rel="noopener noreferrer"><strong>[23.05] CLIP4STR: The Blessing of Multimodality</strong></a></li>
</ul></div></div>
</li>
</ol>
<hr>
<ol start=2>
<li>
<p><strong>Recognize the entire MRZ cropped image at once:</strong></p>
<p>Since the aspect ratio of the MRZ area is not significantly different, we can crop the entire MRZ area and recognize the entire image at once. This situation is particularly suitable for using Transformer-based models to solve the problem.</p>
<p>For example, if you only use the Transformer Encoder architecture, the model design can look like this:</p>
<div align=center><figure style=width:50%><p><img decoding=async loading=lazy alt="Transformer Encoder" src=/en/assets/images/img6-28f672ed73db01e2240821ea41a65085.jpg width=2108 height=2374 class=img_ev3q></figure></div>
<p>Due to the self-attention mechanism, multiple Tokens may point to the same character in the image, which may confuse the model if a typical decoding approach is used:</p>
<blockquote>
<p>It's clearly the image of this character, why should it be decoded as another character?</p>
</blockquote>
<p>Based on our experiments, using CTC for text decoding yields better results in this case, because each Token originates from an image area of "a particular" character, and we just need to merge the outputs at the final stage to get the final text result.</p>
<hr>
<p>Of course, if you dislike CTC and think it’s a hassle, you could consider adopting an Encoder-Decoder architecture. The model design could look like this:</p>
<figure><p><img decoding=async loading=lazy alt="Transformer Encoder-Decoder" src=/en/assets/images/img7-9c8575fcb7e63fceaf879b803032fd90.jpg width=3281 height=2300 class=img_ev3q></figure>
<p>This approach allows direct string decoding without the need for an additional CTC layer, because the tokens input to the Decoder are queries for the characters, and each token is responsible for finding the corresponding character in the correct order.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>The Decoder here can output in parallel, without needing an autoregressive approach.<p>Upon further reflection, the reason we use autoregression is that we need to "base the next prediction on the previous prediction." However, in this case, such operations are clearly unnecessary.<p>Each MRZ character is independent, and the prediction for the first position does not affect the prediction for the second position. All the necessary information is already present in the Encoder’s output, and the Decoder's job is merely to query and retrieve the correct sequence.<p>Of course, simply talking about it isn't enough; we have also tested both parallel output and autoregressive training methods, and the result shows that parallel output converges faster, achieves better performance, and generalizes better.</div></div>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=error-propagation>Error Propagation<a href=#error-propagation class=hash-link aria-label="Direct link to Error Propagation" title="Direct link to Error Propagation">​</a></h3>
<p>This brings us back to the issue of corner points.</p>
<p>All two-stage models face a common challenge: <strong>error propagation</strong>.</p>
<p>No model is 100% accurate. Since we can never fully model the statistical population, every rule has exceptions, and every model has errors. Regardless of the method chosen, the final challenge is the same:</p>
<ul>
<li><strong>Inaccurate corner estimation</strong></li>
</ul>
<p>Inaccurate corner estimation leads to an inaccurate MRZ region after correction. An inaccurate MRZ region then leads to inaccurate text recognition. This becomes a classic example of error propagation.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=single-stage-recognition-model>Single-Stage Recognition Model<a href=#single-stage-recognition-model class=hash-link aria-label="Direct link to Single-Stage Recognition Model" title="Direct link to Single-Stage Recognition Model">​</a></h2>
<p>The primary challenge of a single-stage model is dealing with multi-scale features.</p>
<p>The MRZ region can vary with the user’s capture angle, meaning that before detecting text, we must handle the image at different scales.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=model-architecture>Model Architecture<a href=#model-architecture class=hash-link aria-label="Direct link to Model Architecture" title="Direct link to Model Architecture">​</a></h3>
<p><img decoding=async loading=lazy alt=single-stage src=/en/assets/images/img9-b0fc93a05449de8d10a858a1664eb683.jpg width=8396 height=4221 class=img_ev3q></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=backbone>Backbone<a href=#backbone class=hash-link aria-label="Direct link to Backbone" title="Direct link to Backbone">​</a></h3>
<p><img decoding=async loading=lazy alt=backbone src=/en/assets/images/img8-3d70aa43813018cb370af597d75d7001.jpg width=1324 height=504 class=img_ev3q></p>
<p>Recently, Google released a new paper: <strong>MobileNet-V4</strong>, which is optimized for mobile devices. This is great news for us, and we’ll use it as our Backbone, using the pretrained weights from <code>timm</code>, with an input image size of 512 x 512 in RGB.
<a href=https://docsaid.org/papers/lightweight/mobilenet-v4/ target=_blank rel="noopener noreferrer">https://docsaid.org/papers/lightweight/mobilenet-v4/</a></p>
<ul>
<li><a href=https://docsaid.org/en/papers/lightweight/mobilenet-v4/ target=_blank rel="noopener noreferrer"><strong>[24.04] MobileNet-V4: The Legacy After Five Years</strong></a></li>
<li><a href=https://github.com/huggingface/pytorch-image-models target=_blank rel="noopener noreferrer"><strong>huggingface/pytorch-image-models</strong></a></li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>After testing, we found that at an input resolution of 512 x 512, the text size in the MRZ area is around 4 to 8 pixels. Reducing the resolution further leads to blurry text in the MRZ area, degrading recognition performance.</div></div>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=neck>Neck<a href=#neck class=hash-link aria-label="Direct link to Neck" title="Direct link to Neck">​</a></h3>
<p><img decoding=async loading=lazy alt=neck src=/en/assets/images/img10-34e38a23a3eaa918e094bcba71530b38.jpg width=1322 height=500 class=img_ev3q></p>
<p>To better fuse multi-scale features, we introduced BiFPN. By allowing bidirectional flow of contextual information, BiFPN enhances feature representation. It produces a set of rich, scale-aware feature maps that effectively capture objects at different scales, positively impacting prediction accuracy.</p>
<p>In our ablation study, we tried removing this component and directly using the Backbone output feature maps, but training failed.</p>
<ul>
<li><a href=https://docsaid.org/papers/feature-fusion/bifpn/ target=_blank rel="noopener noreferrer"><strong>[19.11] EfficientDet: BiFPN Is the Key</strong></a></li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=patchify>Patchify<a href=#patchify class=hash-link aria-label="Direct link to Patchify" title="Direct link to Patchify">​</a></h3>
<p>Now for some unconventional experimentation.</p>
<hr>
<p>We need to convert feature maps from each stage into a format that Transformers can process. Here, we use standard convolution operations to turn feature maps into patches.</p>
<p>Here are some of our settings:</p>
<ol>
<li>
<p><strong>Patch Size: 4 x 4.</strong></p>
<p>We measured the text size in the MRZ area and found that small characters are around 4 to 8 pixels. Given this, we set the patch size to 4 x 4.</p>
</li>
<li>
<p><strong>Each feature map has a corresponding Patch Embedding and Position Embedding.</strong></p>
<p>Since each feature map has a different scale, they can’t share the same embedding, or information exchange across scales won’t work properly. We considered designing a shared embedding, but it was too complex, so we abandoned that idea.</p>
<p>We also tested shared weights for Patch Embedding, where all feature maps share the same Conv2d for embedding, but the results were poor.</p>
</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=cross-attention>Cross-Attention<a href=#cross-attention class=hash-link aria-label="Direct link to Cross-Attention" title="Direct link to Cross-Attention">​</a></h3>
<p>Finally, we used Cross-Attention for text recognition.</p>
<p>We randomly initialized 93 tokens.</p>
<ul>
<li><strong>Why 93 tokens?</strong></li>
</ul>
<p>We based this on the longest MRZ format, TD1, which has 90 characters. TD1 has three lines, so we need 2 "separator" characters. Additionally, we need one "end" character, making a total of 93 tokens.</p>
<p>The separator character is <code>&</code>, and the end character is <code>[EOS]</code>. If there are extra positions, we mark them with <code>[EOS]</code>, and the model can predict whatever it wants beyond that point, but we won’t supervise it.</p>
<hr>
<p>For the Transformer decoder, here are the basic settings:</p>
<ul>
<li>Dimension: 256</li>
<li>Layers: 6</li>
<li>Attention heads: 4</li>
<li>Dropout: 0</li>
<li>Normalization: Post-LN</li>
</ul>
<p>The main design philosophy of this architecture is to provide the Decoder with a "multi-scale" feature space so it can freely choose the appropriate scale for text recognition. We don't need to worry about the position of the text in the image—this is up to the model to handle.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=additional-notes>Additional Notes<a href=#additional-notes class=hash-link aria-label="Direct link to Additional Notes" title="Direct link to Additional Notes">​</a></h3>
<p>During the experiment, we kept some records, which may be helpful:</p>
<ol>
<li>
<p><strong>Models with dimensions 64 and 128 can converge, but reducing the dimension doubles the convergence time.</strong></p>
<p>On an RTX4090, training a model with 256 dimensions takes around 50 hours. For 128 dimensions, it takes around 100 hours; for 64 dimensions, about 200 hours.</p>
<p>Why not try 512 dimensions? Because it makes the model too large, exceeding 100 MB, which isn't what we want.</p>
</li>
</ol>
<hr>
<ol start=2>
<li>
<p><strong>Adding extra branches, like Polygon or text center points, can speed up convergence.</strong></p>
<p>But it’s impractical! Collecting data is hard enough; adding MRZ area annotation makes it even more challenging. In the end, the benefits don’t justify the effort.</p>
</li>
</ol>
<hr>
<ol start=3>
<li>
<p><strong>Removing the Neck.</strong></p>
<p>It can still converge, but the time triples, so think carefully.</p>
</li>
</ol>
<hr>
<ol start=4>
<li>
<p><strong>Removing position encoding.</strong></p>
<p>No convergence.</p>
</li>
</ol>
<hr>
<ol start=5>
<li>
<p><strong>Adjusting weight decay from <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=application/x-tex>10^{-5}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8141em></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> to <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mn>10</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><annotation encoding=application/x-tex>10^{-2}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8141em></span><span class=mord>1</span><span class=mord><span class=mord>0</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:0.8141em><span style=top:-3.063em;margin-right:0.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>.</strong></p>
<p>Faster convergence but reduced generalization.</p>
<p>Small models inherently have some regularization, so they don’t need strong weight decay.</p>
</li>
</ol>
<hr>
<ol start=6>
<li>
<p><strong>Using Pre-LN.</strong></p>
<p>Faster convergence but reduced generalization.</p>
<p>Pre-LN reduces model depth to some extent, which isn’t ideal for small models.</p>
</li>
</ol>
<hr>
<ol start=7>
<li>
<p><strong>Increasing data augmentation.</strong></p>
<p>To speed up experiments, we limited the rotation of MRZ images to within ±45 degrees.</p>
<p>We tried using full rotation and more augmentations, but the model couldn’t handle such a heavy load and failed to converge.</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>We believe the current single-stage model design is still missing some key components. We plan to continue reading more literature and conducting further experiments.</p>
<p>Scaling up the model size is probably the most effective solution. The challenge lies in how to meet all these requirements while maintaining a "lightweight" parameter size, which is our next focus.</p>
<p>As mentioned earlier, this problem can already be solved reliably in almost all cases with a <strong>two-stage</strong> solution. If you're serious about this, we still recommend going back to developing a two-stage model, which will save you from many unnecessary headaches.</div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-02-23T03:20:01.000Z itemprop=dateModified>Feb 23, 2025</time></b> by <b>zephyr-sh</b></span></div></div><div style=margin-top:3rem> </div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/docs/mrzscanner/advance><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>Advanced Settings</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/docs/mrzscanner/benchmark><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Evaluation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#two-stage-recognition-model class="table-of-contents__link toc-highlight">Two-Stage Recognition Model</a><ul><li><a href=#localization-model class="table-of-contents__link toc-highlight">Localization Model</a><li><a href=#recognition-model class="table-of-contents__link toc-highlight">Recognition Model</a><li><a href=#error-propagation class="table-of-contents__link toc-highlight">Error Propagation</a></ul><li><a href=#single-stage-recognition-model class="table-of-contents__link toc-highlight">Single-Stage Recognition Model</a><ul><li><a href=#model-architecture class="table-of-contents__link toc-highlight">Model Architecture</a><li><a href=#backbone class="table-of-contents__link toc-highlight">Backbone</a><li><a href=#neck class="table-of-contents__link toc-highlight">Neck</a><li><a href=#patchify class="table-of-contents__link toc-highlight">Patchify</a><li><a href=#cross-attention class="table-of-contents__link toc-highlight">Cross-Attention</a><li><a href=#additional-notes class="table-of-contents__link toc-highlight">Additional Notes</a></ul><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>