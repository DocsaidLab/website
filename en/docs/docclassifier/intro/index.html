<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-docclassifier/intro" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>Introduction | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/docsaid-social-card.jpg><meta data-rh=true property=og:url content=https://docsaid.org/en/docs/docclassifier/intro><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="Introduction | DOCSAID"><meta data-rh=true name=description content="In past project experiences, classification models have been some of the most common machine learning tasks."><meta data-rh=true property=og:description content="In past project experiences, classification models have been some of the most common machine learning tasks."><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/docs/docclassifier/intro><link data-rh=true rel=alternate href=https://docsaid.org/docs/docclassifier/intro hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/docs/docclassifier/intro hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/docs/docclassifier/intro hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/docs/docclassifier/intro hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.0bad0a09.css><script src=/en/assets/js/runtime~main.89a14a64.js defer></script><script src=/en/assets/js/main.2a6aaa04.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/docsaid_logo.png><link rel=preload as=image href=/en/img/docsaid_logo_white.png><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/docs/>Projects</a><a class="navbar__item navbar__link" href=/en/papers/intro>Papers</a><a class="navbar__item navbar__link" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/docs/docclassifier/intro target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/docs/docclassifier/intro target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/docs/docclassifier/intro target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-7ny38l ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex=-1 class=sidebarLogo_isFc href=/en/><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b></b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class=menu__link href=/en/docs/>Open Source Projects</a><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/model-training-guide/>Model Training Guide</a><button aria-label="Expand sidebar category 'Model Training Guide'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/autotraderx/>AutoTraderX</a><button aria-label="Expand sidebar category 'AutoTraderX'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/capybara/>Capybara</a><button aria-label="Expand sidebar category 'Capybara'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/docaligner/>DocAligner</a><button aria-label="Expand sidebar category 'DocAligner'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/en/docs/docclassifier/>DocClassifier</a><button aria-label="Collapse sidebar category 'DocClassifier'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/en/docs/docclassifier/intro>Introduction</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/installation>Installation</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/quickstart>Quick Start</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/advance>Advanced</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/model_arch>Model Design</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/benchmark>Evaluation</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/discussion>Discussion</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/training_env>Training</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/en/docs/docclassifier/summit_data>Submission</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/gmailsummary/>GmailSummary</a><button aria-label="Expand sidebar category 'GmailSummary'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/mrzscanner/>MRZScanner</a><button aria-label="Expand sidebar category 'MRZScanner'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/nginx-notes/>Nginx Notes</a><button aria-label="Expand sidebar category 'Nginx Notes'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/en/docs/wordcanvas/>WordCanvas</a><button aria-label="Expand sidebar category 'WordCanvas'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav><button type=button title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width=20 height=20 aria-hidden=true class=collapseSidebarButtonIcon_kv0_><g fill=#7a7a7a><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"/><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"/></g></svg></button></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/en/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/en/docs/docclassifier/><span itemprop=name>DocClassifier</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>Introduction</span><meta itemprop=position content=2></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Introduction</h1></header>
<p>In past project experiences, classification models have been some of the most common machine learning tasks.</p>
<p>Classification models are not difficult to implement. First, we build a backbone, then map the final output to multiple specific categories. Finally, we evaluate the model’s performance using several metrics, such as accuracy, recall, F1-score, and so on.</p>
<p>Although this may sound straightforward, in practical applications, we often encounter some issues. Let’s take the topic of this project as an example:</p>
<ul>
<li>
<p><strong>Category Definition</strong></p>
<p>In classification tasks, if the categories we define are highly similar, the model may have difficulty distinguishing between them. For example, “Company A insurance document” and “Company B insurance document.” Both categories belong to documents from a company, and their differences may be minimal, making it hard for the model to distinguish between the two.</p>
</li>
<li>
<p><strong>Data Imbalance</strong></p>
<p>In most scenarios, data collection can be the most challenging problem, especially when dealing with documents containing personal information. Data imbalance further leads to poor prediction performance for minority categories.</p>
</li>
<li>
<p><strong>Data Augmentation</strong></p>
<p>In our daily life, we are surrounded by a large number of documents, and we constantly want to add more document categories.</p>
<p>However, every time we add a new category, the entire model needs to be retrained or fine-tuned, which incurs high costs, including data collection, labeling, retraining, reevaluation, deployment, and so on. All of these processes must be repeated.</p>
</li>
<li>
<p><strong>Sub-labels for Categories</strong></p>
<p>Customers' needs are often unpredictable.</p>
<p>Let’s assume there’s a customer who first defines a document type, let’s call it Document A. Then, the customer wants to add more sub-labels for Document A, such as:</p>
<ul>
<li>Damaged Document A</li>
<li>Glare Document A</li>
<li>First-generation format Document A</li>
<li>Second-generation format Document A</li>
<li>...</li>
</ul>
<p>Let’s not even discuss the fact that adding each sub-label would require retraining the model.</p>
<p>From the perspective of model engineering, it’s “irrational” to treat these labels as independent categories since they all belong to Document A. Likewise, it’s “unreasonable” to treat them as a multi-class problem because sub-labels corresponding to different document formats may vary.</p>
</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"/></svg></span>tip</div><div class=admonitionContent_BuS1><p>So you might think: Since we can’t solve the problem, let’s solve the person who raised it!<blockquote>
<p>No, you can’t!</p>
</blockquote><p>This is a machine learning problem.</div></div>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=metric-learning>Metric Learning<a href=#metric-learning class=hash-link aria-label="Direct link to Metric Learning" title="Direct link to Metric Learning">​</a></h2>
<p>Stepping out of the document classification problem, you’ll realize that what we’re really discussing here is <strong>Metric Learning</strong>.</p>
<p>The primary goal of metric learning is to learn the optimal distance measure to evaluate the similarity between samples. In traditional machine learning, metric learning typically involves mapping data from the original feature space to a new feature space, where similar objects are closer together and dissimilar objects are farther apart. This is usually achieved by learning a distance function that better reflects the true similarity between samples.</p>
<p>To summarize in one sentence: <strong>Metric learning is a method for learning similarity</strong>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=application-scenarios>Application Scenarios<a href=#application-scenarios class=hash-link aria-label="Direct link to Application Scenarios" title="Direct link to Application Scenarios">​</a></h3>
<p>A well-known application of metric learning is <strong>Face Recognition</strong>.</p>
<p>As mentioned earlier, the number of faces continues to grow, and we can’t always retrain the model. Therefore, by using a metric learning framework, we can learn a better distance function to improve the accuracy of face recognition.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=solving-the-problem>Solving the Problem<a href=#solving-the-problem class=hash-link aria-label="Direct link to Solving the Problem" title="Direct link to Solving the Problem">​</a></h2>
<p>Although not every classification problem is suited to elevate to the level of metric learning, in this project, the weapon of metric learning can indeed help us overcome the obstacles mentioned earlier.</p>
<ul>
<li>
<p><strong>Obstacle 1: Category Definition</strong></p>
<p>The goal of our learning is to learn a better distance function, which helps us better distinguish similar categories. Therefore, we no longer need to define categories. The objects we want to classify will ultimately only become registered data.</p>
</li>
<li>
<p><strong>Obstacle 2: Data Imbalance</strong></p>
<p>We no longer need to collect massive amounts of data because our model doesn’t rely on large samples. We only need one sample, which serves as our registered data, and other parts can be trained using different training data.</p>
</li>
<li>
<p><strong>Obstacle 3: Category Expansion</strong></p>
<p>Expanding categories only requires registering new data, without the need to retrain the model. This design greatly reduces the training cost.</p>
</li>
<li>
<p><strong>Obstacle 4: Sub-labels for Categories</strong></p>
<p>This issue can be well addressed within the metric learning framework. We can treat sub-labels as new registered data, which will not affect the original model. The distance between sub-labels and the main label in the feature space may be close, but not identical, thus effectively distinguishing these two categories.</p>
</li>
</ul>
<hr>
<p>We initially introduced the metric learning framework: <a href=https://arxiv.org/abs/2203.15565 target=_blank rel="noopener noreferrer"><strong>PartialFC</strong></a>, which combines technologies like <a href=https://arxiv.org/abs/1801.09414 target=_blank rel="noopener noreferrer"><strong>CosFace</strong></a> and <a href=https://arxiv.org/abs/1801.07698 target=_blank rel="noopener noreferrer"><strong>ArcFace</strong></a>, enabling precise classification without the need for pre-defined categories.</p>
<p>Next, in further experiments, we introduced the <a href=https://www.image-net.org/ target=_blank rel="noopener noreferrer"><strong>ImageNet-1K dataset</strong></a> and <a href=https://arxiv.org/abs/2103.00020 target=_blank rel="noopener noreferrer"><strong>CLIP model</strong></a>. We used ImageNet-1K as the base, treating each image as a category. This operation expanded the number of categories to about 1.3 million, providing the model with richer visual variations and increasing data diversity.</p>
<p>In the TPR@FPR=1e-4 benchmark, the performance improved by about 4.1% (77.2% -> 81.3%) compared to the baseline model. By introducing the CLIP model on top of ImageNet-1K, and conducting knowledge distillation during training, the performance improved by an additional 4.6% (81.3% -> 85.9%) in the same benchmark.</p>
<p>In the latest experiments, we combined BatchNorm and LayerNorm, achieving encouraging results. On top of the CLIP distillation model, the TPR@FPR=1e-4 performance improved by about 4.4% (85.9% -> 90.3%).</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=why-not-contrastive-learning>Why Not Contrastive Learning?<a href=#why-not-contrastive-learning class=hash-link aria-label="Direct link to Why Not Contrastive Learning?" title="Direct link to Why Not Contrastive Learning?">​</a></h2>
<p>Contrastive Learning and Metric Learning are both methods for learning the similarity between samples.</p>
<p>So why did we choose not to use contrastive learning this time?</p>
<p>It’s not because it’s not good; we just believe that at this stage, metric learning is a better fit.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=benefits-of-contrastive-learning>Benefits of Contrastive Learning<a href=#benefits-of-contrastive-learning class=hash-link aria-label="Direct link to Benefits of Contrastive Learning" title="Direct link to Benefits of Contrastive Learning">​</a></h3>
<p>The greatest advantage of contrastive learning is its ability to handle unlabeled data well. For scenarios where data labeling is difficult or the dataset is enormous, it’s essentially a “lifesaver.”</p>
<p>Moreover, it excels at learning general features, which can be applied not only to classification tasks but also across tasks, such as object detection, semantic segmentation, and so on.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=but-there-are-drawbacks>But There Are Drawbacks<a href=#but-there-are-drawbacks class=hash-link aria-label="Direct link to But There Are Drawbacks" title="Direct link to But There Are Drawbacks">​</a></h3>
<p>First, contrastive learning heavily relies on the design of negative samples. If the selection of negative samples is poor, either too simple or too complex, the model’s training performance may suffer.</p>
<p>Additionally, contrastive learning has high resource requirements because it needs a large number of negative samples to help the model understand “what is different,” leading to high computational costs. This is especially true when large training batches are required to provide enough negative samples, posing a challenge for our hardware resources.</p>
<p>Furthermore, contrastive learning is limited by its self-supervised design for unlabeled data, so it’s difficult for the model to learn highly precise features (such as an error rate of one in ten thousand). This is evident in the leaderboard of face recognition, where metric learning methods still dominate.</p>
<hr>
<p>In conclusion, we chose “metric learning” to solve the problem. In the future, we’ll allocate time to explore the application of contrastive learning and may even combine the strengths of both methods, allowing the model to learn general features while possessing powerful similarity judgment abilities.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=final-thoughts>Final Thoughts<a href=#final-thoughts class=hash-link aria-label="Direct link to Final Thoughts" title="Direct link to Final Thoughts">​</a></h2>
<p>In testing, our model demonstrated over 90% accuracy at a TPR@FPR=1e-4 error rate, with no need to retrain when adding new category types.</p>
<p>In simple terms, we’ve essentially transferred the operation process from a face recognition system over to this!</p>
<p>During the development, we often jokingly asked ourselves, “Can this really work?” As mentioned earlier, the first-generation framework (first author) had some effect but was still unstable. By the time this project was released, it was already the third-generation model (second author), and the overall performance showed significant improvement, making it a solid result.</p>
<p>Compared to our previous “standard” projects, this one is full of fun.</p>
<p>Therefore, we’ve decided to release the project’s framework and experimental results, hoping it will inspire you. If you also find a new application scenario from the design concepts of this project, feel free to share it with us.</div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class=col></div><div class="col lastUpdated_JAkA"><span class=theme-last-updated>Last updated<!-- --> on <b><time datetime=2025-01-07T10:28:17.000Z itemprop=dateModified>Jan 7, 2025</time></b> by <b>zephyr-sh</b></span></div></div><div style=margin-top:3rem> </div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/docs/docclassifier/><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>DocClassifier</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/docs/docclassifier/installation><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Installation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#metric-learning class="table-of-contents__link toc-highlight">Metric Learning</a><ul><li><a href=#application-scenarios class="table-of-contents__link toc-highlight">Application Scenarios</a></ul><li><a href=#solving-the-problem class="table-of-contents__link toc-highlight">Solving the Problem</a><li><a href=#why-not-contrastive-learning class="table-of-contents__link toc-highlight">Why Not Contrastive Learning?</a><ul><li><a href=#benefits-of-contrastive-learning class="table-of-contents__link toc-highlight">Benefits of Contrastive Learning</a><li><a href=#but-there-are-drawbacks class="table-of-contents__link toc-highlight">But There Are Drawbacks</a></ul><li><a href=#final-thoughts class="table-of-contents__link toc-highlight">Final Thoughts</a></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>