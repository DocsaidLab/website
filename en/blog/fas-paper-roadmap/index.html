<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.8.1"><title data-rh=true>Face Anti-Spoofing Technology Map | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://docsaid.org/en/blog/fas-paper-roadmap><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docusaurus_tag content=default><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docsearch:docusaurus_tag content=default><meta data-rh=true property=og:title content="Face Anti-Spoofing Technology Map | DOCSAID"><meta data-rh=true name=description content="A guide to 40 papers from FAS."><meta data-rh=true property=og:description content="A guide to 40 papers from FAS."><meta data-rh=true property=og:image content=https://docsaid.org/en/img/2025/0401.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/2025/0401.jpg><meta data-rh=true property=og:type content=article><meta data-rh=true property=article:published_time content=2025-04-01T00:00:00.000Z><meta data-rh=true property=article:author content=https://github.com/zephyr-sh><meta data-rh=true property=article:tag content=face-anti-spoofing,liveness-detection><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/blog/fas-paper-roadmap><link data-rh=true rel=alternate href=https://docsaid.org/blog/fas-paper-roadmap hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/blog/fas-paper-roadmap hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/blog/fas-paper-roadmap hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/blog/fas-paper-roadmap hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://docsaid.org/en/blog/fas-paper-roadmap","@type":"BlogPosting","author":{"@type":"Person","description":"Dosaid maintainer, Full-Stack AI Engineer","image":"https://github.com/zephyr-sh.png","name":"Z. Yuan","url":"https://github.com/zephyr-sh"},"datePublished":"2025-04-01T00:00:00.000Z","description":"A guide to 40 papers from FAS.","headline":"Face Anti-Spoofing Technology Map","image":{"@id":"https://docsaid.org/en/img/2025/0401.jpg","@type":"ImageObject","caption":"title image for the blog post: Face Anti-Spoofing Technology Map","contentUrl":"https://docsaid.org/en/img/2025/0401.jpg","url":"https://docsaid.org/en/img/2025/0401.jpg"},"isPartOf":{"@id":"https://docsaid.org/en/blog","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://docsaid.org/en/blog/fas-paper-roadmap","name":"Face Anti-Spoofing Technology Map","url":"https://docsaid.org/en/blog/fas-paper-roadmap"}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.e52f1f88.css><script src=/en/assets/js/runtime~main.6f3b99d3.js defer></script><script src=/en/assets/js/main.eabfffa3.js defer></script><body class=navigation-with-keyboard><svg xmlns=http://www.w3.org/2000/svg style="display: none;"><defs>
<symbol id=theme-svg-external-link viewBox="0 0 24 24"><path fill=currentColor d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light",e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a class="navbar__item navbar__link" href=/en/papers/intro>Papers</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/services>Services</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/blog/fas-paper-roadmap target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/blog/fas-paper-roadmap target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/blog/fas-paper-roadmap target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-mc1tut ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=blog-hero-fullwidth><div class=postHero_mmE7 style=background-image:url(/en/img/2025/0401.jpg)><div class=postHeroOverlay_UDxJ><h1 class=postTitle_weFP>Face Anti-Spoofing Technology Map</h1><div class=postMeta_oUa9><div class=postAuthors_wLk4><div class=postAuthor_NvIn><img class=postAuthorImg_omQD src=https://github.com/zephyr-sh.png alt="Z. Yuan"><div class=postAuthorText_C6S8><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=postAuthorLink_uKn3><span class=postAuthorName_SaVw>Z. Yuan</span></a><span class=postAuthorTitle_UTso>Dosaid maintainer, Full-Stack AI Engineer</span></div></div></div><div class=postMetaInfo__nS4><div class=postMetaRow_zK0w><span class=postDate_B0aP>2025年4月1日</span><span class=postReadingTime_roVj>20<!-- --> min read</span></div><div class=postTags_nipL><a class=postTag_inik href=/en/blog/tags/face-anti-spoofing>face-anti-spoofing</a><a class=postTag_inik href=/en/blog/tags/liveness-detection>liveness-detection</a></div></div></div></div></div></div><div class="container margin-vert--lg"><div class=row><main class="col col--9"><article class=markdown style="max-width:800px;margin:2rem auto"><article class=""><div><div id=__blog-post-container class=markdown><p>What is Face Anti-Spoofing? Why is it important? How do I get started?</p>
<p>This article is a comprehensive roadmap I’ve put together after reading a substantial amount of literature, designed for those who are learning, researching, or developing FAS systems.</p>
<p>I have selected the 40 most representative papers, divided into eight major themes based on time and technological advancements. Each paper includes reasons to read, key contributions, and the appropriate positioning. From traditional LBP, rPPG, and CNN to Transformer, CLIP, and Vision-Language Models, you will get the full scope.</p>
<p>Later, I will share the details of each paper in the "Paper Notes" section. Let’s first get a grasp of the overall context.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-1-the-dawn-of-low-resolution-light>Chapter 1: The Dawn of Low-Resolution Light<a href=#chapter-1-the-dawn-of-low-resolution-light class=hash-link aria-label="Direct link to Chapter 1: The Dawn of Low-Resolution Light" title="Direct link to Chapter 1: The Dawn of Low-Resolution Light">​</a></h2>
<blockquote>
<p><strong>From traditional feature engineering to the first glimmer of deep learning</strong></p>
</blockquote>
<p>Early research on Face Anti-Spoofing primarily relied on traditional image processing techniques. Researchers used handcrafted features such as texture, contrast, and frequency to describe the authenticity of faces, performing binary classification with classic classifiers.</p>
<ol>
<li>
<p><a href=https://parnec.nuaa.edu.cn/_upload/article/files/4d/43/8a227f2c46bda4c20da97715f010/db1eef47-b25f-4af9-88d4-a8afeccda889.pdf target=_blank rel="noopener noreferrer"><strong>[10.09] Face Liveness Detection from a Single Image with Sparse Low Rank Bilinear Discriminative Model</strong></a>
Using the Lambertian model and sparse low-rank representation to construct feature space, effectively separating real faces from photos, providing theoretical and practical basis for early single-image liveness detection.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/slrbd/ target=_blank rel="noopener noreferrer"><strong>[10.09] SLRBD: Silent Reflective Light</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/6313548 target=_blank rel="noopener noreferrer"><strong>[12.09] On the Effectiveness of Local Binary Patterns in Face Anti-Spoofing</strong></a>
Utilizing LBP and its variants, this paper recognizes flat photos and screen replay attacks and establishes the REPLAY-ATTACK dataset, one of the earliest publicly available datasets and classic baselines.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/lbp/ target=_blank rel="noopener noreferrer"><strong>[12.09] LBP: Lively Micro-textures</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/6810829 target=_blank rel="noopener noreferrer"><strong>[14.05] Spoofing Face Recognition with 3D Masks</strong></a>
A systematic analysis of the attack effects of 3D masks on different face recognition systems (2D/2.5D/3D), pointing out that the traditional assumption of flat fake faces is no longer valid with 3D printing technologies.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/three-d-mad/ target=_blank rel="noopener noreferrer"><strong>[14.05] 3DMAD: The Real Mask</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/1909.08848 target=_blank rel="noopener noreferrer"><strong>[19.09] Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network</strong></a>
Proposing a multi-channel CNN architecture that combines RGB, depth, infrared, and thermal signals for recognition, and releasing the WMCA dataset to enhance detection of advanced fake faces (e.g., silicone masks).</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/wmca/ target=_blank rel="noopener noreferrer"><strong>[19.09] WMCA: The Invisible Face</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/abstract/document/9925105 target=_blank rel="noopener noreferrer"><strong>[22.10] Deep Learning for Face Anti-Spoofing: A Survey</strong></a>
The first systematic survey in the FAS field focusing on deep learning, covering pixel-wise supervision, multi-modal sensors, and domain generalization trends, establishing a comprehensive knowledge base.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/fas-survey/ target=_blank rel="noopener noreferrer"><strong>[22.10] FAS Survey: A Chronicle of Attacks and Defenses</strong></a></div></div>
</li>
</ol>
<hr>
<p>Although these methods are simple, they laid the foundation for recognizing flat fake faces (e.g., photos and screen replays) and set the conceptual framework for the later introduction of deep learning techniques.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-2-the-real-world-stage>Chapter 2: The Real-World Stage<a href=#chapter-2-the-real-world-stage class=hash-link aria-label="Direct link to Chapter 2: The Real-World Stage" title="Direct link to Chapter 2: The Real-World Stage">​</a></h2>
<blockquote>
<p><strong>A milestone for FAS technology moving from the lab to real-world scenarios</strong></p>
</blockquote>
<p>Datasets and benchmarks determine whether a field can grow steadily.</p>
<p>FAS technology expanded from a single scene to multiple devices, lighting conditions, and attack methods, driven by these representative public datasets.</p>
<ol start=6>
<li>
<p><a href=https://ieeexplore.ieee.org/document/7961798 target=_blank rel="noopener noreferrer"><strong>[17.06] OULU-NPU: A Mobile Face Presentation Attack Database with Real-World Variations</strong></a>
A mobile-specific FAS dataset designed for real-world factors such as device, environmental lighting, and attack methods, with four testing protocols, becoming a milestone in "generalization ability" evaluation.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/oulu-npu/ target=_blank rel="noopener noreferrer"><strong>[17.06] OULU-NPU: Four Challenges</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2003.05136 target=_blank rel="noopener noreferrer"><strong>[20.03] CASIA-SURF CeFA: A Benchmark for Multi-modal Cross-ethnicity Face Anti-Spoofing</strong></a>
The world’s first large-scale multi-modal FAS dataset with "ethnicity annotations," covering RGB, Depth, IR, and multiple attack types, specifically used to study ethnic bias and modality fusion strategies.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/cefa/ target=_blank rel="noopener noreferrer"><strong>[20.03] CeFA: Discrimination in Models</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2007.12342 target=_blank rel="noopener noreferrer"><strong>[20.07] CelebASpoof: Large-scale Face Anti-Spoofing Dataset with Rich Annotations</strong></a>
The largest FAS dataset currently, with over 620,000 images and 10 types of spoof annotations, along with 40 attributes from the original CelebA, enabling multi-task and spoof trace learning.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/celeba-spoof/ target=_blank rel="noopener noreferrer"><strong>[20.07] CelebA-Spoof: Large-Scale Anti-Spoofing Trials</strong></a></div></div>
</li>
<li>
<p><a href=https://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Belli_A_Personalized_Benchmark_for_Face_Anti-Spoofing_WACVW_2022_paper.html target=_blank rel="noopener noreferrer"><strong>[22.01] A Personalized Benchmark for Face Anti-Spoofing</strong></a>
Advocating for including liveness images from user registration in the recognition process, proposing two new test configurations, CelebA-Spoof-Enroll and SiW-Enroll, exploring the possibility of personalized FAS systems.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/personalized-fas/ target=_blank rel="noopener noreferrer"><strong>[22.01] Personalized-FAS: Personalized Attempt</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2402.04178 target=_blank rel="noopener noreferrer"><strong>[24.02] SHIELD: An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models</strong></a>
Combining LLM and multi-modal inputs, proposing a QA task format to evaluate the reasoning ability of MLLMs in spoof/forgery detection, opening a new field of "understanding attacks with language modeling."</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/shield/ target=_blank rel="noopener noreferrer"><strong>[24.02] SHIELD: Tell me, why?</strong></a></div></div>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-3-the-cross-domain-battleground>Chapter 3: The Cross-Domain Battleground<a href=#chapter-3-the-cross-domain-battleground class=hash-link aria-label="Direct link to Chapter 3: The Cross-Domain Battleground" title="Direct link to Chapter 3: The Cross-Domain Battleground">​</a></h2>
<blockquote>
<p><strong>From single-domain learning to core technologies for multi-scene deployment</strong></p>
</blockquote>
<p>One of the most challenging problems in Face Anti-Spoofing is generalization—how to make models not only effective on training data but also capable of handling new devices, environments, and attacks.</p>
<ol start=11>
<li>
<p><a href=https://arxiv.org/abs/2004.14043 target=_blank rel="noopener noreferrer"><strong>[20.04] Single-Side Domain Generalization for Face Anti-Spoofing</strong></a>
Proposing a one-sided adversarial learning strategy, aligning only real faces across domains, allowing fake face features to naturally scatter across domains, and preventing over-compression of erroneous information. This is an enlightening direction for DG design.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/ssdg/ target=_blank rel="noopener noreferrer"><strong>[20.04] SSDG: Stable Realness</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2105.02453 target=_blank rel="noopener noreferrer"><strong>[21.05] Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</strong></a>
Not assuming known domain labels, but using instance normalization and MMD for unsupervised clustering and alignment, achieving a generalization training process that does not rely on manual grouping.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/d2am/ target=_blank rel="noopener noreferrer"><strong>[21.05] D²AM: Thousand-Domain Soul Forging</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2303.13662 target=_blank rel="noopener noreferrer"><strong>[23.03] Rethinking Domain Generalization for Face Anti-Spoofing: Separability and Alignment</strong></a>
Proposing the SA-FAS framework, emphasizing maintaining feature separability across different domains while ensuring that the live-to-spoof transition path is consistent across domains, a deep application of IRM theory in FAS.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/sa-fas/ target=_blank rel="noopener noreferrer"><strong>[23.03] SA-FAS: The Law of the Hyperplane</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2402.19298 target=_blank rel="noopener noreferrer"><strong>[24.02] Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing</strong></a>
A deep analysis of the multi-modal DG problem, using U-Adapter to suppress unstable modal interference, paired with ReGrad to dynamically adjust the convergence speed of each modality, providing a complete solution for modality imbalance and reliability issues.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/mmdg/ target=_blank rel="noopener noreferrer"><strong>[24.02] MMDG: Trust Management</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2403.14333 target=_blank rel="noopener noreferrer"><strong>[24.03] CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing</strong></a>
Focuses on a prompt learning approach that emphasizes class-free prompt design, eliminating the need for manually defined categories. This represents a new direction in leveraging language prompts to enhance the generalization ability of FAS models.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/cfpl-fas/ target=_blank rel="noopener noreferrer"><strong>[24.03] CFPL-FAS: Class-Free Prompt Learning</strong></a></div></div>
</li>
</ol>
<hr>
<p>These five papers form the core technical axis under the current Domain Generalization (DG) theme, from one-sided adversarial, label-free clustering, separability analysis, to supervisory methods that integrate language, presenting a complete strategy to address cross-domain challenges.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-4-the-rise-of-a-new-world>Chapter 4: The Rise of a New World<a href=#chapter-4-the-rise-of-a-new-world class=hash-link aria-label="Direct link to Chapter 4: The Rise of a New World" title="Direct link to Chapter 4: The Rise of a New World">​</a></h2>
<blockquote>
<p><strong>From CNN to ViT, the architectural innovation path of FAS models</strong></p>
</blockquote>
<p>The rise of Vision Transformers (ViT) has ushered in an era of global modeling for image tasks, shifting away from local convolutions. Face Anti-Spoofing (FAS) is no exception.</p>
<ol start=16>
<li>
<p><a href=https://openaccess.thecvf.com/content/WACV2023/papers/Liao_Domain_Invariant_Vision_Transformer_Learning_for_Face_Anti-Spoofing_WACV_2023_paper.pdf target=_blank rel="noopener noreferrer"><strong>[23.01] Domain Invariant Vision Transformer Learning for Face Anti-Spoofing</strong></a>
Proposed the DiVT architecture, which enhances cross-domain generalization through two core loss functions. It aggregates genuine face features to form more consistent domain-invariant representations. Experiments show that DiVT achieves state-of-the-art results on various DG-FAS tasks, offering a streamlined yet effective approach to capturing key information for cross-domain recognition.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/divt/ target=_blank rel="noopener noreferrer"><strong>[23.01] DiVT: All-Star Championship</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2302.05744 target=_blank rel="noopener noreferrer"><strong>[23.02] Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing</strong></a>
A comprehensive review of the core issues of ViT in multimodal FAS, including input design, pre-training strategies, and fine-tuning processes. The paper proposes the AMA adapter and M2A2E pre-training architecture to construct cross-modal, label-free self-supervised workflows.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/m2a2e/ target=_blank rel="noopener noreferrer"><strong>[23.02] M²A²E: Drawing Parallels</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2304.07549 target=_blank rel="noopener noreferrer"><strong>[23.04] MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing</strong></a>
Using a single-branch early fusion architecture, this paper implements modality-agnostic recognition ability through Modal-Disentangle Attention and Cross-Modal Attention, balancing memory efficiency and flexible deployment, marking an important step in ViT's practicality.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/ma-vit/ target=_blank rel="noopener noreferrer"><strong>[23.04] MA-ViT: All appearances Are Illusions</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2309.04038 target=_blank rel="noopener noreferrer"><strong>[23.09] S-Adapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens</strong></a>
Using an Efficient Parameter Transfer Learning architecture, this approach inserts statistical adapters into ViT while fixing the main network parameters. Token Style Regularization helps suppress style differences, providing a lightweight solution for cross-domain FAS.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/s-adapter/ target=_blank rel="noopener noreferrer"><strong>[23.09] S-Adapter: Real Notebook</strong></a></div></div>
</li>
<li>
<p><a href=https://dl.acm.org/doi/pdf/10.1145/3664647.3680856 target=_blank rel="noopener noreferrer"><strong>[24.10] FM-CLIP: Flexible Modal CLIP for Face Anti-Spoofing</strong></a>
By utilizing Cross-Modal Spoofing Enhancer (CMS-Enhancer) and text-guided (LGPA) dynamic alignment of spoofing cues, it maintains high detection accuracy across multi-modal training and single or multi-modal testing, demonstrating excellent generalization ability across multiple datasets.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/fm-clip/ target=_blank rel="noopener noreferrer"><strong>[24.10] FM-CLIP: Guidance from Language</strong></a></div></div>
</li>
</ol>
<hr>
<p>These five papers demonstrate how the Transformer architecture handles critical challenges in multimodal input, modality loss, cross-domain style, and local patch representations, representing a comprehensive shift in the logic of FAS model design.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-5-the-battle-of-styles>Chapter 5: The Battle of Styles<a href=#chapter-5-the-battle-of-styles class=hash-link aria-label="Direct link to Chapter 5: The Battle of Styles" title="Direct link to Chapter 5: The Battle of Styles">​</a></h2>
<blockquote>
<p><strong>When spoofing comes from different worlds, how to build a style-agnostic model?</strong></p>
</blockquote>
<p>The generalization of FAS models is challenged not only by domain shift but also by the interference caused by the asymmetry of information between different styles.</p>
<p>This chapter focuses on style disentanglement, adversarial learning, test-time adaptation, and instance-aware design. These methods aim to help the model maintain stable recognition performance under unknown styles and sample distributions.</p>
<ol start=21>
<li>
<p><a href=https://arxiv.org/abs/2203.05340 target=_blank rel="noopener noreferrer"><strong>[22.03] Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing</strong></a>
Employs a content-style separation strategy, reorganizing the style space to simulate style shift. Combined with contrastive learning that emphasizes style related to liveness, this is a significant breakthrough in style-aware domain generalization (DG) design.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/ssan/ target=_blank rel="noopener noreferrer"><strong>[22.03] SSAN: The Shadow of Style</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2212.03651 target=_blank rel="noopener noreferrer"><strong>[22.12] Cyclically Disentangled Feature Translation for Face Anti-spoofing</strong></a>
Proposes CDFTN, which separates liveness and style components through adversarial learning, generating pseudo-labeled samples that combine real labels and target domain appearances. This significantly improves the accuracy and robustness of cross-domain spoof detection.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/cdftn/ target=_blank rel="noopener noreferrer"><strong>[22.12] CDFTN: The Entanglement of Style</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2304.05640 target=_blank rel="noopener noreferrer"><strong>[23.04] Instance-Aware Domain Generalization for Face Anti-Spoofing</strong></a>
Abandons coarse domain labels in favor of instance-level style alignment strategies. Through asymmetric whitening, style enhancement, and dynamic kernel design, this refines recognition features that are insensitive to style.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/iadg/ target=_blank rel="noopener noreferrer"><strong>[23.04] IADG: A Monologue of Styles</strong></a></div></div>
</li>
<li>
<p><a href=https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Towards_Unsupervised_Domain_Generalization_for_Face_Anti-Spoofing_ICCV_2023_paper.html target=_blank rel="noopener noreferrer"><strong>[23.10] Towards Unsupervised Domain Generalization for Face Anti-Spoofing</strong></a>
Incorporates unlabeled data into the learning process, using segmentation and cross-domain similarity searching mechanisms to extract generalized representations that adapt to multiple unlabeled scenarios, achieving true unsupervised domain generalization (DG) for FAS.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/udg-fas/ target=_blank rel="noopener noreferrer"><strong>[23.10] UDG-FAS: Fragments of Style</strong></a></div></div>
</li>
<li>
<p><a href=https://papers.bmvc2023.org/0379.pdf target=_blank rel="noopener noreferrer"><strong>[23.11] Test-Time Adaptation for Robust Face Anti-Spoofing</strong></a>
Dynamically adjusts the model at inference time for new scenes, combining activation-based pseudo-labeling and contrastive learning to prevent forgetting, allowing pre-trained FAS models to self-optimize during testing, improving sensitivity to unknown attacks.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/three-a-tta/ target=_blank rel="noopener noreferrer"><strong>[23.11] 3A-TTA: Surviving the Wilderness</strong></a></div></div>
</li>
</ol>
<hr>
<p>These five papers challenge the theme of "style generalization" from different angles, especially with their attempts in instance-based and test-time adaptation, gradually approaching the demands of practical application scenarios.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-6-the-summoning-of-multimodality>Chapter 6: The Summoning of Multimodality<a href=#chapter-6-the-summoning-of-multimodality class=hash-link aria-label="Direct link to Chapter 6: The Summoning of Multimodality" title="Direct link to Chapter 6: The Summoning of Multimodality">​</a></h2>
<blockquote>
<p><strong>When images are no longer the only modality, sound and physiological signals come into play</strong></p>
</blockquote>
<p>When traditional RGB models face bottlenecks in high-fidelity attacks and cross-domain challenges, the FAS community began exploring non-visual signals, such as <strong>rPPG, physiological signals, and acoustic echoes</strong>, to establish recognition bases that are harder to forge, starting from "human-centered signals."</p>
<p>This chapter features five representative papers spanning physiological signals, 3D geometry, and acoustic perception, showcasing the potential and future of multimodal FAS technology.</p>
<ol start=26>
<li>
<p><a href=https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1223.pdf target=_blank rel="noopener noreferrer"><strong>[16.12] Generalized face anti-spoofing by detecting pulse from face videos</strong></a>
In the early FAS (Face Anti-Spoofing) scenario, this paper demonstrated how facial heartbeat signals alone, without depth or infrared sensors, can be used to detect fake faces, highlighting the potential of rPPG.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/rppg/ target=_blank rel="noopener noreferrer"><strong>[16.12] rPPG: The Flicker of Life</strong></a></div></div>
</li>
<li>
<p><a href=https://dl.acm.org/doi/10.1007/978-3-030-01270-0_34 target=_blank rel="noopener noreferrer"><strong>[18.09] Remote Photoplethysmography Correspondence Feature for 3D Mask Face Presentation Attack Detection</strong></a>
Introducing CFrPPG (Correspondence rPPG) features to enhance liveness signal acquisition, ensuring accurate heart rate tracking even under low light or camera shake, showing strong performance against 3D mask attacks.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/cfrppg target=_blank rel="noopener noreferrer"><strong>[18.09] CFrPPG: The Echo of a Heartbeat</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/8761776 target=_blank rel="noopener noreferrer"><strong>[19.05] Multi-Modal Face Authentication Using Deep Visual and Acoustic Features</strong></a>
Using the built-in speakers and microphones of smartphones, this method emits ultrasound and analyzes facial echoes, combined with CNN-extracted image features, creating a dual-modal authentication system that requires no additional hardware.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/vafas target=_blank rel="noopener noreferrer"><strong>[19.05] VA-FAS: Faces in Sound Waves</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/9868051 target=_blank rel="noopener noreferrer"><strong>[22.08] Beyond the Pixel World: A Novel Acoustic-Based Face Anti-Spoofing System for Smartphones</strong></a>
Creating the Echo-Spoof acoustic FAS dataset and designing the Echo-FAS framework, which uses sound waves to reconstruct 3D geometry and material information, entirely independent of cameras, showcasing a low-cost and high-resilience mobile device application.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/echo-fas target=_blank rel="noopener noreferrer"><strong>[22.08] Echo-FAS: The Echo of Spoofing</strong></a></div></div>
</li>
<li>
<p><a href=https://dl.acm.org/doi/10.1145/3643510 target=_blank rel="noopener noreferrer"><strong>[24.03] AFace: Range-Flexible Anti-Spoofing Face Authentication via Smartphone Acoustic Sensing</strong></a>
Extending the Echo-FAS concept, incorporating an iso-depth model and distance-adaptive algorithm to combat 3D printed masks, and adjusting based on user distance, this is a crucial design in the practical implementation of acoustic-based liveness verification.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/aface target=_blank rel="noopener noreferrer"><strong>[24.03] AFace: The Boundary of Waves</strong></a></div></div>
</li>
</ol>
<hr>
<p>These five papers mark the beginning of the significant role non-image modalities play in FAS, and if you wish to bypass the limitations of traditional cameras, this is a promising direction worth exploring.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-7-decoding-the-trace-of-deception>Chapter 7: Decoding the Trace of Deception<a href=#chapter-7-decoding-the-trace-of-deception class=hash-link aria-label="Direct link to Chapter 7: Decoding the Trace of Deception" title="Direct link to Chapter 7: Decoding the Trace of Deception">​</a></h2>
<blockquote>
<p><strong>Deeply modeling the structure and semantics of spoofing to enhance model discrimination</strong></p>
</blockquote>
<p>As Face Anti-Spoofing (FAS) models face the dual challenges of interpretability and generalization, researchers have begun to focus on the concept of the "spoof trace" — subtle patterns left by fake faces in images, such as color deviations, edge contours, or frequency anomalies.</p>
<p>The five papers in this chapter approach the problem from the perspective of <strong>disentangled representation</strong>, attempting to separate spoof features from genuine facial content, enabling reconstruction, analysis, and even synthesis of spoof samples, allowing models to truly learn to "see through the disguise."</p>
<ol start=31>
<li>
<p><a href=https://arxiv.org/abs/2003.04092 target=_blank rel="noopener noreferrer"><strong>[20.03] Searching Central Difference Convolutional Networks for Face Anti-Spoofing</strong></a>
Proposes the Central Difference Convolution (CDC) method: by manually defining the hypothesis that "spoof traces should leave differences in local gradients," it disentangles gradient signals of real faces from potential spoofs. Combined with a multiscale attention module, it achieves efficient deployment and strong cross-dataset generalization in face anti-spoofing (FAS) tasks. This work has received a high number of citations.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/cdcn target=_blank rel="noopener noreferrer"><strong>[20.03] CDCN: Between Truth and Falsehood</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2007.09273 target=_blank rel="noopener noreferrer"><strong>[20.07] On Disentangling Spoof Trace for Generic Face Anti-Spoofing</strong></a>
Proposes a multi-scale spoof trace separation model that treats spoof signals as combinations of multi-layer patterns. Through adversarial learning, it reconstructs genuine faces and spoof masks, which can be used to synthesize new attack samples. This work is a representative example of spoof-aware representation learning.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/stdn target=_blank rel="noopener noreferrer"><strong>[20.07] STDN: Traces of Disguise</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2008.08250 target=_blank rel="noopener noreferrer"><strong>[20.08] Face Anti-Spoofing via Disentangled Representation Learning</strong></a>
Decomposes facial features into two subspaces: liveness and identity. Using a CNN architecture to separate low-level and high-level signals, it builds a more transferable live-face classifier, improving stability across different attack types.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/disentangle-fas target=_blank rel="noopener noreferrer"><strong>[20.08] Disentangle-FAS: Untangling the Soul Knot</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2110.09157 target=_blank rel="noopener noreferrer"><strong>[21.10] Disentangled representation with dual-stage feature learning for face anti-spoofing</strong></a>
Employs a dual-stage disentanglement training mechanism to separate facial images into two subspaces—related and unrelated to liveness—and effectively enhances the model’s ability to recognize unseen attack types. This is a key design to strengthen generalization performance.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/dualstage target=_blank rel="noopener noreferrer"><strong>[21.10] DualStage: The Technique of Dual Disentanglement</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2112.00568 target=_blank rel="noopener noreferrer"><strong>[21.12] Dual spoof disentanglement generation for face anti-spoofing with depth uncertainty learning</strong></a>
Introduces the DSDG generation framework that factorizes latent representations of identity and attack texture via a Variational Autoencoder (VAE). It can synthesize large-scale diverse spoof images and incorporates a depth uncertainty module to stabilize depth supervision, serving as a paradigm for "generative adversarial spoofing."</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/dsdg target=_blank rel="noopener noreferrer"><strong>[21.12] DSDG: The Eve of Illusion Recombination</strong></a></div></div>
</li>
</ol>
<hr>
<p>This chapter marks a critical turning point: from detecting liveness → analyzing spoofing → simulating attacks, Face Anti-Spoofing research is gradually progressing toward a next phase that is “generative, interpretable, and controllable.” These approaches not only improve model accuracy but may also inspire future evolutionary paths in attack and defense strategies.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-8-the-chaotic-landscape-of-the-future>Chapter 8: The Chaotic Landscape of the Future<a href=#chapter-8-the-chaotic-landscape-of-the-future class=hash-link aria-label="Direct link to Chapter 8: The Chaotic Landscape of the Future" title="Direct link to Chapter 8: The Chaotic Landscape of the Future">​</a></h2>
<blockquote>
<p><strong>From CLIP to human perception, the next frontier of FAS</strong></p>
</blockquote>
<p>As single-modal and single-attack-type solutions fail to meet real-world needs, FAS is stepping into higher-level challenges: <strong>physical + digital dual attacks, semantic-driven recognition, and zero-shot generalization in diverse environments</strong>.</p>
<p>These five representative works are the three major development axes for the future of FAS: <strong>fusion recognition, language modeling, and human-centered perception</strong>.</p>
<ol start=36>
<li>
<p><a href=https://arxiv.org/abs/2309.16649 target=_blank rel="noopener noreferrer"><strong>[23.09] FLIP: Cross-domain Face Anti-Spoofing with Language Guidance</strong></a>
Applies the CLIP model to the FAS task, guiding visual representation spaces through natural language descriptions to improve cross-domain generalization. The paper proposes semantic alignment and multimodal contrastive learning strategies, achieving true zero-shot FAS under language guidance.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/flip target=_blank rel="noopener noreferrer"><strong>[23.09] FLIP: The Defense Spell</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2404.08450 target=_blank rel="noopener noreferrer"><strong>[24.04] Joint Physical-Digital Facial Attack Detection via Simulating Spoofing Clues</strong></a>
Proposes SPSC and SDSC data augmentation strategies to simulate both physical and digital attack clues, enabling a single model to learn to recognize both types of attacks. This won the CVPR 2024 competition, setting a new paradigm for fusion models.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/pd-fas target=_blank rel="noopener noreferrer"><strong>[24.04] PD-FAS: The Illusionary Arena</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2404.06211 target=_blank rel="noopener noreferrer"><strong>[24.04] Unified Physical-Digital Attack Detection Challenge</strong></a>
Launched the first unified attack detection challenge, releasing the 28,000-entry UniAttackData complex attack dataset and analyzing model architectures, catalyzing the research community toward Unified Attack Detection.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/fas-challenge target=_blank rel="noopener noreferrer"><strong>[24.04] FAS-Challenge: Arsenal</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2408.12793 target=_blank rel="noopener noreferrer"><strong>[24.08] La-SoftMoE CLIP for Unified Physical-Digital Face Attack Detection</strong></a>
Combines CLIP with the Mixture of Experts architecture, introducing a soft-adaptive mechanism to dynamically assign sub-models for complex decision boundaries, providing an efficient parameter selection solution for physical and digital attack fusion handling.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/la-softmoe target=_blank rel="noopener noreferrer"><strong>[24.08] La-SoftMoE: Sparse Cracks</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/2501.01720 target=_blank rel="noopener noreferrer"><strong>[25.01] Interpretable Face Anti-Spoofing: Enhancing Generalization with Multimodal Large Language Models</strong></a>
Proposes a novel architecture I-FAS that integrates multimodal large language models, transforming face anti-spoofing into an interpretable visual question answering task. Through semantic annotation, an asymmetric language loss, and a globally aware connector, it significantly improves cross-domain generalization and reasoning capabilities of the model.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>: <a href=https://docsaid.org/en/papers/face-antispoofing/i-fas target=_blank rel="noopener noreferrer"><strong>[25.01] I-FAS: The Final Chapter of Classification</strong></a></div></div>
</li>
</ol>
<hr>
<p>This chapter signifies the future trend in the FAS field: <strong>from recognizing fake faces → inferring attack types → understanding semantics → combining multimodal language logic reasoning</strong>. Research is evolving from "visual understanding" to "semantic cognition," and attacks are shifting from single-mode to complex hybrid models.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>The real world is never short of malice. As long as there is a demand for face recognition, the need for anti-spoofing will never stop.</p>
<p>From the initial texture analysis and light-shadow modeling to the advent of convolutional networks, and now to the introduction of ViT, CLIP, sound waves, and human perception, FAS technology continues to expand its boundaries. These papers are not only a collection of classics and trends but also a map that spans decades of technological evolution, connecting the past, present, and future.</p>
<p>On this map, we see:</p>
<ul>
<li><strong>From single-modal to multimodal</strong>: Not just seeing the image but sensing depth, sound, pulse, and material.</li>
<li><strong>From classification to disentanglement</strong>: Not just determining real or fake, but attempting to understand the structure of each disguise.</li>
<li><strong>From recognition to reasoning</strong>: Not just distinguishing liveness, but starting to understand the semantics, materials, and language descriptions behind the truth.</li>
<li><strong>From defense to generation</strong>: Not just passive defense, but starting to simulate, reconstruct, and intervene proactively.</li>
</ul>
<p>If you're planning to enter this field, this technical guide won't give you "a one-size-fits-all solution," but it will help you find your starting point: are you fascinated by the visualization of spoof traces? Or do you want to explore how CLIP can assist in secure recognition? Or perhaps you're interested in sound waves and material recognition?</p>
<p>No matter what your background is, FAS is an intersection of image recognition, biometrics, human perception, semantic reasoning, and cross-modal fusion.</p>
<p>This battle is far from over.<section class=ctaSection_iCjC><div class="
        simpleCta_ji_Y
        simple-cta__coffee_YwC8
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>☕ Fuel my writing with a coffee</h3><p class=simple-cta__subtitle_ol86>Your support keeps my AI & full-stack guides coming.<div class=simple-cta__buttonWrapper_jk1Y><img src=/en/img/bmc-logo.svg alt=cta-button class=simple-cta__buttonImg_Q9VV></div></div><div class="ant-row ant-row-stretch cardsSection_wRaP css-mc1tut" style=margin-left:-8px;margin-right:-8px;row-gap:16px><div style=padding-left:8px;padding-right:8px;display:flex class="ant-col ant-col-xs-24 css-mc1tut"><div class="ant-card ant-card-bordered card_gKx9 fadeInUp_n33J hoverTransform_Mozy css-mc1tut" style=flex:1;display:flex;flex-direction:column><div class=ant-card-body><div style=text-align:center;margin-top:1rem><img src=/en/img/icons/all_in.svg alt="AI / Full-Stack / Custom — All In icon" style=width:48px;height:48px></div><span class="ant-tag ant-tag-orange card__tag_PLj3 css-mc1tut">All-in</span><h4 class=card__title_SQBY>AI / Full-Stack / Custom — All In</h4><p class=card__concept_Ak8F>From idea to launch—efficient systems that are future-ready.<div class=card__bulletHeader_b6cf><h5 class=card__bulletTitle_R_wg>All-In Bundle</h5></div><ul class=card__bulletList_SrNN><li class=card__bulletItem_wCRd>Consulting + Dev + Deploy<li class=card__bulletItem_wCRd>Maintenance & upgrades</ul></div></div></div></div><div class="
        simpleCta_ji_Y
        simple-cta__outro_AXbn
        fadeInUp_n33J
        hoverTransform_Mozy
      " style=cursor:pointer><h3 class=simple-cta__title_i7hn>🚀 Ready for your next project?</h3><p class=simple-cta__subtitle_ol86>Need a tech partner or custom solution? Let's connect.</div></section><div style=margin-top:3rem> </div></div></div><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button></article></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href=/en/blog/colorful-cli-with-ansi-escape-codes><div class=pagination-nav__sublabel>Newer Post</div><div class=pagination-nav__label>Your Terminal Shouldn't Just Be Black and White</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/en/blog/should-you-choose-docusaurus><div class=pagination-nav__sublabel>Older Post</div><div class=pagination-nav__label>Should You Choose Docusaurus?</div></a></nav></main><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All our posts</div><div role=group><h3 class=yearGroupHeading_rMGB>2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/read-papers-lightly>Reading Papers, No Need to Try Too Hard</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/cgi-injection-log-analysis>A Technical Profile of CGI Attacks</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/closure-in-python>What is Closure?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/react-hook-vs-python>What exactly is React Hook hooking?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/colorful-cli-with-ansi-escape-codes>Your Terminal Shouldn't Just Be Black and White</a><li class=sidebarItem__DBe><a aria-current=page class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href=/en/blog/fas-paper-roadmap>Face Anti-Spoofing Technology Map</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/should-you-choose-docusaurus>Should You Choose Docusaurus?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/looking-up-the-ten-steps-of-a-master>Looking Up the Ten Steps of a Master</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/build-a-resume>Write a Resume with JS!</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/pydantic-intro>Pydantic Introduction: Python Data Validation and Management</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/builds-dashboard-system>I, an AI Engineer, Actually Built a Backend System?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/amazon-ses-setting-dns-on-namecheap>Setting Up Amazon SES DNS on Namecheap</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/download-from-google-drive-using-python>Download Files from Google Drive Using Python</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/mount-disk-on-ubuntu>Mounting a USB Drive on Ubuntu</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/github-markdown-advanced-syntax>Useful GitHub Markdown Syntax</a></ul></div><div role=group><h3 class=yearGroupHeading_rMGB>2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/extract-font-info-by-python>Extract Font File Information</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/flexible-video-conversion-by-python>Batch Video Conversion</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/system-status-checking-by-chatgpt>Automating Ubuntu System Status Checks with ChatGPT</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/customized-docusaurus-author-to-plugin-content-docs>Add Author Info to Docusaurus Docs</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/graph-convolutional-networks>A Brief Introduction to Graph Convolutional Networks</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/fourier-transform>A Brief Introduction to Fourier Transform</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/fixed-pyenv-install-error>Fixing pyenv Build Errors</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/update-docusaurus-to-3-6-0>Update Docusaurus to 3.6.0</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/file-crawler-python-implementation>Python Implementation of a Web File Downloader</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/customized-docusaurus-sidebars-auto-count>Automatically Count Articles in Docusaurus Sidebar</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/customized-docusaurus-404-page>Customizing the Docusaurus 404 Page</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/torch-layernorm-mismatch>Discrepancy in LayerNorm Calculations?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/get-taiwan-all-stocks-info>Get All Stock Code Information from TWSE</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/windows-python-settings>Simple Configuration of Python Environment on Win11</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/latex-usage>LaTeX Syntax Quick Reference</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/impl-normalized-levenshtein-similarity>Implementing ANLS</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/python-js-basic-command-equivalents>Equivalent Basic Commands between Python and JS</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/vscode-settings>Common VSCode Configuration Settings</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/setting-up-nextcloud>Setting Up Nextcloud: A Guide</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/pytorch-training-out-of-memory>The PyTorch List Trap</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/convert-pdf-to-images>Convert PDF to Images with Python</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/opencv-imread>Reading HEIC Images and Accelerating Loading with Python</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/error-record>Daily Error Troubleshooting Log</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/gosu-usage>User Switching Tool in Containers: gosu</a></ul></div><div role=group><h3 class=yearGroupHeading_rMGB>2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/buy-a-new-computer>Building a New Computer</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/pyenv-installation>Managing Python Versions with pyenv</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/python-env-info-collector>Recording and Troubleshooting Model Training Environment Issues</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/setting-up-pypiserver-on-ubuntu-with-docker>Setting Up PyPiServer on Ubuntu with Docker</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/ubuntu-install-ssh>Set Up SSH Server on Ubuntu</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/ubuntu-github-runner-systemd>Auto-Run GitHub Runner</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/login-rtf8207w>Login to RTF8207W Router</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/fail2ban-settings>Fail2ban: Protecting SSH Services</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/unicode-table>Unicode Table</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/mac-selective-vpn-routing>Configuring Selective Traffic Routing for VPN</a></ul></div></nav></aside><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#chapter-1-the-dawn-of-low-resolution-light class="table-of-contents__link toc-highlight">Chapter 1: The Dawn of Low-Resolution Light</a><li><a href=#chapter-2-the-real-world-stage class="table-of-contents__link toc-highlight">Chapter 2: The Real-World Stage</a><li><a href=#chapter-3-the-cross-domain-battleground class="table-of-contents__link toc-highlight">Chapter 3: The Cross-Domain Battleground</a><li><a href=#chapter-4-the-rise-of-a-new-world class="table-of-contents__link toc-highlight">Chapter 4: The Rise of a New World</a><li><a href=#chapter-5-the-battle-of-styles class="table-of-contents__link toc-highlight">Chapter 5: The Battle of Styles</a><li><a href=#chapter-6-the-summoning-of-multimodality class="table-of-contents__link toc-highlight">Chapter 6: The Summoning of Multimodality</a><li><a href=#chapter-7-decoding-the-trace-of-deception class="table-of-contents__link toc-highlight">Chapter 7: Decoding the Trace of Deception</a><li><a href=#chapter-8-the-chaotic-landscape-of-the-future class="table-of-contents__link toc-highlight">Chapter 8: The Chaotic Landscape of the Future</a><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>