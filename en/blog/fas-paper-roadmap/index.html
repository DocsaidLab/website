<!doctype html><html lang=en dir=ltr class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.7.0"><title data-rh=true>Face Anti-Spoofing Technology Map | DOCSAID</title><meta data-rh=true name=viewport content="width=device-width, initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://docsaid.org/en/blog/fas-paper-roadmap><meta data-rh=true property=og:locale content=en><meta data-rh=true property=og:locale:alternate content=zh_hant><meta data-rh=true property=og:locale:alternate content=ja><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docusaurus_tag content=default><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docsearch:docusaurus_tag content=default><meta data-rh=true property=og:title content="Face Anti-Spoofing Technology Map | DOCSAID"><meta data-rh=true name=description content="A guide to 40 papers from traditional to future advancements."><meta data-rh=true property=og:description content="A guide to 40 papers from traditional to future advancements."><meta data-rh=true property=og:image content=https://docsaid.org/en/img/2025/0401.jpg><meta data-rh=true name=twitter:image content=https://docsaid.org/en/img/2025/0401.jpg><meta data-rh=true property=og:type content=article><meta data-rh=true property=article:published_time content=2025-04-01T00:00:00.000Z><meta data-rh=true property=article:author content=https://github.com/zephyr-sh><meta data-rh=true property=article:tag content=face-anti-spoofing,liveness-detection><link data-rh=true rel=icon href=/en/img/favicon.ico><link data-rh=true rel=canonical href=https://docsaid.org/en/blog/fas-paper-roadmap><link data-rh=true rel=alternate href=https://docsaid.org/blog/fas-paper-roadmap hreflang=zh-hant><link data-rh=true rel=alternate href=https://docsaid.org/en/blog/fas-paper-roadmap hreflang=en><link data-rh=true rel=alternate href=https://docsaid.org/ja/blog/fas-paper-roadmap hreflang=ja><link data-rh=true rel=alternate href=https://docsaid.org/blog/fas-paper-roadmap hreflang=x-default><link data-rh=true rel=preconnect href=https://S9NC0RYCHF-dsn.algolia.net crossorigin=anonymous><script data-rh=true type=application/ld+json>{"@context":"https://schema.org","@id":"https://docsaid.org/en/blog/fas-paper-roadmap","@type":"BlogPosting","author":{"@type":"Person","description":"Dosaid maintainer, Full-Stack AI Engineer","image":"https://github.com/zephyr-sh.png","name":"Z. Yuan","url":"https://github.com/zephyr-sh"},"datePublished":"2025-04-01T00:00:00.000Z","description":"A guide to 40 papers from traditional to future advancements.","headline":"Face Anti-Spoofing Technology Map","image":{"@id":"https://docsaid.org/en/img/2025/0401.jpg","@type":"ImageObject","caption":"title image for the blog post: Face Anti-Spoofing Technology Map","contentUrl":"https://docsaid.org/en/img/2025/0401.jpg","url":"https://docsaid.org/en/img/2025/0401.jpg"},"isPartOf":{"@id":"https://docsaid.org/en/blog","@type":"Blog","name":"Blog"},"keywords":[],"mainEntityOfPage":"https://docsaid.org/en/blog/fas-paper-roadmap","name":"Face Anti-Spoofing Technology Map","url":"https://docsaid.org/en/blog/fas-paper-roadmap"}</script><link rel=alternate type=application/rss+xml href=/en/blog/rss.xml title="DOCSAID RSS Feed"><link rel=alternate type=application/atom+xml href=/en/blog/atom.xml title="DOCSAID Atom Feed"><link rel=preconnect href=https://www.google-analytics.com><link rel=preconnect href=https://www.googletagmanager.com><script async src="https://www.googletagmanager.com/gtag/js?id=G-RDF83L9R4M"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RDF83L9R4M",{anonymize_ip:!0})</script><link rel=search type=application/opensearchdescription+xml title=DOCSAID href=/en/opensearch.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css type=text/css integrity=sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM crossorigin=anonymous><link rel=stylesheet href=/en/assets/css/styles.8b5c2e41.css><script src=/en/assets/js/runtime~main.99c1f173.js defer></script><script src=/en/assets/js/main.2a0e2200.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><link rel=preload as=image href=/en/img/docsaid_logo.png><link rel=preload as=image href=/en/img/docsaid_logo_white.png><link rel=preload as=image href=https://github.com/zephyr-sh.png><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top navbarHideable_jvwV"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/en/><div class=navbar__logo><img src=/en/img/docsaid_logo.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src=/en/img/docsaid_logo_white.png alt="Docsaid Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href=/en/docs/>Projects</a><a class="navbar__item navbar__link" href=/en/papers/intro>Papers</a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/en/blog>Blog</a><a class="navbar__item navbar__link" href=/en/playground/intro>Playground</a><a class="navbar__item navbar__link" href=/en/aboutus>About Us</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href=# aria-haspopup=true aria-expanded=false role=button class=navbar__link><svg viewBox="0 0 24 24" width=20 height=20 aria-hidden=true class=iconLanguage_nlXk><path fill=currentColor d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"/></svg>English</a><ul class=dropdown__menu><li><a href=/blog/fas-paper-roadmap target=_self rel="noopener noreferrer" class=dropdown__link lang=zh-hant>繁體中文</a><li><a href=/en/blog/fas-paper-roadmap target=_self rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang=en>English</a><li><a href=/ja/blog/fas-paper-roadmap target=_self rel="noopener noreferrer" class=dropdown__link lang=ja>日本語</a></ul></div><div class=navbarSearchContainer_dCNk><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div><button type=button class="ant-btn css-7ny38l ant-btn-circle ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-btn-icon-only"><span class=ant-btn-icon><span role=img aria-label=user style=font-size:18px class="anticon anticon-user"><svg viewBox="64 64 896 896" focusable=false data-icon=user width=1em height=1em fill=currentColor aria-hidden=true><path d="M858.5 763.6a374 374 0 00-80.6-119.5 375.63 375.63 0 00-119.5-80.6c-.4-.2-.8-.3-1.2-.5C719.5 518 760 444.7 760 362c0-137-111-248-248-248S264 225 264 362c0 82.7 40.5 156 102.8 201.1-.4.2-.8.3-1.2.5-44.8 18.9-85 46-119.5 80.6a375.63 375.63 0 00-80.6 119.5A371.7 371.7 0 00136 901.8a8 8 0 008 8.2h60c4.4 0 7.9-3.5 8-7.8 2-77.2 33-149.5 87.8-204.3 56.7-56.7 132-87.9 212.2-87.9s155.5 31.2 212.2 87.9C779 752.7 810 825 812 902.2c.1 4.4 3.6 7.8 8 7.8h60a8 8 0 008-8.2c-1-47.8-10.9-94.3-29.5-138.2zM512 534c-45.9 0-89.1-17.9-121.6-50.4S340 407.9 340 362c0-45.9 17.9-89.1 50.4-121.6S466.1 190 512 190s89.1 17.9 121.6 50.4S684 316.1 684 362c0 45.9-17.9 89.1-50.4 121.6S557.9 534 512 534z"/></svg></span></span></button></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_eExm"><div class=blog-hero-fullwidth><div class=postHero_mmE7 style=background-image:url(/en/img/2025/0401.jpg)><div class=postHeroOverlay_UDxJ><h1 class=postTitle_weFP>Face Anti-Spoofing Technology Map</h1><div class=postMeta_oUa9><div class=postAuthors_wLk4><div class=postAuthor_NvIn><img class=postAuthorImg_omQD src=https://github.com/zephyr-sh.png alt="Z. Yuan"><div class=postAuthorText_C6S8><a href=https://github.com/zephyr-sh target=_blank rel="noopener noreferrer" class=postAuthorLink_uKn3><span class=postAuthorName_SaVw>Z. Yuan</span></a><span class=postAuthorTitle_UTso>Dosaid maintainer, Full-Stack AI Engineer</span></div></div></div><div class=postMetaInfo__nS4><div class=postMetaRow_zK0w><span class=postDate_B0aP>2025年4月1日</span><span class=postReadingTime_roVj>15<!-- --> min read</span></div><div class=postTags_nipL><a class=postTag_inik href=/en/blog/tags/face-anti-spoofing>face-anti-spoofing</a><a class=postTag_inik href=/en/blog/tags/liveness-detection>liveness-detection</a></div></div></div></div></div></div><div class="container margin-vert--lg"><div class=row><main class="col col--9"><article class=markdown style="max-width:800px;margin:2rem auto"><article class=""><div><div id=__blog-post-container class=markdown><p>What is Face Anti-Spoofing? Why is it important? How do I get started?</p>
<p>This article is a comprehensive roadmap I’ve put together after reading a substantial amount of literature, designed for those who are learning, researching, or developing FAS systems.</p>
<p>I have selected the 40 most representative papers, divided into eight major themes based on time and technological advancements. Each paper includes reasons to read, key contributions, and the appropriate positioning. From traditional LBP, rPPG, and CNN to Transformer, CLIP, and Vision-Language Models, you will get the full scope.</p>
<p>Later, I will share the details of each paper in the "Paper Notes" section. Let’s first get a grasp of the overall context.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-1-the-dawn-of-low-resolution-light>Chapter 1: The Dawn of Low-Resolution Light<a href=#chapter-1-the-dawn-of-low-resolution-light class=hash-link aria-label="Direct link to Chapter 1: The Dawn of Low-Resolution Light" title="Direct link to Chapter 1: The Dawn of Low-Resolution Light">​</a></h2>
<blockquote>
<p><strong>From traditional feature engineering to the first glimmer of deep learning</strong></p>
</blockquote>
<p>Early research on Face Anti-Spoofing primarily relied on traditional image processing techniques. Researchers used handcrafted features such as texture, contrast, and frequency to describe the authenticity of faces, performing binary classification with classic classifiers.</p>
<ol>
<li>
<p><a href=https://parnec.nuaa.edu.cn/_upload/article/files/4d/43/8a227f2c46bda4c20da97715f010/db1eef47-b25f-4af9-88d4-a8afeccda889.pdf target=_blank rel="noopener noreferrer"><strong>[10.09] Face Liveness Detection from a Single Image with Sparse Low Rank Bilinear Discriminative Model</strong></a>
Using the Lambertian model and sparse low-rank representation to construct feature space, effectively separating real faces from photos, providing theoretical and practical basis for early single-image liveness detection.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/slrbd/ target=_blank rel="noopener noreferrer"><strong>[10.09] SLRBD: Silent Reflective Light</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/6313548 target=_blank rel="noopener noreferrer"><strong>[12.09] On the Effectiveness of Local Binary Patterns in Face Anti-Spoofing</strong></a>
Utilizing LBP and its variants, this paper recognizes flat photos and screen replay attacks and establishes the REPLAY-ATTACK dataset, one of the earliest publicly available datasets and classic baselines.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/lbp/ target=_blank rel="noopener noreferrer"><strong>[12.09] LBP: Lively Micro-textures</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/6810829 target=_blank rel="noopener noreferrer"><strong>[14.05] Spoofing Face Recognition with 3D Masks</strong></a>
A systematic analysis of the attack effects of 3D masks on different face recognition systems (2D/2.5D/3D), pointing out that the traditional assumption of flat fake faces is no longer valid with 3D printing technologies.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/three-d-mad/ target=_blank rel="noopener noreferrer"><strong>[14.05] 3DMAD: The Real Mask</strong></a></div></div>
</li>
<li>
<p><a href=https://arxiv.org/abs/1909.08848 target=_blank rel="noopener noreferrer"><strong>[19.09] Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network</strong></a>
Proposing a multi-channel CNN architecture that combines RGB, depth, infrared, and thermal signals for recognition, and releasing the WMCA dataset to enhance detection of advanced fake faces (e.g., silicone masks).</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/wmca/ target=_blank rel="noopener noreferrer"><strong>[19.09] WMCA: The Invisible Face</strong></a></div></div>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/abstract/document/9925105 target=_blank rel="noopener noreferrer"><strong>[22.10] Deep Learning for Face Anti-Spoofing: A Survey</strong></a>
The first systematic survey in the FAS field focusing on deep learning, covering pixel-wise supervision, multi-modal sensors, and domain generalization trends, establishing a comprehensive knowledge base.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 14 16"><path fill-rule=evenodd d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg></span>info</div><div class=admonitionContent_BuS1><p><strong>Paper Notes</strong>：<a href=https://docsaid.org/en/papers/face-antispoofing/fas-survey/ target=_blank rel="noopener noreferrer"><strong>[22.10] FAS Survey: A Chronicle of Attacks and Defenses</strong></a></div></div>
</li>
</ol>
<hr>
<p>Although these methods are simple, they laid the foundation for recognizing flat fake faces (e.g., photos and screen replays) and set the conceptual framework for the later introduction of deep learning techniques.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-2-the-real-world-stage>Chapter 2: The Real-World Stage<a href=#chapter-2-the-real-world-stage class=hash-link aria-label="Direct link to Chapter 2: The Real-World Stage" title="Direct link to Chapter 2: The Real-World Stage">​</a></h2>
<blockquote>
<p><strong>A milestone for FAS technology moving from the lab to real-world scenarios</strong></p>
</blockquote>
<p>Datasets and benchmarks determine whether a field can grow steadily.</p>
<p>FAS technology expanded from a single scene to multiple devices, lighting conditions, and attack methods, driven by these representative public datasets.</p>
<ol start=6>
<li>
<p><a href=https://ieeexplore.ieee.org/document/7961798 target=_blank rel="noopener noreferrer"><strong>[17.06] OULU-NPU: A Mobile Face Presentation Attack Database with Real-World Variations</strong></a>
A mobile-specific FAS dataset designed for real-world factors such as device, environmental lighting, and attack methods, with four testing protocols, becoming a milestone in "generalization ability" evaluation.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2003.05136 target=_blank rel="noopener noreferrer"><strong>[20.03] CASIA-SURF CeFA: A Benchmark for Multi-modal Cross-ethnicity Face Anti-Spoofing</strong></a>
The world’s first large-scale multi-modal FAS dataset with "ethnicity annotations," covering RGB, Depth, IR, and multiple attack types, specifically used to study ethnic bias and modality fusion strategies.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2007.12342 target=_blank rel="noopener noreferrer"><strong>[20.07] CelebASpoof: Large-scale Face Anti-Spoofing Dataset with Rich Annotations</strong></a>
The largest FAS dataset currently, with over 620,000 images and 10 types of spoof annotations, along with 40 attributes from the original CelebA, enabling multi-task and spoof trace learning.</p>
</li>
<li>
<p><a href=https://openaccess.thecvf.com/content/WACV2022W/MAP-A/html/Belli_A_Personalized_Benchmark_for_Face_Anti-Spoofing_WACVW_2022_paper.html target=_blank rel="noopener noreferrer"><strong>[22.01] A Personalized Benchmark for Face Anti-Spoofing</strong></a>
Advocating for including liveness images from user registration in the recognition process, proposing two new test configurations, CelebA-Spoof-Enroll and SiW-Enroll, exploring the possibility of personalized FAS systems.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2402.04178 target=_blank rel="noopener noreferrer"><strong>[24.02] SHIELD: An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models</strong></a>
Combining LLM and multi-modal inputs, proposing a QA task format to evaluate the reasoning ability of MLLMs in spoof/forgery detection, opening a new field of "understanding attacks with language modeling."</p>
</li>
</ol>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-3-the-cross-domain-battleground>Chapter 3: The Cross-Domain Battleground<a href=#chapter-3-the-cross-domain-battleground class=hash-link aria-label="Direct link to Chapter 3: The Cross-Domain Battleground" title="Direct link to Chapter 3: The Cross-Domain Battleground">​</a></h2>
<blockquote>
<p><strong>From single-domain learning to core technologies for multi-scene deployment</strong></p>
</blockquote>
<p>One of the most challenging problems in Face Anti-Spoofing is generalization—how to make models not only effective on training data but also capable of handling new devices, environments, and attacks.</p>
<ol start=11>
<li>
<p><a href=https://arxiv.org/abs/2004.14043 target=_blank rel="noopener noreferrer"><strong>[20.04] Single-Side Domain Generalization for Face Anti-Spoofing</strong></a>
Proposing a one-sided adversarial learning strategy, aligning only real faces across domains, allowing fake face features to naturally scatter across domains, and preventing over-compression of erroneous information. This is an enlightening direction for DG design.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2105.02453 target=_blank rel="noopener noreferrer"><strong>[21.05] Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing</strong></a>
Not assuming known domain labels, but using instance normalization and MMD for unsupervised clustering and alignment, achieving a generalization training process that does not rely on manual grouping.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2303.13662 target=_blank rel="noopener noreferrer"><strong>[23.03] Rethinking Domain Generalization for Face Anti-Spoofing: Separability and Alignment</strong></a>
Proposing the SA-FAS framework, emphasizing maintaining feature separability across different domains while ensuring that the live-to-spoof transition path is consistent across domains, a deep application of IRM theory in FAS.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2402.19298 target=_blank rel="noopener noreferrer"><strong>[24.02] Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing</strong></a>
A deep analysis of the multi-modal DG problem, using U-Adapter to suppress unstable modal interference, paired with ReGrad to dynamically adjust the convergence speed of each modality, providing a complete solution for modality imbalance and reliability issues.</p>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/10448156 target=_blank rel="noopener noreferrer"><strong>[24.04] VL-FAS: Domain Generalization via Vision-Language Model for Face Anti-Spoofing</strong></a>
Introducing Vision-Language mechanisms for the first time, guiding attention to face regions via semantic guidance, combined with image-text contrastive learning (SLVT) for semantic layer generalization, significantly improving ViT's cross-domain stability.</p>
</li>
</ol>
<hr>
<p>These five papers form the core technical axis under the current Domain Generalization (DG) theme, from one-sided adversarial, label-free clustering, separability analysis, to supervisory methods that integrate language, presenting a complete strategy to address cross-domain challenges.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-4-the-rise-of-a-new-world>Chapter 4: The Rise of a New World<a href=#chapter-4-the-rise-of-a-new-world class=hash-link aria-label="Direct link to Chapter 4: The Rise of a New World" title="Direct link to Chapter 4: The Rise of a New World">​</a></h2>
<blockquote>
<p><strong>From CNN to ViT, the architectural innovation path of FAS models</strong></p>
</blockquote>
<p>The rise of Vision Transformers (ViT) has ushered in an era of global modeling for image tasks, shifting away from local convolutions. Face Anti-Spoofing (FAS) is no exception.</p>
<ol start=16>
<li>
<p><a href=https://arxiv.org/abs/2302.05744 target=_blank rel="noopener noreferrer"><strong>[23.02] Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing</strong></a>
A comprehensive review of the core issues of ViT in multimodal FAS, including input design, pre-training strategies, and fine-tuning processes. The paper proposes the AMA adapter and M2A2E pre-training architecture to construct cross-modal, label-free self-supervised workflows.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2304.07549 target=_blank rel="noopener noreferrer"><strong>[23.04] Ma-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing</strong></a>
Using a single-branch early fusion architecture, this paper implements modality-agnostic recognition ability through Modal-Disentangle Attention and Cross-Modal Attention, balancing memory efficiency and flexible deployment, marking an important step in ViT's practicality.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2305.03277 target=_blank rel="noopener noreferrer"><strong>[23.05] FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing</strong></a>
To solve the issues of modality loss and high-fidelity attacks, the paper introduces a cross-modal attention design (MMA + MFA), which strengthens the focus on spoof patches while preserving the characteristics of each modality, serving as a model for deployment flexibility.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2309.04038 target=_blank rel="noopener noreferrer"><strong>[23.09] Sadapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens</strong></a>
Using an Efficient Parameter Transfer Learning architecture, this approach inserts statistical adapters into ViT while fixing the main network parameters. Token Style Regularization helps suppress style differences, providing a lightweight solution for cross-domain FAS.</p>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/10222330 target=_blank rel="noopener noreferrer"><strong>[23.10] LDCFormer: Incorporating Learnable Descriptive Convolution to Vision Transformer for Face Anti-Spoofing</strong></a>
Combining learnable descriptive convolution (LDC) with ViT to enhance local detail representation, the paper introduces a decoupled optimization version (LDCformerD), achieving state-of-the-art performance across multiple benchmarks.</p>
</li>
</ol>
<hr>
<p>These five papers demonstrate how the Transformer architecture handles critical challenges in multimodal input, modality loss, cross-domain style, and local patch representations, representing a comprehensive shift in the logic of FAS model design.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-5-the-battle-of-styles>Chapter 5: The Battle of Styles<a href=#chapter-5-the-battle-of-styles class=hash-link aria-label="Direct link to Chapter 5: The Battle of Styles" title="Direct link to Chapter 5: The Battle of Styles">​</a></h2>
<blockquote>
<p><strong>When spoofing comes from different worlds, how can we build style-invariant models?</strong></p>
</blockquote>
<p>The generalization of FAS models is not only challenged by domain shifts but also by the interference caused by asymmetric information between different styles.</p>
<p>This chapter focuses on style decoupling, adversarial learning, test-time adaptation, and instance-aware designs. These approaches attempt to enable models to maintain stable recognition performance even under unknown styles and sample distributions.</p>
<ol start=21>
<li>
<p><a href=https://www.sciencedirect.com/science/article/abs/pii/S0031320321000753 target=_blank rel="noopener noreferrer"><strong>[21.07] Unified Unsupervised and Semi-Supervised Domain Adaptation Network for Cross-Scenario Face Anti-Spoofing</strong></a>
Proposing the USDAN framework, which supports both unsupervised and semi-supervised settings, and learns generalized representations compatible with different task configurations through marginal and conditional alignment modules, along with adversarial training.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2203.05340 target=_blank rel="noopener noreferrer"><strong>[22.03] Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing</strong></a>
Adopting content and style separation strategies, this paper reshuffles the style space to simulate style shifts, emphasizing live-related styles through contrastive learning. It represents a significant breakthrough in style-aware domain generalization (DG) design.</p>
</li>
<li>
<p><a href=https://link.springer.com/article/10.1007/s11263-023-01778-x target=_blank rel="noopener noreferrer"><strong>[23.03] Adversarial Learning Domain-Invariant Conditional Features for Robust Face Anti-Spoofing</strong></a>
Not only aligning marginal distributions, but also introducing adversarial structures for conditional alignment, learning distinguishable cross-domain shared representations at the class level, effectively solving misalignment issues.</p>
</li>
<li>
<p><a href=https://www.sciencedirect.com/science/article/abs/pii/S0957417422021248 target=_blank rel="noopener noreferrer"><strong>[23.03] Style Selective Normalization with Meta Learning for Test-Time Adaptive Face Anti-Spoofing</strong></a>
Utilizing statistical information to estimate the style of input images, this method dynamically selects normalization parameters for test-time adaptation, and combines meta-learning to pre-simulate the transfer process for unknown domains.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2304.05640 target=_blank rel="noopener noreferrer"><strong>[23.04] Instance-Aware Domain Generalization for Face Anti-Spoofing</strong></a>
Discarding coarse domain labels, this paper adopts an instance-level style alignment strategy, refining style-invariant recognition features through asymmetric whitening, style enhancement, and dynamic kernel designs.</p>
</li>
</ol>
<hr>
<p>These five papers challenge the "style generalization" theme from different angles, particularly with attempts at instance-based and test-time adaptation, gradually approaching the demands of real-world applications.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-6-the-summoning-of-multimodality>Chapter 6: The Summoning of Multimodality<a href=#chapter-6-the-summoning-of-multimodality class=hash-link aria-label="Direct link to Chapter 6: The Summoning of Multimodality" title="Direct link to Chapter 6: The Summoning of Multimodality">​</a></h2>
<blockquote>
<p><strong>When images are no longer the only modality, sound and physiological signals come into play</strong></p>
</blockquote>
<p>When traditional RGB models face bottlenecks in high-fidelity attacks and cross-domain challenges, the FAS community began exploring non-visual signals, such as <strong>rPPG, physiological signals, and acoustic echoes</strong>, to establish recognition bases that are harder to forge, starting from "human-centered signals."</p>
<p>This chapter features five representative papers spanning physiological signals, 3D geometry, and acoustic perception, showcasing the potential and future of multimodal FAS technology.</p>
<ol start=26>
<li>
<p><a href=https://dl.acm.org/doi/10.1007/978-3-030-01270-0_34 target=_blank rel="noopener noreferrer"><strong>[18.09] Remote Photoplethysmography Correspondence Feature for 3D Mask Face Presentation Attack Detection</strong></a>
Introducing CFrPPG (Correspondence rPPG) features to enhance liveness signal acquisition, ensuring accurate heart rate tracking even under low light or camera shake, showing strong performance against 3D mask attacks.</p>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/8761776 target=_blank rel="noopener noreferrer"><strong>[19.05] Multi-Modal Face Authentication Using Deep Visual and Acoustic Features</strong></a>
Using the built-in speakers and microphones of smartphones, this method emits ultrasound and analyzes facial echoes, combined with CNN-extracted image features, creating a dual-modal authentication system that requires no additional hardware.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2104.06148 target=_blank rel="noopener noreferrer"><strong>[21.04] Contrastive Context-Aware Learning for 3D High-Fidelity Mask Face Presentation Attack Detection</strong></a>
To address the challenge of high-fidelity 3D masks, the HiFiMask dataset is introduced, along with a Contrastive Context-Aware Learning method, using context information (person, material, lighting) to enhance attack detection capability.</p>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/9868051 target=_blank rel="noopener noreferrer"><strong>[22.08] Beyond the Pixel World: A Novel Acoustic-Based Face Anti-Spoofing System for Smartphones</strong></a>
Creating the Echo-Spoof acoustic FAS dataset and designing the Echo-FAS framework, which uses sound waves to reconstruct 3D geometry and material information, entirely independent of cameras, showcasing a low-cost and high-resilience mobile device application.</p>
</li>
<li>
<p><a href=https://dl.acm.org/doi/10.1145/3643510 target=_blank rel="noopener noreferrer"><strong>[24.03] AFace: Range-Flexible Anti-Spoofing Face Authentication via Smartphone Acoustic Sensing</strong></a>
Extending the Echo-FAS concept, incorporating an iso-depth model and distance-adaptive algorithm to combat 3D printed masks, and adjusting based on user distance, this is a crucial design in the practical implementation of acoustic-based liveness verification.</p>
</li>
</ol>
<hr>
<p>These five papers mark the beginning of the significant role non-image modalities play in FAS, and if you wish to bypass the limitations of traditional cameras, this is a promising direction worth exploring.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-7-decoding-the-trace-of-deception>Chapter 7: Decoding the Trace of Deception<a href=#chapter-7-decoding-the-trace-of-deception class=hash-link aria-label="Direct link to Chapter 7: Decoding the Trace of Deception" title="Direct link to Chapter 7: Decoding the Trace of Deception">​</a></h2>
<blockquote>
<p><strong>Deeply modeling the structure and semantics of spoofing to enhance model discriminability</strong></p>
</blockquote>
<p>As FAS models face dual challenges of interpretability and generalization, researchers have begun to focus on the concept of "spoof trace": the subtle patterns left by fake faces in images, such as color biases, edge contours, or frequency anomalies.</p>
<p>The five papers in this chapter all approach this from the perspective of <strong>representation disentanglement</strong>, attempting to separate spoof features from facial content, then reconstruct, analyze, or even synthesize spoof samples, allowing models to truly "see through the disguise."</p>
<ol start=31>
<li>
<p><a href=https://arxiv.org/abs/2007.09273 target=_blank rel="noopener noreferrer"><strong>[20.07] On Disentangling Spoof Trace for Generic Face Anti-Spoofing</strong></a>
Proposes a multi-scale spoof trace separation model, treating spoof signals as multi-layered patterns. Through adversarial learning, it reconstructs real faces and spoof masks, applicable for synthesizing new attack samples. It is a representative work in spoof-aware representation learning.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2008.08250 target=_blank rel="noopener noreferrer"><strong>[20.08] Face Anti-Spoofing via Disentangled Representation Learning</strong></a>
Decomposes facial features into two subspaces: liveness and identity. Through a CNN structure, it separates low- and high-level signals to build a more transferable liveness classifier, improving stability across different attack types.</p>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/9779478 target=_blank rel="noopener noreferrer"><strong>[22.03] Spoof Trace Disentanglement for Generic Face Anti-Spoofing</strong></a>
Models spoof traces as additive and repairable patterns, proposing a two-stage disentanglement framework that incorporates frequency domain information to strengthen low-level spoof detection, also useful for spoof data augmentation to enhance long-tail attack generalization.</p>
</li>
<li>
<p><a href=https://ieeexplore.ieee.org/document/9859657 target=_blank rel="noopener noreferrer"><strong>[22.07] Learning to Augment Face Presentation Attack Dataset via Disentangled Feature Learning from Limited Spoof Data</strong></a>
Proposes a disentangled remix strategy for limited spoof samples, generating in the separated liveness and identity feature spaces, and using contrastive learning to maintain discriminability, significantly improving recognition performance in small-sample scenarios.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2212.03943 target=_blank rel="noopener noreferrer"><strong>[22.12] Learning Polysemantic Spoof Trace: A Multi-Modal Disentanglement Network for Face Anti-Spoofing</strong></a>
Extends the spoof trace disentanglement framework to multimodal settings, designing an RGB/Depth dual-network to capture complementary spoof clues and integrating cross-modality fusion to combine their semantics, offering a forward-looking solution for universal FAS models.</p>
</li>
</ol>
<hr>
<p>This chapter marks a key turning point: from recognizing liveness → analyzing disguises → simulating attacks, Face Anti-Spoofing research is gradually moving toward the next stage of "generative, interpretable, and controllable" models. These methods not only improve model accuracy but may also inspire the future evolution of offense and defense strategies.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=chapter-8-the-chaotic-landscape-of-the-future>Chapter 8: The Chaotic Landscape of the Future<a href=#chapter-8-the-chaotic-landscape-of-the-future class=hash-link aria-label="Direct link to Chapter 8: The Chaotic Landscape of the Future" title="Direct link to Chapter 8: The Chaotic Landscape of the Future">​</a></h2>
<blockquote>
<p><strong>From CLIP to human perception, the next frontier of FAS</strong></p>
</blockquote>
<p>As single-modal and single-attack-type solutions fail to meet real-world needs, FAS is stepping into higher-level challenges: <strong>physical + digital dual attacks, semantic-driven recognition, and zero-shot generalization in diverse environments</strong>.</p>
<p>These five representative works are the three major development axes for the future of FAS: <strong>fusion recognition, language modeling, and human-centered perception</strong>.</p>
<ol start=36>
<li>
<p><a href=https://arxiv.org/abs/2007.02157 target=_blank rel="noopener noreferrer"><strong>[20.07] Face Anti-Spoofing with Human Material Perception</strong></a>
Integrates material perception into FAS model design, with the BCN architecture simulating human perception at macro and micro levels to judge material differences (skin, paper, silicone), enhancing the model's semantic interpretability and cross-material recognition ability.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2309.16649 target=_blank rel="noopener noreferrer"><strong>[23.09] FLIP: Cross-domain Face Anti-Spoofing with Language Guidance</strong></a>
Applies the CLIP model to the FAS task, guiding visual representation spaces through natural language descriptions to improve cross-domain generalization. The paper proposes semantic alignment and multimodal contrastive learning strategies, achieving true zero-shot FAS under language guidance.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2404.08450 target=_blank rel="noopener noreferrer"><strong>[24.04] Joint Physical-Digital Facial Attack Detection via Simulating Spoofing Clues</strong></a>
Proposes SPSC and SDSC data augmentation strategies to simulate both physical and digital attack clues, enabling a single model to learn to recognize both types of attacks. This won the CVPR 2024 competition, setting a new paradigm for fusion models.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2404.06211 target=_blank rel="noopener noreferrer"><strong>[24.04] Unified Physical-Digital Attack Detection Challenge</strong></a>
Launched the first unified attack detection challenge, releasing the 28,000-entry UniAttackData complex attack dataset and analyzing model architectures, catalyzing the research community toward Unified Attack Detection.</p>
</li>
<li>
<p><a href=https://arxiv.org/abs/2408.12793 target=_blank rel="noopener noreferrer"><strong>[24.08] La-SoftMoE CLIP for Unified Physical-Digital Face Attack Detection</strong></a>
Combines CLIP with the Mixture of Experts architecture, introducing a soft-adaptive mechanism to dynamically assign sub-models for complex decision boundaries, providing an efficient parameter selection solution for physical and digital attack fusion handling.</p>
</li>
</ol>
<hr>
<p>This chapter signifies the future trend in the FAS field: <strong>from recognizing fake faces → inferring attack types → understanding semantics → combining multimodal language logic reasoning</strong>. Research is evolving from "visual understanding" to "semantic cognition," and attacks are shifting from single-mode to complex hybrid models.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id=conclusion>Conclusion<a href=#conclusion class=hash-link aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>The real world is never short of malice. As long as there is a demand for face recognition, the need for anti-spoofing will never stop.</p>
<p>From the initial texture analysis and light-shadow modeling to the advent of convolutional networks, and now to the introduction of ViT, CLIP, sound waves, and human perception, FAS technology continues to expand its boundaries. These papers are not only a collection of classics and trends but also a map that spans decades of technological evolution, connecting the past, present, and future.</p>
<p>On this map, we see:</p>
<ul>
<li><strong>From single-modal to multimodal</strong>: Not just seeing the image but sensing depth, sound, pulse, and material.</li>
<li><strong>From classification to disentanglement</strong>: Not just determining real or fake, but attempting to understand the structure of each disguise.</li>
<li><strong>From recognition to reasoning</strong>: Not just distinguishing liveness, but starting to understand the semantics, materials, and language descriptions behind the truth.</li>
<li><strong>From defense to generation</strong>: Not just passive defense, but starting to simulate, reconstruct, and intervene proactively.</li>
</ul>
<p>If you're planning to enter this field, this technical guide won't give you "a one-size-fits-all solution," but it will help you find your starting point: are you fascinated by the visualization of spoof traces? Or do you want to explore how CLIP can assist in secure recognition? Or perhaps you're interested in sound waves and material recognition?</p>
<p>No matter what your background is, FAS is an intersection of image recognition, biometrics, human perception, semantic reasoning, and cross-modal fusion.</p>
<p>This battle is far from over.</div></div><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button></article></article><div style=margin-top:3rem> </div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href=/en/blog/should-you-choose-docusaurus><div class=pagination-nav__sublabel>Older Post</div><div class=pagination-nav__label>Should You Choose Docusaurus?</div></a></nav></main><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All our posts</div><div role=group><h3 class=yearGroupHeading_rMGB>2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a aria-current=page class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href=/en/blog/fas-paper-roadmap>Face Anti-Spoofing Technology Map</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/should-you-choose-docusaurus>Should You Choose Docusaurus?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/looking-up-the-ten-steps-of-a-master>Looking Up the Ten Steps of a Master</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/build-a-resume>Write a Resume with JS!</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/pydantic-intro>Pydantic Introduction: Python Data Validation and Management</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/builds-dashboard-system>I, an AI Engineer, Actually Built a Backend System?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/amazon-ses-setting-dns-on-namecheap>Setting Up Amazon SES DNS on Namecheap</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/download-from-google-drive-using-python>Download Files from Google Drive Using Python</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/mount-disk-on-ubuntu>Mounting a USB Drive on Ubuntu</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/github-markdown-advanced-syntax>Useful GitHub Markdown Syntax</a></ul></div><div role=group><h3 class=yearGroupHeading_rMGB>2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/extract-font-info-by-python>Extract Font File Information</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/flexible-video-conversion-by-python>Batch Video Conversion</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/system-status-checking-by-chatgpt>Automating Ubuntu System Status Checks with ChatGPT</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/customized-docusaurus-author-to-plugin-content-docs>Add Author Info to Docusaurus Docs</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/graph-convolutional-networks>A Brief Introduction to Graph Convolutional Networks</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/fourier-transform>A Brief Introduction to Fourier Transform</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/fixed-pyenv-install-error>Fixing pyenv Build Errors</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/update-docusaurus-to-3-6-0>Update Docusaurus to 3.6.0</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/file-crawler-python-implementation>Python Implementation of a Web File Downloader</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/customized-docusaurus-sidebars-auto-count>Automatically Count Articles in Docusaurus Sidebar</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/customized-docusaurus-404-page>Customizing the Docusaurus 404 Page</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/torch-layernorm-mismatch>Discrepancy in LayerNorm Calculations?</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/get-taiwan-all-stocks-info>Get All Stock Code Information from TWSE</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/windows-python-settings>Simple Configuration of Python Environment on Win11</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/latex-usage>LaTeX Syntax Quick Reference</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/impl-normalized-levenshtein-similarity>Implementing ANLS</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/python-js-basic-command-equivalents>Equivalent Basic Commands between Python and JS</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/vscode-settings>Common VSCode Configuration Settings</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/setting-up-nextcloud>Setting Up Nextcloud: A Guide</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/pytorch-training-out-of-memory>The PyTorch List Trap</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/convert-pdf-to-images>Convert PDF to Images with Python</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/opencv-imread>Reading HEIC Images and Accelerating Loading with Python</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/error-record>Daily Error Troubleshooting Log</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/gosu-usage>User Switching Tool in Containers: gosu</a></ul></div><div role=group><h3 class=yearGroupHeading_rMGB>2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/buy-a-new-computer>Building a New Computer</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/pyenv-installation>Managing Python Versions with pyenv</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/python-env-info-collector>Recording and Troubleshooting Model Training Environment Issues</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/setting-up-pypiserver-on-ubuntu-with-docker>Setting Up PyPiServer on Ubuntu with Docker</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/ubuntu-install-ssh>Set Up SSH Server on Ubuntu</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/ubuntu-github-runner-systemd>Auto-Run GitHub Runner</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/login-rtf8207w>Login to RTF8207W Router</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/fail2ban-settings>Fail2ban: Protecting SSH Services</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/unicode-table>Unicode Table</a><li class=sidebarItem__DBe><a class=sidebarItemLink_mo7H href=/en/blog/mac-selective-vpn-routing>Configuring Selective Traffic Routing for VPN</a></ul></div></nav></aside><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#chapter-1-the-dawn-of-low-resolution-light class="table-of-contents__link toc-highlight">Chapter 1: The Dawn of Low-Resolution Light</a><li><a href=#chapter-2-the-real-world-stage class="table-of-contents__link toc-highlight">Chapter 2: The Real-World Stage</a><li><a href=#chapter-3-the-cross-domain-battleground class="table-of-contents__link toc-highlight">Chapter 3: The Cross-Domain Battleground</a><li><a href=#chapter-4-the-rise-of-a-new-world class="table-of-contents__link toc-highlight">Chapter 4: The Rise of a New World</a><li><a href=#chapter-5-the-battle-of-styles class="table-of-contents__link toc-highlight">Chapter 5: The Battle of Styles</a><li><a href=#chapter-6-the-summoning-of-multimodality class="table-of-contents__link toc-highlight">Chapter 6: The Summoning of Multimodality</a><li><a href=#chapter-7-decoding-the-trace-of-deception class="table-of-contents__link toc-highlight">Chapter 7: Decoding the Trace of Deception</a><li><a href=#chapter-8-the-chaotic-landscape-of-the-future class="table-of-contents__link toc-highlight">Chapter 8: The Chaotic Landscape of the Future</a><li><a href=#conclusion class="table-of-contents__link toc-highlight">Conclusion</a></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__links text--center"><div class=footer__links><a class=footer__link-item href=/en/docs>Projects</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/papers/intro>Papers</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/blog>Blog</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/terms-of-service>TermsOfUse</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/privacy-policy>Privacy Policy</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/become-an-author>Become an author</a><span class=footer__link-separator>·</span><a class=footer__link-item href=/en/worklog>Worklog</a></div></div><div class="footer__bottom text--center"><div class=footer__copyright>Copyright © 2024 DOCSAID.</div></div></div></footer></div>